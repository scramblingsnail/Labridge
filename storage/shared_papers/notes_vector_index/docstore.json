{"docstore/data": {"root_node": {"__data__": {"id_": "root_node", "embedding": null, "metadata": {"node_type": "root_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"5": [{"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}]}, "text": "Root node for the paper notes.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0caafa2-03a1-459a-89e1-708df0be7966": {"__data__": {"id_": "a0caafa2-03a1-459a-89e1-708df0be7966", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2929254c-844e-4c67-b6d5-6e8bd409bee6", "node_type": "1", "metadata": {}, "hash": "9772c90624024d9c5dade7b251bf9725e172b6388926df04c3d98b957d11c5ee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1\nIntroduction\nIn recent years, several different approaches have been proposed for reinforcement learning with\nneural network function approximators. The leading contenders are deep Q-learning [Mni+15],\n\u201cvanilla\u201d policy gradient methods [Mni+16], and trust region / natural policy gradient methods\n[Sch+15b]. However, there is room for improvement in developing a method that is scalable (to\nlarge models and parallel implementations), data efficient, and robust (i.e., successful on a variety\nof problems without hyperparameter tuning).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2929254c-844e-4c67-b6d5-6e8bd409bee6": {"__data__": {"id_": "2929254c-844e-4c67-b6d5-6e8bd409bee6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0caafa2-03a1-459a-89e1-708df0be7966", "node_type": "1", "metadata": {}, "hash": "fcdea9897a07b42e27ac4a8a689695f08ca0869580a4395b2b4e25a41d22d9a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e0761f7-06e8-4475-acd5-025aeb072fb7", "node_type": "1", "metadata": {}, "hash": "4b426ca3331517c431f99a60f8323a1ffa5215daa2fecf94943d2cc24ee70f65", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Q-learning (with function approximation) fails on\nmany simple problems1 and is poorly understood, vanilla policy gradient methods have poor data\neffiency and robustness; and trust region policy optimization (TRPO) is relatively complicated,\nand is not compatible with architectures that include noise (such as dropout) or parameter sharing\n(between the policy and value function, or with auxiliary tasks).\nThis paper seeks to improve the current state of affairs by introducing an algorithm that attains\nthe data efficiency and reliable performance of TRPO, while using only first-order optimization.", "mimetype": "text/plain", "start_char_idx": 539, "end_char_idx": 1139, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e0761f7-06e8-4475-acd5-025aeb072fb7": {"__data__": {"id_": "8e0761f7-06e8-4475-acd5-025aeb072fb7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2929254c-844e-4c67-b6d5-6e8bd409bee6", "node_type": "1", "metadata": {}, "hash": "9772c90624024d9c5dade7b251bf9725e172b6388926df04c3d98b957d11c5ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bfc782a-4235-49bc-966f-0b279ec4e6d1", "node_type": "1", "metadata": {}, "hash": "62e6c998e7319d8f9fe0855d3fb0a248873f464cc2759c920df34ae74c098c2c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We propose a novel objective with clipped probability ratios, which forms a pessimistic estimate\n(i.e., lower bound) of the performance of the policy. To optimize policies, we alternate between\nsampling data from the policy and performing several epochs of optimization on the sampled data.\nOur experiments compare the performance of various different versions of the surrogate objec-\ntive, and find that the version with the clipped probability ratios performs best. We also compare\nPPO to several previous algorithms from the literature. On continuous control tasks, it performs\nbetter than the algorithms we compare against.", "mimetype": "text/plain", "start_char_idx": 1140, "end_char_idx": 1767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bfc782a-4235-49bc-966f-0b279ec4e6d1": {"__data__": {"id_": "6bfc782a-4235-49bc-966f-0b279ec4e6d1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e0761f7-06e8-4475-acd5-025aeb072fb7", "node_type": "1", "metadata": {}, "hash": "4b426ca3331517c431f99a60f8323a1ffa5215daa2fecf94943d2cc24ee70f65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04666a9b-f40b-4843-804c-80d445aedceb", "node_type": "1", "metadata": {}, "hash": "b5addfcec8db9f6200b127e3da3ecd325ae189e6ede09fb404bfadb367083fef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "On Atari, it performs significantly better (in terms\nof sample complexity) than A2C and similarly to ACER though it is much simpler.\n1While DQN works well on game environments like the Arcade Learning Environment [Bel+15] with discrete\naction spaces, it has not been demonstrated to perform well on continuous control benchmarks such as those in\nOpenAI Gym [Bro+16] and described by Duan et al. [Dua+16].", "mimetype": "text/plain", "start_char_idx": 1768, "end_char_idx": 2172, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04666a9b-f40b-4843-804c-80d445aedceb": {"__data__": {"id_": "04666a9b-f40b-4843-804c-80d445aedceb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bfc782a-4235-49bc-966f-0b279ec4e6d1", "node_type": "1", "metadata": {}, "hash": "62e6c998e7319d8f9fe0855d3fb0a248873f464cc2759c920df34ae74c098c2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c051e5fe-53c2-49b2-93be-866ba68773ac", "node_type": "1", "metadata": {}, "hash": "1cf48983976d1738d6073af79d8ecd6f18fcbb989e0da23626c03ef69b22c383", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1\narXiv:1707.06347v2  [cs.LG]  28 Aug 2017\n2\nBackground: Policy Optimization\n2.1\nPolicy Gradient Methods\nPolicy gradient methods work by computing an estimator of the policy gradient and plugging it\ninto a stochastic gradient ascent algorithm.", "mimetype": "text/plain", "start_char_idx": 2173, "end_char_idx": 2416, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c051e5fe-53c2-49b2-93be-866ba68773ac": {"__data__": {"id_": "c051e5fe-53c2-49b2-93be-866ba68773ac", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04666a9b-f40b-4843-804c-80d445aedceb", "node_type": "1", "metadata": {}, "hash": "b5addfcec8db9f6200b127e3da3ecd325ae189e6ede09fb404bfadb367083fef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62315ef2-ad40-479b-9888-6c077637caec", "node_type": "1", "metadata": {}, "hash": "88be73694a7740d9b48298b2bc292105aeb99d6940395b149e8192a0a7726959", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The most commonly used gradient estimator has the\nform\n\u02c6\ng = \u02c6\nEt\n\ufffd\n\u2207\u03b8 log \u03c0\u03b8(at | st) \u02c6\nAt\n\ufffd\n(1)\nwhere \u03c0\u03b8 is a stochastic policy and \u02c6\nAt is an estimator of the advantage function at timestep t.\nHere, the expectation \u02c6\nEt[. . .] indicates the empirical average over a finite batch of samples, in an\nalgorithm that alternates between sampling and optimization.", "mimetype": "text/plain", "start_char_idx": 2417, "end_char_idx": 2777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62315ef2-ad40-479b-9888-6c077637caec": {"__data__": {"id_": "62315ef2-ad40-479b-9888-6c077637caec", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c051e5fe-53c2-49b2-93be-866ba68773ac", "node_type": "1", "metadata": {}, "hash": "1cf48983976d1738d6073af79d8ecd6f18fcbb989e0da23626c03ef69b22c383", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb403b59-c170-4877-8a45-6fcdb3416f3d", "node_type": "1", "metadata": {}, "hash": "3521224869a24a6c58fec089ba746327e78eacf8e9416f7185514a0c5a672d7c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Implementations that use automatic\ndifferentiation software work by constructing an objective function whose gradient is the policy\ngradient estimator; the estimator \u02c6\ng is obtained by differentiating the objective\nLPG(\u03b8) = \u02c6\nEt\n\ufffd\nlog \u03c0\u03b8(at | st) \u02c6\nAt\n\ufffd\n.", "mimetype": "text/plain", "start_char_idx": 2778, "end_char_idx": 3033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb403b59-c170-4877-8a45-6fcdb3416f3d": {"__data__": {"id_": "bb403b59-c170-4877-8a45-6fcdb3416f3d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62315ef2-ad40-479b-9888-6c077637caec", "node_type": "1", "metadata": {}, "hash": "88be73694a7740d9b48298b2bc292105aeb99d6940395b149e8192a0a7726959", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4aa3c1bb-2890-427e-9444-efbf4752b9fc", "node_type": "1", "metadata": {}, "hash": "79b7bbacc6c4dd5c46904fa0e5c0dbd85483f29cd7f5394c52074f0524c441f8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(2)\nWhile it is appealing to perform multiple steps of optimization on this loss LPG using the same\ntrajectory, doing so is not well-justified, and empirically it often leads to destructively large policy\nupdates (see Section 6.1; results are not shown but were similar or worse than the \u201cno clipping or\npenalty\u201d setting).\n2.2\nTrust Region Methods\nIn TRPO [Sch+15b], an objective function (the \u201csurrogate\u201d objective) is maximized subject to a\nconstraint on the size of the policy update.", "mimetype": "text/plain", "start_char_idx": 3034, "end_char_idx": 3521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4aa3c1bb-2890-427e-9444-efbf4752b9fc": {"__data__": {"id_": "4aa3c1bb-2890-427e-9444-efbf4752b9fc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb403b59-c170-4877-8a45-6fcdb3416f3d", "node_type": "1", "metadata": {}, "hash": "3521224869a24a6c58fec089ba746327e78eacf8e9416f7185514a0c5a672d7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a7a7070-481f-44e4-b23e-931b8fb814ac", "node_type": "1", "metadata": {}, "hash": "d4102a9145ddd9f000dc0ae1649e55915e8c36ff41be574ee5f97b4ab109523c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Specifically,\nmaximize\n\u03b8\n\u02c6\nEt\n\ufffd\u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6\nAt\n\ufffd\n(3)\nsubject to\n\u02c6\nEt[KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]] \u2264\u03b4.\n(4)\nHere, \u03b8old is the vector of policy parameters before the update. This problem can efficiently be\napproximately solved using the conjugate gradient algorithm, after making a linear approximation\nto the objective and a quadratic approximation to the constraint.", "mimetype": "text/plain", "start_char_idx": 3522, "end_char_idx": 3905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a7a7070-481f-44e4-b23e-931b8fb814ac": {"__data__": {"id_": "9a7a7070-481f-44e4-b23e-931b8fb814ac", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4aa3c1bb-2890-427e-9444-efbf4752b9fc", "node_type": "1", "metadata": {}, "hash": "79b7bbacc6c4dd5c46904fa0e5c0dbd85483f29cd7f5394c52074f0524c441f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bb61a03-40f8-4bf3-ac28-9e1b5f349e87", "node_type": "1", "metadata": {}, "hash": "561d854f6adb3c0f65c75f1f2653283bff4ef72b2d3784a8afbda87e3fe050bc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e.\nsolving the unconstrained optimization problem\nmaximize\n\u03b8\n\u02c6\nEt\n\ufffd\u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6\nAt \u2212\u03b2 KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]\n\ufffd\n(5)\nfor some coefficient \u03b2. This follows from the fact that a certain surrogate objective (which computes\nthe max KL over states instead of the mean) forms a lower bound (i.e.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bb61a03-40f8-4bf3-ac28-9e1b5f349e87": {"__data__": {"id_": "3bb61a03-40f8-4bf3-ac28-9e1b5f349e87", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a7a7070-481f-44e4-b23e-931b8fb814ac", "node_type": "1", "metadata": {}, "hash": "d4102a9145ddd9f000dc0ae1649e55915e8c36ff41be574ee5f97b4ab109523c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "084f6aa2-2a95-4e36-ae95-7b8bdb900fca", "node_type": "1", "metadata": {}, "hash": "230f5a37b4d01a11a53d130be9eca933d1fa3785a540e6e0cdf15b90edba511d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "a pessimistic bound) on the\nperformance of the policy \u03c0. TRPO uses a hard constraint rather than a penalty because it is hard\nto choose a single value of \u03b2 that performs well across different problems\u2014or even within a single\nproblem, where the the characteristics change over the course of learning. Hence, to achieve our goal\nof a first-order algorithm that emulates the monotonic improvement of TRPO, experiments show\nthat it is not sufficient to simply choose a fixed penalty coefficient \u03b2 and optimize the penalized\nobjective Equation (5) with SGD; additional modifications are required.", "mimetype": "text/plain", "start_char_idx": 4306, "end_char_idx": 4897, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "084f6aa2-2a95-4e36-ae95-7b8bdb900fca": {"__data__": {"id_": "084f6aa2-2a95-4e36-ae95-7b8bdb900fca", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bb61a03-40f8-4bf3-ac28-9e1b5f349e87", "node_type": "1", "metadata": {}, "hash": "561d854f6adb3c0f65c75f1f2653283bff4ef72b2d3784a8afbda87e3fe050bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89fa1b5b-ec50-470c-882f-f81ffeacaaa6", "node_type": "1", "metadata": {}, "hash": "5bdeaf15bcbf80d39b26bfffe3ff7bccf528604b0db26d995f0d2a1b5179158e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2\n3\nClipped Surrogate Objective\nLet rt(\u03b8) denote the probability ratio rt(\u03b8) =\n\u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st), so r(\u03b8old) = 1.\nTRPO maximizes a\n\u201csurrogate\u201d objective\nLCPI(\u03b8) = \u02c6\nEt\n\ufffd\u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6\nAt\n\ufffd\n= \u02c6\nEt\n\ufffd\nrt(\u03b8) \u02c6\nAt\n\ufffd\n.", "mimetype": "text/plain", "start_char_idx": 4898, "end_char_idx": 5133, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89fa1b5b-ec50-470c-882f-f81ffeacaaa6": {"__data__": {"id_": "89fa1b5b-ec50-470c-882f-f81ffeacaaa6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "084f6aa2-2a95-4e36-ae95-7b8bdb900fca", "node_type": "1", "metadata": {}, "hash": "230f5a37b4d01a11a53d130be9eca933d1fa3785a540e6e0cdf15b90edba511d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c0bba9e-7133-4636-ba81-f84c210bc32c", "node_type": "1", "metadata": {}, "hash": "1e262427960a40e692e9f9371404334bd35378fd85fc95875b5b412415d50165", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(6)\nThe superscript CPI refers to conservative policy iteration [KL02], where this objective was pro-\nposed.\nWithout a constraint, maximization of LCPI would lead to an excessively large policy\nupdate; hence, we now consider how to modify the objective, to penalize changes to the policy that\nmove rt(\u03b8) away from 1.", "mimetype": "text/plain", "start_char_idx": 5134, "end_char_idx": 5450, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c0bba9e-7133-4636-ba81-f84c210bc32c": {"__data__": {"id_": "5c0bba9e-7133-4636-ba81-f84c210bc32c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89fa1b5b-ec50-470c-882f-f81ffeacaaa6", "node_type": "1", "metadata": {}, "hash": "5bdeaf15bcbf80d39b26bfffe3ff7bccf528604b0db26d995f0d2a1b5179158e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eac61337-ab4b-43c4-b972-ce0b9ed21ca8", "node_type": "1", "metadata": {}, "hash": "91fa2d290a4c2fcd9273b3cd6a6df962546240e266a21a26b9c738bd865f7820", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The main objective we propose is the following:\nLCLIP (\u03b8) = \u02c6\nEt\n\ufffd\nmin(rt(\u03b8) \u02c6\nAt, clip(rt(\u03b8), 1 \u2212\u03f5, 1 + \u03f5) \u02c6\nAt)\n\ufffd\n(7)\nwhere epsilon is a hyperparameter, say, \u03f5 = 0.2. The motivation for this objective is as follows. The\nfirst term inside the min is LCPI.", "mimetype": "text/plain", "start_char_idx": 5451, "end_char_idx": 5707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eac61337-ab4b-43c4-b972-ce0b9ed21ca8": {"__data__": {"id_": "eac61337-ab4b-43c4-b972-ce0b9ed21ca8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c0bba9e-7133-4636-ba81-f84c210bc32c", "node_type": "1", "metadata": {}, "hash": "1e262427960a40e692e9f9371404334bd35378fd85fc95875b5b412415d50165", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a922a896-544c-41b9-aae5-36ed21bd1c2e", "node_type": "1", "metadata": {}, "hash": "16acd160ad0ab355e516595e1144b4a7ee4d6d7cb9fd01ea2582aed2501dc9e5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The second term, clip(rt(\u03b8), 1\u2212\u03f5, 1+\u03f5) \u02c6\nAt, modifies the surrogate\nobjective by clipping the probability ratio, which removes the incentive for moving rt outside of the\ninterval [1 \u2212\u03f5, 1 + \u03f5]. Finally, we take the minimum of the clipped and unclipped objective, so the\nfinal objective is a lower bound (i.e., a pessimistic bound) on the unclipped objective.", "mimetype": "text/plain", "start_char_idx": 5708, "end_char_idx": 6066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a922a896-544c-41b9-aae5-36ed21bd1c2e": {"__data__": {"id_": "a922a896-544c-41b9-aae5-36ed21bd1c2e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eac61337-ab4b-43c4-b972-ce0b9ed21ca8", "node_type": "1", "metadata": {}, "hash": "91fa2d290a4c2fcd9273b3cd6a6df962546240e266a21a26b9c738bd865f7820", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb0b5c96-3abd-4026-8d8a-bc0fd325aca8", "node_type": "1", "metadata": {}, "hash": "40d3224203327e980494776b81cc578a6de9763ee182ec39cb31af7af5d01ddf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "With this\nscheme, we only ignore the change in probability ratio when it would make the objective improve,\nand we include it when it makes the objective worse. Note that LCLIP (\u03b8) = LCPI(\u03b8) to first order\naround \u03b8old (i.e., where r = 1), however, they become different as \u03b8 moves away from \u03b8old.", "mimetype": "text/plain", "start_char_idx": 6067, "end_char_idx": 6362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb0b5c96-3abd-4026-8d8a-bc0fd325aca8": {"__data__": {"id_": "eb0b5c96-3abd-4026-8d8a-bc0fd325aca8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a922a896-544c-41b9-aae5-36ed21bd1c2e", "node_type": "1", "metadata": {}, "hash": "16acd160ad0ab355e516595e1144b4a7ee4d6d7cb9fd01ea2582aed2501dc9e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f715b14-44dc-46d3-b48c-439335722505", "node_type": "1", "metadata": {}, "hash": "94d958082b9298b5a472363b584f68b0317565105ec29bb0613643e7178b5657", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Figure 1\nplots a single term (i.e., a single t) in LCLIP ; note that the probability ratio r is clipped at 1 \u2212\u03f5\nor 1 + \u03f5 depending on whether the advantage is positive or negative.", "mimetype": "text/plain", "start_char_idx": 6363, "end_char_idx": 6543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f715b14-44dc-46d3-b48c-439335722505": {"__data__": {"id_": "7f715b14-44dc-46d3-b48c-439335722505", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb0b5c96-3abd-4026-8d8a-bc0fd325aca8", "node_type": "1", "metadata": {}, "hash": "40d3224203327e980494776b81cc578a6de9763ee182ec39cb31af7af5d01ddf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ea6e0be-82dd-4b21-8f07-8679f9c3cd63", "node_type": "1", "metadata": {}, "hash": "e90324dcb68f172f3d84b6eb2a10639ac4410c695b5eb739b7ee9f32ac75f099", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "r\nLCLIP\n0\n1 1 + \u03f5\nA > 0\nr\nLCLIP\n0\n1\n1 \u2212\u03f5\nA < 0\nFigure 1: Plots showing one term (i.e., a single timestep) of the surrogate function LCLIP as a function of\nthe probability ratio r, for positive advantages (left) and negative advantages (right). The red circle on each\nplot shows the starting point for the optimization, i.e., r = 1. Note that LCLIP sums many of these terms.", "mimetype": "text/plain", "start_char_idx": 6544, "end_char_idx": 6917, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ea6e0be-82dd-4b21-8f07-8679f9c3cd63": {"__data__": {"id_": "6ea6e0be-82dd-4b21-8f07-8679f9c3cd63", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f715b14-44dc-46d3-b48c-439335722505", "node_type": "1", "metadata": {}, "hash": "94d958082b9298b5a472363b584f68b0317565105ec29bb0613643e7178b5657", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7c3a139-ad6b-47e7-8515-6595d9d30330", "node_type": "1", "metadata": {}, "hash": "d49b52fcb601658805f03cddfe1dcaa63e6bf8efb4e4eb4a05d4376f12eda753", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Figure 2 provides another source of intuition about the surrogate objective LCLIP . It shows how\nseveral objectives vary as we interpolate along the policy update direction, obtained by proximal\npolicy optimization (the algorithm we will introduce shortly) on a continuous control problem. We\ncan see that LCLIP is a lower bound on LCPI, with a penalty for having too large of a policy\nupdate.\n3\n0\n1\nLinear interpolation factor\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.", "mimetype": "text/plain", "start_char_idx": 6918, "end_char_idx": 7383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7c3a139-ad6b-47e7-8515-6595d9d30330": {"__data__": {"id_": "f7c3a139-ad6b-47e7-8515-6595d9d30330", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ea6e0be-82dd-4b21-8f07-8679f9c3cd63", "node_type": "1", "metadata": {}, "hash": "e90324dcb68f172f3d84b6eb2a10639ac4410c695b5eb739b7ee9f32ac75f099", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a5aae2a-3bea-4c7a-b7d1-fe0148de4301", "node_type": "1", "metadata": {}, "hash": "5ee00f20deb20586bf4698994e8de946e3ac3932243908fd15b955a12faaff92", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "12\nEt[KLt]\nLCPI = Et[rtAt]\nEt[clip(rt, 1\n, 1 + )At]\nLCLIP = Et[min(rtAt, clip(rt, 1\n, 1 + )At)]\nFigure 2: Surrogate objectives, as we interpolate between the initial policy parameter \u03b8old, and the updated\npolicy parameter, which we compute after one iteration of PPO. The updated policy has a KL divergence of\nabout 0.02 from the initial policy, and this is the point at which LCLIP is maximal.", "mimetype": "text/plain", "start_char_idx": 7383, "end_char_idx": 7777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a5aae2a-3bea-4c7a-b7d1-fe0148de4301": {"__data__": {"id_": "7a5aae2a-3bea-4c7a-b7d1-fe0148de4301", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7c3a139-ad6b-47e7-8515-6595d9d30330", "node_type": "1", "metadata": {}, "hash": "d49b52fcb601658805f03cddfe1dcaa63e6bf8efb4e4eb4a05d4376f12eda753", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16598399-70e9-4bec-bf7e-c884323b1790", "node_type": "1", "metadata": {}, "hash": "f85e6043e25728094dd026a0a67949ccbc8ae9820f17524d7a6864b0f078ef49", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This plot corresponds\nto the first policy update on the Hopper-v1 problem, using hyperparameters provided in Section 6.1.\n4\nAdaptive KL Penalty Coefficient\nAnother approach, which can be used as an alternative to the clipped surrogate objective, or in\naddition to it, is to use a penalty on KL divergence, and to adapt the penalty coefficient so that we\nachieve some target value of the KL divergence dtarg each policy update.", "mimetype": "text/plain", "start_char_idx": 7778, "end_char_idx": 8204, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16598399-70e9-4bec-bf7e-c884323b1790": {"__data__": {"id_": "16598399-70e9-4bec-bf7e-c884323b1790", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a5aae2a-3bea-4c7a-b7d1-fe0148de4301", "node_type": "1", "metadata": {}, "hash": "5ee00f20deb20586bf4698994e8de946e3ac3932243908fd15b955a12faaff92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec79c6a9-2dd3-4c95-aec9-3949c24e085f", "node_type": "1", "metadata": {}, "hash": "e068dc3ed455727def344ea9bb26c3c9038315e992e791171b4c696dc8971b61", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In our experiments, we\nfound that the KL penalty performed worse than the clipped surrogate objective, however, we\u2019ve\nincluded it here because it\u2019s an important baseline.\nIn the simplest instantiation of this algorithm, we perform the following steps in each policy\nupdate:\n\u2022 Using several epochs of minibatch SGD, optimize the KL-penalized objective\nLKLPEN(\u03b8) = \u02c6\nEt\n\ufffd\u03c0\u03b8(at | st)\n\u03c0\u03b8old(at | st)\n\u02c6\nAt \u2212\u03b2 KL[\u03c0\u03b8old(\u00b7 | st),", "mimetype": "text/plain", "start_char_idx": 8205, "end_char_idx": 8626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec79c6a9-2dd3-4c95-aec9-3949c24e085f": {"__data__": {"id_": "ec79c6a9-2dd3-4c95-aec9-3949c24e085f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16598399-70e9-4bec-bf7e-c884323b1790", "node_type": "1", "metadata": {}, "hash": "f85e6043e25728094dd026a0a67949ccbc8ae9820f17524d7a6864b0f078ef49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12501e2c-d18e-45d0-b4df-b04b171ac8df", "node_type": "1", "metadata": {}, "hash": "fb2b4a243b9464253a0c17c49d61ab48b855f0a5df3ca2190134e2d4a145ff01", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\u03c0\u03b8(\u00b7 | st)]\n\ufffd\n(8)\n\u2022 Compute d = \u02c6\nEt[KL[\u03c0\u03b8old(\u00b7 | st), \u03c0\u03b8(\u00b7 | st)]]\n\u2013 If d < dtarg/1.5, \u03b2 \u2190\u03b2/2\n\u2013 If d > dtarg \u00d7 1.5, \u03b2 \u2190\u03b2 \u00d7 2\nThe updated \u03b2 is used for the next policy update. With this scheme, we occasionally see policy\nupdates where the KL divergence is significantly different from dtarg, however, these are rare, and\n\u03b2 quickly adjusts.", "mimetype": "text/plain", "start_char_idx": 8627, "end_char_idx": 8966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12501e2c-d18e-45d0-b4df-b04b171ac8df": {"__data__": {"id_": "12501e2c-d18e-45d0-b4df-b04b171ac8df", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec79c6a9-2dd3-4c95-aec9-3949c24e085f", "node_type": "1", "metadata": {}, "hash": "e068dc3ed455727def344ea9bb26c3c9038315e992e791171b4c696dc8971b61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a666b670-4745-4078-a0c7-27a7e13a0565", "node_type": "1", "metadata": {}, "hash": "58252a9469c50a28776f13c7a2fd8f85cd5e3d9ca869c9007af1f37095403519", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The parameters 1.5 and 2 above are chosen heuristically, but the algorithm is\nnot very sensitive to them. The initial value of \u03b2 is a another hyperparameter but is not important\nin practice because the algorithm quickly adjusts it.\n5\nAlgorithm\nThe surrogate losses from the previous sections can be computed and differentiated with a minor\nchange to a typical policy gradient implementation. For implementations that use automatic dif-\nferentation, one simply constructs the loss LCLIP or LKLPEN instead of LPG, and one performs\nmultiple steps of stochastic gradient ascent on this objective.", "mimetype": "text/plain", "start_char_idx": 8967, "end_char_idx": 9559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a666b670-4745-4078-a0c7-27a7e13a0565": {"__data__": {"id_": "a666b670-4745-4078-a0c7-27a7e13a0565", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12501e2c-d18e-45d0-b4df-b04b171ac8df", "node_type": "1", "metadata": {}, "hash": "fb2b4a243b9464253a0c17c49d61ab48b855f0a5df3ca2190134e2d4a145ff01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11f6fd5b-1ffd-4c0f-a81d-385b5f9a29ea", "node_type": "1", "metadata": {}, "hash": "b5c7618d14e71f965d99be7daf353ae6c653150966a4092f7856d8c60a432e8a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Most techniques for computing variance-reduced advantage-function estimators make use a\nlearned state-value function V (s); for example, generalized advantage estimation [Sch+15a], or the\n4\nfinite-horizon estimators in [Mni+16]. If using a neural network architecture that shares parameters\nbetween the policy and value function, we must use a loss function that combines the policy\nsurrogate and a value function error term. This objective can further be augmented by adding\nan entropy bonus to ensure sufficient exploration, as suggested in past work [Wil92; Mni+16].", "mimetype": "text/plain", "start_char_idx": 9560, "end_char_idx": 10129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11f6fd5b-1ffd-4c0f-a81d-385b5f9a29ea": {"__data__": {"id_": "11f6fd5b-1ffd-4c0f-a81d-385b5f9a29ea", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a666b670-4745-4078-a0c7-27a7e13a0565", "node_type": "1", "metadata": {}, "hash": "58252a9469c50a28776f13c7a2fd8f85cd5e3d9ca869c9007af1f37095403519", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80a0e15b-3324-483b-83cf-05ba7c5a709c", "node_type": "1", "metadata": {}, "hash": "bb7178399f7bcd17b3e0cb484435e25e0ba4ae6053fe0284644381d5d122e287", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Combining these terms, we obtain the following objective, which is (approximately) maximized\neach iteration:\nLCLIP+V F+S\nt\n(\u03b8) = \u02c6\nEt\n\ufffd\nLCLIP\nt\n(\u03b8) \u2212c1LV F\nt\n(\u03b8) + c2S[\u03c0\u03b8](st)\n\ufffd\n,\n(9)\nwhere c1, c2 are coefficients, and S denotes an entropy bonus, and LV F\nt\nis a squared-error loss\n(V\u03b8(st) \u2212V targ\nt\n)2.", "mimetype": "text/plain", "start_char_idx": 10130, "end_char_idx": 10433, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80a0e15b-3324-483b-83cf-05ba7c5a709c": {"__data__": {"id_": "80a0e15b-3324-483b-83cf-05ba7c5a709c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11f6fd5b-1ffd-4c0f-a81d-385b5f9a29ea", "node_type": "1", "metadata": {}, "hash": "b5c7618d14e71f965d99be7daf353ae6c653150966a4092f7856d8c60a432e8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25a0d61f-165b-4bef-be9c-2ae95bf4b427", "node_type": "1", "metadata": {}, "hash": "a0b22893b0cad829cf244a724c12ff029d461811ef2857a544822562d1ccf879", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "One style of policy gradient implementation, popularized in [Mni+16] and well-suited for use\nwith recurrent neural networks, runs the policy for T timesteps (where T is much less than the\nepisode length), and uses the collected samples for an update.", "mimetype": "text/plain", "start_char_idx": 10434, "end_char_idx": 10684, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25a0d61f-165b-4bef-be9c-2ae95bf4b427": {"__data__": {"id_": "25a0d61f-165b-4bef-be9c-2ae95bf4b427", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80a0e15b-3324-483b-83cf-05ba7c5a709c", "node_type": "1", "metadata": {}, "hash": "bb7178399f7bcd17b3e0cb484435e25e0ba4ae6053fe0284644381d5d122e287", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2b8cfaa-16dd-4daa-a610-4fe00cdbc1f6", "node_type": "1", "metadata": {}, "hash": "bebae52b1c2c1dfa4d4f04d0e5c46d6de117b10e0443434e6cfa6ad09dd143c8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This style requires an advantage\nestimator that does not look beyond timestep T. The estimator used by [Mni+16] is\n\u02c6\nAt = \u2212V (st) + rt + \u03b3rt+1 + \u00b7 \u00b7 \u00b7 + \u03b3T\u2212t+1rT\u22121 + \u03b3T\u2212tV (sT )\n(10)\nwhere t specifies the time index in [0, T], within a given length-T trajectory segment.", "mimetype": "text/plain", "start_char_idx": 10685, "end_char_idx": 10955, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2b8cfaa-16dd-4daa-a610-4fe00cdbc1f6": {"__data__": {"id_": "a2b8cfaa-16dd-4daa-a610-4fe00cdbc1f6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25a0d61f-165b-4bef-be9c-2ae95bf4b427", "node_type": "1", "metadata": {}, "hash": "a0b22893b0cad829cf244a724c12ff029d461811ef2857a544822562d1ccf879", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80f926a7-357e-4b05-8df5-0811620a625e", "node_type": "1", "metadata": {}, "hash": "02540d0fc2d8217ad007e7051e7acd058572602acd86e56864759802d0a4eab7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Generalizing\nthis choice, we can use a truncated version of generalized advantage estimation, which reduces to\nEquation (10) when \u03bb = 1:\n\u02c6\nAt = \u03b4t + (\u03b3\u03bb)\u03b4t+1 + \u00b7 \u00b7 \u00b7 + \u00b7 \u00b7 \u00b7 + (\u03b3\u03bb)T\u2212t+1\u03b4T\u22121,\n(11)\nwhere\n\u03b4t = rt + \u03b3V (st+1) \u2212V (st)\n(12)\nA proximal policy optimization (PPO) algorithm that uses fixed-length trajectory segments is\nshown below.", "mimetype": "text/plain", "start_char_idx": 10956, "end_char_idx": 11296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80f926a7-357e-4b05-8df5-0811620a625e": {"__data__": {"id_": "80f926a7-357e-4b05-8df5-0811620a625e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2b8cfaa-16dd-4daa-a610-4fe00cdbc1f6", "node_type": "1", "metadata": {}, "hash": "bebae52b1c2c1dfa4d4f04d0e5c46d6de117b10e0443434e6cfa6ad09dd143c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d957296-e747-4823-8909-6284d238ef93", "node_type": "1", "metadata": {}, "hash": "8f3d0edf294eb52f9d8214e5e1dd26e601cc80c6a765b7c32f72821f5e675e0b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Each iteration, each of N (parallel) actors collect T timesteps of data. Then we\nconstruct the surrogate loss on these NT timesteps of data, and optimize it with minibatch SGD\n(or usually for better performance, Adam [KB14]), for K epochs.\nAlgorithm 1 PPO, Actor-Critic Style\nfor iteration=1, 2, . . . do\nfor actor=1, 2, . . . , N do\nRun policy \u03c0\u03b8old in environment for T timesteps\nCompute advantage estimates \u02c6\nA1, . .", "mimetype": "text/plain", "start_char_idx": 11297, "end_char_idx": 11716, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d957296-e747-4823-8909-6284d238ef93": {"__data__": {"id_": "7d957296-e747-4823-8909-6284d238ef93", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80f926a7-357e-4b05-8df5-0811620a625e", "node_type": "1", "metadata": {}, "hash": "02540d0fc2d8217ad007e7051e7acd058572602acd86e56864759802d0a4eab7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27a0b647-3732-4092-8b98-1658cf568749", "node_type": "1", "metadata": {}, "hash": "668a878e3fcadfec054667f87d2b3dbd174ba45b543cfbb89a04f7b0080e852d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": ". , \u02c6\nAT\nend for\nOptimize surrogate L wrt \u03b8, with K epochs and minibatch size M \u2264NT\n\u03b8old \u2190\u03b8\nend for\n6\nExperiments\n6.1\nComparison of Surrogate Objectives\nFirst, we compare several different surrogate objectives under different hyperparameters. Here, we\ncompare the surrogate objective LCLIP to several natural variations and ablated versions.", "mimetype": "text/plain", "start_char_idx": 11717, "end_char_idx": 12058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27a0b647-3732-4092-8b98-1658cf568749": {"__data__": {"id_": "27a0b647-3732-4092-8b98-1658cf568749", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d957296-e747-4823-8909-6284d238ef93", "node_type": "1", "metadata": {}, "hash": "8f3d0edf294eb52f9d8214e5e1dd26e601cc80c6a765b7c32f72821f5e675e0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d4af09d-ff11-4898-904d-d6ec9fb04372", "node_type": "1", "metadata": {}, "hash": "09fb82a400eed03fe5d8e951dcf07a0154dd9087375997a3c315fde537200a2d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "No clipping or penalty:\nLt(\u03b8) = rt(\u03b8) \u02c6\nAt\nClipping:\nLt(\u03b8) = min(rt(\u03b8) \u02c6\nAt, clip(rt(\u03b8)), 1 \u2212\u03f5, 1 + \u03f5) \u02c6\nAt\nKL penalty (fixed or adaptive)\nLt(\u03b8) = rt(\u03b8) \u02c6\nAt \u2212\u03b2 KL[\u03c0\u03b8old, \u03c0\u03b8]\n5\nFor the KL penalty,", "mimetype": "text/plain", "start_char_idx": 12059, "end_char_idx": 12255, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d4af09d-ff11-4898-904d-d6ec9fb04372": {"__data__": {"id_": "9d4af09d-ff11-4898-904d-d6ec9fb04372", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27a0b647-3732-4092-8b98-1658cf568749", "node_type": "1", "metadata": {}, "hash": "668a878e3fcadfec054667f87d2b3dbd174ba45b543cfbb89a04f7b0080e852d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c62002c7-d883-438c-aa54-5c80f7c1515f", "node_type": "1", "metadata": {}, "hash": "edd4be9031873c60237c5662eee87bd5fc5a681da6c14d5fe8f77bab71964fb1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "one can either use a fixed penalty coefficient \u03b2 or an adaptive coefficient as\ndescribed in Section 4 using target KL value dtarg. Note that we also tried clipping in log space,\nbut found the performance to be no better.\nBecause we are searching over hyperparameters for each algorithm variant, we chose a compu-\ntationally cheap benchmark to test the algorithms on. Namely, we used 7 simulated robotics tasks2\nimplemented in OpenAI Gym [Bro+16], which use the MuJoCo [TET12] physics engine. We do\none million timesteps of training on each one.", "mimetype": "text/plain", "start_char_idx": 12256, "end_char_idx": 12800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c62002c7-d883-438c-aa54-5c80f7c1515f": {"__data__": {"id_": "c62002c7-d883-438c-aa54-5c80f7c1515f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d4af09d-ff11-4898-904d-d6ec9fb04372", "node_type": "1", "metadata": {}, "hash": "09fb82a400eed03fe5d8e951dcf07a0154dd9087375997a3c315fde537200a2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a044f4c4-f33d-49f8-8ae9-00c8dc0366ab", "node_type": "1", "metadata": {}, "hash": "3d26abb4b3200c9974929e5a9976e3926df6af35c5d2a3899af53ef3e6a0d10f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Besides the hyperparameters used for clipping (\u03f5)\nand the KL penalty (\u03b2, dtarg), which we search over, the other hyperparameters are provided in in\nTable 3.\nTo represent the policy, we used a fully-connected MLP with two hidden layers of 64 units,\nand tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard\ndeviations, following [Sch+15b; Dua+16]. We don\u2019t share parameters between the policy and value\nfunction (so coefficient c1 is irrelevant), and we don\u2019t use an entropy bonus.", "mimetype": "text/plain", "start_char_idx": 12801, "end_char_idx": 13319, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a044f4c4-f33d-49f8-8ae9-00c8dc0366ab": {"__data__": {"id_": "a044f4c4-f33d-49f8-8ae9-00c8dc0366ab", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c62002c7-d883-438c-aa54-5c80f7c1515f", "node_type": "1", "metadata": {}, "hash": "edd4be9031873c60237c5662eee87bd5fc5a681da6c14d5fe8f77bab71964fb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56a890c1-62f2-4950-96ec-ef865a7028da", "node_type": "1", "metadata": {}, "hash": "04cb6b4df120179ba4c8a74002144e1c9f73fbcce0c5633e829a1a9814d2314b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Each algorithm was run on all 7 environments, with 3 random seeds on each. We scored each\nrun of the algorithm by computing the average total reward of the last 100 episodes. We shifted\nand scaled the scores for each environment so that the random policy gave a score of 0 and the best\nresult was set to 1, and averaged over 21 runs to produce a single scalar for each algorithm setting.\nThe results are shown in Table 1.", "mimetype": "text/plain", "start_char_idx": 13320, "end_char_idx": 13741, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56a890c1-62f2-4950-96ec-ef865a7028da": {"__data__": {"id_": "56a890c1-62f2-4950-96ec-ef865a7028da", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a044f4c4-f33d-49f8-8ae9-00c8dc0366ab", "node_type": "1", "metadata": {}, "hash": "3d26abb4b3200c9974929e5a9976e3926df6af35c5d2a3899af53ef3e6a0d10f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d47d970d-a799-488f-8c02-ef44d3547b43", "node_type": "1", "metadata": {}, "hash": "ad9724d1f409f537bca0f8592e5d22fbc261b4ec4641a5db82db2f61a43d7f06", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Note that the score is negative for the setting without clipping\nor penalties, because for one environment (half cheetah) it leads to a very negative score, which is\nworse than the initial random policy.\nalgorithm\navg.", "mimetype": "text/plain", "start_char_idx": 13742, "end_char_idx": 13960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d47d970d-a799-488f-8c02-ef44d3547b43": {"__data__": {"id_": "d47d970d-a799-488f-8c02-ef44d3547b43", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56a890c1-62f2-4950-96ec-ef865a7028da", "node_type": "1", "metadata": {}, "hash": "04cb6b4df120179ba4c8a74002144e1c9f73fbcce0c5633e829a1a9814d2314b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c286534f-0ba3-42de-9e00-271c4aae63ff", "node_type": "1", "metadata": {}, "hash": "68b65e90c8d016273c7a5820804a50634786673ca57f17b9f2cb69e78f6be310", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "normalized score\nNo clipping or penalty\n-0.39\nClipping, \u03f5 = 0.1\n0.76\nClipping, \u03f5 = 0.2\n0.82\nClipping, \u03f5 = 0.3\n0.70\nAdaptive KL dtarg = 0.003\n0.68\nAdaptive KL dtarg = 0.01\n0.74\nAdaptive KL dtarg = 0.03\n0.71\nFixed KL, \u03b2 = 0.3\n0.62\nFixed KL, \u03b2 = 1.", "mimetype": "text/plain", "start_char_idx": 13961, "end_char_idx": 14206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c286534f-0ba3-42de-9e00-271c4aae63ff": {"__data__": {"id_": "c286534f-0ba3-42de-9e00-271c4aae63ff", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d47d970d-a799-488f-8c02-ef44d3547b43", "node_type": "1", "metadata": {}, "hash": "ad9724d1f409f537bca0f8592e5d22fbc261b4ec4641a5db82db2f61a43d7f06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5645c50-741d-46fa-8745-425d711dd8b6", "node_type": "1", "metadata": {}, "hash": "65135c7e687cfb29b1f1dad30879acbf88283792af2cb6543cf1cd9a698c9b5d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "0.71\nFixed KL, \u03b2 = 3.\n0.72\nFixed KL, \u03b2 = 10.\n0.69\nTable 1: Results from continuous control benchmark.\nAverage normalized scores (over 21 runs of the\nalgorithm, on 7 environments) for each algorithm / hyperparameter setting . \u03b2 was initialized at 1.\n6.2\nComparison to Other Algorithms in the Continuous Domain\nNext, we compare PPO (with the \u201cclipped\u201d surrogate objective from Section 3) to several other\nmethods from the literature, which are considered to be effective for continuous problems.", "mimetype": "text/plain", "start_char_idx": 14207, "end_char_idx": 14700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5645c50-741d-46fa-8745-425d711dd8b6": {"__data__": {"id_": "a5645c50-741d-46fa-8745-425d711dd8b6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c286534f-0ba3-42de-9e00-271c4aae63ff", "node_type": "1", "metadata": {}, "hash": "68b65e90c8d016273c7a5820804a50634786673ca57f17b9f2cb69e78f6be310", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b29399e-6a43-447a-9b97-3ffc55928232", "node_type": "1", "metadata": {}, "hash": "d8404ead2e39e4375e2055ce7a3db362d1c138a91558ce97cc5a379b49d85118", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We com-\npared against tuned implementations of the following algorithms: trust region policy optimization\n[Sch+15b], cross-entropy method (CEM) [SL06], vanilla policy gradient with adaptive stepsize3,\n2HalfCheetah, Hopper, InvertedDoublePendulum, InvertedPendulum, Reacher, Swimmer, and Walker2d, all \u201c-v1\u201d\n3After each batch of data, the Adam stepsize is adjusted based on the KL divergence of the original and updated\npolicy, using a rule similar to the one shown in Section 4.", "mimetype": "text/plain", "start_char_idx": 14701, "end_char_idx": 15179, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b29399e-6a43-447a-9b97-3ffc55928232": {"__data__": {"id_": "8b29399e-6a43-447a-9b97-3ffc55928232", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5645c50-741d-46fa-8745-425d711dd8b6", "node_type": "1", "metadata": {}, "hash": "65135c7e687cfb29b1f1dad30879acbf88283792af2cb6543cf1cd9a698c9b5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c096904-122c-4e70-8ea0-f9522b7f06a6", "node_type": "1", "metadata": {}, "hash": "0e59dbe66243b2b55b24dd49be7b4bb6ebf224a8d95e7d6865937d080926ea88", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "An implementation is available at https://github.com/\nberkeleydeeprlcourse/homework/tree/master/hw4.\n6\nA2C [Mni+16], A2C with trust region [Wan+16]. A2C stands for advantage actor critic, and is\na synchronous version of A3C, which we found to have the same or better performance than the\nasynchronous version. For PPO, we used the hyperparameters from the previous section, with\n\u03f5 = 0.2.", "mimetype": "text/plain", "start_char_idx": 15180, "end_char_idx": 15567, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c096904-122c-4e70-8ea0-f9522b7f06a6": {"__data__": {"id_": "3c096904-122c-4e70-8ea0-f9522b7f06a6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b29399e-6a43-447a-9b97-3ffc55928232", "node_type": "1", "metadata": {}, "hash": "d8404ead2e39e4375e2055ce7a3db362d1c138a91558ce97cc5a379b49d85118", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2628aef-f602-4142-ab19-51b9c291faf2", "node_type": "1", "metadata": {}, "hash": "5d26f39c4e7a16ebf0c1a7d254ef73ec2f53e8a66797d593b56c9762f146ca3c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We see that PPO outperforms the previous methods on almost all the continuous control\nenvironments.\n0\n1000000\n500\n0\n500\n1000\n1500\n2000\nHalfCheetah-v1\n0\n1000000\n0\n500\n1000\n1500\n2000\n2500\nHopper-v1\n0\n1000000\n0", "mimetype": "text/plain", "start_char_idx": 15568, "end_char_idx": 15775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2628aef-f602-4142-ab19-51b9c291faf2": {"__data__": {"id_": "b2628aef-f602-4142-ab19-51b9c291faf2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c096904-122c-4e70-8ea0-f9522b7f06a6", "node_type": "1", "metadata": {}, "hash": "0e59dbe66243b2b55b24dd49be7b4bb6ebf224a8d95e7d6865937d080926ea88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a699796d-44eb-4ede-8c66-3b790c2117eb", "node_type": "1", "metadata": {}, "hash": "62b976524a71657d7de25006dced1f9f69aa49efa995248971b434c636e8c090", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2000\n4000\n6000\n8000\nInvertedDoublePendulum-v1\n0\n1000000\n0\n200\n400\n600\n800\n1000\nInvertedPendulum-v1\n0\n1000000\n120\n100\n80\n60\n40\n20", "mimetype": "text/plain", "start_char_idx": 15776, "end_char_idx": 15904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a699796d-44eb-4ede-8c66-3b790c2117eb": {"__data__": {"id_": "a699796d-44eb-4ede-8c66-3b790c2117eb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2628aef-f602-4142-ab19-51b9c291faf2", "node_type": "1", "metadata": {}, "hash": "5d26f39c4e7a16ebf0c1a7d254ef73ec2f53e8a66797d593b56c9762f146ca3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72e1ac06-19c5-48ae-bf7c-821a8a508093", "node_type": "1", "metadata": {}, "hash": "7ab58b54f24238e645e86c7417b726a4f30e61af356b007855de6c9b6cf346dc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Reacher-v1\n0\n1000000\n0\n20\n40\n60\n80\n100\n120\nSwimmer-v1\n0\n1000000\n0\n1000\n2000\n3000\nWalker2d-v1\nA2C\nA2C + Trust Region\nCEM\nPPO (Clip)\nVanilla PG,", "mimetype": "text/plain", "start_char_idx": 15905, "end_char_idx": 16047, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72e1ac06-19c5-48ae-bf7c-821a8a508093": {"__data__": {"id_": "72e1ac06-19c5-48ae-bf7c-821a8a508093", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a699796d-44eb-4ede-8c66-3b790c2117eb", "node_type": "1", "metadata": {}, "hash": "62b976524a71657d7de25006dced1f9f69aa49efa995248971b434c636e8c090", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4789cdc-b420-4538-9a12-7363e468e8ee", "node_type": "1", "metadata": {}, "hash": "840fad4846a6c222039d7e58310adb99fe482d225f384b4a8799a0bccd54bee0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Adaptive\nTRPO\nFigure 3: Comparison of several algorithms on several MuJoCo environments, training for one million\ntimesteps.\n6.3\nShowcase in the Continuous Domain: Humanoid Running and Steering\nTo showcase the performance of PPO on high-dimensional continuous control problems, we train\non a set of problems involving a 3D humanoid, where the robot must run, steer, and get up\noff the ground, possibly while being pelted by cubes.", "mimetype": "text/plain", "start_char_idx": 16048, "end_char_idx": 16478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4789cdc-b420-4538-9a12-7363e468e8ee": {"__data__": {"id_": "d4789cdc-b420-4538-9a12-7363e468e8ee", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72e1ac06-19c5-48ae-bf7c-821a8a508093", "node_type": "1", "metadata": {}, "hash": "7ab58b54f24238e645e86c7417b726a4f30e61af356b007855de6c9b6cf346dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0189774e-86b0-4277-aaf5-98cffdfda3e0", "node_type": "1", "metadata": {}, "hash": "17aa761684296f932a266ff745dbb1789000bdae4e26b267a2ba43de68e975eb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The three tasks we test on are (1) Ro-\nboschoolHumanoid: forward locomotion only, (2) RoboschoolHumanoidFlagrun: position of target\nis randomly varied every 200 timesteps or whenever the goal is reached, (3) RoboschoolHumanoid-\nFlagrunHarder, where the robot is pelted by cubes and needs to get up off the ground. See Figure 5\nfor still frames of a learned policy, and Figure 4 for learning curves on the three tasks. Hyperpa-\nrameters are provided in Table 4.", "mimetype": "text/plain", "start_char_idx": 16479, "end_char_idx": 16939, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0189774e-86b0-4277-aaf5-98cffdfda3e0": {"__data__": {"id_": "0189774e-86b0-4277-aaf5-98cffdfda3e0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4789cdc-b420-4538-9a12-7363e468e8ee", "node_type": "1", "metadata": {}, "hash": "840fad4846a6c222039d7e58310adb99fe482d225f384b4a8799a0bccd54bee0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "383cdbf4-b658-4f92-94e4-c1bdf1d79a9b", "node_type": "1", "metadata": {}, "hash": "bec539e3a2b5c09c0f15411162783e57f5d47430d6332adb08864d10c4684c5b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In concurrent work, Heess et al. [Hee+17] used the adaptive KL\nvariant of PPO (Section 4) to learn locomotion policies for 3D robots.", "mimetype": "text/plain", "start_char_idx": 16940, "end_char_idx": 17073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "383cdbf4-b658-4f92-94e4-c1bdf1d79a9b": {"__data__": {"id_": "383cdbf4-b658-4f92-94e4-c1bdf1d79a9b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0189774e-86b0-4277-aaf5-98cffdfda3e0", "node_type": "1", "metadata": {}, "hash": "17aa761684296f932a266ff745dbb1789000bdae4e26b267a2ba43de68e975eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ec73f66-494f-4a41-9ba6-0d7a9cde8a8c", "node_type": "1", "metadata": {}, "hash": "963cc28a2d656a28a023b9e61bc68bc7a37e3fce16bcea1a4693074c02f8da89", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "0\n50M\nTimestep\n0\n1000\n2000\n3000\n4000\nRoboschoolHumanoid-v0\n0\n100M\nTimestep\n0\n500\n1000\n1500\n2000\n2500\nRoboschoolHumanoidFlagrun-v0\n0\n100M\nTimestep\n0\n1000\n2000\n3000\nRoboschoolHumanoidFlagrunHarder-v0\nFigure 4: Learning curves from PPO on 3D humanoid control tasks, using Roboschool.", "mimetype": "text/plain", "start_char_idx": 17074, "end_char_idx": 17354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ec73f66-494f-4a41-9ba6-0d7a9cde8a8c": {"__data__": {"id_": "7ec73f66-494f-4a41-9ba6-0d7a9cde8a8c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "383cdbf4-b658-4f92-94e4-c1bdf1d79a9b", "node_type": "1", "metadata": {}, "hash": "bec539e3a2b5c09c0f15411162783e57f5d47430d6332adb08864d10c4684c5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2952169b-ea4d-4e3a-b1f9-a8deb75d3136", "node_type": "1", "metadata": {}, "hash": "70f30c7f3511e87835f0952bb0b8f4181d718bf0c34bbbe87b0c601db160af8a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "7\nFigure 5: Still frames of the policy learned from RoboschoolHumanoidFlagrun. In the first six frames, the\nrobot runs towards a target. Then the position is randomly changed, and the robot turns and runs toward\nthe new target.\n6.4\nComparison to Other Algorithms on the Atari Domain\nWe also ran PPO on the Arcade Learning Environment [Bel+15] benchmark and compared against\nwell-tuned implementations of A2C [Mni+16] and ACER [Wan+16].", "mimetype": "text/plain", "start_char_idx": 17355, "end_char_idx": 17790, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2952169b-ea4d-4e3a-b1f9-a8deb75d3136": {"__data__": {"id_": "2952169b-ea4d-4e3a-b1f9-a8deb75d3136", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ec73f66-494f-4a41-9ba6-0d7a9cde8a8c", "node_type": "1", "metadata": {}, "hash": "963cc28a2d656a28a023b9e61bc68bc7a37e3fce16bcea1a4693074c02f8da89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3d71d6c-29bf-47fd-828b-99ac385c81c3", "node_type": "1", "metadata": {}, "hash": "bbfa47c38f5cfa6f31483f91bf84cefb0dbdd0acb6ebac42851cc38a79f56608", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For all three algorithms, we\nused the same policy network architechture as used in [Mni+16]. The hyperparameters for PPO\nare provided in Table 5. For the other two algorithms, we used hyperparameters that were tuned\nto maximize performance on this benchmark.", "mimetype": "text/plain", "start_char_idx": 17791, "end_char_idx": 18049, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3d71d6c-29bf-47fd-828b-99ac385c81c3": {"__data__": {"id_": "f3d71d6c-29bf-47fd-828b-99ac385c81c3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2952169b-ea4d-4e3a-b1f9-a8deb75d3136", "node_type": "1", "metadata": {}, "hash": "70f30c7f3511e87835f0952bb0b8f4181d718bf0c34bbbe87b0c601db160af8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29fc2a74-fbc7-40c2-a0fc-84d8c7d55e07", "node_type": "1", "metadata": {}, "hash": "86723c1536c19ce49847a8261f66d0bf96f461d1157bd7e38877bbfd4d5a430d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A table of results and learning curves for all 49 games is provided in Appendix B. We consider\nthe following two scoring metrics: (1) average reward per episode over entire training period (which\nfavors fast learning), and (2) average reward per episode over last 100 episodes of training (which\nfavors final performance). Table 2 shows the number of games \u201cwon\u201d by each algorithm, where we\ncompute the victor by averaging the scoring metric across three trials.\nA2C\nACER\nPPO\nTie\n(1) avg.", "mimetype": "text/plain", "start_char_idx": 18050, "end_char_idx": 18538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29fc2a74-fbc7-40c2-a0fc-84d8c7d55e07": {"__data__": {"id_": "29fc2a74-fbc7-40c2-a0fc-84d8c7d55e07", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3d71d6c-29bf-47fd-828b-99ac385c81c3", "node_type": "1", "metadata": {}, "hash": "bbfa47c38f5cfa6f31483f91bf84cefb0dbdd0acb6ebac42851cc38a79f56608", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d88c2feb-d935-41e5-b5a0-21e8444ed341", "node_type": "1", "metadata": {}, "hash": "0250acdf2377a625021c7c211aaf4ee725ce34764c153b6e007f60232716ff27", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "episode reward over all of training\n1\n18\n30\n0\n(2) avg. episode reward over last 100 episodes\n1\n28\n19\n1\nTable 2: Number of games \u201cwon\u201d by each algorithm, where the scoring metric is averaged across three trials.\n7\nConclusion\nWe have introduced proximal policy optimization, a family of policy optimization methods that use\nmultiple epochs of stochastic gradient ascent to perform each policy update.", "mimetype": "text/plain", "start_char_idx": 18539, "end_char_idx": 18937, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d88c2feb-d935-41e5-b5a0-21e8444ed341": {"__data__": {"id_": "d88c2feb-d935-41e5-b5a0-21e8444ed341", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "c10569ca4556388ccbb15e9c3bc8a3a64babc26a93157fe9956254c362dcc588", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29fc2a74-fbc7-40c2-a0fc-84d8c7d55e07", "node_type": "1", "metadata": {}, "hash": "86723c1536c19ce49847a8261f66d0bf96f461d1157bd7e38877bbfd4d5a430d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/bigcomp57234.2023.00015", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "These methods have\nthe stability and reliability of trust-region methods but are much simpler to implement, requiring\nonly few lines of code change to a vanilla policy gradient implementation, applicable in more general\nsettings (for example, when using a joint architecture for the policy and value function), and have\nbetter overall performance.\n8\nAcknowledgements\nThanks to Rocky Duan, Peter Chen, and others at OpenAI for insightful comments.\n8", "mimetype": "text/plain", "start_char_idx": 18938, "end_char_idx": 19386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10.1109/bigcomp57234.2023.00015": {"__data__": {"id_": "10.1109/bigcomp57234.2023.00015", "embedding": null, "metadata": {"node_type": "doi_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"4": {"node_id": "root_node", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "a0caafa2-03a1-459a-89e1-708df0be7966", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2929254c-844e-4c67-b6d5-6e8bd409bee6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8e0761f7-06e8-4475-acd5-025aeb072fb7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6bfc782a-4235-49bc-966f-0b279ec4e6d1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "04666a9b-f40b-4843-804c-80d445aedceb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c051e5fe-53c2-49b2-93be-866ba68773ac", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "62315ef2-ad40-479b-9888-6c077637caec", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bb403b59-c170-4877-8a45-6fcdb3416f3d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4aa3c1bb-2890-427e-9444-efbf4752b9fc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9a7a7070-481f-44e4-b23e-931b8fb814ac", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3bb61a03-40f8-4bf3-ac28-9e1b5f349e87", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "084f6aa2-2a95-4e36-ae95-7b8bdb900fca", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "89fa1b5b-ec50-470c-882f-f81ffeacaaa6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5c0bba9e-7133-4636-ba81-f84c210bc32c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "eac61337-ab4b-43c4-b972-ce0b9ed21ca8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a922a896-544c-41b9-aae5-36ed21bd1c2e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "eb0b5c96-3abd-4026-8d8a-bc0fd325aca8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7f715b14-44dc-46d3-b48c-439335722505", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6ea6e0be-82dd-4b21-8f07-8679f9c3cd63", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f7c3a139-ad6b-47e7-8515-6595d9d30330", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7a5aae2a-3bea-4c7a-b7d1-fe0148de4301", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "16598399-70e9-4bec-bf7e-c884323b1790", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ec79c6a9-2dd3-4c95-aec9-3949c24e085f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "12501e2c-d18e-45d0-b4df-b04b171ac8df", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a666b670-4745-4078-a0c7-27a7e13a0565", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "11f6fd5b-1ffd-4c0f-a81d-385b5f9a29ea", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "80a0e15b-3324-483b-83cf-05ba7c5a709c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "25a0d61f-165b-4bef-be9c-2ae95bf4b427", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a2b8cfaa-16dd-4daa-a610-4fe00cdbc1f6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "80f926a7-357e-4b05-8df5-0811620a625e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7d957296-e747-4823-8909-6284d238ef93", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "27a0b647-3732-4092-8b98-1658cf568749", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9d4af09d-ff11-4898-904d-d6ec9fb04372", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c62002c7-d883-438c-aa54-5c80f7c1515f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a044f4c4-f33d-49f8-8ae9-00c8dc0366ab", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "56a890c1-62f2-4950-96ec-ef865a7028da", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d47d970d-a799-488f-8c02-ef44d3547b43", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c286534f-0ba3-42de-9e00-271c4aae63ff", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a5645c50-741d-46fa-8745-425d711dd8b6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8b29399e-6a43-447a-9b97-3ffc55928232", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3c096904-122c-4e70-8ea0-f9522b7f06a6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b2628aef-f602-4142-ab19-51b9c291faf2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a699796d-44eb-4ede-8c66-3b790c2117eb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "72e1ac06-19c5-48ae-bf7c-821a8a508093", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d4789cdc-b420-4538-9a12-7363e468e8ee", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0189774e-86b0-4277-aaf5-98cffdfda3e0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "383cdbf4-b658-4f92-94e4-c1bdf1d79a9b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7ec73f66-494f-4a41-9ba6-0d7a9cde8a8c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2952169b-ea4d-4e3a-b1f9-a8deb75d3136", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f3d71d6c-29bf-47fd-828b-99ac385c81c3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "29fc2a74-fbc7-40c2-a0fc-84d8c7d55e07", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d88c2feb-d935-41e5-b5a0-21e8444ed341", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}]}, "text": "DOI: 10.1109/bigcomp57234.2023.00015", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffdeaf3b-c475-43e3-a2c0-0e0472671663": {"__data__": {"id_": "ffdeaf3b-c475-43e3-a2c0-0e0472671663", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f463d6a-73b7-4257-a0c2-a03497302442", "node_type": "1", "metadata": {}, "hash": "f0dee1b9456c3e6f31268777326e44a6896e48d25505e17ca658b222c8e69660", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Yu Xing\u2217\u2020\u2021\u00a7\u00b6, Jian Weng\u2020, Yushun Wang\u2020, Lingzhi Sui\u2020, Yi Shan\u2020, Yu Wang\u2217\u2021\u00a7\u2225\n\u2217Department of Electronic Engineering, Tsinghua University, Beijing, China\n\u2020Xilinx, Beijing, China\n\u2021Beijing National Research Center for Information Science and Technology, Beijing, China\n\u00a7Center for Intelligent Connected Vehicles and Transportation, Tsinghua University, Beijing, China\n\u00b6xingy16@mails.tsinghua.edu.cn\n\u2225yu-wang@tsinghua.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f463d6a-73b7-4257-a0c2-a03497302442": {"__data__": {"id_": "3f463d6a-73b7-4257-a0c2-a03497302442", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffdeaf3b-c475-43e3-a2c0-0e0472671663", "node_type": "1", "metadata": {}, "hash": "f3e0ba701bc640121e21289c4a83c207c5c45ef01790631f01603ba4e75c0d32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d9c9c93-9028-4a08-8da6-6d1ab15344cc", "node_type": "1", "metadata": {}, "hash": "1dfbef56e4864f276f1ee133275486747466158bdebbabd6af53fe7d10dcbaba", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "edu.cn\nAbstract\u2014Deep neural networks (DNNs) are currently the\nfoundation for many artificial intelligence tasks. The difficulty of\nmapping NN models to high-performance hardware implementa-\ntions arises from factors ranging from the computation complex-\nity of multiple operations to different hardware features such as\nmemory hierarchy and parallelism. In this article, we present a\ngeneric compiler process flow and make an in-depth comparison\nof compiler frameworks regarding their domain-specific language\n(DSL), intermediate representations (IRs), optimization strategies\nand autoscheduling methods.", "mimetype": "text/plain", "start_char_idx": 412, "end_char_idx": 1016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d9c9c93-9028-4a08-8da6-6d1ab15344cc": {"__data__": {"id_": "4d9c9c93-9028-4a08-8da6-6d1ab15344cc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f463d6a-73b7-4257-a0c2-a03497302442", "node_type": "1", "metadata": {}, "hash": "f0dee1b9456c3e6f31268777326e44a6896e48d25505e17ca658b222c8e69660", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25f5135a-4134-4374-9d01-24c9a8465078", "node_type": "1", "metadata": {}, "hash": "5df8d4839a6657603e601c8beee2d41b90c11d58a5704530a5a9020fdf092644", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We reimplement typical NN models\nbased on these compiler frameworks and evaluate the resulting\nperformance. We also review our previous work(Deep Neural\nNetwork Virtual Machine, DNNVM) on compiler frameworks\nand optimization for a custom FPGA-based accelerator to gain\ninspiration regarding the difference between compiler design for\ngeneral-purpose processors and that of FPGA-based accelerators.\nIndex Terms\u2014Compiler, deep neural network, optimization\nI. INTRODUCTION\nThe development of deep neural networks (DNNs) is driving\nan explosion in multiple artificial intelligence (AI) domains.", "mimetype": "text/plain", "start_char_idx": 1017, "end_char_idx": 1607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25f5135a-4134-4374-9d01-24c9a8465078": {"__data__": {"id_": "25f5135a-4134-4374-9d01-24c9a8465078", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d9c9c93-9028-4a08-8da6-6d1ab15344cc", "node_type": "1", "metadata": {}, "hash": "1dfbef56e4864f276f1ee133275486747466158bdebbabd6af53fe7d10dcbaba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea2469e4-fcb7-47c1-adf3-357653468b6b", "node_type": "1", "metadata": {}, "hash": "0171376d73bb125b473dfae0c21dda04333bc03d5ea2f9d85ebdffebc5f26541", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "DNNs currently achieve state-of-the-art performance in multi-\nple AI applications, such as computer vision, robotics and nat-\nural language processing. To date, many well-designed, high-\nperformance machine learning systems such as TensorFlow[1],\nCaffe[2], and PyTorch[3] exist and allow programmers to\nexperiment with various DNN algorithms in a quick and\nelegant way.\nNevertheless, the appealing accuracy and ability of DNN\ncomes at the cost of high computational complexity.", "mimetype": "text/plain", "start_char_idx": 1608, "end_char_idx": 2085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea2469e4-fcb7-47c1-adf3-357653468b6b": {"__data__": {"id_": "ea2469e4-fcb7-47c1-adf3-357653468b6b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25f5135a-4134-4374-9d01-24c9a8465078", "node_type": "1", "metadata": {}, "hash": "5df8d4839a6657603e601c8beee2d41b90c11d58a5704530a5a9020fdf092644", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "620ea7d9-283d-4ca6-b2e4-cca42871147c", "node_type": "1", "metadata": {}, "hash": "23d7bcf20219ff1d44561ef4a4a89ac209b443e2e1b1bf35e01e6aa5eeee87c9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In\ngeneral, most implementations of DNNs are based on ex-\nisting general-purpose computation engines, especially CPU\nand GPU platforms. When applications identify the needs\nfor custom computation, improved efficiency in computation,\nlower power consumption/design cost, or physical system size,\nit is a tedious task to optimize the source code of algorithms\ntargeting CPUs and GPUs. In recent years, there has been a\nsignificant trend in designing specialized processing units such\nas FPGA-based accelerators[4], [5] or ASICs[6], [7] to meet\nthese aggressive platform requirements and accelerate DNNs.", "mimetype": "text/plain", "start_char_idx": 2086, "end_char_idx": 2687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "620ea7d9-283d-4ca6-b2e4-cca42871147c": {"__data__": {"id_": "620ea7d9-283d-4ca6-b2e4-cca42871147c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea2469e4-fcb7-47c1-adf3-357653468b6b", "node_type": "1", "metadata": {}, "hash": "0171376d73bb125b473dfae0c21dda04333bc03d5ea2f9d85ebdffebc5f26541", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b67bea5a-52e5-462f-a205-bbbf673641ac", "node_type": "1", "metadata": {}, "hash": "78594eb723f2dfda840f595f26f0585d917985629c0c09a37f89f7706c3d5963", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Despite the advantages of custom platforms, the intricacy of\ndesign flows remains a barrier to the adoption of custom\naccelerators.\nTo transform each segment of applications to an optimized\nversion of implementation, compilers have been used for\nseveral decades[8]. Benefiting from the mature economy in-\nvolving CPUs and GPUs, compilers are capable of generating\nplatform-dependent code efficiently from high-level program-\nming languages, while optimized implementations of DNNs\nare provided by linear algebra acceleration libraries such as\nEigen[9], MKL[10] and OpenBLAS[11].", "mimetype": "text/plain", "start_char_idx": 2688, "end_char_idx": 3266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b67bea5a-52e5-462f-a205-bbbf673641ac": {"__data__": {"id_": "b67bea5a-52e5-462f-a205-bbbf673641ac", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "620ea7d9-283d-4ca6-b2e4-cca42871147c", "node_type": "1", "metadata": {}, "hash": "23d7bcf20219ff1d44561ef4a4a89ac209b443e2e1b1bf35e01e6aa5eeee87c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d23e8c7d-60bf-4541-b639-33eec97ead8c", "node_type": "1", "metadata": {}, "hash": "65ad8eb4408e1096fad7df5ecd0df897c9229c6d520e18d8a7629f8f747e5cb1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In addition, in the\ndesign of custom accelerators, computer-aided tools play a\nkey role in mapping different DNNs into hardware blocks and\ngenerating efficient instructions executed by platforms.\nUnfortunately, neural networks are computationally inten-\nsive and involve latency-critical tasks. It is a tedious process\nfor users to write algorithms to fit the linear algebra acceler-\nation libraries.", "mimetype": "text/plain", "start_char_idx": 3267, "end_char_idx": 3667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d23e8c7d-60bf-4541-b639-33eec97ead8c": {"__data__": {"id_": "d23e8c7d-60bf-4541-b639-33eec97ead8c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b67bea5a-52e5-462f-a205-bbbf673641ac", "node_type": "1", "metadata": {}, "hash": "78594eb723f2dfda840f595f26f0585d917985629c0c09a37f89f7706c3d5963", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecb2a8c5-dc92-4170-bb37-e2a825ed9e31", "node_type": "1", "metadata": {}, "hash": "b7bfb710d27b93d8b9f901e1ff0e6bc7e3126891d9f72e04fac45e200d76d452", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "It is also very hard for existing compilers to\nintegrate newly introduced optimization methods to keep up\nwith the pace of the rapid development of algorithms; they\ncannot readily provide a sufficient acceleration rate to bridge\nthe gap between the written algorithms and target hardware.\nBuilding on these considerations, to improve the throughput\nof devices and enhance productivity, several compiler-inspired\nframeworks have appeared in recent years that intelligently\nsimplify the realization of optimized neural network perfor-\nmance. In this paper, we leverage an in-depth comparison of\nthe processing flow of these compiler frameworks and provide\nan analysis of their optimization methods.", "mimetype": "text/plain", "start_char_idx": 3668, "end_char_idx": 4364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecb2a8c5-dc92-4170-bb37-e2a825ed9e31": {"__data__": {"id_": "ecb2a8c5-dc92-4170-bb37-e2a825ed9e31", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d23e8c7d-60bf-4541-b639-33eec97ead8c", "node_type": "1", "metadata": {}, "hash": "65ad8eb4408e1096fad7df5ecd0df897c9229c6d520e18d8a7629f8f747e5cb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b153da7-104b-4f01-833c-78e94149c4e0", "node_type": "1", "metadata": {}, "hash": "9306a0fc82a423e7b7fb3cdd64f987c078cff87f6b0b9791bba4932cef23ee91", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We also perform ex-\ntensive experiments using several mainstream neural networks\nand present our practical experience. For this purpose, we\nuse best-effort reimplementations based on the original papers\nand tutorials. Our main contributions can be summarized as\nfollows:\n\u2022 We present a generic compiler process flow and explain\nthe challenges of compilers for deep neural networks on\nhardware.\n\u2022 We analyze the difference of optimization strategies used\nin existing compiler frameworks[4], [5], [12]\u2013[19].", "mimetype": "text/plain", "start_char_idx": 4365, "end_char_idx": 4870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b153da7-104b-4f01-833c-78e94149c4e0": {"__data__": {"id_": "0b153da7-104b-4f01-833c-78e94149c4e0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecb2a8c5-dc92-4170-bb37-e2a825ed9e31", "node_type": "1", "metadata": {}, "hash": "b7bfb710d27b93d8b9f901e1ff0e6bc7e3126891d9f72e04fac45e200d76d452", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d08751ae-097a-4b6a-90e2-c855556061ce", "node_type": "1", "metadata": {}, "hash": "0b37facd6144812b785115cc2b5813ba7257d53fbd2502f161876bb84c85dcf2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\u2022 We fairly and empirically evaluate these compilers, tar-\ngeting general-purpose processors (GPPs) or specialized\naccelerators on frequently used neural networks. We\nhighlight the difference between the achieved throughput\nof GPPs and custom accelerators.\nTo the best of our knowledge, this paper is the first study to\ncompare newly designed compilers for deep neural networks.\nThe remainder of this paper is organized as follows. Section\n978-1-7281-2437-7/19/$31.00 c\n\u20dd2019 IEEE\nAuthorized licensed use limited to: Nanjing University.", "mimetype": "text/plain", "start_char_idx": 4871, "end_char_idx": 5407, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d08751ae-097a-4b6a-90e2-c855556061ce": {"__data__": {"id_": "d08751ae-097a-4b6a-90e2-c855556061ce", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b153da7-104b-4f01-833c-78e94149c4e0", "node_type": "1", "metadata": {}, "hash": "9306a0fc82a423e7b7fb3cdd64f987c078cff87f6b0b9791bba4932cef23ee91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2c584ff-b29b-4966-97bf-dd610fb95927", "node_type": "1", "metadata": {}, "hash": "bfff28b0d790cf7bf957429742737b4649ce508335d5d56d868361686cd7fe8a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Downloaded on June 02,2024 at 08:51:44 UTC from IEEE Xplore.  Restrictions apply. \n2\nFig. 1: Generic Compiler Frameworks for Deep Neural Networks\n2 reviews the challenges and universal processing flow of\ncompiler frameworks for DNNs. Section 3 presents the details\nof existing compilers and the difference between them. We\nintroduce our experimental environments in Section 5 and\npresent the analysis and results of the evaluation. Section 6\nsummarizes and concludes the paper.\nII.", "mimetype": "text/plain", "start_char_idx": 5408, "end_char_idx": 5889, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2c584ff-b29b-4966-97bf-dd610fb95927": {"__data__": {"id_": "b2c584ff-b29b-4966-97bf-dd610fb95927", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d08751ae-097a-4b6a-90e2-c855556061ce", "node_type": "1", "metadata": {}, "hash": "0b37facd6144812b785115cc2b5813ba7257d53fbd2502f161876bb84c85dcf2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae6f136c-33d0-43c8-8155-0e69e26e20d2", "node_type": "1", "metadata": {}, "hash": "7055c2df7d22501b59a672954dc0987fb050f707dd45a446d9a09e963e2694b5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "COMPILER OPTIMIZATION CONCEPT\nA. Generic Compiler Frameworks for Deep Neural Networks\nCompiler frameworks for deep neural networks work in the\ncontext of high-level DNN specifications, especially models\nfrom deep learning frameworks such as Caffe[2], Tensor-\nFlow[1], MXNet[20], and PyTorch[3].", "mimetype": "text/plain", "start_char_idx": 5890, "end_char_idx": 6184, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae6f136c-33d0-43c8-8155-0e69e26e20d2": {"__data__": {"id_": "ae6f136c-33d0-43c8-8155-0e69e26e20d2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2c584ff-b29b-4966-97bf-dd610fb95927", "node_type": "1", "metadata": {}, "hash": "bfff28b0d790cf7bf957429742737b4649ce508335d5d56d868361686cd7fe8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48ac672e-3193-42e8-968c-f0703a53f97f", "node_type": "1", "metadata": {}, "hash": "15ba523791d876fa063112f3b7c634aa0b85059486fd19194f135c21b31bf6d5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The optimization steps\nare applied at different stages of the compilation process,\nand the processing flow of these compiler frameworks can\nbe categorized into five layers: 1) front end, 2) intermediate\nrepresentation (IR), 3) high-level optimization, 4) low-level\noptimization and 5) back end.\nFirst, front ends transform high-level specifications of deep\nneural networks into compiler-specific IRs.", "mimetype": "text/plain", "start_char_idx": 6185, "end_char_idx": 6585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48ac672e-3193-42e8-968c-f0703a53f97f": {"__data__": {"id_": "48ac672e-3193-42e8-968c-f0703a53f97f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae6f136c-33d0-43c8-8155-0e69e26e20d2", "node_type": "1", "metadata": {}, "hash": "7055c2df7d22501b59a672954dc0987fb050f707dd45a446d9a09e963e2694b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "baca1f34-761d-476b-8849-eea3e8d49820", "node_type": "1", "metadata": {}, "hash": "c3acabb1a4e4ca40bef216b974cef911e5ab3f35a780c34a09018cdd1ae40174", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "These IRs are\nusually in the form of directed acyclic graphs[1], [3], in\nwhich each node represents a computation operation and each\nedge denotes the data dependency between operations. As\na result, graph-level algorithms[13], [19] can be used upon\nthese IRs to fuse operations and optimize data layouts. Apart\nfrom the high-level IRs, multiple extensive IRs[21]\u2013[24] are\nadopted in the optimization process of compilers.", "mimetype": "text/plain", "start_char_idx": 6586, "end_char_idx": 7007, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baca1f34-761d-476b-8849-eea3e8d49820": {"__data__": {"id_": "baca1f34-761d-476b-8849-eea3e8d49820", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48ac672e-3193-42e8-968c-f0703a53f97f", "node_type": "1", "metadata": {}, "hash": "15ba523791d876fa063112f3b7c634aa0b85059486fd19194f135c21b31bf6d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a64459c0-9aa7-4d65-b338-01a91a5f07a1", "node_type": "1", "metadata": {}, "hash": "09ebb77cb06e6e2e4e86c0d3ff3a60d2330e26c64319ace8bb87e67ef1d7ba1e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Deep neural\nnetwork workloads can be decomposed into tensor operations,\nsuch as matrix-vector and matrix-matrix multiplication. Low-\nlevel optimization methods[12], [13] are used to optimize the\nschedule for enhancing data locality and making full utilization\nof the parallelism of hardware platforms. This problem turns\ninto what optimization to use and which parameters to choose\nfrom (e.g., tiling size, fusion strategies, and vectorization).\nHundreds of low-level optimization steps may be applied\nduring the compilation phases.", "mimetype": "text/plain", "start_char_idx": 7008, "end_char_idx": 7540, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a64459c0-9aa7-4d65-b338-01a91a5f07a1": {"__data__": {"id_": "a64459c0-9aa7-4d65-b338-01a91a5f07a1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "baca1f34-761d-476b-8849-eea3e8d49820", "node_type": "1", "metadata": {}, "hash": "c3acabb1a4e4ca40bef216b974cef911e5ab3f35a780c34a09018cdd1ae40174", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8853a2dc-0d1d-43f3-bcc6-ebbcc09d66a3", "node_type": "1", "metadata": {}, "hash": "e989839284dd8e51b54836f72e2de870b7688205642f78f4678470f25565c1a9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Finally, the back end[24], [25]\nis responsible for mapping optimized implementations into\nmachine-dependent executable instructions.\nB. Challenges\n1) Intermediate Representations (IRs): Well-known deep\nlearning frameworks offer high-level abstraction for deep\nneural networks expressed as a computing graph. TensorFlow\nemploys a static dataflow graph of operators and offers highly\noptimized implementations, in which GPUs and other spe-\ncialized accelerators are transparent to the users. However, a\nstatic computing graph cannot support computations that are\nnot explicitly specified.", "mimetype": "text/plain", "start_char_idx": 7541, "end_char_idx": 8127, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8853a2dc-0d1d-43f3-bcc6-ebbcc09d66a3": {"__data__": {"id_": "8853a2dc-0d1d-43f3-bcc6-ebbcc09d66a3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a64459c0-9aa7-4d65-b338-01a91a5f07a1", "node_type": "1", "metadata": {}, "hash": "09ebb77cb06e6e2e4e86c0d3ff3a60d2330e26c64319ace8bb87e67ef1d7ba1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c05cf077-62c9-4238-bf7c-4f75ed74523f", "node_type": "1", "metadata": {}, "hash": "8fe8a99e01f31598610b50263e305199b37691f49a36f6a02457d598f68a97ea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For example, the size of the input\nand output needs to be specified, and all required data should\nbe loaded on-chip before execution of an operation. Dynamic\nframeworks such as PyTorch and Chainer adopt a define-by-\nrun computing graph to alleviate this problem, but the control\nflow is lost due to the dynamic computing graphs.\nIn addition, these deep learning frameworks lack efficiency\nin the instances where researchers need to develop a custom\noperator. Hundreds of lines of codes need to be written man-\nually to express the algorithm.", "mimetype": "text/plain", "start_char_idx": 8128, "end_char_idx": 8669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c05cf077-62c9-4238-bf7c-4f75ed74523f": {"__data__": {"id_": "c05cf077-62c9-4238-bf7c-4f75ed74523f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8853a2dc-0d1d-43f3-bcc6-ebbcc09d66a3", "node_type": "1", "metadata": {}, "hash": "e989839284dd8e51b54836f72e2de870b7688205642f78f4678470f25565c1a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "547c03f7-4cc7-47a8-9830-22151ce042c6", "node_type": "1", "metadata": {}, "hash": "d3bc778630cf22ffc981f68a798c3787ca3eb4a231d8f8254d9ad9466293b92b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The computing graphs in these\nframeworks lack features and information from the hardware\nas well. The computation efficiency decreases dramatically\nwhen the operators do not fit the preoptimized version of\nthe library functions. At the very least, we need to make\nO(Nf \u00b7Np) efforts to optimize operations in Nf deep learning\nframeworks targeting Np hardware platforms.\nTo solve these problems, an effective design mentality is to\ndecouple the algorithm description with deep learning frame-\nworks and hardware platforms.", "mimetype": "text/plain", "start_char_idx": 8670, "end_char_idx": 9190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "547c03f7-4cc7-47a8-9830-22151ce042c6": {"__data__": {"id_": "547c03f7-4cc7-47a8-9830-22151ce042c6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c05cf077-62c9-4238-bf7c-4f75ed74523f", "node_type": "1", "metadata": {}, "hash": "8fe8a99e01f31598610b50263e305199b37691f49a36f6a02457d598f68a97ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b032fa00-a193-41da-a194-7776be2d46fd", "node_type": "1", "metadata": {}, "hash": "c8b89455ad6c1c0e83d7aebd433bf3d83634e090a136c34cecdca1ecb1f3d558", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "IRs should not only provide\nconcise, portable and expressive syntax to represent the NN\nmodels and control flow but also provide a powerful abstrac-\ntion containing both the features of algorithms and hardware\nplatforms for the following analysis and optimizations.\n2) Scheduling Pipelines: Given the expression of algo-\nrithms, schedulers contain the rules to map computation\ndescriptions to implementations for different hardware plat-\nforms.", "mimetype": "text/plain", "start_char_idx": 9191, "end_char_idx": 9635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b032fa00-a193-41da-a194-7776be2d46fd": {"__data__": {"id_": "b032fa00-a193-41da-a194-7776be2d46fd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "547c03f7-4cc7-47a8-9830-22151ce042c6", "node_type": "1", "metadata": {}, "hash": "d3bc778630cf22ffc981f68a798c3787ca3eb4a231d8f8254d9ad9466293b92b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6152888b-1e9f-4ec1-bf9a-39bd5ecf6a19", "node_type": "1", "metadata": {}, "hash": "949dd866ae84c546ff3ff7b811e0addad79b24156c31271fe639e71011909cb5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Preoptimized libraries such as Eigen[9] and cuDNN[25]\nprovide various reliable and fast implementations for linear\nalgebra, but these libraries lack optimization across operators,\nand the execution of each operation varies dramatically for\ndifferent data sizes, data layouts, configurations for operators,\nmemory hierarchies and specific hardware features. Determin-\ning when the functions should be computed, where the data\nshould be stored, and how long they should be cached, in\naddition to configuring the trade-off between recomputation\nand data locality, are the main challenges for the scheduler.", "mimetype": "text/plain", "start_char_idx": 9636, "end_char_idx": 10239, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6152888b-1e9f-4ec1-bf9a-39bd5ecf6a19": {"__data__": {"id_": "6152888b-1e9f-4ec1-bf9a-39bd5ecf6a19", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b032fa00-a193-41da-a194-7776be2d46fd", "node_type": "1", "metadata": {}, "hash": "c8b89455ad6c1c0e83d7aebd433bf3d83634e090a136c34cecdca1ecb1f3d558", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68a75f01-dabb-4d9b-885d-143a437b0d1a", "node_type": "1", "metadata": {}, "hash": "2344270fe3d79a3069d51ad1062e227f55c15cb8a8edcfb0f59bf37c5502fa4e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The combination of fusion and tiling are the most common\nmethods to enhance producer-consumer locality and make full\nutilization of parallelism, but the optimization space is too\nlarge to be explored.\n3) Autotuning: Let the optimization sequence of the sched-\nuler in a compiler contain n optimization passes. If we\nfocus on whether to apply the optimization, then we have 2n\noptimization options to select from. Furthermore, if each opti-\nmization pass has a many-choice option with m variants, then\nthe total optimization space becomes \ufffdn\ni=0 mi.", "mimetype": "text/plain", "start_char_idx": 10240, "end_char_idx": 10788, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68a75f01-dabb-4d9b-885d-143a437b0d1a": {"__data__": {"id_": "68a75f01-dabb-4d9b-885d-143a437b0d1a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6152888b-1e9f-4ec1-bf9a-39bd5ecf6a19", "node_type": "1", "metadata": {}, "hash": "949dd866ae84c546ff3ff7b811e0addad79b24156c31271fe639e71011909cb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e125a4bd-9498-41af-b170-87ac268881a5", "node_type": "1", "metadata": {}, "hash": "d983a19d4e4b38cba9a8e994f33f513a13c5957485e0860af2dbd5e9976158a1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "If we take the\nordering of optimization steps into account, the optimization\nsearch possibilities become n! due to the permutations. Hence,\ndifficulties often arise from the combinatorial explosion of\noptimization choices. If all optimization steps and scheduling\nAuthorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:51:44 UTC from IEEE Xplore.  Restrictions apply. \n3\nare manually specified, it would incur a high engineering cost\nto achieve an ideal performance even for the most experienced\nengineers.", "mimetype": "text/plain", "start_char_idx": 10789, "end_char_idx": 11329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e125a4bd-9498-41af-b170-87ac268881a5": {"__data__": {"id_": "e125a4bd-9498-41af-b170-87ac268881a5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68a75f01-dabb-4d9b-885d-143a437b0d1a", "node_type": "1", "metadata": {}, "hash": "2344270fe3d79a3069d51ad1062e227f55c15cb8a8edcfb0f59bf37c5502fa4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50601c31-e74d-4160-b2e6-214aed85138b", "node_type": "1", "metadata": {}, "hash": "9a388282284b795315a66dacd433f3bbe7ebb8cde4185311782dc0c795baa683", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Autotuning refers to a methodology incorporating\na model with which users can traverse the entire optimization\nspace efficiently. For autotuning, it is challenging to develop\nan approach that is able to traverse all potentially profitable\noptimization choices incorporating a precise execution cost\nwith finite time complexity.\n4) Back Ends and Code Generation: The back end is\nresponsible for emitting machine code for the optimized\nimplementations. In general, the IRs and programs are trans-\nformed into LLVM[24] or CUDA[25]/OpenCL[26] source\ncode.", "mimetype": "text/plain", "start_char_idx": 11330, "end_char_idx": 11881, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50601c31-e74d-4160-b2e6-214aed85138b": {"__data__": {"id_": "50601c31-e74d-4160-b2e6-214aed85138b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e125a4bd-9498-41af-b170-87ac268881a5", "node_type": "1", "metadata": {}, "hash": "d983a19d4e4b38cba9a8e994f33f513a13c5957485e0860af2dbd5e9976158a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e09bab72-b099-484b-8dae-b70e1b650332", "node_type": "1", "metadata": {}, "hash": "e03e50e5deab2f48f8ce6acc064f224fae03592b03f66f20cd5f9d2c34f8f745", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Unfortunately, several patterns of the implementations\nmay generate poor code when passed directly to LLVM.\nAdditionally, due to the custom instruction set architectures\nfor specialized accelerators, back ends for custom accelerators\nneed to be designed explicitly from scratch. In some extreme\ncases, the end implementations might not target an individual\nCPU or GPU kernel, and the hybrid execution of CPU/G-\nPU/FPGA/ASIC platforms introduces new challenges. A small\nchange in the implementation can affect the memory manage-\nment, communication between devices, synchronization and\noptimization choices.\nIII.", "mimetype": "text/plain", "start_char_idx": 11882, "end_char_idx": 12493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e09bab72-b099-484b-8dae-b70e1b650332": {"__data__": {"id_": "e09bab72-b099-484b-8dae-b70e1b650332", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50601c31-e74d-4160-b2e6-214aed85138b", "node_type": "1", "metadata": {}, "hash": "9a388282284b795315a66dacd433f3bbe7ebb8cde4185311782dc0c795baa683", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ca02669-dabf-4246-9691-62ed76bd22d8", "node_type": "1", "metadata": {}, "hash": "a801a7a27fe34d73b8f8c82fa0982efad4f54a0ab4f9ffb5788a57cbc77a4273", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "IMPLEMENTATIONS OF COMPILER FRAMEWORKS\nIn this section, we analyze 4 full-stack compiler frameworks\nfor deep neural networks to generate optimized implementa-\ntions on CPUs and GPUs. We focus on the domain-specific\nrepresentations, high-level transformations and data scheduling\noptimization. In addition, we present 5 compiler architectures\nfor specialized FPGA-based accelerators. Other compilers,\nsuch as Intel\u2019s nGraph[27] and TensorFlow XLA[16], are still\nexperimental and in active development, and we do not discuss\nthem in this paper.", "mimetype": "text/plain", "start_char_idx": 12494, "end_char_idx": 13036, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ca02669-dabf-4246-9691-62ed76bd22d8": {"__data__": {"id_": "9ca02669-dabf-4246-9691-62ed76bd22d8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e09bab72-b099-484b-8dae-b70e1b650332", "node_type": "1", "metadata": {}, "hash": "e03e50e5deab2f48f8ce6acc064f224fae03592b03f66f20cd5f9d2c34f8f745", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3470e867-a066-418e-8705-fbd1a4b89481", "node_type": "1", "metadata": {}, "hash": "f2ad7a364c57fe483b4184a198d9af190ab4cd59f2e91d6290cd789eff83275f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "There is not much literature or source code\nfor compilers targeting ASICs such as TPU[7] or Diannao[28]\nfor reference, so we do not include them either.\nA. Halide\nThe Halide compiler[12] was originally designed for image\nprocessing, and neural networks have many similarities with\nimage processing. They are both composed of a long compu-\ntation sequence of many operations, and they both combine\nthe challenges of stencil computation and stream programs.", "mimetype": "text/plain", "start_char_idx": 13037, "end_char_idx": 13492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3470e867-a066-418e-8705-fbd1a4b89481": {"__data__": {"id_": "3470e867-a066-418e-8705-fbd1a4b89481", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ca02669-dabf-4246-9691-62ed76bd22d8", "node_type": "1", "metadata": {}, "hash": "a801a7a27fe34d73b8f8c82fa0982efad4f54a0ab4f9ffb5788a57cbc77a4273", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8505cb06-bca1-4b0b-9832-fdaf92dfb907", "node_type": "1", "metadata": {}, "hash": "4cfee1041ef582844254c5cd4df141e4ba903d74ee7719dc85a1040d18910420", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Thousands of lines of code for the optimized pipeline must\nbe written manually in C, CUDA or assembly for each\ncomplex operation to achieve the peak performance, even by\nan experienced engineer. The optimized pipeline cannot be\nported to other architectures. In consideration of these aspects,\nHalide applies high-level abstraction and efficient schedule\nmethods to improve portability and composability.\n1) Domain-specific Language (DSL) and Representations:\nHalide\u2018s DSL decouples the algorithm definition from the\nexecution strategy.", "mimetype": "text/plain", "start_char_idx": 13493, "end_char_idx": 14029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8505cb06-bca1-4b0b-9832-fdaf92dfb907": {"__data__": {"id_": "8505cb06-bca1-4b0b-9832-fdaf92dfb907", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3470e867-a066-418e-8705-fbd1a4b89481", "node_type": "1", "metadata": {}, "hash": "f2ad7a364c57fe483b4184a198d9af190ab4cd59f2e91d6290cd789eff83275f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "711dacfd-483b-45af-8ade-1864aa723de5", "node_type": "1", "metadata": {}, "hash": "0a437b66385b53a8b470799b3dc82adf64e0db121579f46d36abc8d2b5065bbb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Instead of providing specific values to\ndescribe a function, algorithms are defined as pure functions\nover an infinite integer domain. The DSL avoids higher-\norder functions, dynamic recursion and complex data structure.\nTo express operations such as convolution and pooling in\nneural networks, a recursive reduction function is applied by\ndefining the minimum and maximum value for the targeting\ndimension. The scheduling representations provide efficient\ndescriptions for the implementations of parallel, vectorized,\ntiled, fused and reordered functions. Halides DSL is simpler\nthan most functional languages and is sufficient to describe\nmost operations and scheduling methods for neural networks.", "mimetype": "text/plain", "start_char_idx": 14030, "end_char_idx": 14730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "711dacfd-483b-45af-8ade-1864aa723de5": {"__data__": {"id_": "711dacfd-483b-45af-8ade-1864aa723de5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8505cb06-bca1-4b0b-9832-fdaf92dfb907", "node_type": "1", "metadata": {}, "hash": "4cfee1041ef582844254c5cd4df141e4ba903d74ee7719dc85a1040d18910420", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3d40b0c-3805-4d3b-8d23-d4dd4f2deccd", "node_type": "1", "metadata": {}, "hash": "6dfb9832a63baa5576fd6dc978f2ddbfbb3105a912aaf384c3b906be9dc1f5b8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Listing 1: Algorithm description of convolution written\nmanually in Halide DSL. By setting the potential range of\nshapes of the input Buffer and V ariables, the\nautoscheduler can search for valid CPU execution strategies\nautomatically.\n1 / / Data\nare\ns t o r e d\nin\nan on\u2212chip\nb u f f e r .", "mimetype": "text/plain", "start_char_idx": 14731, "end_char_idx": 15021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3d40b0c-3805-4d3b-8d23-d4dd4f2deccd": {"__data__": {"id_": "a3d40b0c-3805-4d3b-8d23-d4dd4f2deccd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "711dacfd-483b-45af-8ade-1864aa723de5", "node_type": "1", "metadata": {}, "hash": "0a437b66385b53a8b470799b3dc82adf64e0db121579f46d36abc8d2b5065bbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7796bbc4-a338-43bd-bd86-5369a20e7c54", "node_type": "1", "metadata": {}, "hash": "46bd2a681bb73e088f6cc504cfc220d36ff104b006dc4b9e6d6840cf53267792", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2 Input<Buffer<f l o a t >\n> input , wtB , b i a s ;\n3 Input<f l o a t > pad l , pad t , stride w , s t r i d e\nh ;\n4 / / Set\nthe\nboundary\nc o n d i t i o n\nf o r\nf u n c t i o n s .", "mimetype": "text/plain", "start_char_idx": 15022, "end_char_idx": 15204, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7796bbc4-a338-43bd-bd86-5369a20e7c54": {"__data__": {"id_": "7796bbc4-a338-43bd-bd86-5369a20e7c54", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3d40b0c-3805-4d3b-8d23-d4dd4f2deccd", "node_type": "1", "metadata": {}, "hash": "6dfb9832a63baa5576fd6dc978f2ddbfbb3105a912aaf384c3b906be9dc1f5b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a05685f1-eb4c-4e53-a231-f63d381fa96f", "node_type": "1", "metadata": {}, "hash": "30794e51f12d7f410db21917e0b8044264fda6a87b770a7267af4740889d63a2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5 Func\nin = BoundaryConditions : : c o n s t a n t\ne x t e r i o r (\ninput ,\n0) ;\n6 Func in w = BoundaryConditions : : repeat edge ( wtB ) ;\n7 Func\nin b = BoundaryConditions : : repeat edge ( b i a s ) ;\n8 / / Pre\u2212d e c l a r e\nthe\nv a r i a b l e s\nand\nf u n c t i o n s .", "mimetype": "text/plain", "start_char_idx": 15205, "end_char_idx": 15478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a05685f1-eb4c-4e53-a231-f63d381fa96f": {"__data__": {"id_": "a05685f1-eb4c-4e53-a231-f63d381fa96f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7796bbc4-a338-43bd-bd86-5369a20e7c54", "node_type": "1", "metadata": {}, "hash": "46bd2a681bb73e088f6cc504cfc220d36ff104b006dc4b9e6d6840cf53267792", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb1b185a-6e04-4913-bbbb-318f66778f2e", "node_type": "1", "metadata": {}, "hash": "07a45b95edd0532e124fd36b4fd89a105c8905c2db7ca29a96ab0fddc2e886fb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "9 Var w, h , c , oc , kw , kh ;\n10 Func fw , fh , conv2d , convsum ;\n11 / / Algorithm\nd e s c r i p t i o n\nof\nc o n v o l u t i o n .\n12 fw (w) = w\u2217s t r i d e\nw \u2212pad l ;\n13 fh ( h ) = h\u2217s t r i d e\nh \u2212pad t ;\n14 RDom k e r n e l (0 , wtB . width ( ) ,0 , wtB .", "mimetype": "text/plain", "start_char_idx": 15479, "end_char_idx": 15741, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb1b185a-6e04-4913-bbbb-318f66778f2e": {"__data__": {"id_": "eb1b185a-6e04-4913-bbbb-318f66778f2e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a05685f1-eb4c-4e53-a231-f63d381fa96f", "node_type": "1", "metadata": {}, "hash": "30794e51f12d7f410db21917e0b8044264fda6a87b770a7267af4740889d63a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e6e0720-d784-4adf-a0ec-4585d5a44b5d", "node_type": "1", "metadata": {}, "hash": "52c3e1e1075c8910bbc3fd8e4d1dad440a462f14e3eefaec4d9a7efffca16808", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "h e i g h t ( ) ) ;\n15 conv2d (w, h , c , oc ) += in ( fw (w) + k e r n e l . x , fh ( h ) + k e r n e l\n. y , c ) \u2217in w ( k e r n e l . x , k e r n e l . y , c , oc ) ;\n16 / / Reduction\nof\nthe\ndimension\nof\nthe\nchannel .\n17 RDom channel (0 ,\ni n p u t .", "mimetype": "text/plain", "start_char_idx": 15742, "end_char_idx": 15995, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e6e0720-d784-4adf-a0ec-4585d5a44b5d": {"__data__": {"id_": "9e6e0720-d784-4adf-a0ec-4585d5a44b5d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb1b185a-6e04-4913-bbbb-318f66778f2e", "node_type": "1", "metadata": {}, "hash": "07a45b95edd0532e124fd36b4fd89a105c8905c2db7ca29a96ab0fddc2e886fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e18a3d2b-d41d-4df9-a484-058727ff9c07", "node_type": "1", "metadata": {}, "hash": "dca3e7b644622b24829088c3b2bd15f9d623ca5def4516c490138a20ba53baff", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "channels ( ) ) ;\n18 convsum (w, h , oc ) += conv2d (w, h , channel , oc ) ;\n19 / / Add the\nb i a s .\n20 outp ut (w, h , oc ) = convsum (w, h , oc ) + in b ( oc ) ;\n21 / / Set\nbounds estimate\nf o r\npre\u2212d e f i n e d\nb u f f e r s .", "mimetype": "text/plain", "start_char_idx": 15996, "end_char_idx": 16226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e18a3d2b-d41d-4df9-a484-058727ff9c07": {"__data__": {"id_": "e18a3d2b-d41d-4df9-a484-058727ff9c07", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e6e0720-d784-4adf-a0ec-4585d5a44b5d", "node_type": "1", "metadata": {}, "hash": "52c3e1e1075c8910bbc3fd8e4d1dad440a462f14e3eefaec4d9a7efffca16808", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25cc674e-c95e-4e1a-973b-e04f9abfa371", "node_type": "1", "metadata": {}, "hash": "9596ef566cbaeb7d21d544869a8fe7ccd9235caef645459c3827130b7617e1aa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "22 / / Halide\no p t i m i z e s\nthe\nschedule\nunder\nsuch\nc o n s t r a i n t s .\n23 i n p u t . dim ( i ) . s e t\nb o u n d s\ne s t i m a t e (0 , value\n{ i }) ;\n. . .\n24 outp ut . e s t i m a t e (w, 0 , 1 6 ) .", "mimetype": "text/plain", "start_char_idx": 16227, "end_char_idx": 16438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25cc674e-c95e-4e1a-973b-e04f9abfa371": {"__data__": {"id_": "25cc674e-c95e-4e1a-973b-e04f9abfa371", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e18a3d2b-d41d-4df9-a484-058727ff9c07", "node_type": "1", "metadata": {}, "hash": "dca3e7b644622b24829088c3b2bd15f9d623ca5def4516c490138a20ba53baff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab10ee88-e59f-4c36-a5d7-904ed0a9d327", "node_type": "1", "metadata": {}, "hash": "0936e2684f16e09d520231ae8d3cdb8c99650ebcaa51b73b76897b241356063c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "e s t i m a t e ( h , 0 , 1 6 ) . e s t i m a t e (\noc ,0 ,4096) ;\n2) Autoscheduler: At the very beginning, the optimization\nin Halide is semiautomatic, and the schedule is specified by\nthe user. The schedule search space in Halide is enormous due\nto the combination of choices of marking the implementations\nof loop parallel, reorder, vectorized or unrolled, caching be-\nhaviors and hybrid code generation for various devices.", "mimetype": "text/plain", "start_char_idx": 16439, "end_char_idx": 16866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab10ee88-e59f-4c36-a5d7-904ed0a9d327": {"__data__": {"id_": "ab10ee88-e59f-4c36-a5d7-904ed0a9d327", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25cc674e-c95e-4e1a-973b-e04f9abfa371", "node_type": "1", "metadata": {}, "hash": "9596ef566cbaeb7d21d544869a8fe7ccd9235caef645459c3827130b7617e1aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43914e07-f635-4db0-b54c-4a66dc2d1a6a", "node_type": "1", "metadata": {}, "hash": "f02e1b3204d38e03fc9826c774232cc1361c9d8f4098af0fef88b23d9bec9399", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The\nprimary autotuner[12] in Halide applies stochastic search and\ngenetic algorithms to optimize schedulers for pipelines. It\nstarts from seeding potentially profitable schedules to initial\npopulations of a fixed size (128) and constructs a new gener-\nation with crossover elitism, mutated and random individuals.\nIn recent work, Mullapudi et al. proposed a model-driven\nautoscheduler[29] for Halide. The heuristic autoscheduler first\ndetermines the best tile size for each group to maximize input\ndata reuse.", "mimetype": "text/plain", "start_char_idx": 16867, "end_char_idx": 17376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43914e07-f635-4db0-b54c-4a66dc2d1a6a": {"__data__": {"id_": "43914e07-f635-4db0-b54c-4a66dc2d1a6a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab10ee88-e59f-4c36-a5d7-904ed0a9d327", "node_type": "1", "metadata": {}, "hash": "0936e2684f16e09d520231ae8d3cdb8c99650ebcaa51b73b76897b241356063c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d282d820-b8fc-4b98-b9ea-35ffcd3a322c", "node_type": "1", "metadata": {}, "hash": "cb118a5770e0457277afff0dd46b66b0546af0f68b686173e4d34e345267cad0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Then, the scheduler enumerates all valid grouping\nopportunities with a direct producer-consumer relationship to\nreduce memory communications.", "mimetype": "text/plain", "start_char_idx": 17377, "end_char_idx": 17518, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d282d820-b8fc-4b98-b9ea-35ffcd3a322c": {"__data__": {"id_": "d282d820-b8fc-4b98-b9ea-35ffcd3a322c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43914e07-f635-4db0-b54c-4a66dc2d1a6a", "node_type": "1", "metadata": {}, "hash": "f02e1b3204d38e03fc9826c774232cc1361c9d8f4098af0fef88b23d9bec9399", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0acfc550-8ace-443a-9130-de2731b81860", "node_type": "1", "metadata": {}, "hash": "ab5832ef7cf8c0ec8e0ce71c94a3ea6d8d7cce260bd6a03492e79d2882ae28d1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "However, different tile sizes\nfor the grouping operations introduce additional recomputa-\ntion, so the authors set some rules for candidate tiling: 1) the\nminimum of the innermost dimension of tiling should be more\nthan VECTOR_WIDTH so that the loop nests can be efficiently\nvectorized, 2) the number of tiles should be larger than the\nnumber of CPU cores to enhance parallelism, and 3) the mem-\nory footprint is dependent on the last-level CACHE_SIZE.\nThen, the autoscheduler provides a cost function to estimate\nAuthorized licensed use limited to: Nanjing University.", "mimetype": "text/plain", "start_char_idx": 17519, "end_char_idx": 18088, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0acfc550-8ace-443a-9130-de2731b81860": {"__data__": {"id_": "0acfc550-8ace-443a-9130-de2731b81860", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d282d820-b8fc-4b98-b9ea-35ffcd3a322c", "node_type": "1", "metadata": {}, "hash": "cb118a5770e0457277afff0dd46b66b0546af0f68b686173e4d34e345267cad0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffbdc075-f38f-4c62-a564-8608463c752c", "node_type": "1", "metadata": {}, "hash": "e8aa5c21bdc478bcd9759632731a145783f9490988cfbfedde8fe87c61c022c4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Downloaded on June 02,2024 at 08:51:44 UTC from IEEE Xplore.  Restrictions apply. \n4\nthe performance improvement of grouping and tiling by adding\nthe arithmetic cost of implementations ARITH_COST to the\ntotal number of loads LOAD_COST. The performance benefit\nof inlining a function to consumers is also considered by the\nscheduler. Finally, the autotuner selects the optimal schedule\nstrategies with the minimal cost.", "mimetype": "text/plain", "start_char_idx": 18089, "end_char_idx": 18507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffbdc075-f38f-4c62-a564-8608463c752c": {"__data__": {"id_": "ffbdc075-f38f-4c62-a564-8608463c752c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0acfc550-8ace-443a-9130-de2731b81860", "node_type": "1", "metadata": {}, "hash": "ab5832ef7cf8c0ec8e0ce71c94a3ea6d8d7cce260bd6a03492e79d2882ae28d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b38ed1d-3ff4-4abb-a672-c426bb82ada8", "node_type": "1", "metadata": {}, "hash": "37b399001c50e2829f8f0fdda76760d342067a898580add69916241518d05aa2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "3) Portability\nto\nDifferent\nArchitectures:\nThe\nHalide\nautoscheduler provides the ability to generate an opti-\nmized\npipeline\ntargeting\nvarious\nhardware\narchitectures.\nBy altering the values of PARALLELISM_THRESHOLD,\nVECTOR_WIDTH, CACHE_SIZE and LOAD_COST as men-\ntioned above, the autoscheduler can be adapted to CPUs\nsuch as the Xeon and ARM CPUs.", "mimetype": "text/plain", "start_char_idx": 18508, "end_char_idx": 18856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b38ed1d-3ff4-4abb-a672-c426bb82ada8": {"__data__": {"id_": "2b38ed1d-3ff4-4abb-a672-c426bb82ada8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffbdc075-f38f-4c62-a564-8608463c752c", "node_type": "1", "metadata": {}, "hash": "e8aa5c21bdc478bcd9759632731a145783f9490988cfbfedde8fe87c61c022c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d24b05d3-94b4-4b7a-b890-8ffba811e88f", "node_type": "1", "metadata": {}, "hash": "31c97d65dfa7b84594a743bdae6d42b0e6877d2b637e058ae1a29d2cf411b9cc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Alone, the autoscheduler\ncan generate schedules for GPUs in a similar way, and\nthe number of threads per GPU thread block is constrained\nby MAX_THREADS_PER_BLOCK to avoid generating invalid\nstrategies.\nB. TVM\nTVM is an end-to-end full-stack compiler framework that\nmaps high-level specifications of deep neural networks from\nmultiple deep learning frameworks to low-level optimized\ncode for a diverse set of hardware back ends. In recent months,\nTVM has become a community that attracts many developers\nto optimize their models based on TVM stacks.", "mimetype": "text/plain", "start_char_idx": 18857, "end_char_idx": 19405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d24b05d3-94b4-4b7a-b890-8ffba811e88f": {"__data__": {"id_": "d24b05d3-94b4-4b7a-b890-8ffba811e88f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b38ed1d-3ff4-4abb-a672-c426bb82ada8", "node_type": "1", "metadata": {}, "hash": "37b399001c50e2829f8f0fdda76760d342067a898580add69916241518d05aa2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9458e849-1aec-43ec-b3a4-7464f384192e", "node_type": "1", "metadata": {}, "hash": "0f0595fc8cc87e48f0103070bc43178a917333bb8b1be1b183bb8ad26ddd119d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1) Relay[22]: Relay is the front end of TVM. The com-\nputing graphs in TensorFlow are static graphs with a fixed\ntopology. It is easy to optimize each operation, but users can\nconstruct their own operations only in a deeply embedded\nDSL. Dynamic computing graphs as adopted by PyTorch and\nChainer[30] provide the convenience to describe the operators,\nbut it would be very hard to leverage optimization across the\noperation and hardware platforms.", "mimetype": "text/plain", "start_char_idx": 19406, "end_char_idx": 19853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9458e849-1aec-43ec-b3a4-7464f384192e": {"__data__": {"id_": "9458e849-1aec-43ec-b3a4-7464f384192e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d24b05d3-94b4-4b7a-b890-8ffba811e88f", "node_type": "1", "metadata": {}, "hash": "31c97d65dfa7b84594a743bdae6d42b0e6877d2b637e058ae1a29d2cf411b9cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb16379e-43d3-421a-ba36-953e693b4d50", "node_type": "1", "metadata": {}, "hash": "a82f3c1d8b81dc8e1f621b7fe6db60ff9e5f33302a490e7e622280e8428cc1b6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As a result, Relay presents\na new high-level IR to provide expressiveness and efficient\ncompilation from the perspective of a programming language\ninstead of the previous dataflow representations.\n2) Graph-level Optimization: Graph-level optimization of\nthe computing graphs of deep neural networks has been\ndemonstrated to be effective by many off-the-shelf tools[31].\nTVM focuses on operation fusion and data layout transforma-\ntion.", "mimetype": "text/plain", "start_char_idx": 19854, "end_char_idx": 20289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb16379e-43d3-421a-ba36-953e693b4d50": {"__data__": {"id_": "bb16379e-43d3-421a-ba36-953e693b4d50", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9458e849-1aec-43ec-b3a4-7464f384192e", "node_type": "1", "metadata": {}, "hash": "0f0595fc8cc87e48f0103070bc43178a917333bb8b1be1b183bb8ad26ddd119d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1669afa0-c886-43d1-8cfb-16eca718dbaf", "node_type": "1", "metadata": {}, "hash": "12f99ae87b023307f476ae1d3e368567af26d0b3491454c0c9fff67a5ad3dda5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Operator fusion in TVM combines adjacent operators:\n1) those that can be precomputed, 2) reduction functions,\nand 3) pointwise operations to reduce data transfer between\nthe on-chip buffer and off-chip memory. Data layout can be\nconverted for better execution on the target hardware. Based\non TVM, Amazon[32] has presented a new data layout method\nto accelerate an NN on a CPU.\n3) Automating Operator-level Optimization:\nUnlike the\nhigh-level representations, optimized implementations of each\noperator are opaque to users.", "mimetype": "text/plain", "start_char_idx": 20290, "end_char_idx": 20813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1669afa0-c886-43d1-8cfb-16eca718dbaf": {"__data__": {"id_": "1669afa0-c886-43d1-8cfb-16eca718dbaf", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb16379e-43d3-421a-ba36-953e693b4d50", "node_type": "1", "metadata": {}, "hash": "a82f3c1d8b81dc8e1f621b7fe6db60ff9e5f33302a490e7e622280e8428cc1b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f757e0d3-1100-40f0-9301-b235b47072d9", "node_type": "1", "metadata": {}, "hash": "9d09f402976b9628f1f7ffcd26ff2d4d5236973b19bf99cdebf145f5984420c9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Internally, TVM reuses helpful\nschedule primitives in Halide and extends the primitives to\noptimize the GPU and specialized accelerator performance.\nFor example, TVM provides schedule primitives that assign\ndata into the shared memory in GPU so that groups of threads\ncan fetch the data they need cooperatively. Additionally, TVM\ndecouples the schedule primitives with the hardware intrinsi-\ncally, so the compiler is capable of matching schedule patterns\nwith hardware implementations.\nGiven the schedule primitives, users could leverage op-\ntimization for scheduling either manually or based on the\nexperience provided by TVM developers.", "mimetype": "text/plain", "start_char_idx": 20814, "end_char_idx": 21453, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f757e0d3-1100-40f0-9301-b235b47072d9": {"__data__": {"id_": "f757e0d3-1100-40f0-9301-b235b47072d9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1669afa0-c886-43d1-8cfb-16eca718dbaf", "node_type": "1", "metadata": {}, "hash": "12f99ae87b023307f476ae1d3e368567af26d0b3491454c0c9fff67a5ad3dda5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c75c395-bb32-4e4d-b4fa-defe48874fdf", "node_type": "1", "metadata": {}, "hash": "c1d69871b79925d0b6c3e6c2dbef998ee12560aac4d08e7f58875515af1dcaa2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Such an approach is\ninefficient, so the remaining problem is to find an automatic\nmethod for scheduling optimization. By comparing random\nsearch and blackbox genetic algorithms, which are similar\nto the methods in the previous edition of Halide with ML-\nbased models[33], [34], users can find an ML-based model:\na gradient tree boosting model that provides high speedup\nquality and requires little training cost.\n4) Hybrid Execution: TVM supports multiple types of\nhardware platforms, including a server-class CPU/GPU and an\nembedded-class CPU/GPU.", "mimetype": "text/plain", "start_char_idx": 21454, "end_char_idx": 22002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c75c395-bb32-4e4d-b4fa-defe48874fdf": {"__data__": {"id_": "8c75c395-bb32-4e4d-b4fa-defe48874fdf", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f757e0d3-1100-40f0-9301-b235b47072d9", "node_type": "1", "metadata": {}, "hash": "9d09f402976b9628f1f7ffcd26ff2d4d5236973b19bf99cdebf145f5984420c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9011f739-c3ca-4321-b88d-00e36e4548ad", "node_type": "1", "metadata": {}, "hash": "cb8af7b0baaf9e6292258dafeaac863f40eb3bce178130140a9ac473e5213c70", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In particular, the TVM group pro-\nvides a codesign of the hardware and software tool VTA[35],\nwhich can generate hardware architecture based on FPGA\nthrough high-level synthesis (HLS) and generate hybrid im-\nplementations for both the ARM CPU and FPGA.\nC. DLVM\nThe IR in DLVM is a graph-based, modular representation\nthat has a set of hierarchy of abstractions, including mod-\nule, function, basic block and instruction.", "mimetype": "text/plain", "start_char_idx": 22003, "end_char_idx": 22423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9011f739-c3ca-4321-b88d-00e36e4548ad": {"__data__": {"id_": "9011f739-c3ca-4321-b88d-00e36e4548ad", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c75c395-bb32-4e4d-b4fa-defe48874fdf", "node_type": "1", "metadata": {}, "hash": "c1d69871b79925d0b6c3e6c2dbef998ee12560aac4d08e7f58875515af1dcaa2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd2db383-b0c5-41f9-8aa8-2f303690613d", "node_type": "1", "metadata": {}, "hash": "04c77220df97ce47ba4453934211cc8067ddb1e1f6859138e772b4304451f1b9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In particular, each\nmodule contains type definitions and functions, each function\ncontains a control flow graph formed by basic blocks, and each\nbasic block contains instructions with data dependencies in\nthe form of a DAG. The virtual instructions in DLVM include\nbasic fine-grained math operators, which can be categorized\ninto 1) elementwise operators such as add and tanh and 2)\ncomplex operators such as dot and convolve. Optimization\nsteps in DLVM include algebra simplification, linear algebra\nfusion, matrix multiplication reordering and some traditional\ncompiler optimization steps.", "mimetype": "text/plain", "start_char_idx": 22424, "end_char_idx": 23015, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd2db383-b0c5-41f9-8aa8-2f303690613d": {"__data__": {"id_": "fd2db383-b0c5-41f9-8aa8-2f303690613d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9011f739-c3ca-4321-b88d-00e36e4548ad", "node_type": "1", "metadata": {}, "hash": "cb8af7b0baaf9e6292258dafeaac863f40eb3bce178130140a9ac473e5213c70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0f91275-eefe-48be-95ac-2f2cc648f1e1", "node_type": "1", "metadata": {}, "hash": "96444b841c17d628cc4747fe559ffb351f166b0b5fd44e06bfaf52d57297ebcc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Ultimately, DLVM IR exists at a\nlower level than implementations of BLAS and computation\nkernels of LLVM for code generation.\nD. Tensor Comprehension (TC)\nOn GPUs, the achieved performance of parallel execution of\noperations through the preoptimized library functions depends\non the data size, data layout and various hardware features\nof the GPUs. However, creating a library that covers all\ntransformations and optimizations for a combination of these\nfeatures and leverages optimizations across operators is fea-\nsible. All scheduling transformations of algorithms for GPUs\nshould be written manually in Halide.", "mimetype": "text/plain", "start_char_idx": 23016, "end_char_idx": 23630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0f91275-eefe-48be-95ac-2f2cc648f1e1": {"__data__": {"id_": "a0f91275-eefe-48be-95ac-2f2cc648f1e1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd2db383-b0c5-41f9-8aa8-2f303690613d", "node_type": "1", "metadata": {}, "hash": "04c77220df97ce47ba4453934211cc8067ddb1e1f6859138e772b4304451f1b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f07ca8e-d4a5-4db8-a318-40c946baac6c", "node_type": "1", "metadata": {}, "hash": "69faf7f6c0131d907a8e5fa8eb192e2ad2cc61693788606db33043e541541b78", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "More recent deep neural\nnetwork compilers such as XLA and Latte[36] cannot readily\nachieve the ideal throughput even though they take the data\nsize and cross-layer optimization into consideration. Building\non these considerations, TC presents an expressive DSL that\ncan efficiently describe the algorithms. Based on the DSL,\nthe compilation flow maps the high-level representation into\nthe polyhedral model to explore scheduling of the optimization\nspace and generates highly optimized GPU code automatically.", "mimetype": "text/plain", "start_char_idx": 23631, "end_char_idx": 24140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f07ca8e-d4a5-4db8-a318-40c946baac6c": {"__data__": {"id_": "0f07ca8e-d4a5-4db8-a318-40c946baac6c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0f91275-eefe-48be-95ac-2f2cc648f1e1", "node_type": "1", "metadata": {}, "hash": "96444b841c17d628cc4747fe559ffb351f166b0b5fd44e06bfaf52d57297ebcc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9279c0f9-a48e-4994-aaed-f2fd7ebb2952", "node_type": "1", "metadata": {}, "hash": "715b853a8251cf3737449e45575be42e4aceb8c2fd96b3e28a108ab800eca963", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1) High-level Representations and DSL: Instead of design-\ning an embedded DLS such as Halide (embedded in C++),\nTC avoids a verbose process when addressing debugging\nand warnings of the embedded DSL. The presentation of\ncomputing and multiarrays in TC is inspired by OoLaLa[37],\nTACO[38] and Einstein notation[39], which prevents users\nAuthorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:51:44 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 24141, "end_char_idx": 24614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9279c0f9-a48e-4994-aaed-f2fd7ebb2952": {"__data__": {"id_": "9279c0f9-a48e-4994-aaed-f2fd7ebb2952", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f07ca8e-d4a5-4db8-a318-40c946baac6c", "node_type": "1", "metadata": {}, "hash": "69faf7f6c0131d907a8e5fa8eb192e2ad2cc61693788606db33043e541541b78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44452aae-48f9-4dbe-9f02-a0c337763b66", "node_type": "1", "metadata": {}, "hash": "f7a17e8389c225009dee0d711372d7b6744526aa1a06247d52d4f56bfe2a8f38", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5\nfrom predeclaring variables and functions, simplifies the re-\nduction operations and eliminates the influence of evaluation\nsequence of points on the output. To integrate TC into deep\nlearning frameworks, they provide APIs to transform the NN\nmodel from deep learning frameworks into TC. A single\nTC corresponds to a node in the computing graph in the\nML framework. When adding a new operator, users can\nwrite their own TC implementation instead of using back-end\nimplementation.\nListing 2: Algorithm description of convolution written\nmanually in TC DSL.", "mimetype": "text/plain", "start_char_idx": 24616, "end_char_idx": 25173, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44452aae-48f9-4dbe-9f02-a0c337763b66": {"__data__": {"id_": "44452aae-48f9-4dbe-9f02-a0c337763b66", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9279c0f9-a48e-4994-aaed-f2fd7ebb2952", "node_type": "1", "metadata": {}, "hash": "715b853a8251cf3737449e45575be42e4aceb8c2fd96b3e28a108ab800eca963", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28ded4bf-d587-48c8-90fc-c32fc5afda76", "node_type": "1", "metadata": {}, "hash": "6e18fd930938c22670a24c33c5d89919867cccc729f84269b913c6ba85657370", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The autotuner in TC traverses\nscheduling opportunities based on hardware features and\nleverages a generic algorithm to find the optimal execution\nstrategies.\n1 / /\nNo p r e d e c l a r a t i o n\nof\nv a r i a b l e s\nl i k e B, C,H or W\n2 def\nconv relu ( f l o a t (B, C,H,W)\nInput , f l o a t (CO, C,KH,K\nW\n)\nWeights , f l o a t (CO)\nBias ,", "mimetype": "text/plain", "start_char_idx": 25174, "end_char_idx": 25514, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28ded4bf-d587-48c8-90fc-c32fc5afda76": {"__data__": {"id_": "28ded4bf-d587-48c8-90fc-c32fc5afda76", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44452aae-48f9-4dbe-9f02-a0c337763b66", "node_type": "1", "metadata": {}, "hash": "f7a17e8389c225009dee0d711372d7b6744526aa1a06247d52d4f56bfe2a8f38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e208ca0-6c5b-4703-84af-8c4caf5cc87a", "node_type": "1", "metadata": {}, "hash": "f4ed36ea8c5e07f9c601267186acda4c0ecc940bdf6b15ca029505539b18bca7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "f l o a t ( 1 )\nkernel w ,\nf l o a t ( 1 )\nkernel h )\u2212>(Output ) : {\n3 / / I n d i c e s\nt h a t\nappear\non\nthe\nr i g h t\nbut\nnot on\nthe\nl e f t\nare\nassumed\nto\nbe\nreduced\ndimensions .", "mimetype": "text/plain", "start_char_idx": 25515, "end_char_idx": 25697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e208ca0-6c5b-4703-84af-8c4caf5cc87a": {"__data__": {"id_": "7e208ca0-6c5b-4703-84af-8c4caf5cc87a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28ded4bf-d587-48c8-90fc-c32fc5afda76", "node_type": "1", "metadata": {}, "hash": "6e18fd930938c22670a24c33c5d89919867cccc729f84269b913c6ba85657370", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85cdb7f4-646a-4e90-bd6b-c32044d60bbd", "node_type": "1", "metadata": {}, "hash": "6d3e180809269176cac5b058ea029d4ce1671cc473e6c20dbcf929d8d3287ac2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4 Output ( b , co , h ,w) += Input ( b , r c , h\u2217s t r i d e\nh [0]+ r kh ,\nw\u2217s t r i d e\nw [0]+ r kw ) \u2217Weights ( co , r c , r kh , r kw )\n5 Output ( b , co , h ,w) = fmax ( 0 .", "mimetype": "text/plain", "start_char_idx": 25698, "end_char_idx": 25875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85cdb7f4-646a-4e90-bd6b-c32044d60bbd": {"__data__": {"id_": "85cdb7f4-646a-4e90-bd6b-c32044d60bbd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e208ca0-6c5b-4703-84af-8c4caf5cc87a", "node_type": "1", "metadata": {}, "hash": "f4ed36ea8c5e07f9c601267186acda4c0ecc940bdf6b15ca029505539b18bca7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "908332bf-44e8-4dec-b0f9-36b2e1add3dc", "node_type": "1", "metadata": {}, "hash": "53524b7662dd2342fb54c66f468b21a7b0cb344257ff3705aa72a4d6b9a3ac5f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "0 , Output ( b , co , h ,w) +Bias (\nco ) ) }\n6 / / TC a u t o t u n e s\nthe\nschedule\ns t r a t e g i e s\nf o r\na\ns p e c i f i c\nshape\nof\nthe\ndata\neach\ntime .\n7 H,W, C, B, F ,CO,KH,K\nW = 224 ,224 ,3 ,1 ,32 ,3 ,3\n8\nt c .", "mimetype": "text/plain", "start_char_idx": 25876, "end_char_idx": 26095, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "908332bf-44e8-4dec-b0f9-36b2e1add3dc": {"__data__": {"id_": "908332bf-44e8-4dec-b0f9-36b2e1add3dc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85cdb7f4-646a-4e90-bd6b-c32044d60bbd", "node_type": "1", "metadata": {}, "hash": "6d3e180809269176cac5b058ea029d4ce1671cc473e6c20dbcf929d8d3287ac2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abbd5bc3-434c-4649-b569-73a6f38ffd3b", "node_type": "1", "metadata": {}, "hash": "435d886164a7b7128c0776d6dfbc90c18ad0f43b5df95a60f1ddb4917060f319", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "autotune and compile ( conv relu ,\ninput ,\nweights ,\nbias ,\nt u n e r\nc o n f i g )\n2) Polyhedral Compilation: Halide explores the interaction\nof grouping and slicing based on Halide IR to optimize the\nscheduling[29]; more recent work[40] presents a fusion model\nwith potentially profitable fusion strategies that are not covered\nby the previous Halide approach. Instead, the TC compiler\ntransforms the TC representations into Halide IR and then\nlowers Halides IR into a polyhedral IR.", "mimetype": "text/plain", "start_char_idx": 26096, "end_char_idx": 26581, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abbd5bc3-434c-4649-b569-73a6f38ffd3b": {"__data__": {"id_": "abbd5bc3-434c-4649-b569-73a6f38ffd3b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "908332bf-44e8-4dec-b0f9-36b2e1add3dc", "node_type": "1", "metadata": {}, "hash": "53524b7662dd2342fb54c66f468b21a7b0cb344257ff3705aa72a4d6b9a3ac5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd0a8396-0811-4e4c-b5c8-4d758f48924b", "node_type": "1", "metadata": {}, "hash": "2d396def40a2a04a56b01ea989defe68ad8b1053eb148682652be07185eaec0a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This process is realized\nby PENCIL[41] and pet libraries, but the PENCIL IR is\nultimately bypassed to bridge the mismatch between high-level\noperations and polyhedral code. The core polyhedral schedul-\ning in TC is provided by isl[42], and a data dependency\ngraph is generated internally by analyzing the algorithm and\nloops. A combination of affine transformations such as tiling,\nmapping, shifting, fusion, distribution and interchange upon\nthe loops without changing the data dependency exposes more\nopportunities for scheduling optimization.", "mimetype": "text/plain", "start_char_idx": 26582, "end_char_idx": 27127, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd0a8396-0811-4e4c-b5c8-4d758f48924b": {"__data__": {"id_": "cd0a8396-0811-4e4c-b5c8-4d758f48924b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abbd5bc3-434c-4649-b569-73a6f38ffd3b", "node_type": "1", "metadata": {}, "hash": "435d886164a7b7128c0776d6dfbc90c18ad0f43b5df95a60f1ddb4917060f319", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b318501a-bd7c-4e9d-9772-98754952167c", "node_type": "1", "metadata": {}, "hash": "556c2b255d432b7bb46eb5df7239e45a62e91b8966a13b529a1864cef46401f9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "TC extends the isl\nby providing more fine-grained control, and additional custom\nconstraints can be inserted into the program. All affine maps\ncan be integrated into a schedule tree, and the schedule tree\ncan be used to present loop tiling, mapping to blocks and\nthreads, and mapping tensors into shared or private memory\nand registers. The mapping algorithms are borrowed from\nPPCG[43], but more complex scheduling methods and opti-\nmization opportunities regarding imperfectly nested structures\nare taken into consideration by TC.", "mimetype": "text/plain", "start_char_idx": 27128, "end_char_idx": 27660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b318501a-bd7c-4e9d-9772-98754952167c": {"__data__": {"id_": "b318501a-bd7c-4e9d-9772-98754952167c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd0a8396-0811-4e4c-b5c8-4d758f48924b", "node_type": "1", "metadata": {}, "hash": "2d396def40a2a04a56b01ea989defe68ad8b1053eb148682652be07185eaec0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34297c71-1f08-4ea1-9408-279b2a29b126", "node_type": "1", "metadata": {}, "hash": "144df14818b6cf20d623de440a882e127d07005c217aadb4e748ac8dfa1dbadc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "3) Autotuning Methods: In Halide, the value of the data\nsize is not specified, and the algorithm is described over\nan infinite integer domain. However, in TC, the generated\ncode depends on the specific input shapes and other options\nsuch as hardware features, including the shared memory size\nand register size. After setting these options as an entry key,\nFig.", "mimetype": "text/plain", "start_char_idx": 27661, "end_char_idx": 28022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34297c71-1f08-4ea1-9408-279b2a29b126": {"__data__": {"id_": "34297c71-1f08-4ea1-9408-279b2a29b126", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b318501a-bd7c-4e9d-9772-98754952167c", "node_type": "1", "metadata": {}, "hash": "556c2b255d432b7bb46eb5df7239e45a62e91b8966a13b529a1864cef46401f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "704aa060-2129-4118-80c7-6465ef9ea9a1", "node_type": "1", "metadata": {}, "hash": "848fa43e06f0a8fb5956066e0443475e2bdfb55d9964777e3d7299f251189498", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2: Generic Compiler Frameworks for FPGA-based\nAccelerators\nthe autotuner in TC sets up candidate configurations about\nthe tile size, block mapping, fusion strategies and shared\nmemory usage randomly, and then each tuning is compiled\nand profiled on the GPUs. After obtaining the implementation\ncosts of each tuning, TC leverages a genetic search to find\nthe optimal execution strategy. Finally, the compilation cache\nstores the generated CUDA code, which holds the fastest\nknown implementations for each entry key to enable reuse.", "mimetype": "text/plain", "start_char_idx": 28023, "end_char_idx": 28553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "704aa060-2129-4118-80c7-6465ef9ea9a1": {"__data__": {"id_": "704aa060-2129-4118-80c7-6465ef9ea9a1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34297c71-1f08-4ea1-9408-279b2a29b126", "node_type": "1", "metadata": {}, "hash": "144df14818b6cf20d623de440a882e127d07005c217aadb4e748ac8dfa1dbadc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fb83834-55d4-4f6b-9327-73830378823b", "node_type": "1", "metadata": {}, "hash": "c3c9b4dae73b0f614d267687b3aaa4b470b220f05eb1f720485e76ae1a9c681e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "E. Overview of fpgaConvNet, DNNWeaver, DLA[18], xfDNN,\nand DNNVM\nIn Figure.2, the function of compilers for the FPGA-based\nspecialized accelerators can be divided into two categories,\nwhich are to map neural networks into 1) hardware blocks[4],\n[5], [18] and 2) instructions executed on hardware[17]\u2013[19].", "mimetype": "text/plain", "start_char_idx": 28554, "end_char_idx": 28859, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fb83834-55d4-4f6b-9327-73830378823b": {"__data__": {"id_": "5fb83834-55d4-4f6b-9327-73830378823b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "704aa060-2129-4118-80c7-6465ef9ea9a1", "node_type": "1", "metadata": {}, "hash": "848fa43e06f0a8fb5956066e0443475e2bdfb55d9964777e3d7299f251189498", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65744212-880c-4a95-b85d-b31f439da4ba", "node_type": "1", "metadata": {}, "hash": "0a29173fa7a2acd71cafb0d55175abfcfceaf16f9b2aec45008d8ff542070f27", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The compiler framework of fpgaConvNet is based on the\nsynchronous dataflow (SDF) paradigm graph[44], which is\nable to obtain a predictable amount of required on-chip\nmemory and provide a static execution strategy. By analyzing\nthe interaction between the DAG of the DNN model and the\nplatform-specific resource constraints, the compiler transforms\nthe computing graph into an SDF hardware IR, in which each\nnode represents a hardware block. The compiler tiles the DNN\nmodel into segments and generates an optimized hardware\nblock for each subgraph separately.", "mimetype": "text/plain", "start_char_idx": 28860, "end_char_idx": 29419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65744212-880c-4a95-b85d-b31f439da4ba": {"__data__": {"id_": "65744212-880c-4a95-b85d-b31f439da4ba", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fb83834-55d4-4f6b-9327-73830378823b", "node_type": "1", "metadata": {}, "hash": "c3c9b4dae73b0f614d267687b3aaa4b470b220f05eb1f720485e76ae1a9c681e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e451cf6-7fde-48d0-849a-bf867df138a1", "node_type": "1", "metadata": {}, "hash": "1ba34525437586df222f565b9257e2af4819f22f3538f83e48d20cd47efd6990", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "DNNWeavers compiler\nadopts a dataflow graph generated from Caffe. When given\na specific NN model, DNNWeaver leverages an automatic\nresource optimization algorithm to maximize the performance\nby varying 1) the number of PEs per PU and 2) the output\nslice. Additionally, DNNWeaver provides a custom ISA to\ndecouple accelerators with different FPGA platforms.\nDLA presents a compiler and FPGA overlay for deep neural\nnetwork acceleration.", "mimetype": "text/plain", "start_char_idx": 29420, "end_char_idx": 29855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e451cf6-7fde-48d0-849a-bf867df138a1": {"__data__": {"id_": "7e451cf6-7fde-48d0-849a-bf867df138a1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65744212-880c-4a95-b85d-b31f439da4ba", "node_type": "1", "metadata": {}, "hash": "0a29173fa7a2acd71cafb0d55175abfcfceaf16f9b2aec45008d8ff542070f27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18e70dd0-d18b-406f-b432-7b645699e39a", "node_type": "1", "metadata": {}, "hash": "eb9b7db8fa466a0c77f7ad2def9992c9bcc46e35845417ab88aaca3e83a1c1af", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "On the hardware side, DLA proposes a\nvery long instruction word (VLIW) that introduces negligible\noverhead. By continuously fetching the VLIW from external\nmemory, the VLIW is decomposed into segments and sent to\nmodular hardware kernels connected by Xbar. DLAs hardware\nsupports vectorized and parallel implementations. By changing\nthe parallelism in the width Q_VEC, height P_VEC, input\nchannel C_VEC, and output channel K_VEC\ndimensions,\nthe overlay achieves the optimal implementation efficiency.", "mimetype": "text/plain", "start_char_idx": 29856, "end_char_idx": 30356, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18e70dd0-d18b-406f-b432-7b645699e39a": {"__data__": {"id_": "18e70dd0-d18b-406f-b432-7b645699e39a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e451cf6-7fde-48d0-849a-bf867df138a1", "node_type": "1", "metadata": {}, "hash": "1ba34525437586df222f565b9257e2af4819f22f3538f83e48d20cd47efd6990", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d331d13-7298-4e53-b43b-8e246e25c6c8", "node_type": "1", "metadata": {}, "hash": "87590916153628fccbee1cee8177f7770aae0df05e5756ed72ddf5db2d3ede35", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "On the software side, the DLA compiler slices the feature\nAuthorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:51:44 UTC from IEEE Xplore.  Restrictions apply. \n6\nHalide\nTF(XLA)\nTVM\nTC\nDNNVM\nxfDNN\nfpgaConvNet*\nBenchmark\nCPU\nCPU\nGPU\nCPU\nGPU\nGPU\nFPGA\nFPGA\nFPGA\nVGG\n751.6\n205\n3\n235\n9\n-\n20.1\n24.", "mimetype": "text/plain", "start_char_idx": 30357, "end_char_idx": 30684, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d331d13-7298-4e53-b43b-8e246e25c6c8": {"__data__": {"id_": "0d331d13-7298-4e53-b43b-8e246e25c6c8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18e70dd0-d18b-406f-b432-7b645699e39a", "node_type": "1", "metadata": {}, "hash": "eb9b7db8fa466a0c77f7ad2def9992c9bcc46e35845417ab88aaca3e83a1c1af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0dc8c9af-7ef0-4190-a630-e23b6a30b4d6", "node_type": "1", "metadata": {}, "hash": "330fec37328cb36eae54f3d007674dc02da888b0b1b6ebd8dad6667b7742f19c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "33\n249.4\nResNet50\n600.8\n102\n5\n130\n11\n18.6\n13.5\n12.42\n-\nResNet152\n1683\n287\n13.6\n343\n30\n48.3\n36.4\n34.87\n153.84\nInception v3\n826.1\n137\n10.5\n156\n19\n-\n13.6\n24.62\n-\nMobileNet v1\n197.3\n15\n6.5\n50\n2.9\n3.17\n3.4\n1.", "mimetype": "text/plain", "start_char_idx": 30684, "end_char_idx": 30887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0dc8c9af-7ef0-4190-a630-e23b6a30b4d6": {"__data__": {"id_": "0dc8c9af-7ef0-4190-a630-e23b6a30b4d6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d331d13-7298-4e53-b43b-8e246e25c6c8", "node_type": "1", "metadata": {}, "hash": "87590916153628fccbee1cee8177f7770aae0df05e5756ed72ddf5db2d3ede35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33d33faf-7308-4dfb-b22d-f2009ca99fcf", "node_type": "1", "metadata": {}, "hash": "6f254faed87f9d8b9e33b59dd3486a956b79266197932492dcc11fda3fc6c8b8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5\n-\nSqueezeNet\n139.1\n10.6\n7.6\n20.8\n4\n4.4\n2.5\n-\n-\nFig.", "mimetype": "text/plain", "start_char_idx": 30887, "end_char_idx": 30940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33d33faf-7308-4dfb-b22d-f2009ca99fcf": {"__data__": {"id_": "33d33faf-7308-4dfb-b22d-f2009ca99fcf", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0dc8c9af-7ef0-4190-a630-e23b6a30b4d6", "node_type": "1", "metadata": {}, "hash": "330fec37328cb36eae54f3d007674dc02da888b0b1b6ebd8dad6667b7742f19c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12749957-743b-40a6-83be-6e31eac48e2c", "node_type": "1", "metadata": {}, "hash": "20d66e310a50a688873ac5e932535c5e2698d45bb750d7d57492b1f579f02377", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "3: Performance (ms) comparison of the optimized implementations generated\nby different compilers (autoscheduler of Halide for CPU, TensorFlow (TF) with\nXLA and cuDNN, TVM, autotuner of Tensor Comprehension for GPU, DNNVM\nfor Deephi DPU on ZCU102@330 MHz, xfDNN of Xilinx on VU9P@450 MHz,\nand fpgaConvNet on ZC706@125 MHz) on our benchmarks (all batch = 1). *\ndenotes that the result is achieved in the literature; others are tested by us onboard.", "mimetype": "text/plain", "start_char_idx": 30941, "end_char_idx": 31387, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12749957-743b-40a6-83be-6e31eac48e2c": {"__data__": {"id_": "12749957-743b-40a6-83be-6e31eac48e2c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33d33faf-7308-4dfb-b22d-f2009ca99fcf", "node_type": "1", "metadata": {}, "hash": "6f254faed87f9d8b9e33b59dd3486a956b79266197932492dcc11fda3fc6c8b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60f10906-a74a-4861-b1fe-54a075b91b58", "node_type": "1", "metadata": {}, "hash": "267ec098915de81afd59fd11cd6044caaadc2ff3aaa0f98eb628396e86e73d3d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Fig. 4: Comparison of compilers\naccording to our practical experience.\nThe score increases (1 to 5) from the\ninside out. A higher score is better.\nmaps and weights to fit the stream buffer and filter caches\nto enhance data locality. Then, the compiler fuses adjacent\noperations to reduce the communication between the on-chip\nbuffer and external memory. The sequence of implementations\nwith many branches may affect the data layout and on-chip\nmemory consumption. In DLA, the optimized schedule in\nDLA uses a priority queue to reduce on-chip buffer usage.", "mimetype": "text/plain", "start_char_idx": 31388, "end_char_idx": 31943, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60f10906-a74a-4861-b1fe-54a075b91b58": {"__data__": {"id_": "60f10906-a74a-4861-b1fe-54a075b91b58", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12749957-743b-40a6-83be-6e31eac48e2c", "node_type": "1", "metadata": {}, "hash": "20d66e310a50a688873ac5e932535c5e2698d45bb750d7d57492b1f579f02377", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a2d98f3-08ca-4c89-a5ca-c21641b8da20", "node_type": "1", "metadata": {}, "hash": "9b4496e12d57bcc13613edb76dae2992ebb072ce25d3876514d9f1f395cebcfa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In particular, DLA introduces some neural-network-dependent\noptimizations for ResNet[45].\nxfDNN[17] and DNNVM[19] share similar framework de-\nsign ideas, and they both construct an NN-independent hard-\nware architecture. Their quantization tools transform the data\ninto an 8-bit fixed number from a 32-bit float. The compilation\ntools compile and optimize the networks for efficient inference\ndeployment.", "mimetype": "text/plain", "start_char_idx": 31944, "end_char_idx": 32348, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a2d98f3-08ca-4c89-a5ca-c21641b8da20": {"__data__": {"id_": "6a2d98f3-08ca-4c89-a5ca-c21641b8da20", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60f10906-a74a-4861-b1fe-54a075b91b58", "node_type": "1", "metadata": {}, "hash": "267ec098915de81afd59fd11cd6044caaadc2ff3aaa0f98eb628396e86e73d3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18b9c7c3-c535-423b-a086-2de36ae1cc7f", "node_type": "1", "metadata": {}, "hash": "5972ea7d3810adebee292b293e219df9a5e7923206031aa1ecca2d8d0350d011", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Due to relatively sufficient on-chip resources on\nXilinx VU9P, xfDNN fuses many layers in the DNN model,\nand the entire network, schedule and weights need to be loaded\nonly once onto the FPGA. This one-shot inference elimi-\nnates the CPU calls. DNNVM transforms the optimization\nproblems of computing the graph generation, pipeline and\ndata layout into graph-level problems.", "mimetype": "text/plain", "start_char_idx": 32349, "end_char_idx": 32723, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18b9c7c3-c535-423b-a086-2de36ae1cc7f": {"__data__": {"id_": "18b9c7c3-c535-423b-a086-2de36ae1cc7f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a2d98f3-08ca-4c89-a5ca-c21641b8da20", "node_type": "1", "metadata": {}, "hash": "9b4496e12d57bcc13613edb76dae2992ebb072ce25d3876514d9f1f395cebcfa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b566e6e7-c750-4be2-809a-debc38438160", "node_type": "1", "metadata": {}, "hash": "26ab97a23fd95ee64e8125d8b63f9c88757a82dd7f8d4271cfe59b23c71390d1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "DNNVM transforms\nNN models from different deep learning frameworks into a\nuniform DAG named XGraph, then leverages heuristic sub-\ngraph isomorphism and shortest-path algorithms to traverse\nfusion opportunities and find the optimal execution strategies.\nDespite the frequent data communication between the on-chip\nBRAMs and off-chip DRAM, concurrent computation and\ndata communication greatly alleviate the bandwidth saturation\nand improve the overall throughput of the system.\nIV. EVALUATION\nAs far as we know, no one has presented a fair comparison\namong the aforementioned compiler frameworks for deep\nneural networks.", "mimetype": "text/plain", "start_char_idx": 32724, "end_char_idx": 33344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b566e6e7-c750-4be2-809a-debc38438160": {"__data__": {"id_": "b566e6e7-c750-4be2-809a-debc38438160", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18b9c7c3-c535-423b-a086-2de36ae1cc7f", "node_type": "1", "metadata": {}, "hash": "5972ea7d3810adebee292b293e219df9a5e7923206031aa1ecca2d8d0350d011", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0e9f081-2ead-40c5-9c38-974442a13794", "node_type": "1", "metadata": {}, "hash": "f68beeb85e087f4920cdfffa05e23e3c3941d937ea06726c97908210eea6b049", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In this section, we analyze the performance of\nthe optimized implementations generated by these compilers.\nAccording to our evaluation and practical experience, we\nhope to help developers choose from hardware platforms and\ncompilation technologies for deep neural networks.\nA. Experiment Environment\nAll our experiments are conducted on an Intel(R) Xeon(R)\nCPU E5-2687 W v4 (CPU) and a GeForce GTX TITAN X\n(GPU). Xilinx xfDNN is evaluated on VU9P (FPGA), and\nthis service is provided by AWS Cloud.", "mimetype": "text/plain", "start_char_idx": 33345, "end_char_idx": 33842, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0e9f081-2ead-40c5-9c38-974442a13794": {"__data__": {"id_": "b0e9f081-2ead-40c5-9c38-974442a13794", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b566e6e7-c750-4be2-809a-debc38438160", "node_type": "1", "metadata": {}, "hash": "26ab97a23fd95ee64e8125d8b63f9c88757a82dd7f8d4271cfe59b23c71390d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49cab8c8-6676-4019-bf25-f352c2ac6415", "node_type": "1", "metadata": {}, "hash": "8c97fdb1579c22633eb855022558f8d133f804fbe45def98bcd33151016056ad", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "DNNVM is tested\non ZCU102 (FPGA), which is provided by Deephi, now\npart of Xilinx. The results obtained from the literature are\nhighlighted. We reimplement multiple operations of neural\nnetworks in the DSL of Halide, TC, TVM, etc. We set several\nmainstream DNNs[45]\u2013[49] as the benchmarks, which include\nmany typical operators.\nB. Comparison of Different Compiler Frameworks\nAs shown in Figure 3 and Figure 4, we compare these\ncompiler frameworks from different angles.", "mimetype": "text/plain", "start_char_idx": 33843, "end_char_idx": 34312, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49cab8c8-6676-4019-bf25-f352c2ac6415": {"__data__": {"id_": "49cab8c8-6676-4019-bf25-f352c2ac6415", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0e9f081-2ead-40c5-9c38-974442a13794", "node_type": "1", "metadata": {}, "hash": "f68beeb85e087f4920cdfffa05e23e3c3941d937ea06726c97908210eea6b049", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5a34c06-45e5-4e83-ba35-eb06b7a29214", "node_type": "1", "metadata": {}, "hash": "b7e9bcae248012c8d94d8630b2a7d8b7328a399a8d0d291d0b584e38b98bf4ee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1) Performance, Gops/W: Performance means the through-\nput achieved on our benchmarks. Halide provides a very\nconvenient DSL and an autoscheduler for CPU implementation\noptimization, while the GPU implementations can only be\noptimized manually. We find that Halide is 3-13x slower than\nTF and TVM on the CPU because Halide was originally de-\nsigned for image processing, and the autoscheduler considers\nonly a combination of parallelism, grouping and tiling.", "mimetype": "text/plain", "start_char_idx": 34313, "end_char_idx": 34771, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5a34c06-45e5-4e83-ba35-eb06b7a29214": {"__data__": {"id_": "f5a34c06-45e5-4e83-ba35-eb06b7a29214", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49cab8c8-6676-4019-bf25-f352c2ac6415", "node_type": "1", "metadata": {}, "hash": "8c97fdb1579c22633eb855022558f8d133f804fbe45def98bcd33151016056ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad760e92-7e57-427b-9caf-23d430443131", "node_type": "1", "metadata": {}, "hash": "5b2f4c4d215ae2cb7e2b553aad7e639fade495940b0cd6c3b15f27602797d2f7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In\naddition, Halide\u2019s optimization algorithms are greedy, so some\npotentially valid opportunities may be missed, and the tile\nsizes are limited to several given numbers for narrowing the\noptimization space.\nObviously, CPU implementations generated by Halide, TF\nand TVM require 10-200x the time of GPU implementations\nbecause of the lack of parallelism. Due to multiple customized\ncodes in TF, optimization across operators by XLA and\nassembly-level optimization in cuDNN, TensorFlow with XLA\nand cuDNN show the optimal performance compared with that\nof other implementations on the GPU.", "mimetype": "text/plain", "start_char_idx": 34772, "end_char_idx": 35359, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad760e92-7e57-427b-9caf-23d430443131": {"__data__": {"id_": "ad760e92-7e57-427b-9caf-23d430443131", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5a34c06-45e5-4e83-ba35-eb06b7a29214", "node_type": "1", "metadata": {}, "hash": "b7e9bcae248012c8d94d8630b2a7d8b7328a399a8d0d291d0b584e38b98bf4ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80c9f6b5-456f-43d2-9cde-399ea2abd084", "node_type": "1", "metadata": {}, "hash": "f0ea2b3cbbc187b79dae4fadbfc434f3de87f8846ec03f738ebae29a2a92225a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "TVM integrates all of\nthe optimization experience, provides favorable performance\nand supports most deep learning frameworks and hardware\nplatforms at the same time. Compared with manual implemen-\ntations in TF and the optimization experience needed by TVM,\nTC requires almost no manual experience or intervention for\nscheduling optimization. However, the scheduling strategies\nachieved through TC are dependent on the start point choice\nof the autotuner and the number of seeds and iterations in its\ngenetic algorithm.", "mimetype": "text/plain", "start_char_idx": 35360, "end_char_idx": 35879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80c9f6b5-456f-43d2-9cde-399ea2abd084": {"__data__": {"id_": "80c9f6b5-456f-43d2-9cde-399ea2abd084", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad760e92-7e57-427b-9caf-23d430443131", "node_type": "1", "metadata": {}, "hash": "5b2f4c4d215ae2cb7e2b553aad7e639fade495940b0cd6c3b15f27602797d2f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be070e71-e7e9-41dd-ab09-e70c95c061f3", "node_type": "1", "metadata": {}, "hash": "0ecd3f16874849ac5d8558ba1314ac5293de1577171d5cfae7debe013f141901", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In Figure 3, each operator in TC runs 1000\ntimes with the best scheduling strategies that we trained, and\nthe value we select represents the minimal time consumption.\nAuthorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:51:44 UTC from IEEE Xplore.  Restrictions apply. \n7\nTime(ms)\nTC\nDNNVM\n1\n2\n3\n0\n0.2\n0.4\n0.6\nconv\npool\nconv\nconv\nc", "mimetype": "text/plain", "start_char_idx": 35880, "end_char_idx": 36247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be070e71-e7e9-41dd-ab09-e70c95c061f3": {"__data__": {"id_": "be070e71-e7e9-41dd-ab09-e70c95c061f3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80c9f6b5-456f-43d2-9cde-399ea2abd084", "node_type": "1", "metadata": {}, "hash": "f0ea2b3cbbc187b79dae4fadbfc434f3de87f8846ec03f738ebae29a2a92225a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aed3a0b0-a415-4baa-a2d0-caa4f979f836", "node_type": "1", "metadata": {}, "hash": "482c0b8c1d35c0692076a4289ee70ae06d31635d5f0e964f76bb393062bbdb48", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "onv\nconv\neltwise\nconv\nconv\nconv\neltwise\nconv\nconv\nconv\neltwise\nconv\nconv\nconv\nconv\neltwise\nconv\nconv\nconv\neltwise\nconv\nconv\nconv", "mimetype": "text/plain", "start_char_idx": 36247, "end_char_idx": 36375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aed3a0b0-a415-4baa-a2d0-caa4f979f836": {"__data__": {"id_": "aed3a0b0-a415-4baa-a2d0-caa4f979f836", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be070e71-e7e9-41dd-ab09-e70c95c061f3", "node_type": "1", "metadata": {}, "hash": "0ecd3f16874849ac5d8558ba1314ac5293de1577171d5cfae7debe013f141901", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7a827f5-de3e-49bd-8df0-0d3ea48e7ec2", "node_type": "1", "metadata": {}, "hash": "d0519d0842c4a3683768d892d89e1b597431271509ebd4e33d8274d779890aae", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "eltwise\nconv\nconv\nconv\neltwise\nconv\nconv\nconv\nconv\neltwise\nconv\nconv\nconv\neltwise\nconv\nconv\nconv\neltwise\nconv\nconv\nconv\neltwise", "mimetype": "text/plain", "start_char_idx": 36279, "end_char_idx": 36406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7a827f5-de3e-49bd-8df0-0d3ea48e7ec2": {"__data__": {"id_": "b7a827f5-de3e-49bd-8df0-0d3ea48e7ec2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aed3a0b0-a415-4baa-a2d0-caa4f979f836", "node_type": "1", "metadata": {}, "hash": "482c0b8c1d35c0692076a4289ee70ae06d31635d5f0e964f76bb393062bbdb48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fb5cd21-691f-4418-bc6f-9bc31ccd8f36", "node_type": "1", "metadata": {}, "hash": "051bdaf7b79e1f4cd498ef3a2ef978d479cd4037d586d738fe7b8fe78b2a5a28", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "conv\nconv\nconv\neltwise\nconv\nconv\nconv\neltwise\nconv\nconv\nconv\nconv\neltwise\nconv\nconv\nconv\neltwise\nconv\nconv\nconv\neltwise\npool\nFi", "mimetype": "text/plain", "start_char_idx": 36504, "end_char_idx": 36631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fb5cd21-691f-4418-bc6f-9bc31ccd8f36": {"__data__": {"id_": "1fb5cd21-691f-4418-bc6f-9bc31ccd8f36", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7a827f5-de3e-49bd-8df0-0d3ea48e7ec2", "node_type": "1", "metadata": {}, "hash": "d0519d0842c4a3683768d892d89e1b597431271509ebd4e33d8274d779890aae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab5145ef-8416-4b97-ab7a-9ceebd3ceeb9", "node_type": "1", "metadata": {}, "hash": "1b1b575c53ae8b55d577453447efd0167bd44f7ac158426433e6603840e27aeb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "g. 5: Evaluation of optimized implementations generated by the TC autotuner for the GPU and DNNVM for the FPGA\n(ZCU102@ 330MHz) on ResNet50. We select the best autotuning results, and each operation is run 1000 times on the GPU\nfor TC. The value we choose is the minimal time consumption of each operator.", "mimetype": "text/plain", "start_char_idx": 36631, "end_char_idx": 36936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab5145ef-8416-4b97-ab7a-9ceebd3ceeb9": {"__data__": {"id_": "ab5145ef-8416-4b97-ab7a-9ceebd3ceeb9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fb5cd21-691f-4418-bc6f-9bc31ccd8f36", "node_type": "1", "metadata": {}, "hash": "051bdaf7b79e1f4cd498ef3a2ef978d479cd4037d586d738fe7b8fe78b2a5a28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a107c5f5-57c0-4377-8fcf-9a7d171222fa", "node_type": "1", "metadata": {}, "hash": "a8cd58d9d0310b67d7a3efaaf1673612f00200faa98817a3dddcff1f49ec8cd1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "TC\nDNNVM\nTime(ms)\n1\n2\n3\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nconv\ndepthwise\nconv\ndepthwise\nconv\ndepthwise\nconv\ndepthwise\nconv\ndepthwise\nconv\ndepthwise\nconv\ndepthwise\nconv\ndepthwise\nconv\ndepthwise\nconv\ndepthwise\nconv\ndepthwise\nconv\ndepthwise\nconv\ndepthwise\nconv\npool\nconv\nFig.", "mimetype": "text/plain", "start_char_idx": 36937, "end_char_idx": 37197, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a107c5f5-57c0-4377-8fcf-9a7d171222fa": {"__data__": {"id_": "a107c5f5-57c0-4377-8fcf-9a7d171222fa", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab5145ef-8416-4b97-ab7a-9ceebd3ceeb9", "node_type": "1", "metadata": {}, "hash": "1b1b575c53ae8b55d577453447efd0167bd44f7ac158426433e6603840e27aeb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a4d7fe6-6afd-4b2a-bdfa-30b2cb79dd58", "node_type": "1", "metadata": {}, "hash": "b5157230bcdf1635c95dc3a92db0c54e26aa7bcac69e64ae0a98cc9221dac567", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "6: Evaluation of optimized implementations generated by the TC autotuner for the GPU and DNNVM for the FPGA\n(ZCU102@ 330MHz) on MobileNet. In DNNVM, each depthwise convolution and pooling step is embedded into the\nprevious convolution so that the single depthwise convolution is skipped.", "mimetype": "text/plain", "start_char_idx": 37198, "end_char_idx": 37485, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a4d7fe6-6afd-4b2a-bdfa-30b2cb79dd58": {"__data__": {"id_": "1a4d7fe6-6afd-4b2a-bdfa-30b2cb79dd58", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a107c5f5-57c0-4377-8fcf-9a7d171222fa", "node_type": "1", "metadata": {}, "hash": "a8cd58d9d0310b67d7a3efaaf1673612f00200faa98817a3dddcff1f49ec8cd1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46ff6c09-cfa3-45f7-9472-cf71c94da6bd", "node_type": "1", "metadata": {}, "hash": "15304f205c2a64b3916004dbe2fb182bff200e06ec298cfa157cee2069c1e10e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Instructions generated by DNNVM and xfDNN (D&x)\nexecuted on FPGA platforms show comparable performance\nbut 10x Gops/W relative to GPU implementations on the\nNvidia GeForce Titan X. The hardware architectures of D&x\nare relatively fixed so that the compiler plays a key role\nin optimizing instructions. The hardware architecture of fp-\ngaConvNet or DNNWeaver (f&D) is flexible and depends\non the structure of the NNs. We find that D&x achieves\n10x throughput over f&D.", "mimetype": "text/plain", "start_char_idx": 37486, "end_char_idx": 37953, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46ff6c09-cfa3-45f7-9472-cf71c94da6bd": {"__data__": {"id_": "46ff6c09-cfa3-45f7-9472-cf71c94da6bd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a4d7fe6-6afd-4b2a-bdfa-30b2cb79dd58", "node_type": "1", "metadata": {}, "hash": "b5157230bcdf1635c95dc3a92db0c54e26aa7bcac69e64ae0a98cc9221dac567", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90817541-872b-40c9-a477-10c2d47d5844", "node_type": "1", "metadata": {}, "hash": "cead3f5464d0b5553608517b051fa5ba1f6111f7664f2a0c49cfd17c8180c19e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This performance gap comes from\nthe following: first, the separate optimization for hardware\ndesign and compiler can be much easier than that of NN-\ndependent architectures. Second, the streaming architecture\nof f&D decomposes an NN into subgraphs and generates\na hardware block for each subgraph, which means that the\nhardware resources are distributed for each subgraph so that\neach computation engine is constructed by fewer resources.\nAdditionally, reconfiguration overhead may be introduced.\n2) Compilation Cost: The autotuner of TC searches for\noptimized implementations for each operator with specific\nparameters and data shapes.", "mimetype": "text/plain", "start_char_idx": 37954, "end_char_idx": 38590, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90817541-872b-40c9-a477-10c2d47d5844": {"__data__": {"id_": "90817541-872b-40c9-a477-10c2d47d5844", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46ff6c09-cfa3-45f7-9472-cf71c94da6bd", "node_type": "1", "metadata": {}, "hash": "15304f205c2a64b3916004dbe2fb182bff200e06ec298cfa157cee2069c1e10e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e87ce1b6-9745-4143-bd3e-8ecc6a9fee72", "node_type": "1", "metadata": {}, "hash": "daa3767e00b26814357a6169bfea8bb1d52daf562afdb659e849ad81ba4c78c6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Multiple iterations of the genetic\nalgorithm and profiling process of all seeds cost hours or\neven days to train an optimized implementation for an NN.\nCompilers of f&D need to realize performance gains and\ngenerate hardware architecture through long compilations even\nwith HLS tools. It takes several minutes for Halide to finish\nthe autoschedule process for an NN. D&x spends dozens of\nseconds to generate optimized instructions.\n3) Ease of Use: reflects the difficulty of running a specific\nNN on a processor.", "mimetype": "text/plain", "start_char_idx": 38591, "end_char_idx": 39103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e87ce1b6-9745-4143-bd3e-8ecc6a9fee72": {"__data__": {"id_": "e87ce1b6-9745-4143-bd3e-8ecc6a9fee72", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90817541-872b-40c9-a477-10c2d47d5844", "node_type": "1", "metadata": {}, "hash": "cead3f5464d0b5553608517b051fa5ba1f6111f7664f2a0c49cfd17c8180c19e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20bd3397-fcac-4188-86d8-8ff1e8881b72", "node_type": "1", "metadata": {}, "hash": "b8e3fab29fd5f39d034a7af60ddd3bbd098292fcbd5f61c6837ebbcff63b1ea1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "XLA, TVM and D&x are end-to-end\ncompilers used to map an NN into instructions. Users need\nto transform NN models into TC and Halide manually, which\nmay take some effort. The autoscheduler should also be\ninitialized by the user in Halide. f&D requires basic knowledge\nfor FPGA and HLS tools and can be more difficult to use.\n4) Expandability: Expandability here means the difficulty\nof adding a new operation or an algorithm.", "mimetype": "text/plain", "start_char_idx": 39104, "end_char_idx": 39528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20bd3397-fcac-4188-86d8-8ff1e8881b72": {"__data__": {"id_": "20bd3397-fcac-4188-86d8-8ff1e8881b72", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e87ce1b6-9745-4143-bd3e-8ecc6a9fee72", "node_type": "1", "metadata": {}, "hash": "daa3767e00b26814357a6169bfea8bb1d52daf562afdb659e849ad81ba4c78c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf2f0faa-19f8-4803-9053-60d8768afbd4", "node_type": "1", "metadata": {}, "hash": "228dbc7c820af81de2b2d0f4788cd099341cf6b9adfd3a38b84f00c3b02879df", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "It is extremely\nconvenient to describe an operation using DSL provided by\nHalide, TVM and TC and map the operation into machine\ncode. For f&D, a new algorithm necessitates a new combina-\ntion of hardware blocks and optimization strategies. It takes a\ngreat deal of time to design a new computation engine for a\nnew operator. D&x can reuse computation engines by different\ninstructions to provide slightly easier use than that of f&D.", "mimetype": "text/plain", "start_char_idx": 39529, "end_char_idx": 39962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf2f0faa-19f8-4803-9053-60d8768afbd4": {"__data__": {"id_": "cf2f0faa-19f8-4803-9053-60d8768afbd4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20bd3397-fcac-4188-86d8-8ff1e8881b72", "node_type": "1", "metadata": {}, "hash": "b8e3fab29fd5f39d034a7af60ddd3bbd098292fcbd5f61c6837ebbcff63b1ea1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62e536d3-ca89-41ae-8450-a5ebec2a54d0", "node_type": "1", "metadata": {}, "hash": "ef2d1ec350f19ce6218ec7fd17afcd4cb84868438779da1644c3e2ac1ee365b8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5) Autoschedule Optimization: We propose this property\nto describe the difficulty for users to optimize the schedule\nwhen given a newly designed operation or algorithm. TVM\napplies a machine learning algorithm to autotune the sched-\nule. TC adopts polyhedral representations and explores the\noptimization space by an automatic random search and genetic\nalgorithm. However, TVM and TC cannot achieve throughput\nequivalent to that of manually optimized implementations. The\nschedule of f&D is optimized on the hardware side. Designing\nand optimizing hardware is more difficult than optimizing\ninstructions in D&x design.", "mimetype": "text/plain", "start_char_idx": 39963, "end_char_idx": 40581, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62e536d3-ca89-41ae-8450-a5ebec2a54d0": {"__data__": {"id_": "62e536d3-ca89-41ae-8450-a5ebec2a54d0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf2f0faa-19f8-4803-9053-60d8768afbd4", "node_type": "1", "metadata": {}, "hash": "228dbc7c820af81de2b2d0f4788cd099341cf6b9adfd3a38b84f00c3b02879df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8503468-4dbf-4347-af65-57090e565847", "node_type": "1", "metadata": {}, "hash": "258e9362f96bdb1bba0322b77c32e0ff28125274fa208fda84eef16a31d78ab6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "C. GPU or FPGA?\nAs shown in Figure 5 and Figure 6, we used ResNet50 and\nMobileNet as a case study. We evaluated TC on the GPU\nand DNNVM on the FPGA. In Figure 5, DNNVM shows\na 1.38x system throughput relative to TC. The performance\nimprovement mainly comes from parts marked as 2 due to\nthe highly optimized logic of the specialized accelerators.", "mimetype": "text/plain", "start_char_idx": 40582, "end_char_idx": 40928, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8503468-4dbf-4347-af65-57090e565847": {"__data__": {"id_": "d8503468-4dbf-4347-af65-57090e565847", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62e536d3-ca89-41ae-8450-a5ebec2a54d0", "node_type": "1", "metadata": {}, "hash": "ef2d1ec350f19ce6218ec7fd17afcd4cb84868438779da1644c3e2ac1ee365b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e6de388-701b-47ed-b2db-98d488f6c476", "node_type": "1", "metadata": {}, "hash": "c0b68078435a3dd387f058eb408ee3265e213ffb76b8d4b3b1c3c5566542283a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Unfortunately, we find two typical conditions marked as part\n1 and part 3 in Figure 5, in which specialized accelerators\nachieve poor performance. To accelerate NNs on specialized\naccelerators, developers usually apply parallelism to several\ndimensions of data, such as the width, height, channel and\nkernel. In part 1, the channel of the input feature map of the\nfirst layer in the NN is 3, which cannot satisfy the parallelism\ndegree. As a result, insufficient utilization of parallelism\ncontributes to the inefficiency of the computation.", "mimetype": "text/plain", "start_char_idx": 40929, "end_char_idx": 41470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e6de388-701b-47ed-b2db-98d488f6c476": {"__data__": {"id_": "2e6de388-701b-47ed-b2db-98d488f6c476", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8503468-4dbf-4347-af65-57090e565847", "node_type": "1", "metadata": {}, "hash": "258e9362f96bdb1bba0322b77c32e0ff28125274fa208fda84eef16a31d78ab6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60ffd116-52a2-4465-bcab-6410b5c213a0", "node_type": "1", "metadata": {}, "hash": "94b5cbbacf84540d681382ae6a5fa388023e7505dbbb17d6c7adfc199f34fe0c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In part 3,\nthe operation of elementwise adding needs to load results\nfrom the previous 2 convolutions and leverages the dot-add\nfunction upon these data, which contributes to the bandwidth\nsaturation. Although DNNVM embeds elementwise adding\ninto the previous convolution to reduce data communication\nbetween the off-chip DRAM and on-chip buffers, the much\nAuthorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:51:44 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 41471, "end_char_idx": 41965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60ffd116-52a2-4465-bcab-6410b5c213a0": {"__data__": {"id_": "60ffd116-52a2-4465-bcab-6410b5c213a0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e6de388-701b-47ed-b2db-98d488f6c476", "node_type": "1", "metadata": {}, "hash": "c0b68078435a3dd387f058eb408ee3265e213ffb76b8d4b3b1c3c5566542283a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96e74dd4-4384-4020-ad96-0c16b28bbce0", "node_type": "1", "metadata": {}, "hash": "f080e3b638f5d1152553c518c007556dc9470da050a6dab0d0bfe0844790eb24", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "8\nsmaller bandwidth of the FPGA relative to that of the GPU\nis still the main reason for the performance gap. Similarly,\nin Figure 6, part 1 and part 3 reveal the same condition as\ndescribed above. In subgraphs such as that of part 2, the\noperation of depthwise convolution reduces the amount of\nweights and computation, which shows appealing performance\non GPUs. However, this operation contributes to less data\nreuse.", "mimetype": "text/plain", "start_char_idx": 41967, "end_char_idx": 42386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96e74dd4-4384-4020-ad96-0c16b28bbce0": {"__data__": {"id_": "96e74dd4-4384-4020-ad96-0c16b28bbce0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60ffd116-52a2-4465-bcab-6410b5c213a0", "node_type": "1", "metadata": {}, "hash": "94b5cbbacf84540d681382ae6a5fa388023e7505dbbb17d6c7adfc199f34fe0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b9e58dd-8372-4df9-ab45-d9bfe068896b", "node_type": "1", "metadata": {}, "hash": "9ae5898e3df53f1ba185ba7b67eefbaa67261df32b4df3d0144ebcb8543958fd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Although depthwise convolution is embedded into the\nprevious operator in DNNVM, frequent data transportation and\nan unbalanced data load limit the overall performance.\nV. CONCLUSION\nIn this article, we draw an in-depth comparison of com-\npiler frameworks for DNNs from different angles. These\ncompilers provide convenient DSLs and expressive IRs to\nrepresent NNs and pass information to the following opti-\nmization steps.", "mimetype": "text/plain", "start_char_idx": 42387, "end_char_idx": 42809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b9e58dd-8372-4df9-ab45-d9bfe068896b": {"__data__": {"id_": "6b9e58dd-8372-4df9-ab45-d9bfe068896b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96e74dd4-4384-4020-ad96-0c16b28bbce0", "node_type": "1", "metadata": {}, "hash": "f080e3b638f5d1152553c518c007556dc9470da050a6dab0d0bfe0844790eb24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a988e371-88e7-4e72-9501-fa11de5c7221", "node_type": "1", "metadata": {}, "hash": "f74ab056d57d97ca362d85406ae35a7af131873cb6ad5f9232b169ada94062a6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Although the auto-optimization in compilers\nsuch as Hailde, TVM and TC may require substantial time\nto schedule pipelines and cannot achieve the acceleration rate\nof manual optimization in TF(XLA), they still present a high\nbaseline performance when adding a new operation. It is still\nappealing to explore more efficient and effective algorithms\nfor auto-schedule. Additionally, FPGA-based accelerators with\nDNNVM show comparable system performance to NNs and\n10x Gops/s/W relative to GPUs.", "mimetype": "text/plain", "start_char_idx": 42810, "end_char_idx": 43301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a988e371-88e7-4e72-9501-fa11de5c7221": {"__data__": {"id_": "a988e371-88e7-4e72-9501-fa11de5c7221", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b9e58dd-8372-4df9-ab45-d9bfe068896b", "node_type": "1", "metadata": {}, "hash": "9ae5898e3df53f1ba185ba7b67eefbaa67261df32b4df3d0144ebcb8543958fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cbf4c53-ddf2-4426-bc9a-9f2a4299c63f", "node_type": "1", "metadata": {}, "hash": "3617f2129c56c04685e55ccadb820057e01321133ac28adad4b9b73c4418f304", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "FPGA-based accelerators are\nmore suitable for computation-intensive tasks, while GPUs are\nmore suitable for bandwidth-bound operations.\nVI. ACKNOWLEDGEMENT\nThe authors gratefully acknowledge the support from TOY-\nOTA, Xilinx and Beijing Innovation Center for Future Chips.\nThis work was supported by National Key R&D Program\nof China 2018YFB0105005, National Natural Science Foun-\ndation of China(No. 61622403, 61621091), the project of\nTsinghua University and Toyota Joint Research Center for AI\nTechnology of Automated Vehicle(TT2018-01).", "mimetype": "text/plain", "start_char_idx": 43302, "end_char_idx": 43842, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1cbf4c53-ddf2-4426-bc9a-9f2a4299c63f": {"__data__": {"id_": "1cbf4c53-ddf2-4426-bc9a-9f2a4299c63f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a988e371-88e7-4e72-9501-fa11de5c7221", "node_type": "1", "metadata": {}, "hash": "f74ab056d57d97ca362d85406ae35a7af131873cb6ad5f9232b169ada94062a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8adb01c-fc25-4324-a599-5d5e419f3a17", "node_type": "1", "metadata": {}, "hash": "705759a24f4fd6f2465e8f6dfec222f3929651446918fc511f1f8819612a1be8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "REFERENCES\n[1]\nTensorflow, https://www.tensorflow.org/.\n[2]\nCaffe, http://caffe.berkeleyvision.org.\n[3]\nPytorch, https://pytorch.org.\n[4]\nSharma and et al., \u201cFrom high-level deep neural models to\nFPGAs,\u201d in MICRO, 2016.\n[5]\nVenieris and et al., \u201cfpgaConvNet: A framework for mapping\nconvolutional neural networks on FPGAs,\u201d in FCCM, 2016.", "mimetype": "text/plain", "start_char_idx": 43843, "end_char_idx": 44181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8adb01c-fc25-4324-a599-5d5e419f3a17": {"__data__": {"id_": "c8adb01c-fc25-4324-a599-5d5e419f3a17", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cbf4c53-ddf2-4426-bc9a-9f2a4299c63f", "node_type": "1", "metadata": {}, "hash": "3617f2129c56c04685e55ccadb820057e01321133ac28adad4b9b73c4418f304", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcfa2fc7-6fa2-47ca-8483-dbee871c3c9b", "node_type": "1", "metadata": {}, "hash": "6df70456ed62e4bf16769d1b484c5c2212852a9e550da896f9405bf4530abd21", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[6]\nS. Liu and et al., \u201cCambricon: An instruction set architecture\nfor neural networks,\u201d in ISCA, 2016.\n[7]\nJouppi and et al., \u201cIn-datacenter performance analysis of a\ntensor processing unit,\u201d in ISCA, 2017, pp. 1\u201312.\n[8]\nAshouri and et al., \u201cA survey on compiler autotuning using\nmachine learning,\u201d CSUR, vol. 51, no. 5, pp. 1\u201342, 2018.", "mimetype": "text/plain", "start_char_idx": 44182, "end_char_idx": 44519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcfa2fc7-6fa2-47ca-8483-dbee871c3c9b": {"__data__": {"id_": "fcfa2fc7-6fa2-47ca-8483-dbee871c3c9b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8adb01c-fc25-4324-a599-5d5e419f3a17", "node_type": "1", "metadata": {}, "hash": "705759a24f4fd6f2465e8f6dfec222f3929651446918fc511f1f8819612a1be8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7243164-eb89-4d64-9bce-e55a6e4b99d9", "node_type": "1", "metadata": {}, "hash": "da6925132473f5bdd24c89521841a26bb3510842921b7603079c4e1529757490", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[9]\nEigen, http://eigen.tuxfamily.org/.\n[10]\nMKL, https://software.intel.com/en-us/mkl.\n[11]\nOpenblas, http://www.openblas.net.\n[12]\nRagan-Kelley and et al., \u201cHalide:a language and compiler for\noptimizing parallelism, locality, and recomputation in image\nprocessing pipelines,\u201d Sigplan Notices, 2013.", "mimetype": "text/plain", "start_char_idx": 44520, "end_char_idx": 44820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7243164-eb89-4d64-9bce-e55a6e4b99d9": {"__data__": {"id_": "a7243164-eb89-4d64-9bce-e55a6e4b99d9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcfa2fc7-6fa2-47ca-8483-dbee871c3c9b", "node_type": "1", "metadata": {}, "hash": "6df70456ed62e4bf16769d1b484c5c2212852a9e550da896f9405bf4530abd21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b68904b-d736-4f97-a746-8d7162ccb147", "node_type": "1", "metadata": {}, "hash": "db34bf168096fa83a1d12e5979c404198d93a591529a2a9c27ea7cdd2bbc3b2b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[13]\nT. Chen and et al., \u201cTVM: An automated end-to-end optimiz-\ning compiler for deep learning,\u201d in USENIX & OSDI, 2018.\n[14]\nR. Wei and et al., \u201cDLVM: A modern compiler infrastructure\nfor deep learning systems,\u201d arXiv:1711.03016, 2017.\n[15]\nVasilache and et al., \u201cTensor comprehensions: Framework\nagnostic high-performance machine learning abstractions,\u201d\n2018, https://arxiv.org/pdf/1802.04730.pdf.", "mimetype": "text/plain", "start_char_idx": 44821, "end_char_idx": 45220, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b68904b-d736-4f97-a746-8d7162ccb147": {"__data__": {"id_": "9b68904b-d736-4f97-a746-8d7162ccb147", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7243164-eb89-4d64-9bce-e55a6e4b99d9", "node_type": "1", "metadata": {}, "hash": "da6925132473f5bdd24c89521841a26bb3510842921b7603079c4e1529757490", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "922d55dd-3249-47b3-b670-22a4e15af335", "node_type": "1", "metadata": {}, "hash": "28492644cf635711d1436d8e09b40a2868727e62157720777d96f3666ae07eb4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[16]\nXLA, https://tensorflow.google.cn/xla/.\n[17]\nxfDNN, https://github.com/Xilinx/ml-suite.\n[18]\nM. S. Abdelfattah and et al., \u201cDla: Compiler and fpga overlay\nfor neural network inference acceleration,\u201d in FPL, 2018.\n[19]\nY. Xing and et al., \u201cDNNVM : End-to-end compiler leveraging\nheterogeneous optimizations on FPGA-based cnn accelera-\ntors,\u201d https://arxiv.org/pdf/1902.07463.pdf, 2019.", "mimetype": "text/plain", "start_char_idx": 45221, "end_char_idx": 45610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "922d55dd-3249-47b3-b670-22a4e15af335": {"__data__": {"id_": "922d55dd-3249-47b3-b670-22a4e15af335", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b68904b-d736-4f97-a746-8d7162ccb147", "node_type": "1", "metadata": {}, "hash": "db34bf168096fa83a1d12e5979c404198d93a591529a2a9c27ea7cdd2bbc3b2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19eb1d3d-2306-48a4-baa9-ee9d888b7345", "node_type": "1", "metadata": {}, "hash": "a43f8e5c98bc5137fa20d44223620f4c3ef70f7464a0276b5dad510496171c58", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[20]\nMXNet, http://mxnet.incubator.apache.org.\n[21]\nS. Girbal and et al., \u201cSemi-automatic composition of loop\ntransformations for deep parallelism and memory hierarchies,\u201d\nIJPP, vol. 34, no. 3, pp. 261\u2013317, 2006.\n[22]\nJ. Roesch and et al., \u201cRelay: A new ir for machine learning\nframeworks,\u201d MAPL, 2018.", "mimetype": "text/plain", "start_char_idx": 45611, "end_char_idx": 45913, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19eb1d3d-2306-48a4-baa9-ee9d888b7345": {"__data__": {"id_": "19eb1d3d-2306-48a4-baa9-ee9d888b7345", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "922d55dd-3249-47b3-b670-22a4e15af335", "node_type": "1", "metadata": {}, "hash": "28492644cf635711d1436d8e09b40a2868727e62157720777d96f3666ae07eb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "423abd74-22b7-458c-8f92-d5d3bf8d9580", "node_type": "1", "metadata": {}, "hash": "f5226e836ddfe33b563242053c5ada82449a1859cad2af55b2f2dca9fe926bd7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[23]\nL. N. Pouchet and et al., \u201cPolly - polyhedral optimization in\nLLVM,\u201d in IMPACT, 2011.\n[24]\nThe LLVM compiler infrastructure, https://llvm.org.\n[25]\nCuda, https://developer.nvidia.com/cuda-downloads.\n[26]\nMunshi and et al., \u201cOpencl,\u201d 2011.", "mimetype": "text/plain", "start_char_idx": 45914, "end_char_idx": 46157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "423abd74-22b7-458c-8f92-d5d3bf8d9580": {"__data__": {"id_": "423abd74-22b7-458c-8f92-d5d3bf8d9580", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19eb1d3d-2306-48a4-baa9-ee9d888b7345", "node_type": "1", "metadata": {}, "hash": "a43f8e5c98bc5137fa20d44223620f4c3ef70f7464a0276b5dad510496171c58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ceefa223-c93b-4b61-b17c-c54857a4ce1a", "node_type": "1", "metadata": {}, "hash": "ce0836e7bbdffbe9415914d6b8c277faf5ffcc84ba9424cd97ab2ce940f37877", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[27]\nCyphers and et al., \u201cIntel nGraph: An intermediate repre-\nsentation, compiler, and executor for deep learning,\u201d 2018,\nhttps://arxiv.org/pdf/1801.08058.pdf.\n[28]\nL. Tao and et al., \u201cDaDianNao: A neural network supercom-\nputer,\u201d TOC, vol. 66, no. 1, pp. 1\u20131, 2016.", "mimetype": "text/plain", "start_char_idx": 46158, "end_char_idx": 46425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ceefa223-c93b-4b61-b17c-c54857a4ce1a": {"__data__": {"id_": "ceefa223-c93b-4b61-b17c-c54857a4ce1a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "423abd74-22b7-458c-8f92-d5d3bf8d9580", "node_type": "1", "metadata": {}, "hash": "f5226e836ddfe33b563242053c5ada82449a1859cad2af55b2f2dca9fe926bd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e14016dc-448b-46e4-8e69-5255990910a4", "node_type": "1", "metadata": {}, "hash": "df731388e4a9ccb61bddb8d8d7a856e8f1bf9235dafce34f1cd08e731372170e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[29]\nMullapudi and et al., \u201cAutomatically scheduling halide image\nprocessing pipelines,\u201d Acm Transactions on Graphics, 2016.\n[30]\nChainer, https://chainer.org.\n[31]\nNVIDIA TensorRT, https://developer.nvidia.com/tensorrt.\n[32]\nL. Yizhi and et al., \u201cOptimizing CNN model inference on\nCPUs,\u201d 2018, https://arxiv.org/pdf/1809.02697.pdf.", "mimetype": "text/plain", "start_char_idx": 46426, "end_char_idx": 46758, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e14016dc-448b-46e4-8e69-5255990910a4": {"__data__": {"id_": "e14016dc-448b-46e4-8e69-5255990910a4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ceefa223-c93b-4b61-b17c-c54857a4ce1a", "node_type": "1", "metadata": {}, "hash": "ce0836e7bbdffbe9415914d6b8c277faf5ffcc84ba9424cd97ab2ce940f37877", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00638092-5fc8-4fe5-8b09-d53b7eae4283", "node_type": "1", "metadata": {}, "hash": "eae9a70ca2eff36ee5876357f033717bb5450cbf65f6eca490aaa56becc85694", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[33]\nT. Chen and C. Guestrin, \u201cXgboost: A scalable tree boosting\nsystem,\u201d in SIGKDD, 2016.\n[34]\nK. S. Tai and et al., \u201cImproved semantic representations\nfrom tree-structured long short-term memory networks,\u201d\narXiv:1503.00075, 2015.\n[35]\nT. Moreau and et al., \u201cVta: An open hardware-software stack\nfor deep learning,\u201d 2018, https://arxiv.org/pdf/1807.04188.pdf.", "mimetype": "text/plain", "start_char_idx": 46759, "end_char_idx": 47119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00638092-5fc8-4fe5-8b09-d53b7eae4283": {"__data__": {"id_": "00638092-5fc8-4fe5-8b09-d53b7eae4283", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e14016dc-448b-46e4-8e69-5255990910a4", "node_type": "1", "metadata": {}, "hash": "df731388e4a9ccb61bddb8d8d7a856e8f1bf9235dafce34f1cd08e731372170e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0953a4c1-2cdb-4f6f-b52e-18ac5e0ec5ff", "node_type": "1", "metadata": {}, "hash": "3a9b3c5aa511a99d5b30106b47637d424b833678de738010fdab10104dab2430", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[36]\nL. Truong and et al., \u201cLatte: A language, compiler, and\nruntime for elegant and efficient deep neural networks,\u201d 2016.\n[37]\nM. L. An and et al., \u201cOoLaLa: An object oriented analysis\nand design of numerical linear algebra,\u201d Acm Sigplan Notices,\nvol. 35, no. 10, pp. 229\u2013252, 2000.\n[38]\nF. Kjolstad and et al., \u201cTaco: A tool to generate tensor algebra\nkernels,\u201d in ASE, 2017.", "mimetype": "text/plain", "start_char_idx": 47120, "end_char_idx": 47498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0953a4c1-2cdb-4f6f-b52e-18ac5e0ec5ff": {"__data__": {"id_": "0953a4c1-2cdb-4f6f-b52e-18ac5e0ec5ff", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00638092-5fc8-4fe5-8b09-d53b7eae4283", "node_type": "1", "metadata": {}, "hash": "eae9a70ca2eff36ee5876357f033717bb5450cbf65f6eca490aaa56becc85694", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b479b7d-27d8-4e26-8939-6fc026af37c9", "node_type": "1", "metadata": {}, "hash": "557553474d764a17cec6933624c8105bab87e4f209371ffd8b4755eb454f40a0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[39]\nA. P. Harrison and D. Joseph, \u201cNumeric tensor framework:\nExploiting and extending Einstein notation.,\u201d Journal of Com-\nputational Science, vol. 16, pp. 128\u2013139, 2016.\n[40]\nA. Jangda and U. Bondhugula, \u201cAn effective fusion and tile\nsize model for optimizing image processing pipelines,\u201d in\nPPoPP, 2018.\n[41]\nBaghdadi and et al., \u201cPencil:a platform-neutral compute inter-\nmediate language for accelerator programming,\u201d in PACT\u201915.", "mimetype": "text/plain", "start_char_idx": 47499, "end_char_idx": 47932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b479b7d-27d8-4e26-8939-6fc026af37c9": {"__data__": {"id_": "9b479b7d-27d8-4e26-8939-6fc026af37c9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0953a4c1-2cdb-4f6f-b52e-18ac5e0ec5ff", "node_type": "1", "metadata": {}, "hash": "3a9b3c5aa511a99d5b30106b47637d424b833678de738010fdab10104dab2430", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "600b05d3-80a2-4779-9a67-7324ba7a5f43", "node_type": "1", "metadata": {}, "hash": "057bc89d99d546f33669911cc52a92f3a142d1c585b747540a96d751ce39815d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[42]\nS. Verdoolaege and G. Janssens, Scheduling for PPCG, Report\nCW 706, June 2017.\n[43]\nS. Verdoolaege and et al., \u201cPolyhedral parallel code generation\nfor CUDA,\u201d TACO, vol. 9, no. 4, pp. 1\u201323, 2013.\n[44]\nE. Lee and et al., \u201cSynchronous data flow,\u201d Proceedings of\nthe IEEE, vol. 75, no. 9, pp. 1235\u20131245, 1987.", "mimetype": "text/plain", "start_char_idx": 47933, "end_char_idx": 48244, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "600b05d3-80a2-4779-9a67-7324ba7a5f43": {"__data__": {"id_": "600b05d3-80a2-4779-9a67-7324ba7a5f43", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b479b7d-27d8-4e26-8939-6fc026af37c9", "node_type": "1", "metadata": {}, "hash": "557553474d764a17cec6933624c8105bab87e4f209371ffd8b4755eb454f40a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "463f2f3f-a4c9-42a7-aafb-79cb9b243eaa", "node_type": "1", "metadata": {}, "hash": "a3c84e7bf9e579db720e05c9d0a83e3dc432263122f62afe6c694c2ab7b0afcd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[45]\nK. He and et al., \u201cDeep residual learning for image recogni-\ntion,\u201d in CVPR, 2016.\n[46]\nK. Simonyan and et al., \u201cVery deep convolutional networks\nfor large-scale image recognition,\u201d Computer Science, 2014.\n[47]\nC. Szegedy and et al., \u201cRethinking the inception architecture\nfor computer vision,\u201d in CVPR, 2016.", "mimetype": "text/plain", "start_char_idx": 48245, "end_char_idx": 48559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "463f2f3f-a4c9-42a7-aafb-79cb9b243eaa": {"__data__": {"id_": "463f2f3f-a4c9-42a7-aafb-79cb9b243eaa", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "600b05d3-80a2-4779-9a67-7324ba7a5f43", "node_type": "1", "metadata": {}, "hash": "057bc89d99d546f33669911cc52a92f3a142d1c585b747540a96d751ce39815d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "698fff3f-eff1-4dc6-b379-316790cbe937", "node_type": "1", "metadata": {}, "hash": "17ef0d04b1ff8c6ef8c2c3fac5069eb6d1f3d0ef6f44aae0e9ed6b6820146fa8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[48]\nA. G. Howard and et al., \u201cMobilenets: Efficient convo-\nlutional neural networks for mobile vision applications,\u201d\narXiv:1704.04861, 2017.\n[49]\nF. N. Iandola and et al., \u201cSqueezenet: Alexnet-level accuracy\nwith 50x fewer parameters and < 0.5 MB model size,\u201d\narXiv:1602.07360, 2016.\nAuthorized licensed use limited to: Nanjing University.", "mimetype": "text/plain", "start_char_idx": 48560, "end_char_idx": 48900, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "698fff3f-eff1-4dc6-b379-316790cbe937": {"__data__": {"id_": "698fff3f-eff1-4dc6-b379-316790cbe937", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "cc6115fb13b957dfb769bb81adde2242383a98059254722713dda22e3c7f3d87", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "463f2f3f-a4c9-42a7-aafb-79cb9b243eaa", "node_type": "1", "metadata": {}, "hash": "a3c84e7bf9e579db720e05c9d0a83e3dc432263122f62afe6c694c2ab7b0afcd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/icess.2019.8782480", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Downloaded on June 02,2024 at 08:51:44 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 5408, "end_char_idx": 5489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10.1109/icess.2019.8782480": {"__data__": {"id_": "10.1109/icess.2019.8782480", "embedding": null, "metadata": {"node_type": "doi_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"4": {"node_id": "root_node", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "ffdeaf3b-c475-43e3-a2c0-0e0472671663", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3f463d6a-73b7-4257-a0c2-a03497302442", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4d9c9c93-9028-4a08-8da6-6d1ab15344cc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "25f5135a-4134-4374-9d01-24c9a8465078", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ea2469e4-fcb7-47c1-adf3-357653468b6b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "620ea7d9-283d-4ca6-b2e4-cca42871147c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b67bea5a-52e5-462f-a205-bbbf673641ac", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d23e8c7d-60bf-4541-b639-33eec97ead8c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ecb2a8c5-dc92-4170-bb37-e2a825ed9e31", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0b153da7-104b-4f01-833c-78e94149c4e0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d08751ae-097a-4b6a-90e2-c855556061ce", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b2c584ff-b29b-4966-97bf-dd610fb95927", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ae6f136c-33d0-43c8-8155-0e69e26e20d2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "48ac672e-3193-42e8-968c-f0703a53f97f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "baca1f34-761d-476b-8849-eea3e8d49820", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a64459c0-9aa7-4d65-b338-01a91a5f07a1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8853a2dc-0d1d-43f3-bcc6-ebbcc09d66a3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c05cf077-62c9-4238-bf7c-4f75ed74523f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "547c03f7-4cc7-47a8-9830-22151ce042c6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b032fa00-a193-41da-a194-7776be2d46fd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6152888b-1e9f-4ec1-bf9a-39bd5ecf6a19", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "68a75f01-dabb-4d9b-885d-143a437b0d1a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e125a4bd-9498-41af-b170-87ac268881a5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "50601c31-e74d-4160-b2e6-214aed85138b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e09bab72-b099-484b-8dae-b70e1b650332", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9ca02669-dabf-4246-9691-62ed76bd22d8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3470e867-a066-418e-8705-fbd1a4b89481", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8505cb06-bca1-4b0b-9832-fdaf92dfb907", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "711dacfd-483b-45af-8ade-1864aa723de5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a3d40b0c-3805-4d3b-8d23-d4dd4f2deccd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7796bbc4-a338-43bd-bd86-5369a20e7c54", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a05685f1-eb4c-4e53-a231-f63d381fa96f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "eb1b185a-6e04-4913-bbbb-318f66778f2e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9e6e0720-d784-4adf-a0ec-4585d5a44b5d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e18a3d2b-d41d-4df9-a484-058727ff9c07", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "25cc674e-c95e-4e1a-973b-e04f9abfa371", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ab10ee88-e59f-4c36-a5d7-904ed0a9d327", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "43914e07-f635-4db0-b54c-4a66dc2d1a6a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d282d820-b8fc-4b98-b9ea-35ffcd3a322c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0acfc550-8ace-443a-9130-de2731b81860", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ffbdc075-f38f-4c62-a564-8608463c752c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2b38ed1d-3ff4-4abb-a672-c426bb82ada8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d24b05d3-94b4-4b7a-b890-8ffba811e88f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9458e849-1aec-43ec-b3a4-7464f384192e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bb16379e-43d3-421a-ba36-953e693b4d50", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1669afa0-c886-43d1-8cfb-16eca718dbaf", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f757e0d3-1100-40f0-9301-b235b47072d9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8c75c395-bb32-4e4d-b4fa-defe48874fdf", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9011f739-c3ca-4321-b88d-00e36e4548ad", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fd2db383-b0c5-41f9-8aa8-2f303690613d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a0f91275-eefe-48be-95ac-2f2cc648f1e1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0f07ca8e-d4a5-4db8-a318-40c946baac6c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9279c0f9-a48e-4994-aaed-f2fd7ebb2952", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "44452aae-48f9-4dbe-9f02-a0c337763b66", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "28ded4bf-d587-48c8-90fc-c32fc5afda76", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7e208ca0-6c5b-4703-84af-8c4caf5cc87a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "85cdb7f4-646a-4e90-bd6b-c32044d60bbd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "908332bf-44e8-4dec-b0f9-36b2e1add3dc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "abbd5bc3-434c-4649-b569-73a6f38ffd3b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cd0a8396-0811-4e4c-b5c8-4d758f48924b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b318501a-bd7c-4e9d-9772-98754952167c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "34297c71-1f08-4ea1-9408-279b2a29b126", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "704aa060-2129-4118-80c7-6465ef9ea9a1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5fb83834-55d4-4f6b-9327-73830378823b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "65744212-880c-4a95-b85d-b31f439da4ba", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7e451cf6-7fde-48d0-849a-bf867df138a1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "18e70dd0-d18b-406f-b432-7b645699e39a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0d331d13-7298-4e53-b43b-8e246e25c6c8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0dc8c9af-7ef0-4190-a630-e23b6a30b4d6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "33d33faf-7308-4dfb-b22d-f2009ca99fcf", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "12749957-743b-40a6-83be-6e31eac48e2c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "60f10906-a74a-4861-b1fe-54a075b91b58", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6a2d98f3-08ca-4c89-a5ca-c21641b8da20", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "18b9c7c3-c535-423b-a086-2de36ae1cc7f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b566e6e7-c750-4be2-809a-debc38438160", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b0e9f081-2ead-40c5-9c38-974442a13794", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "49cab8c8-6676-4019-bf25-f352c2ac6415", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f5a34c06-45e5-4e83-ba35-eb06b7a29214", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ad760e92-7e57-427b-9caf-23d430443131", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "80c9f6b5-456f-43d2-9cde-399ea2abd084", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "be070e71-e7e9-41dd-ab09-e70c95c061f3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "aed3a0b0-a415-4baa-a2d0-caa4f979f836", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b7a827f5-de3e-49bd-8df0-0d3ea48e7ec2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1fb5cd21-691f-4418-bc6f-9bc31ccd8f36", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ab5145ef-8416-4b97-ab7a-9ceebd3ceeb9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a107c5f5-57c0-4377-8fcf-9a7d171222fa", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1a4d7fe6-6afd-4b2a-bdfa-30b2cb79dd58", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "46ff6c09-cfa3-45f7-9472-cf71c94da6bd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "90817541-872b-40c9-a477-10c2d47d5844", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e87ce1b6-9745-4143-bd3e-8ecc6a9fee72", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "20bd3397-fcac-4188-86d8-8ff1e8881b72", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cf2f0faa-19f8-4803-9053-60d8768afbd4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "62e536d3-ca89-41ae-8450-a5ebec2a54d0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d8503468-4dbf-4347-af65-57090e565847", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2e6de388-701b-47ed-b2db-98d488f6c476", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "60ffd116-52a2-4465-bcab-6410b5c213a0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "96e74dd4-4384-4020-ad96-0c16b28bbce0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6b9e58dd-8372-4df9-ab45-d9bfe068896b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a988e371-88e7-4e72-9501-fa11de5c7221", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1cbf4c53-ddf2-4426-bc9a-9f2a4299c63f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c8adb01c-fc25-4324-a599-5d5e419f3a17", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fcfa2fc7-6fa2-47ca-8483-dbee871c3c9b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a7243164-eb89-4d64-9bce-e55a6e4b99d9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9b68904b-d736-4f97-a746-8d7162ccb147", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "922d55dd-3249-47b3-b670-22a4e15af335", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "19eb1d3d-2306-48a4-baa9-ee9d888b7345", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "423abd74-22b7-458c-8f92-d5d3bf8d9580", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ceefa223-c93b-4b61-b17c-c54857a4ce1a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e14016dc-448b-46e4-8e69-5255990910a4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "00638092-5fc8-4fe5-8b09-d53b7eae4283", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0953a4c1-2cdb-4f6f-b52e-18ac5e0ec5ff", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9b479b7d-27d8-4e26-8939-6fc026af37c9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "600b05d3-80a2-4779-9a67-7324ba7a5f43", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "463f2f3f-a4c9-42a7-aafb-79cb9b243eaa", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "698fff3f-eff1-4dc6-b379-316790cbe937", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}]}, "text": "DOI: 10.1109/icess.2019.8782480", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a00ab085-4161-40c6-8c30-e3ad607b89fd": {"__data__": {"id_": "a00ab085-4161-40c6-8c30-e3ad607b89fd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3096672-db53-482c-a009-95e6b036cef1", "node_type": "1", "metadata": {}, "hash": "7cfcd2debae66a8bb8918d15456d8f879aa21f5fd45ab5c8512d983cb54128b0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1\nINTRODUCTION\nRecently, Computing-in-Memory (CIM) architectures [14, 25, 40] have emerged as a new\ncomputing paradigm to solve the memory-wall problem of traditional Von Neumann computer\nThis work is supported jointly by National Key Research and Development Program of China under Grant No.\n2022YFB4500303, and National Natural Science Foundation of China (NSFC) under Grants No. 62072198, No. 61825202,\nand No. 61929103.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3096672-db53-482c-a009-95e6b036cef1": {"__data__": {"id_": "a3096672-db53-482c-a009-95e6b036cef1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a00ab085-4161-40c6-8c30-e3ad607b89fd", "node_type": "1", "metadata": {}, "hash": "5818594c74890bc0e89941745ef74d31a2df4c4243986dd2f789f45ffa192a74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6e222c2-08e1-434d-884b-ac775dc6bd98", "node_type": "1", "metadata": {}, "hash": "20b697cee705daa744d8f4bb95be5def488783390bcd2c03c8ede7f0537b5ff4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Author\u2019s address: H. Jin, B. Lei, H. Liu (Corresponding author), X. Liao, Z. Duan, C. Ye, and Y. Zhang, National Engineering\nResearch Center for Big Data Technology and System, Service Computing Technology and System Lab, Cluster and Grid\nComputing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan,\n430074, China; e-mails: {hjin, leibo, hkliu, xfliao, zhduan, yecc, zhyu}@hust.edu.cn.", "mimetype": "text/plain", "start_char_idx": 424, "end_char_idx": 860, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6e222c2-08e1-434d-884b-ac775dc6bd98": {"__data__": {"id_": "b6e222c2-08e1-434d-884b-ac775dc6bd98", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3096672-db53-482c-a009-95e6b036cef1", "node_type": "1", "metadata": {}, "hash": "7cfcd2debae66a8bb8918d15456d8f879aa21f5fd45ab5c8512d983cb54128b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3a6e9ad-0024-4a81-ad9c-73838f7602ac", "node_type": "1", "metadata": {}, "hash": "72af5f0298c3d203ae6ba0a9ec47e2085e50b4dc222df0db5f79f6cd991d3636", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "$\nThis work is licensed under a Creative Commons Attribution-NonCommercial International 4.0 License.\n\u00a9 2023 Copyright held by the owner/author(s).\n1544-3566/2023/10-ART47\nhttps://doi.org/10.1145/3617686\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.\n47:2\nH. Jin et al.\narchitectures.", "mimetype": "text/plain", "start_char_idx": 861, "end_char_idx": 1214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3a6e9ad-0024-4a81-ad9c-73838f7602ac": {"__data__": {"id_": "e3a6e9ad-0024-4a81-ad9c-73838f7602ac", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6e222c2-08e1-434d-884b-ac775dc6bd98", "node_type": "1", "metadata": {}, "hash": "20b697cee705daa744d8f4bb95be5def488783390bcd2c03c8ede7f0537b5ff4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cb7b791-e7c9-4c19-91b8-544737658d2f", "node_type": "1", "metadata": {}, "hash": "7bbe54091048d382cbab2f55cb96cd2f8e003234c290da78399307ece98a6ca8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Most CIM accelerators use Non-volatile memories (NVM) technologies such\nas Resistive Random-access Memory (ReRAM) to architect crossbar arrays, and perform in\nsitu analog computing based on the Kirchhoff\u2019s circuit laws [33]. ReRAM crossbars offer high\nparallelism for bitwise computation, and avoid data movement between storage and processing\nunits [25, 40]. Particularly, ReRAM crossbar structures can perform Matrix-Vector Multipli-\ncation (MVM) operations in the analog domain with a constant time (O(1) complexity).", "mimetype": "text/plain", "start_char_idx": 1215, "end_char_idx": 1735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6cb7b791-e7c9-4c19-91b8-544737658d2f": {"__data__": {"id_": "6cb7b791-e7c9-4c19-91b8-544737658d2f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3a6e9ad-0024-4a81-ad9c-73838f7602ac", "node_type": "1", "metadata": {}, "hash": "72af5f0298c3d203ae6ba0a9ec47e2085e50b4dc222df0db5f79f6cd991d3636", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9525ac8-4de6-4526-a69a-ceaa039ec724", "node_type": "1", "metadata": {}, "hash": "9d0180fd1d5e5468b5e85a63d94eaf1767a6184652cfcc60fa0af7358d59e759", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "They\nhave demonstrated a significant potential to improve performance and energy efficiency for\ndomain-specific applications, such as neuromorphic computing [39], graph computing [37, 45],\ndatabase applications [29, 34, 35], and image processing [24].\nMany neural network and graph computing applications contain a large portion of computing-\nintensive MVM operations [39, 55]. To accelerate these applications with ReRAM crossbars, they\nhave to be re-engineered to offload MVM operations to CIM accelerators.", "mimetype": "text/plain", "start_char_idx": 1736, "end_char_idx": 2245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9525ac8-4de6-4526-a69a-ceaa039ec724": {"__data__": {"id_": "a9525ac8-4de6-4526-a69a-ceaa039ec724", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cb7b791-e7c9-4c19-91b8-544737658d2f", "node_type": "1", "metadata": {}, "hash": "7bbe54091048d382cbab2f55cb96cd2f8e003234c290da78399307ece98a6ca8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9c24d88-1cc9-4fe4-939c-050cedd3510b", "node_type": "1", "metadata": {}, "hash": "84822956542b18a1fd0d8842f1158932ee4e58ad55e99d18d8eb6e45f335c512", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "However, because the\nphysical feature of ReRAM is different from traditional CMOS circuits, and the CIM computing\nparadigm is also different from the traditional Von Neumann\u2019s, many legacy applications are in-\nfeasible for acceleration in CIM architectures. Thus, it is essential to redesign new programming\nmodels, APIs, and compilation tools to program ReRAM-based CIM accelerators.\nThere have been some preliminary studies on programming models for CIM architectures. Yu\net al. [54] provide a specific language and application programming interfaces (APIs) for\nReRAM-based accelerators.", "mimetype": "text/plain", "start_char_idx": 2246, "end_char_idx": 2835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9c24d88-1cc9-4fe4-939c-050cedd3510b": {"__data__": {"id_": "b9c24d88-1cc9-4fe4-939c-050cedd3510b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9525ac8-4de6-4526-a69a-ceaa039ec724", "node_type": "1", "metadata": {}, "hash": "9d0180fd1d5e5468b5e85a63d94eaf1767a6184652cfcc60fa0af7358d59e759", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c98efe5-1ded-4666-a266-a0634c77a03e", "node_type": "1", "metadata": {}, "hash": "a5c66adeb9e42046e5aaf0d0ae08110ac58de27e3b07814e1e9897abbc151445", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Similarly, Ankit et al. [10] design a new instruction set architecture\n(ISA) and a compiler for ReRAM-based accelerators. However, programmers need to modify the\nsource code of legacy software according to new library functions. Ambrosi et al. [8] develop\na software stack to run existing applications in ReRAM accelerators without modifying source\ncodes. However, it only supports neural network applications developed in a specific framework.\nChakraborty et al.", "mimetype": "text/plain", "start_char_idx": 2836, "end_char_idx": 3299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c98efe5-1ded-4666-a266-a0634c77a03e": {"__data__": {"id_": "2c98efe5-1ded-4666-a266-a0634c77a03e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9c24d88-1cc9-4fe4-939c-050cedd3510b", "node_type": "1", "metadata": {}, "hash": "84822956542b18a1fd0d8842f1158932ee4e58ad55e99d18d8eb6e45f335c512", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "284a3228-e38d-4e17-80db-d78802d8f847", "node_type": "1", "metadata": {}, "hash": "e0463475411d4b98a766f60c2858c1abf802e3fb9a68a3b2ebeab3932f94018e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[13] propose a programming framework to convert applications\u2019 source\ncodes into Boolean Decision Diagram (BDD), and then map a portion of computation to\nReRAM crossbars. However, it only supports very limited computing patterns. A state-of-the-art\nwork [47] exploits LLVM intermediate representations (IRs) to identify accelerable parts from\nthe Control Flow Graph (CFG), and provides a compiler [21] for the proposed CIM architecture.\nHowever, it only supports program with source codes, and can not migrate binary executables\nto CIM accelerators.", "mimetype": "text/plain", "start_char_idx": 3300, "end_char_idx": 3848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "284a3228-e38d-4e17-80db-d78802d8f847": {"__data__": {"id_": "284a3228-e38d-4e17-80db-d78802d8f847", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c98efe5-1ded-4666-a266-a0634c77a03e", "node_type": "1", "metadata": {}, "hash": "a5c66adeb9e42046e5aaf0d0ae08110ac58de27e3b07814e1e9897abbc151445", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2f67fa7-b73b-4c7b-b178-d04c434b2f1a", "node_type": "1", "metadata": {}, "hash": "8bf7043d305c0623ce9c57f09c73e45f16918bad39e3ae4811a379010c6ced64", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "These proposals all need to redesign, recompile application source codes to\nadapt to CPU/CIM heterogeneous architectures, impeding the application of CIM accelerators for\na wide range of legacy software.\nIn this article, we propose a new compilation tool to migrate legacy programs to CPU/CIM\nheterogeneous architectures automatically. Our compilation tool relies on the LLVM compiler\ninfrastructure to compile the application\u2019s source codes into LLVM IR. For binary executables\nwithout source codes, we exploit a decompiling tool [3] to generate the programs\u2019 IR.", "mimetype": "text/plain", "start_char_idx": 3849, "end_char_idx": 4413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2f67fa7-b73b-4c7b-b178-d04c434b2f1a": {"__data__": {"id_": "e2f67fa7-b73b-4c7b-b178-d04c434b2f1a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "284a3228-e38d-4e17-80db-d78802d8f847", "node_type": "1", "metadata": {}, "hash": "e0463475411d4b98a766f60c2858c1abf802e3fb9a68a3b2ebeab3932f94018e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4517e9c-4f7f-47a6-817a-4197c697ca82", "node_type": "1", "metadata": {}, "hash": "57fd7faf21dddb312526e4d0cbc73f0c612f20f879e55669751e0344f6b4d09e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We\ndefine and identify several typical computing patterns that can be accelerated by ReRAM-based\naccelerators from the perspective of LLVM IR. Our compilation tool can recognize the accelerable\ncomputing patterns such as MVM and Boolean logical operations, and automatically offload those\ncomputations to CIM accelerators using very simple APIs. We make the following contributions:\n\u2022 For domain-specific applications, we define and abstract several computing patterns that\ncan be accelerated by ReRAM-based crossbars from the perspective of LLVM IR, so that\nour compilation tool can recognize them and offload these computations to ReRAM-based\ncrossbars effectively.", "mimetype": "text/plain", "start_char_idx": 4414, "end_char_idx": 5081, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4517e9c-4f7f-47a6-817a-4197c697ca82": {"__data__": {"id_": "f4517e9c-4f7f-47a6-817a-4197c697ca82", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2f67fa7-b73b-4c7b-b178-d04c434b2f1a", "node_type": "1", "metadata": {}, "hash": "8bf7043d305c0623ce9c57f09c73e45f16918bad39e3ae4811a379010c6ced64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9e854b5-4acf-403a-b8ce-86ab39c90784", "node_type": "1", "metadata": {}, "hash": "bec21d331a9ef624560128dcab8575992758ef416db40c194ab6ddc78c962bd9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\u2022 We further propose a performance model to quantify the performance gain of offload-\ning these accelerable computing patterns to CIM accelerators, and thus can improve the\nefficiency of computation offloading.\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.\nA Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures\n47:3\nFig. 1. ReRAM crossbar array used for matrix-vector multiplication.\nFig. 2. Example of data querying using ReRAM-based Boolean logic operations.", "mimetype": "text/plain", "start_char_idx": 5082, "end_char_idx": 5637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9e854b5-4acf-403a-b8ce-86ab39c90784": {"__data__": {"id_": "f9e854b5-4acf-403a-b8ce-86ab39c90784", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4517e9c-4f7f-47a6-817a-4197c697ca82", "node_type": "1", "metadata": {}, "hash": "57fd7faf21dddb312526e4d0cbc73f0c612f20f879e55669751e0344f6b4d09e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d73297d6-32ce-4901-824c-8fce79c77571", "node_type": "1", "metadata": {}, "hash": "461ca01f416269ae5abf68e2c2024f10c321ad223c47c4c35a6c63fadfdc22c9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\u2022 We develop a compilation tool to migrate legacy programs to CPU/CIM heterogeneous archi-\ntectures automatically, without modifying application source codes. It can also transparently\nconvert binary executables without source codes by exploiting a decompilation tool.\n\u2022 With our compilation tool, experimental results show that the performance of domain-\nspecific programs can be significantly accelerated by up to 51\u00d7, and the energy consumption\ncan be reduced by up to 309\u00d7, compared with general-purpose x86 processors.\nThe remainder of this article is organized as follows. Section 2 introduces the background and\nmotivation of this work.", "mimetype": "text/plain", "start_char_idx": 5638, "end_char_idx": 6281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d73297d6-32ce-4901-824c-8fce79c77571": {"__data__": {"id_": "d73297d6-32ce-4901-824c-8fce79c77571", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9e854b5-4acf-403a-b8ce-86ab39c90784", "node_type": "1", "metadata": {}, "hash": "bec21d331a9ef624560128dcab8575992758ef416db40c194ab6ddc78c962bd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c89d7d84-6dda-4677-9adf-0cc318efdff9", "node_type": "1", "metadata": {}, "hash": "2c6ecb01fce8f5f8db85320850a24afbb982089120cc0f7a419613ef7019abe6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Section 3 shows the architecture of our system and the design of the com-\npilation tool. Section 4 introduces four kinds of ReRAM-accelerable LLVM IR patterns. Section 5\ndescribes how the compilation tool identifies the accelerable patterns and performs the code trans-\nlation. Section 6 shows how binary executables generated by our compilation tool are executed\nat runtime. Section 7 presents experimental results. We discuss the related work in Section 8 and\nconclude in Section 9.", "mimetype": "text/plain", "start_char_idx": 6282, "end_char_idx": 6766, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c89d7d84-6dda-4677-9adf-0cc318efdff9": {"__data__": {"id_": "c89d7d84-6dda-4677-9adf-0cc318efdff9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d73297d6-32ce-4901-824c-8fce79c77571", "node_type": "1", "metadata": {}, "hash": "461ca01f416269ae5abf68e2c2024f10c321ad223c47c4c35a6c63fadfdc22c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c0552e2-bfc9-4a44-9212-048333f21e01", "node_type": "1", "metadata": {}, "hash": "1ed0864b5624b8b32405cfe0df2f2ecf37a1e238f9815bbd9dbb54b1a1967420", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2\nBACKGROUND AND MOTIVATION\n2.1\nCIM Accelerators\nReRAM is a emerging NVM technology that stores data by adjusting its resistance. This feature\nalso enables highly parallel computation based on the Kirchhoff\u2019s circuit laws. Figure 1 shows that\na typical ReRAM crossbar array can complete matrix-vector multiplication in a constant time (O(1)\ncomplexity), while a typical CPU often takes a quadratic time (O(n2) complexity).", "mimetype": "text/plain", "start_char_idx": 6767, "end_char_idx": 7189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c0552e2-bfc9-4a44-9212-048333f21e01": {"__data__": {"id_": "9c0552e2-bfc9-4a44-9212-048333f21e01", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c89d7d84-6dda-4677-9adf-0cc318efdff9", "node_type": "1", "metadata": {}, "hash": "2c6ecb01fce8f5f8db85320850a24afbb982089120cc0f7a419613ef7019abe6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90643c3e-7f32-4875-8584-2071d08b76cc", "node_type": "1", "metadata": {}, "hash": "db81d2ae82271a84b2123cd43d4cb0dc357e9ca027c2e69ecbb0554987b965e1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Therefore, the\nReRAM crossbar-based CIM accelerators have been widely studied to accelerate domain-specific\napplications that contains a large number of MVM operations.\nMoreover, ReRAM can be also used in other architectures to accelerate bitwise Boolean logic\noperations, which are widely used in applications such as databases [29, 35, 46]. Figure 2 shows an\nexample of data querying via ReRAM-based Boolean logic operations.", "mimetype": "text/plain", "start_char_idx": 7190, "end_char_idx": 7617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90643c3e-7f32-4875-8584-2071d08b76cc": {"__data__": {"id_": "90643c3e-7f32-4875-8584-2071d08b76cc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c0552e2-bfc9-4a44-9212-048333f21e01", "node_type": "1", "metadata": {}, "hash": "1ed0864b5624b8b32405cfe0df2f2ecf37a1e238f9815bbd9dbb54b1a1967420", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5f4fba5-3d2f-4fcc-8de2-a1b3369330dc", "node_type": "1", "metadata": {}, "hash": "77ba70f8e513be42867ca0ce4b3eed24d70776be75f7b2f65aedb349a119f5c4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To retrieve all married man\nfrom the data table, only an AND operation is needed for two bitmaps generated from columns\n\u201cGender\u201d and \u201cMarital.\u201d\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.\n47:4\nH. Jin et al.\nFig. 3. Sketch of migrating software to CPU/CIM heterogeneous architectures using our compilation tool.\nBesides the above examples, ReRAM-based CIM architectures can be applied to many other\nscenarios for higher performance and energy efficiency.", "mimetype": "text/plain", "start_char_idx": 7618, "end_char_idx": 8143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5f4fba5-3d2f-4fcc-8de2-a1b3369330dc": {"__data__": {"id_": "b5f4fba5-3d2f-4fcc-8de2-a1b3369330dc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90643c3e-7f32-4875-8584-2071d08b76cc", "node_type": "1", "metadata": {}, "hash": "db81d2ae82271a84b2123cd43d4cb0dc357e9ca027c2e69ecbb0554987b965e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2106144d-642e-4960-b425-558691a5c921", "node_type": "1", "metadata": {}, "hash": "89af8024ab48a88f1c44512db2d76490062602d57cbb615874a4868f859002eb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Therefore, there have been many studies\non the adaption of specific applications to CIM architectures.\n2.2\nMotivation\nTo accelerate applications with CIM accelerators, most previous proposals usually provide spe-\ncific APIs for programmers, and have to modify the source codes for legacy applications. These\nproposals have several limitations to adopt CIM accelerators for a large amount of applications.\n\u2022 Poor Programmability. CIM accelerators usually offer an ISA and new APIs for application\nprogrammers [22]. Programmers have to spend a lot of time to learn new APIs for using CIM\naccelerators.", "mimetype": "text/plain", "start_char_idx": 8144, "end_char_idx": 8743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2106144d-642e-4960-b425-558691a5c921": {"__data__": {"id_": "2106144d-642e-4960-b425-558691a5c921", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5f4fba5-3d2f-4fcc-8de2-a1b3369330dc", "node_type": "1", "metadata": {}, "hash": "77ba70f8e513be42867ca0ce4b3eed24d70776be75f7b2f65aedb349a119f5c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5b89651-6775-409f-a4cd-6b7e9b7335c3", "node_type": "1", "metadata": {}, "hash": "ba124730f5571c48c81759ca6672fda06ed54bc4726556429f145b368b552843", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A rookie programmer may wrongly utilize CIM accelerators, and even causes\nperformance degradation for applications.\n\u2022 Poor Compatibility. Most previous CIM architectures and the corresponding optimizations\nare usually targeted at a given application or a special goal [27, 28, 44]. The poor compati-\nbility limits the adoption of those proposals for a wide range of programs and application\nscenarios.\n\u2022 Poor Portability. All previous proposals have to modify source codes of legacy applica-\ntions. For applications with only binary executables, it is difficult to migrate them to CIM\narchitectures.", "mimetype": "text/plain", "start_char_idx": 8744, "end_char_idx": 9343, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5b89651-6775-409f-a4cd-6b7e9b7335c3": {"__data__": {"id_": "c5b89651-6775-409f-a4cd-6b7e9b7335c3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2106144d-642e-4960-b425-558691a5c921", "node_type": "1", "metadata": {}, "hash": "89af8024ab48a88f1c44512db2d76490062602d57cbb615874a4868f859002eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3cbb9b3-fb5c-43fa-85b6-ad6758f7041b", "node_type": "1", "metadata": {}, "hash": "ae6ace85a3047017ffd91cf43391d4b11b4f9a22a93db8b1bb5dad8bae5eaef8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Although there have been some previous studies [13, 47] on migrating legacy applications to\nCIM architectures, they have to modify applications\u2019 source codes, and can only accelerate very\nlimited computing patterns. In this work, our goal is to facilitate the adoption of CIM accelerators\nfor diverse legacy software. We analyze and identify typical computing patterns that can be accel-\nerated by CIM accelerators at the LLVM IR layer, because LLVM IR has many good features.", "mimetype": "text/plain", "start_char_idx": 9344, "end_char_idx": 9820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3cbb9b3-fb5c-43fa-85b6-ad6758f7041b": {"__data__": {"id_": "e3cbb9b3-fb5c-43fa-85b6-ad6758f7041b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5b89651-6775-409f-a4cd-6b7e9b7335c3", "node_type": "1", "metadata": {}, "hash": "ba124730f5571c48c81759ca6672fda06ed54bc4726556429f145b368b552843", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59ce017a-b229-4751-ae99-2b35be85a5dd", "node_type": "1", "metadata": {}, "hash": "2c9dc6f28c160cd7a349c59288091729a857106348dba97a8404216ee40fd9d0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "First,\nsince LLVM IR offers a low-level and universal abstract model for many programming languages,\ndifferent implementations in source codes for a given task (e.g., different types of loops) can be rep-\nresented by the same LLVM IR. Second, the binary executables of applications can be decompiled\ninto LLVM IRs via reverse engineering tools [1, 3\u20135, 53] even if the source codes of applications are\nunavailable. Third, LLVM IR is portable, and platform/language independent.", "mimetype": "text/plain", "start_char_idx": 9821, "end_char_idx": 10298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59ce017a-b229-4751-ae99-2b35be85a5dd": {"__data__": {"id_": "59ce017a-b229-4751-ae99-2b35be85a5dd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3cbb9b3-fb5c-43fa-85b6-ad6758f7041b", "node_type": "1", "metadata": {}, "hash": "ae6ace85a3047017ffd91cf43391d4b11b4f9a22a93db8b1bb5dad8bae5eaef8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60db44b8-0543-4e04-a8e5-983e194d5060", "node_type": "1", "metadata": {}, "hash": "0a61afcb68525addc4addfd6537320537b59590121a951334f60885049cdc3ab", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "It offers vast oppor-\ntunities to analyze/optimize the program codes with a variety of transformations at compile-time,\nlink-time, or run-time. The low-level abstraction of LLVM IR offers an efficient and simple way to\nrecognize special computing patterns that can be accelerated by ReRAM crossbar arrays.\n3\nSYSTEM OVERVIEW\nFigure 3 shows the flow diagram of legacy software migration using our ReRAM-based CIM\nCompilation Tool (RCCT). At first, we should generate program\u2019s LLVM IR. If the source codes\nof applications are available, then we compile source codes into IRs.", "mimetype": "text/plain", "start_char_idx": 10299, "end_char_idx": 10872, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60db44b8-0543-4e04-a8e5-983e194d5060": {"__data__": {"id_": "60db44b8-0543-4e04-a8e5-983e194d5060", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59ce017a-b229-4751-ae99-2b35be85a5dd", "node_type": "1", "metadata": {}, "hash": "2c9dc6f28c160cd7a349c59288091729a857106348dba97a8404216ee40fd9d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b91e7ef7-2c54-44fa-888d-bc700ea556d0", "node_type": "1", "metadata": {}, "hash": "a124cf939d382d3bf9d22df44ff76ee2509c47032f52b46cb2863f0158fe2f65", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Otherwise, we exploit reverse\nengineering tools to decompile binary executables into IRs. In our system, we mainly compile\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.\nA Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures\n47:5\nC/C++ source codes to LLVM IR (.ll) via Clang/Clang++. For binary executables, we choose a high-\nmaturity decompiler [3] to generate the LLVM IR.", "mimetype": "text/plain", "start_char_idx": 10873, "end_char_idx": 11341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b91e7ef7-2c54-44fa-888d-bc700ea556d0": {"__data__": {"id_": "b91e7ef7-2c54-44fa-888d-bc700ea556d0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60db44b8-0543-4e04-a8e5-983e194d5060", "node_type": "1", "metadata": {}, "hash": "0a61afcb68525addc4addfd6537320537b59590121a951334f60885049cdc3ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21b8ae8b-bb04-436a-add1-104cb8dd909d", "node_type": "1", "metadata": {}, "hash": "0626fb9d112a7559d870a3e0a74b966474848262972502b704b0e9cf792f5a38", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Second, our compilation tool identifies typical\ncode snippets according to predefined patterns, and it determines whether they can be accelerated\nby CIM accelerators based on a simple performance model. For accelerable code snippets, our\ncompilation tool replaces the codes with APIs specified by CIM accelerators. The modified LLVM\nIR can be recompiled into binary executables again. Finally, when the binary executable is executed,\nthe CIM runtime environment (CRE) determines whether each instruction should be executed\non the CPU or ReRAM arrays.", "mimetype": "text/plain", "start_char_idx": 11342, "end_char_idx": 11892, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21b8ae8b-bb04-436a-add1-104cb8dd909d": {"__data__": {"id_": "21b8ae8b-bb04-436a-add1-104cb8dd909d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b91e7ef7-2c54-44fa-888d-bc700ea556d0", "node_type": "1", "metadata": {}, "hash": "a124cf939d382d3bf9d22df44ff76ee2509c47032f52b46cb2863f0158fe2f65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "777dcbac-c3e2-443f-91fd-d02d920f17e5", "node_type": "1", "metadata": {}, "hash": "e018ad2c5da4e290bb80a6e1cd68adac29b9bf82ce093ea68e9440f98d754385", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The CIM runtime also should fetch data into ReRAM arrays from\nmain memory according to the API type and the memory address.\n4\nRERAM-ACCELERABLE LLVM IR PATTERNS\nCurrent ReRAM-based CIM architectures only support multiply-add (MAD) operations in an\nanalog computing paradigm [39, 55], or bitwise Boolean logic operations in a digital computing\nparadigm [29, 35, 46]. They can be adopted in very limited scenarios to accelerate MVM, MMM, and\nbitwise operations for Boolean vectors.", "mimetype": "text/plain", "start_char_idx": 11893, "end_char_idx": 12372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "777dcbac-c3e2-443f-91fd-d02d920f17e5": {"__data__": {"id_": "777dcbac-c3e2-443f-91fd-d02d920f17e5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21b8ae8b-bb04-436a-add1-104cb8dd909d", "node_type": "1", "metadata": {}, "hash": "0626fb9d112a7559d870a3e0a74b966474848262972502b704b0e9cf792f5a38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b86ae0c3-0e53-44ec-a9e8-8e2ca25b0a30", "node_type": "1", "metadata": {}, "hash": "58eb010fe26b395c2437b1edda5a19abe67210634918a7a189d2eb0e176b2249", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In the following, we elaborate the detailed methodologies\nof recognizing four typical accelerable patterns for both compiled IRs and decompiled IRs.\n4.1\nAccelerable Patterns\nTo accelerate the execution of applications, we should find out typical computing patterns in LLVM\nIR that can be accelerated by ReRAM. Although there are an increasing number of application\nscenarios for ReRAM, the fundamental operations that can be accelerated by ReRAM-based CIMs\nare very limited.", "mimetype": "text/plain", "start_char_idx": 12373, "end_char_idx": 12847, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b86ae0c3-0e53-44ec-a9e8-8e2ca25b0a30": {"__data__": {"id_": "b86ae0c3-0e53-44ec-a9e8-8e2ca25b0a30", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "777dcbac-c3e2-443f-91fd-d02d920f17e5", "node_type": "1", "metadata": {}, "hash": "e018ad2c5da4e290bb80a6e1cd68adac29b9bf82ce093ea68e9440f98d754385", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "502ef66e-6dd2-4559-9c21-51caf7bca8c6", "node_type": "1", "metadata": {}, "hash": "ffe8583b1f8bae7d24e4d5aa28244af8e786cb9586dbf42870517aa75dc48450", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "According to the physical characteristics of ReRAM, we recognize four typical\naccelerable computing patterns, i.e., matrix-vector multiplications (MVMs), matrix-matrix\nmultiplications (MMMs), library functions for MVM and MMM operations, and bitmap logical\noperations. Among these patterns, library functions are relatively simple and easy to identify, while\nthe other three patterns are all implemented with more complex loop tactics. Therefore, to identify\nthese accelerable patterns, we have to analyze the loop structure in the IR.", "mimetype": "text/plain", "start_char_idx": 12848, "end_char_idx": 13383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "502ef66e-6dd2-4559-9c21-51caf7bca8c6": {"__data__": {"id_": "502ef66e-6dd2-4559-9c21-51caf7bca8c6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b86ae0c3-0e53-44ec-a9e8-8e2ca25b0a30", "node_type": "1", "metadata": {}, "hash": "58eb010fe26b395c2437b1edda5a19abe67210634918a7a189d2eb0e176b2249", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a248eb7d-7045-48f9-b481-3521d40ad33f", "node_type": "1", "metadata": {}, "hash": "14d808ac5d53de74cd287dc70979d2f1faa2b4850ecb13535b2b100cb5a51303", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For example, a MVM is\ntypically implemented by a two-level nested loop, and the iteration statements in the inner loop\nbody are related to vectors and matrices. Thus, to identify a MVM, we have to identify the pattern\nof the 2-level nested loop at first, and then recognize the MAD operation of two vectors in the\ninner loop.\nFortunately, since the LLVM IR has the ability of low-level abstraction for different programming\nlanguages, it can be exploited to recognize computing patterns that can be accelerated by ReRAM\ncrossbar arrays.", "mimetype": "text/plain", "start_char_idx": 13384, "end_char_idx": 13920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a248eb7d-7045-48f9-b481-3521d40ad33f": {"__data__": {"id_": "a248eb7d-7045-48f9-b481-3521d40ad33f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "502ef66e-6dd2-4559-9c21-51caf7bca8c6", "node_type": "1", "metadata": {}, "hash": "ffe8583b1f8bae7d24e4d5aa28244af8e786cb9586dbf42870517aa75dc48450", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe131852-5ce7-4d09-9faa-715544830cb0", "node_type": "1", "metadata": {}, "hash": "666ab2592856baf6ffdaffd3051e3b51731616558b109269b0bad91259e748e2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Their structural and critical information can be found in different blocks of the\nLLVM IR. One IR file usually contains many blocks and each block consists of multiple statements.\nFigure 4 shows the logic blocks of the two-level nested loop in the LLVM IR. The circles with\ndifferent colors represent different types of blocks. The green, orange, and purple circles represent\nthe logic blocks of the loop condition, the loop body, and the loop end. These blocks are connected\nby jump labels (statements), including unconditional jump and conditional jump.", "mimetype": "text/plain", "start_char_idx": 13921, "end_char_idx": 14476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe131852-5ce7-4d09-9faa-715544830cb0": {"__data__": {"id_": "fe131852-5ce7-4d09-9faa-715544830cb0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a248eb7d-7045-48f9-b481-3521d40ad33f", "node_type": "1", "metadata": {}, "hash": "14d808ac5d53de74cd287dc70979d2f1faa2b4850ecb13535b2b100cb5a51303", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8e6dd4c-902a-489f-911f-fc016ac3b818", "node_type": "1", "metadata": {}, "hash": "addefa84f70744909b0215cdb41c3d916e0c2b853d58a9436d2391256dd1f89b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Therefore, we\nanalyze these blocks in LLVM to identify accelerable MVM operations, and convert them into\nAPIs defined by CIM accelerators.\nThese blocks can be easily identified in the LLVM IR, because it maintains some useful informa-\ntion during the compiling. For example, we can infer the type of the block from its name directly\n(e.g., for.cond, for.body, and for.end).", "mimetype": "text/plain", "start_char_idx": 14477, "end_char_idx": 14850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8e6dd4c-902a-489f-911f-fc016ac3b818": {"__data__": {"id_": "c8e6dd4c-902a-489f-911f-fc016ac3b818", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe131852-5ce7-4d09-9faa-715544830cb0", "node_type": "1", "metadata": {}, "hash": "666ab2592856baf6ffdaffd3051e3b51731616558b109269b0bad91259e748e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56afba75-0e7c-41f6-8310-43fb22925ca6", "node_type": "1", "metadata": {}, "hash": "da4422466591d61644603a8cb9ae46715bc8dd0ec0529ed8758bc78280572f35", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "However, it is hard to identify loop structures in an IR de-\ncompiled from an binary executable, because the decompilation tool does not provide sufficient\nsemantic information about blocks (all blocks have similar names like \u201cblock_4006f 7\u201d). Further-\nmore, the number of instructions generated from reverse engineering often increases by several\ntimes, and instructions also become more complex compared with the IR compiled from source\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.\n47:6\nH. Jin et al.", "mimetype": "text/plain", "start_char_idx": 14851, "end_char_idx": 15424, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56afba75-0e7c-41f6-8310-43fb22925ca6": {"__data__": {"id_": "56afba75-0e7c-41f6-8310-43fb22925ca6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8e6dd4c-902a-489f-911f-fc016ac3b818", "node_type": "1", "metadata": {}, "hash": "addefa84f70744909b0215cdb41c3d916e0c2b853d58a9436d2391256dd1f89b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e6022f1-114a-44b0-b4f8-070f850231cb", "node_type": "1", "metadata": {}, "hash": "01f37ffd94308d932c3d34d267fea5cb017870ceaba298e64f90ff48bb5380a8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Fig. 4. Logic tree of blocks in a MVM operation represented by a two-level nested loop.\nPattern 1: A typical MVM pattern in LLVM IR generated from source codes\n1: loop.condA:\n; \u201cloop\u201d can be \u201cfor\u201d or \u201cwhile\u201d\n2:\n%range_1 = load <i_ty>, <i_ty>* [i]\n3:\n%cmp1 = icmp <cmp> <i_ty> %range_1, [r_1]\n4:\nbr i1 %cmp1, label %loop.bodyB, label %loop.", "mimetype": "text/plain", "start_char_idx": 15425, "end_char_idx": 15764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e6022f1-114a-44b0-b4f8-070f850231cb": {"__data__": {"id_": "1e6022f1-114a-44b0-b4f8-070f850231cb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56afba75-0e7c-41f6-8310-43fb22925ca6", "node_type": "1", "metadata": {}, "hash": "da4422466591d61644603a8cb9ae46715bc8dd0ec0529ed8758bc78280572f35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc7673ad-6e8a-41ab-86ff-a347ac735b2b", "node_type": "1", "metadata": {}, "hash": "4c48efc17a3d892e1c49246aef4b60f1492fbaacec61f730d71181b849880a49", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "endX\n5: loop.bodyB:\n6:\nbr label %loop.condC\n7: loop.condC:\n8:\n%range_2 = load <i_ty>, <i_ty>* [j]\n9:\n%cmp2 = icmp <cmp> <i_ty> %range_2, [r_2]\n10:\nbr i1 %cmp2, label %loop.bodyD, label %loop.endY\n11: loop.bodyD:\n12:\n%arrayidx1 = getelementptr inbounds <ty>, <ty>* addr, <i_ty> idx,", "mimetype": "text/plain", "start_char_idx": 15764, "end_char_idx": 16045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc7673ad-6e8a-41ab-86ff-a347ac735b2b": {"__data__": {"id_": "dc7673ad-6e8a-41ab-86ff-a347ac735b2b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e6022f1-114a-44b0-b4f8-070f850231cb", "node_type": "1", "metadata": {}, "hash": "01f37ffd94308d932c3d34d267fea5cb017870ceaba298e64f90ff48bb5380a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce91620e-7eb7-4b78-9e15-53c97f6f9f5d", "node_type": "1", "metadata": {}, "hash": "10d7ea0c9b1a81a11a8db31be6f92dc5e2fb181606148d69e4ae69197f6d90ec", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "<i_ty> idx1\n13:\n%val_1 = load <ty>, <ty>* %arrayidx1\n; %val_2 and %val_3 are loaded similarly\n14:\n%mul = mul/fmul <ty> %val_1, %val_2\n15:\n%add = add/fadd <ty> %val_3, %mul_e\ncodes. Moreover, blocks generated by decompiling tools usually are out of order, and often lose the\nprogram semantic in the high-level programming language.", "mimetype": "text/plain", "start_char_idx": 16046, "end_char_idx": 16376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce91620e-7eb7-4b78-9e15-53c97f6f9f5d": {"__data__": {"id_": "ce91620e-7eb7-4b78-9e15-53c97f6f9f5d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc7673ad-6e8a-41ab-86ff-a347ac735b2b", "node_type": "1", "metadata": {}, "hash": "4c48efc17a3d892e1c49246aef4b60f1492fbaacec61f730d71181b849880a49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54e970ed-7a06-43b4-bf52-ca552d9a7b8b", "node_type": "1", "metadata": {}, "hash": "dbee43a5c8f2df0d265448550239a645fed3208c00d5eee9057b66525e1f41f3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To address this problem, we conduct\nmassive experiments to get IRs from both compiling and decompiling for the same program, and\nthen reorder the blocks in the decompiled IR. At last, we analyze the type of blocks according to\nsome critical instructions, and determine whether these blocks match the accelerable computing\npatterns.\n4.2\nMatrix-Vector Multiplication\nMVM is a typical computing pattern that can be accelerated by CIM accelerators. For most neural\nnetworks and graph processing applications, massive MVM operations often have a significant\nimpact on the performance and energy consumption.", "mimetype": "text/plain", "start_char_idx": 16377, "end_char_idx": 16979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54e970ed-7a06-43b4-bf52-ca552d9a7b8b": {"__data__": {"id_": "54e970ed-7a06-43b4-bf52-ca552d9a7b8b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce91620e-7eb7-4b78-9e15-53c97f6f9f5d", "node_type": "1", "metadata": {}, "hash": "10d7ea0c9b1a81a11a8db31be6f92dc5e2fb181606148d69e4ae69197f6d90ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b6aaf40-171d-4c09-bd70-a207e25cb551", "node_type": "1", "metadata": {}, "hash": "cf1a7f9941ce2bedcdb65c903311caf7997e1251d28aa4561f6f0c9d5fbd69ce", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Since ReRAM crossbar arrays can perform a\nMVM operation in a constant time (O(1) complexity) via the analog computing model, they have\nbeen widely exploited to accelerate these applications.\nAt first, we analyze a program\u2019s IR generated from its source codes. Pattern 1 shows the\nMVM pattern in the LLVM IR. We can find the same logical structure in Figure 4. Loop.condA\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.", "mimetype": "text/plain", "start_char_idx": 16980, "end_char_idx": 17466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b6aaf40-171d-4c09-bd70-a207e25cb551": {"__data__": {"id_": "2b6aaf40-171d-4c09-bd70-a207e25cb551", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54e970ed-7a06-43b4-bf52-ca552d9a7b8b", "node_type": "1", "metadata": {}, "hash": "dbee43a5c8f2df0d265448550239a645fed3208c00d5eee9057b66525e1f41f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c8d0d18-2d69-4ab7-9e31-420aea81192f", "node_type": "1", "metadata": {}, "hash": "eca107c67ea63b3edf998d087a9b29e5d65e817af5a2a2a0e402c788219490fd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures\n47:7\nPattern 2: An MVM pattern in the IR decompiled from an executable binary\n1: block_A:\n2:\n%a = phi %struct.Memory* [%n1, block_M], [%n2, .]\n3:\n%b = load i32 i32* [r_1]\n4:\n%d = icmp <cmp> i32 %c, i32 0\n5:\n%f = select i1 %e, i64 num1,", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c8d0d18-2d69-4ab7-9e31-420aea81192f": {"__data__": {"id_": "2c8d0d18-2d69-4ab7-9e31-420aea81192f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b6aaf40-171d-4c09-bd70-a207e25cb551", "node_type": "1", "metadata": {}, "hash": "cf1a7f9941ce2bedcdb65c903311caf7997e1251d28aa4561f6f0c9d5fbd69ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c096f8d-52cd-48ab-8802-7364d73cafa2", "node_type": "1", "metadata": {}, "hash": "a89a295aea4797a18888db952a65a8fe35692a40fd8fb5a58b3e75fcc472097b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "i64 6\n6:\n%br i1 %g, label %block_B, label %block_X\n7: block_B:\n8:\nstore i32 [num], i32 * %r_2\n9:\nbr label %block_C\n10: block_C:.\n11: block_D:\n12:\n%o_1 = mul i32 %a_1, %col\n; %col represents the column size of the matrix\n13:\n%add_mid = add i64 %o_1, %m_addr\n14:\n%v1_addr = add i64 %a_2,", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c096f8d-52cd-48ab-8802-7364d73cafa2": {"__data__": {"id_": "3c096f8d-52cd-48ab-8802-7364d73cafa2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c8d0d18-2d69-4ab7-9e31-420aea81192f", "node_type": "1", "metadata": {}, "hash": "eca107c67ea63b3edf998d087a9b29e5d65e817af5a2a2a0e402c788219490fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2df2b22-d8be-40e2-8dd1-90773fc1c113", "node_type": "1", "metadata": {}, "hash": "367378fe23ed3c5338e695fb03d01591bdce7bd26a21d4d242c5c9ae15748241", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "%add_mid\n15:\n%v2_addr = add i64 %input_addr, %a_1\n16:\n%mul = mul/fmul <ty> %val_1, %val_2\n17:\n%v3_addr = add i64 %a_2, %output_addr\n18:\n%add = add/fadd <ty> %val_3, %mul_e\nand loop.condC are loop condition blocks of outer and inner loops, while loop.bodyB and\nloop.bodyD are loop body blocks of outer and inner loops.", "mimetype": "text/plain", "start_char_idx": 18072, "end_char_idx": 18389, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2df2b22-d8be-40e2-8dd1-90773fc1c113": {"__data__": {"id_": "c2df2b22-d8be-40e2-8dd1-90773fc1c113", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c096f8d-52cd-48ab-8802-7364d73cafa2", "node_type": "1", "metadata": {}, "hash": "a89a295aea4797a18888db952a65a8fe35692a40fd8fb5a58b3e75fcc472097b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f50e789-1e08-4641-8cd4-b28d5f6b3460", "node_type": "1", "metadata": {}, "hash": "5b610672f5c79975b4a03de6661e9c412e965d45b8ed47f28417a36ac74abae0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We note that the block for updating\nthe loop control variable is not presented in Pattern 1. The prefix loop. represents loops generated\nfrom \u201cfor\u201d or \u201cwhile\u201d statements. The label A-D can be letters or numbers. loop.bodyB is mainly\nused to initialize the second layer of loop variables, while loop.bodyD is used to complete the\ninnermost loop where the application logic is implemented. Moreover, some critical instructions\nand dependencies are constrained in loop.bodyD.", "mimetype": "text/plain", "start_char_idx": 18390, "end_char_idx": 18862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f50e789-1e08-4641-8cd4-b28d5f6b3460": {"__data__": {"id_": "0f50e789-1e08-4641-8cd4-b28d5f6b3460", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2df2b22-d8be-40e2-8dd1-90773fc1c113", "node_type": "1", "metadata": {}, "hash": "367378fe23ed3c5338e695fb03d01591bdce7bd26a21d4d242c5c9ae15748241", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53e5906f-de74-465a-90c1-75e65dd77205", "node_type": "1", "metadata": {}, "hash": "8cba7b8a390670d3d9e956f98306670e328672866eb47efcbb087d68e604fb66", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For example, arrayidx# denotes that the data\ninvolved in the MVM operation comes from a matrix or a vector. In the statements of the IR,\n[r_#] represents the loop condition variable, including local variables and global variables, and\n%range_# represents the corresponding values. [v_#] represents a variable that stores the range\nof the loop. <cmp> represents the type of a comparison, such as less and less than or equal to.", "mimetype": "text/plain", "start_char_idx": 18863, "end_char_idx": 19289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53e5906f-de74-465a-90c1-75e65dd77205": {"__data__": {"id_": "53e5906f-de74-465a-90c1-75e65dd77205", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f50e789-1e08-4641-8cd4-b28d5f6b3460", "node_type": "1", "metadata": {}, "hash": "5b610672f5c79975b4a03de6661e9c412e965d45b8ed47f28417a36ac74abae0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4629a79-99fd-4883-ae92-e054b85facc5", "node_type": "1", "metadata": {}, "hash": "a8e932b0c116fa3fb706c3f9782fc728efd2507b2b3d37eaba8eb8b21938540d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The icmp instruction is used to compare two variables (e.g., %range_1 and [v_1]) and stores the\noutput in a boolean variable (e.g., cmp1). This critical instruction is used to determine whether\nthe two-layer loop can continue. <ty> represents the data type and <i_ty> refers to the integer\ntype specifically. The result of vector multiplication is assigned to the output vector and then\naccumulated.", "mimetype": "text/plain", "start_char_idx": 19290, "end_char_idx": 19689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4629a79-99fd-4883-ae92-e054b85facc5": {"__data__": {"id_": "f4629a79-99fd-4883-ae92-e054b85facc5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53e5906f-de74-465a-90c1-75e65dd77205", "node_type": "1", "metadata": {}, "hash": "8cba7b8a390670d3d9e956f98306670e328672866eb47efcbb087d68e604fb66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6222df82-1d98-47ff-a0c9-db189f8c6937", "node_type": "1", "metadata": {}, "hash": "98fda56908e91e5e6edf77810849a86f1eeb710f5ea455432c92f9fd5e0d37f8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Thus, the dependence between mul and add should be constrained in loop.bodyD,\nwhere mul/add and fmul/fadd represent operations for integers and floating-point numbers,\nrespectively. We note that %mul_e is just an alias of %mul, or %mul_e is a multiple of %mul.\nOverall, to recognize the MVM pattern, we have to identify the two-level nested loop structure at\nfirst, and then identify the multiplication and addition instructions in the inner loop.", "mimetype": "text/plain", "start_char_idx": 19690, "end_char_idx": 20137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6222df82-1d98-47ff-a0c9-db189f8c6937": {"__data__": {"id_": "6222df82-1d98-47ff-a0c9-db189f8c6937", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4629a79-99fd-4883-ae92-e054b85facc5", "node_type": "1", "metadata": {}, "hash": "a8e932b0c116fa3fb706c3f9782fc728efd2507b2b3d37eaba8eb8b21938540d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8cfadd58-f42a-4802-b49e-3d3a2ed8aaea", "node_type": "1", "metadata": {}, "hash": "393f3b623b313a1d4b78dae79ceabad0a7bc94fe0804e3d563f5798000b2db54", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Second, we decompile the same program\u2019s binary executable to generate its IR, and illustrate\nthe MVM pattern of the IR in Pattern 2. We can find the same logical structure shown in Figure 4,\nbut the name and key instructions of each block are significantly different from the IR in Pattern 1.\nWe note that these key instructions in the blocks are formalized by a high-level abstraction. We\ncan find that the critical instructions in the loop condition block block_A is completely differ-\nent from the loop.condA block in Pattern 1.", "mimetype": "text/plain", "start_char_idx": 20138, "end_char_idx": 20669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8cfadd58-f42a-4802-b49e-3d3a2ed8aaea": {"__data__": {"id_": "8cfadd58-f42a-4802-b49e-3d3a2ed8aaea", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6222df82-1d98-47ff-a0c9-db189f8c6937", "node_type": "1", "metadata": {}, "hash": "98fda56908e91e5e6edf77810849a86f1eeb710f5ea455432c92f9fd5e0d37f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04a5abff-9386-4cbd-b72d-85593cd205a3", "node_type": "1", "metadata": {}, "hash": "c1605c5c91ee6f3613cef3329861479521388e396cd1580d3ff4934c9a04b4da", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The significant difference is mainly caused by the\ndecompiling tool. However, we can still infer that this block is the loop condition based on the\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.\n47:8\nH. Jin et al.", "mimetype": "text/plain", "start_char_idx": 20670, "end_char_idx": 20952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04a5abff-9386-4cbd-b72d-85593cd205a3": {"__data__": {"id_": "04a5abff-9386-4cbd-b72d-85593cd205a3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8cfadd58-f42a-4802-b49e-3d3a2ed8aaea", "node_type": "1", "metadata": {}, "hash": "393f3b623b313a1d4b78dae79ceabad0a7bc94fe0804e3d563f5798000b2db54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e8528e1-bf1b-4bf5-81da-28e65910d1b6", "node_type": "1", "metadata": {}, "hash": "a6aa438e839cb967a7ccc5654182844763890616c80b7a6b1fb034b89a9d0c6f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Pattern 3: An MMM pattern in the IR generated from source codes\n1: loop.condA:\n...\n2: loop.bodyB:\n...\n3: loop.condC:\n...\n4: loop.bodyD:\n...\n5: loop.condE:\n...\n6: loop.bodyF:\n7:\n...\n8:\n%mul = mul/fmul <ty> %val_1, %val_2\n9:\n%add = add/fadd <ty> %val_3, %mul_e\nicmp instruction and the pattern of following blocks.", "mimetype": "text/plain", "start_char_idx": 20953, "end_char_idx": 21265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e8528e1-bf1b-4bf5-81da-28e65910d1b6": {"__data__": {"id_": "2e8528e1-bf1b-4bf5-81da-28e65910d1b6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04a5abff-9386-4cbd-b72d-85593cd205a3", "node_type": "1", "metadata": {}, "hash": "c1605c5c91ee6f3613cef3329861479521388e396cd1580d3ff4934c9a04b4da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a76354e5-746f-4b01-becf-8353e6f0d923", "node_type": "1", "metadata": {}, "hash": "385c167c7ddac6b1750b5f0d9f51b496ecfd0a015067a8e6f871bfb7a93b51a2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Also, similar to Pattern 1, [%r_#] represents\nloop condition variables. Moreover, the instructions in the loop body block (block_D) have also\nchanged significantly. Unlike the key variable arrayidx_n in Pattern 1, no instruction can directly\nindicate vectors or matrices in the innermost loop body block. However, we find there are total\nfour add instructions involved in memory addressing, and two other instructions are used to per-\nform multiplication and addition operations.", "mimetype": "text/plain", "start_char_idx": 21266, "end_char_idx": 21745, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a76354e5-746f-4b01-becf-8353e6f0d923": {"__data__": {"id_": "a76354e5-746f-4b01-becf-8353e6f0d923", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e8528e1-bf1b-4bf5-81da-28e65910d1b6", "node_type": "1", "metadata": {}, "hash": "a6aa438e839cb967a7ccc5654182844763890616c80b7a6b1fb034b89a9d0c6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b2e36ba-aa82-4011-8a43-ad89f8807cf4", "node_type": "1", "metadata": {}, "hash": "450cef19c3804e4cc20041a30bbc29d140c5701960629342fb552ee9de664bac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The first three add instructions (lines 14\u201316) are used\nto calculate the element\u2019s address of the matrix and the input vector, and one more add instruc-\ntion (line 18) is used to address the element of the output vector, where %m_addr, input_addr, and\noutput_addr represent the starting address of the matrix, the input vector, and the output vector,\nrespectively. a_# represents the index of the element in vectors or matrices.", "mimetype": "text/plain", "start_char_idx": 21746, "end_char_idx": 22174, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b2e36ba-aa82-4011-8a43-ad89f8807cf4": {"__data__": {"id_": "1b2e36ba-aa82-4011-8a43-ad89f8807cf4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a76354e5-746f-4b01-becf-8353e6f0d923", "node_type": "1", "metadata": {}, "hash": "385c167c7ddac6b1750b5f0d9f51b496ecfd0a015067a8e6f871bfb7a93b51a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f57d00d-c394-42c0-a3e8-7b29cf00fd12", "node_type": "1", "metadata": {}, "hash": "22f33dd1aad87e0bce17ad0bfe359480167898d703737446677707a4b557d8f4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Their elements can be\naccessed directly by the index from the starting address of a. The value of the variable %val_# cor-\nresponds to the virtual address %v#_addr. Since these instructions are tightly coupled with a high\ndependency, we can infer that these block actually performs an MVM operation by recognizing\nthe pattern of these seven instructions.\nOverall, although instructions in the compiled IR is different from that in the decompiled IR,\nthe MAC operations and constraints among the input vector, the input matrix, and the output\nvector are immutable.", "mimetype": "text/plain", "start_char_idx": 22175, "end_char_idx": 22738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f57d00d-c394-42c0-a3e8-7b29cf00fd12": {"__data__": {"id_": "8f57d00d-c394-42c0-a3e8-7b29cf00fd12", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b2e36ba-aa82-4011-8a43-ad89f8807cf4", "node_type": "1", "metadata": {}, "hash": "450cef19c3804e4cc20041a30bbc29d140c5701960629342fb552ee9de664bac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "beddeded-41a9-41e2-b0d7-4d68fbef9c8a", "node_type": "1", "metadata": {}, "hash": "68e2ac0d1d85a0e6ca1f9a37292465c83bf2af5973c30b61b34f4c973d21be8b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To recognize this pattern, we should first identify the two-level nested loop\nstructure, and then recognize the key instructions and their dependencies.\n4.3\nMatrix-Matrix Multiplication\nMMM is a common operation in neural network applications. In traditional computer architectures,\nMMM is costly in terms of execution time and energy consumption. It can be also accelerated by\nReRAM [26] by decomposing an MMM into multiple MVMs. Thus, we also recognize the MMM\npattern in the LLVM IR.\nThe logical structure of MMM is similar to MVM.", "mimetype": "text/plain", "start_char_idx": 22739, "end_char_idx": 23273, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "beddeded-41a9-41e2-b0d7-4d68fbef9c8a": {"__data__": {"id_": "beddeded-41a9-41e2-b0d7-4d68fbef9c8a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f57d00d-c394-42c0-a3e8-7b29cf00fd12", "node_type": "1", "metadata": {}, "hash": "22f33dd1aad87e0bce17ad0bfe359480167898d703737446677707a4b557d8f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe4ff507-003c-478d-8d23-7c649516db2b", "node_type": "1", "metadata": {}, "hash": "df19504ad79ff8d81c4bb9539da96c6ce01e39072c44a7f06e52bc8f12b3f1ec", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "An MMM operation should be performed\nin a three-level nested loop where the key instructions are executed in the innermost loop body.\nPattern 3 shows the MMM pattern of the IR compiled from source codes. Similar to the structure of\nMVM in Pattern 1, loop.condX and loop.bodyX represent the loop condition block and the loop\nbody block in each level, respectively. Except for loop.bodyF, they have the same roles as that in\nPattern 1, so that we omit the instructions in these blocks.", "mimetype": "text/plain", "start_char_idx": 23274, "end_char_idx": 23757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe4ff507-003c-478d-8d23-7c649516db2b": {"__data__": {"id_": "fe4ff507-003c-478d-8d23-7c649516db2b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "beddeded-41a9-41e2-b0d7-4d68fbef9c8a", "node_type": "1", "metadata": {}, "hash": "68e2ac0d1d85a0e6ca1f9a37292465c83bf2af5973c30b61b34f4c973d21be8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2f79ecd-d00e-4320-bb3d-53171ac5ed78", "node_type": "1", "metadata": {}, "hash": "801ad4e39fcdc87cc796659a62f7a6cf66110e5a1f7ff390d0edcd393f162198", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The key instructions in loop.bodyF are\nalso similar to that in the MVM, but val_# are all numbers in matrices. To recognize the pattern of\nan MMM, we only have to identify the three-level loop structure and the multiplication/addition\noperations in the innermost loop.\nThe MMM pattern of the IR decompiled from a binary executable is shown in Pattern 4. It has\nthe same structure as Pattern 3. However, the key instructions in different blocks are similar to the\nMVM pattern (Pattern 2), so that we omit blocks A\u2013E.", "mimetype": "text/plain", "start_char_idx": 23758, "end_char_idx": 24273, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2f79ecd-d00e-4320-bb3d-53171ac5ed78": {"__data__": {"id_": "f2f79ecd-d00e-4320-bb3d-53171ac5ed78", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe4ff507-003c-478d-8d23-7c649516db2b", "node_type": "1", "metadata": {}, "hash": "df19504ad79ff8d81c4bb9539da96c6ce01e39072c44a7f06e52bc8f12b3f1ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5a01699-da84-4666-a2e6-f3696cc085bd", "node_type": "1", "metadata": {}, "hash": "f570b3d1e197f73d35eb23d167cde5ab748d0b8e66e50292b92a1b6ce157e516", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In block_F, the memory addressing of each\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.\nA Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures\n47:9\nPattern 4: An MMM pattern in the IR decompiled from a binary executable\n1: block_A:\n2:\n.\n3: block_F:\n4:\n%addr_mid1 = add i64 %o_1, %m1_addr\n5:\n%v1_addr = add i64 %a_2,", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5a01699-da84-4666-a2e6-f3696cc085bd": {"__data__": {"id_": "e5a01699-da84-4666-a2e6-f3696cc085bd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2f79ecd-d00e-4320-bb3d-53171ac5ed78", "node_type": "1", "metadata": {}, "hash": "801ad4e39fcdc87cc796659a62f7a6cf66110e5a1f7ff390d0edcd393f162198", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8af28542-ba25-47f0-ba2f-6bf41ab2e18e", "node_type": "1", "metadata": {}, "hash": "529ef3caedbbe18cb2ca450c53db574fb39528bbe4beab10c940124a0ad07e0c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "%addr_mid1\n6:\n%addr_mid2 = add i64 %o_2, %m2_addr\n7:\n%v2_addr = add i64 %a_3, %addr_mid2\n8:\n%mul = mul/fmul <ty> %val_1, %val_2\n9:\n%addr_mid3 = add i64 %a_4, %m3_addr\n10:\n%v3_addr = add i64 %a_3, %addr_mid3\n11:\n%add = add/fadd <ty> %val_3,", "mimetype": "text/plain", "start_char_idx": 24686, "end_char_idx": 24925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8af28542-ba25-47f0-ba2f-6bf41ab2e18e": {"__data__": {"id_": "8af28542-ba25-47f0-ba2f-6bf41ab2e18e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5a01699-da84-4666-a2e6-f3696cc085bd", "node_type": "1", "metadata": {}, "hash": "f570b3d1e197f73d35eb23d167cde5ab748d0b8e66e50292b92a1b6ce157e516", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c04abd6-cef9-4c1c-8754-b992b75de0a9", "node_type": "1", "metadata": {}, "hash": "d4e09388bb0880992af24a3f9b642e6e58b30711be32c9b70da19a7cac39cc7a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "%mul_e\nPattern 5: A library function pattern in the IR\n1: call ret_type @function_name(<ty> p_1, . <ty> p_n)\nelement in a matrix is also similar to that in an MVM. The difference is that total six add instructions\nare used for memory addressing in the innermost loop body. According to these tightly coupled\ninstructions, we can easily recognize the MMM pattern in the IR.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c04abd6-cef9-4c1c-8754-b992b75de0a9": {"__data__": {"id_": "7c04abd6-cef9-4c1c-8754-b992b75de0a9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8af28542-ba25-47f0-ba2f-6bf41ab2e18e", "node_type": "1", "metadata": {}, "hash": "529ef3caedbbe18cb2ca450c53db574fb39528bbe4beab10c940124a0ad07e0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f33879f1-570c-4f85-9e46-5e17602302a4", "node_type": "1", "metadata": {}, "hash": "7e3f267118ede8d8b41a2088955887950323831f8e708f868a52b74019a5966c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4.4\nLibrary Functions of MVM and MMM\nMany programming libraries provide MVM and MMM functions to improve developer\u2019s produc-\ntivity, such as CBLAs library functions in the Caffe framework, GEMV and GEMM in OpenBL e[52],\nand other library functions [41, 49]. We can simply convert these library functions into APIs\nprovided by CIM accelerators. Therefore, we can directly recognize this accelerable pattern by\nhooking these function calls. Pattern 5 shows the pattern of library functions in the IR.", "mimetype": "text/plain", "start_char_idx": 25302, "end_char_idx": 25800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f33879f1-570c-4f85-9e46-5e17602302a4": {"__data__": {"id_": "f33879f1-570c-4f85-9e46-5e17602302a4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c04abd6-cef9-4c1c-8754-b992b75de0a9", "node_type": "1", "metadata": {}, "hash": "d4e09388bb0880992af24a3f9b642e6e58b30711be32c9b70da19a7cac39cc7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49b1d941-ef85-4e88-8b11-d1834a0ced47", "node_type": "1", "metadata": {}, "hash": "944808556e2f83209a59432f942f4863683afec0d9114378ad57ead78864ff7d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Both\ncompiling and decompiling generate the same name of function calls in the IR. p_n denotes the\nnth argument of the library function. Our compilation tool translates those accelerable library\nfunctions into predefined CIM APIs according to the function\u2019s name and arguments.\n4.5\nBitmap Logical Operations\nBesides arithmetic operations, a number of studies have demonstrated that ReRAM can perform\nlogical operations efficiently [12, 31, 32]. Particularly, ReRAM crossbar arrays are well-suited to\naccelerating Boolean logical operations, which are commonly used for data querying in databases.", "mimetype": "text/plain", "start_char_idx": 25801, "end_char_idx": 26397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49b1d941-ef85-4e88-8b11-d1834a0ced47": {"__data__": {"id_": "49b1d941-ef85-4e88-8b11-d1834a0ced47", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f33879f1-570c-4f85-9e46-5e17602302a4", "node_type": "1", "metadata": {}, "hash": "7e3f267118ede8d8b41a2088955887950323831f8e708f868a52b74019a5966c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "065d50ae-16a0-428e-95e2-756112bd9d65", "node_type": "1", "metadata": {}, "hash": "729061d276aeb979b7875c7e4702445ebddf87e61de29819e2e9c24775a105ef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As described in Section 2.1, a quarry in a data table can be converted into a Boolean logical opera-\ntion for two bitmaps [35]. In a ReRAM crossbar array, this bitwise operation can be performed in\nparallel with only one step.\nPattern 6 shows the IR pattern of Boolean logical operations for two bitmaps generated from\nsource codes. %val_# is an element of a bitmap, and logic_op represents a logical operator, such\nas OR, AND.", "mimetype": "text/plain", "start_char_idx": 26398, "end_char_idx": 26825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "065d50ae-16a0-428e-95e2-756112bd9d65": {"__data__": {"id_": "065d50ae-16a0-428e-95e2-756112bd9d65", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49b1d941-ef85-4e88-8b11-d1834a0ced47", "node_type": "1", "metadata": {}, "hash": "944808556e2f83209a59432f942f4863683afec0d9114378ad57ead78864ff7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "518c0cd7-d67a-4f81-827c-d5d132d6ad34", "node_type": "1", "metadata": {}, "hash": "5cbd138efe17e84d6ec68b9306efaab4ca0c241baa0fe48c7d05f22f56d334fc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In traditional computing model without using SIMD instructions, the Boolean logical\noperation for two bitmaps is often performed sequentially in a single-level loop. For the IR\ndecompiled from a binary executable, the pattern of Boolean logical operation shows the same\nloop structure and logical operator as that in Pattern 6. The only difference is the instructions for\ndata addressing, which do not affect the pattern recognition. Thus, to recognize this accelerable\npattern, our tool only needs to identify the single-level loop structure and the Boolean logical\noperator.\nACM Transactions on Architecture and Code Optimization, Vol. 20, No.", "mimetype": "text/plain", "start_char_idx": 26826, "end_char_idx": 27471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "518c0cd7-d67a-4f81-827c-d5d132d6ad34": {"__data__": {"id_": "518c0cd7-d67a-4f81-827c-d5d132d6ad34", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "065d50ae-16a0-428e-95e2-756112bd9d65", "node_type": "1", "metadata": {}, "hash": "729061d276aeb979b7875c7e4702445ebddf87e61de29819e2e9c24775a105ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fef4540-a8f1-45bb-98fd-1c5cfabc8776", "node_type": "1", "metadata": {}, "hash": "6f3f9c42f2add41cffc4eaf5f2ff26b3888cc86f0b7ca347b055f361369e4e09", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4, Article 47. Publication date: October 2023.\n47:10\nH. Jin et al.\nPattern 6: A pattern of Boolean logical operations in the IR compiled from source codes\n1: loop.condA: .\n2: loop.bodyB:\n3:\n%val_1 = load <ty>, <ty>* %arrayidx_1\n4:\n%val_2 = load <ty>, <ty>* %arrayidx_2\n5:\n%logic = logic_op <ty> %val_1, %val_2\n4.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fef4540-a8f1-45bb-98fd-1c5cfabc8776": {"__data__": {"id_": "9fef4540-a8f1-45bb-98fd-1c5cfabc8776", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "518c0cd7-d67a-4f81-827c-d5d132d6ad34", "node_type": "1", "metadata": {}, "hash": "5cbd138efe17e84d6ec68b9306efaab4ca0c241baa0fe48c7d05f22f56d334fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bc940a3-bd3f-49df-adfa-b3977e4a8a35", "node_type": "1", "metadata": {}, "hash": "6a1894a89a555f1e70cb9f04f353ea2821590fbe282d2705d1c2a7c9f25d7051", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "6\nPerformance Model\nAlthough we can recognize four typical accelerable patterns, we still have to check whether these\npatterns can be really accelerated by ReRAM crossbar arrays, because the operation of data map-\nping to ReRAM is usually costly and may offset the performance gain of CIM accelerators. For\nexample, only when the size of matrix is large enough, an MVM operation can be accelerated by\nReRAM crossbar arrays [9, 38]. Thus, we propose performance models to estimate the net benefit\nof computation offloading based on the data size of matrices and vectors [38].", "mimetype": "text/plain", "start_char_idx": 27786, "end_char_idx": 28360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0bc940a3-bd3f-49df-adfa-b3977e4a8a35": {"__data__": {"id_": "0bc940a3-bd3f-49df-adfa-b3977e4a8a35", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fef4540-a8f1-45bb-98fd-1c5cfabc8776", "node_type": "1", "metadata": {}, "hash": "6f3f9c42f2add41cffc4eaf5f2ff26b3888cc86f0b7ca347b055f361369e4e09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ad79ff6-aeef-4ff5-a3bd-312024872057", "node_type": "1", "metadata": {}, "hash": "4d8c8db8505dce950cb941af94729c1675f39e96c65786ece44638b0af37fd00", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We note that the capacity of each ReRAM cell, the size of each crossbar array, the total number of\ncrossbar arrays, and various approaches of titling/blocking matrices/vectors all affect the latency of\nReRAM writes and computation. For example, since each ReRAM cell can only store limited bits, a\nvalue is usually split into multiple bits and mapped to multiple columns in ReRAM crossbar arrays.\nThis affects the number of writes to ReRAM crossbars and the number of crossbar computations,\nand also induces additional shift-and-add latency.", "mimetype": "text/plain", "start_char_idx": 28361, "end_char_idx": 28902, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ad79ff6-aeef-4ff5-a3bd-312024872057": {"__data__": {"id_": "0ad79ff6-aeef-4ff5-a3bd-312024872057", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0bc940a3-bd3f-49df-adfa-b3977e4a8a35", "node_type": "1", "metadata": {}, "hash": "6a1894a89a555f1e70cb9f04f353ea2821590fbe282d2705d1c2a7c9f25d7051", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f83664e-f4e5-4c94-a082-2a48f14fb158", "node_type": "1", "metadata": {}, "hash": "06348db0f5e602eff55c46c1883e8ac8d74dd1db4f94d4d9b0ce7d77de7a9752", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Moreover, a large matrix can be partitioned\ninto multiple sub-matrices and mapped into multiple crossbar arrays simultaneously. Although\ndifferent rows in a single sub-matrix should be written to a crossbar array sequentially, the rows\nof different sub-matrices can be written to multiple crossbar arrays concurrently. The size of the\nCPU cache, the cache hierarchy, and the size of accelerator buffer also have a significant impact on\nthe latency of loading a large matrix.", "mimetype": "text/plain", "start_char_idx": 28903, "end_char_idx": 29377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f83664e-f4e5-4c94-a082-2a48f14fb158": {"__data__": {"id_": "0f83664e-f4e5-4c94-a082-2a48f14fb158", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ad79ff6-aeef-4ff5-a3bd-312024872057", "node_type": "1", "metadata": {}, "hash": "4d8c8db8505dce950cb941af94729c1675f39e96c65786ece44638b0af37fd00", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c83150f1-456a-4ea4-bf4b-7ded0d77cefa", "node_type": "1", "metadata": {}, "hash": "99b05fbbf7584e891808d0b8c9c3aa6133b2b303c1c95073ca9a21acb391c1a6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To simplify the performance model, we assume that the size\nof the accelerator buffer is the same as that of the CPU cache, and thus the time spent in loading a\nmatrix to the CPU cache and to the buffer of the ReRAM-based accelerator are similar. In this way,\nwe do not consider these latencies of data loading in the performance model, because they can be\ncounteracted.\nTaking the MVM operation as an example, Equation (1) shows the performance model.", "mimetype": "text/plain", "start_char_idx": 29378, "end_char_idx": 29829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c83150f1-456a-4ea4-bf4b-7ded0d77cefa": {"__data__": {"id_": "c83150f1-456a-4ea4-bf4b-7ded0d77cefa", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f83664e-f4e5-4c94-a082-2a48f14fb158", "node_type": "1", "metadata": {}, "hash": "06348db0f5e602eff55c46c1883e8ac8d74dd1db4f94d4d9b0ce7d77de7a9752", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28973d5e-e798-45b9-a244-8d8a2e33c7aa", "node_type": "1", "metadata": {}, "hash": "cf325d94df29071c38cec0129e92d4bdc1874ecd4d970eed55f7fcc6db6591ff", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The\nMVM operation can be offloaded to ReRAM crossbar arrays only when the net benefit Bene f itMV M\nis positive. TCPU and TReRAM represent the total execution times of an MVM operation on CPU\nand ReRAM, respectively.", "mimetype": "text/plain", "start_char_idx": 29830, "end_char_idx": 30046, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28973d5e-e798-45b9-a244-8d8a2e33c7aa": {"__data__": {"id_": "28973d5e-e798-45b9-a244-8d8a2e33c7aa", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c83150f1-456a-4ea4-bf4b-7ded0d77cefa", "node_type": "1", "metadata": {}, "hash": "99b05fbbf7584e891808d0b8c9c3aa6133b2b303c1c95073ca9a21acb391c1a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73de1ec2-b29c-4505-8822-68e4e0215bdf", "node_type": "1", "metadata": {}, "hash": "adc90ea98547162057053933d9b0bb4883af3ccaf3631db1be9f7b20c14eb6a0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "TReRAM mainly consists of the time for mapping an matrix to ReRAM\ncrossbar arrays (LDataMap), and the execution time of an MVM operation on ReRAM crossbar\narrays (including analog computing and ADC operations, i.e., Lcom&ADC):\nBene f itMV M = TCPU \u2212TReRAM\n= TCPU \u2212(LDataMap + Lcom&ADC)\n= m \u00d7 n \u00d7 Lmul + (m \u22121) \u00d7 n \u00d7 Ladd \u2212(r \u00d7 Lwrite\u2212a\u2212row + Lcom + k \u00d7 LADC).", "mimetype": "text/plain", "start_char_idx": 30047, "end_char_idx": 30406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73de1ec2-b29c-4505-8822-68e4e0215bdf": {"__data__": {"id_": "73de1ec2-b29c-4505-8822-68e4e0215bdf", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28973d5e-e798-45b9-a244-8d8a2e33c7aa", "node_type": "1", "metadata": {}, "hash": "cf325d94df29071c38cec0129e92d4bdc1874ecd4d970eed55f7fcc6db6591ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dcf1dd35-d48b-453a-a2a8-e0c57f97bffc", "node_type": "1", "metadata": {}, "hash": "dff1962c2cbe2e7ed20e9214ce14c91c23386a0d56388ff748990e313094bb8f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(1)\nAssume the size of a crossbar array is r \u00d7 r, an m \u00d7 n matrix (m rows and n columns) should\nbe mapped into \u2308m/r\u2309ReRAM crossbar arrays. We can write a vector to one row of the crossbar\narray at a time, and the write latency (Lwrite\u2212a\u2212row) can be deemed as a constant.", "mimetype": "text/plain", "start_char_idx": 30407, "end_char_idx": 30677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dcf1dd35-d48b-453a-a2a8-e0c57f97bffc": {"__data__": {"id_": "dcf1dd35-d48b-453a-a2a8-e0c57f97bffc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73de1ec2-b29c-4505-8822-68e4e0215bdf", "node_type": "1", "metadata": {}, "hash": "adc90ea98547162057053933d9b0bb4883af3ccaf3631db1be9f7b20c14eb6a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c62a6ac3-6952-44eb-97c7-9f869b5fa962", "node_type": "1", "metadata": {}, "hash": "c4cb5a0c5343b3c3ad015a48f34da9f8beee3a03a4a8ca54fd02e25064950d4c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Since we can\nmap \u2308m/r\u2309sub-matrices to \u2308m/r\u2309ReRAM crossbar arrays simultaneously, totalr rows in the whole\nmatrix should be written to crossbar arrays sequentially, and the latency of data mapping becomes\nr \u00d7 Lwrite\u2212a\u2212row. For matrices with different sizes, the latency of analog MVM operation in the\nReRAM array (Lcom) is usually the same (O(1) complexity), and is much lower than the latency\nACM Transactions on Architecture and Code Optimization, Vol. 20, No.", "mimetype": "text/plain", "start_char_idx": 30678, "end_char_idx": 31139, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c62a6ac3-6952-44eb-97c7-9f869b5fa962": {"__data__": {"id_": "c62a6ac3-6952-44eb-97c7-9f869b5fa962", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dcf1dd35-d48b-453a-a2a8-e0c57f97bffc", "node_type": "1", "metadata": {}, "hash": "dff1962c2cbe2e7ed20e9214ce14c91c23386a0d56388ff748990e313094bb8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "882bf157-6082-4418-a2d6-bdcd1b788533", "node_type": "1", "metadata": {}, "hash": "05ad16b88a70d54c345b22d6313f85fa63f9acb1626cd8d2a4ceb929688aa110", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4, Article 47. Publication date: October 2023.\nA Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures\n47:11\nof data mapping. After the analog MVM operation is completed, the analog values in the output\nvector should be converted into digital values through ADCs. Since the die area of an ADC device\nis often too large relative to the ReRAM array, we assume k columns share one ADC to convert the\nelements of the output vector sequentially, and the latency of ADC is LADC.", "mimetype": "text/plain", "start_char_idx": 31140, "end_char_idx": 31635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "882bf157-6082-4418-a2d6-bdcd1b788533": {"__data__": {"id_": "882bf157-6082-4418-a2d6-bdcd1b788533", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c62a6ac3-6952-44eb-97c7-9f869b5fa962", "node_type": "1", "metadata": {}, "hash": "c4cb5a0c5343b3c3ad015a48f34da9f8beee3a03a4a8ca54fd02e25064950d4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95fdd86a-5a07-446b-ae20-0fb1ab62c1b2", "node_type": "1", "metadata": {}, "hash": "219d4532add713786b23f8f15bea1d5a1d90c597b96cce2ba29b53c1293cf078", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Thus, the total latency\nof ADC operations becomes k \u00d7 LADC, which is mainly determined by the number of columns in\nthe matrix.\nFor the m \u00d7 n matrix, total m \u00d7 n multiplications and (m \u22121) \u00d7 n additions are required for an\nMVM operation on CPU, respectively. Assume Ladd and Lmul represent the clock cycles required\nfor each addition and multiplication instruction [18], respectively. The total execution time of an\nMVM on CPU TCPU can be estimated by m \u00d7 n \u00d7 Lmul + (m \u22121) \u00d7 n \u00d7 Ladd.", "mimetype": "text/plain", "start_char_idx": 31636, "end_char_idx": 32120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95fdd86a-5a07-446b-ae20-0fb1ab62c1b2": {"__data__": {"id_": "95fdd86a-5a07-446b-ae20-0fb1ab62c1b2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "882bf157-6082-4418-a2d6-bdcd1b788533", "node_type": "1", "metadata": {}, "hash": "05ad16b88a70d54c345b22d6313f85fa63f9acb1626cd8d2a4ceb929688aa110", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78eeca5b-10d3-4bd2-beba-6c6331f17a3c", "node_type": "1", "metadata": {}, "hash": "b626a82891533d1ff6e084df464f43c209feaf87677821664ac7a48bc5a25888", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Since Lmul, Ladd, Lwrite\u2212a\u2212row, Lcom, LADC can be all deemed as constants, the benefit of MVM\noffloading is mainly determined by the matrix size (i.e., m, n). Thus, we can exploit the simple per-\nformance model in Equation (1) to determine whether an MVM operation can be really accelerated\nby the CIM accelerator.\nAs a crossbar array can only perform an MVM operation or multiple MAD operations for vec-\ntors, it cannot perform MMM operations directly. Fortunately, we can decompose an MMM into\nmultiple MVM operations.", "mimetype": "text/plain", "start_char_idx": 32121, "end_char_idx": 32641, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78eeca5b-10d3-4bd2-beba-6c6331f17a3c": {"__data__": {"id_": "78eeca5b-10d3-4bd2-beba-6c6331f17a3c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95fdd86a-5a07-446b-ae20-0fb1ab62c1b2", "node_type": "1", "metadata": {}, "hash": "219d4532add713786b23f8f15bea1d5a1d90c597b96cce2ba29b53c1293cf078", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "254e8095-883b-4a57-8435-68a9c5ab72ce", "node_type": "1", "metadata": {}, "hash": "26de45f28acd0af624973e70819ef63d67681ed0e6791f960bd182c4e20b8dab", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Assume the size of two matrices are l \u00d7m and m \u00d7n, this MMM can be\ndecomposed to l MVM operations. According to Equation (1), we can simply estimate the benefit\nof offloading an MMM operation to the CIM accelerator, i.e.,\nBene f itMMM = l \u00d7 Bene f itMV M.\n(2)\nWe also offer a simple performance model for bitmap-based bitwise operations. Assume a bitmap\ncontains s bits, the size of a crossbar array is r \u00d7 r, and there are total t crossbar arrays in a CIM\naccelerator.", "mimetype": "text/plain", "start_char_idx": 32642, "end_char_idx": 33111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "254e8095-883b-4a57-8435-68a9c5ab72ce": {"__data__": {"id_": "254e8095-883b-4a57-8435-68a9c5ab72ce", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78eeca5b-10d3-4bd2-beba-6c6331f17a3c", "node_type": "1", "metadata": {}, "hash": "b626a82891533d1ff6e084df464f43c209feaf87677821664ac7a48bc5a25888", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92dd26ee-30fe-4f33-92f6-5c8b12d4867a", "node_type": "1", "metadata": {}, "hash": "c1844f005498b327546f2f1b69e90a6edba28e8c5d0887c8ddf8a0362788311f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Thus, we can concurrently write total t \u00d7 r bits to t crossbar arrays at a time. The\ntotal latency of ReRAM writes for the bitmap is\ns\nt\u00d7r Lwrite\u2212a\u2212row. We note that all kinds of logical\noperations can be converted into a series of NOR operations. The latency of bitwise operation\n(Lbitwise) for two rows in a crossbar array can be deemed as a constant.", "mimetype": "text/plain", "start_char_idx": 33112, "end_char_idx": 33465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92dd26ee-30fe-4f33-92f6-5c8b12d4867a": {"__data__": {"id_": "92dd26ee-30fe-4f33-92f6-5c8b12d4867a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "254e8095-883b-4a57-8435-68a9c5ab72ce", "node_type": "1", "metadata": {}, "hash": "26de45f28acd0af624973e70819ef63d67681ed0e6791f960bd182c4e20b8dab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "472c7f79-4cfc-43b2-9f2f-0827a8962d77", "node_type": "1", "metadata": {}, "hash": "96f4e7193219b2a230f948aa5e4a042ce22dfdb181c7314af5d4cd06a69719ed", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "At the host side, we\nassume the CPU uses a 256-bit SIMD unit with SSE/AVX for bitwise operation acceleration, and the\nSIMD latency is LSI MD. We can model the benefit of offloading a bitmap-based bitwise operation\nin Equation (3):\nBene f itbitwise = TCPU \u2212TReRAM =\ns\n256 \u00d7 LSI MD \u2212\ns\nt \u00d7 r (Lwrite\u2212a\u2212row + Lbitwise).", "mimetype": "text/plain", "start_char_idx": 33466, "end_char_idx": 33782, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "472c7f79-4cfc-43b2-9f2f-0827a8962d77": {"__data__": {"id_": "472c7f79-4cfc-43b2-9f2f-0827a8962d77", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92dd26ee-30fe-4f33-92f6-5c8b12d4867a", "node_type": "1", "metadata": {}, "hash": "c1844f005498b327546f2f1b69e90a6edba28e8c5d0887c8ddf8a0362788311f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ba2b0c0-174f-4b19-baeb-84ad56a59329", "node_type": "1", "metadata": {}, "hash": "8bbe70d0ba1543498be38c015896321409c4fce435bd705f130a2f76294434fa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(3)\n5\nAUTOMATIC CODE CONVERSION IN THE IR\nWhen an accelerable pattern is found, the IR code should be replaced with the corresponding API\nprovided by CIM accelerators. In this section, we describe the details of code conversion, including\nfinding key variables, resolving data dependencies, and API construction and replacement.\n5.1\nIR Preprocessing\nAs mentioned in Section 4.1, unlike the compiled IR, blocks in the decompiled IR are often out of\norder, and they often have a different code layout of the control flow graph relative to the compiled\nIR code.", "mimetype": "text/plain", "start_char_idx": 33783, "end_char_idx": 34341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ba2b0c0-174f-4b19-baeb-84ad56a59329": {"__data__": {"id_": "7ba2b0c0-174f-4b19-baeb-84ad56a59329", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "472c7f79-4cfc-43b2-9f2f-0827a8962d77", "node_type": "1", "metadata": {}, "hash": "96f4e7193219b2a230f948aa5e4a042ce22dfdb181c7314af5d4cd06a69719ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9437fd62-8259-41e4-8953-57e2c81f337e", "node_type": "1", "metadata": {}, "hash": "dad3757b378021a7676673b8a166e77b892bbc51d8d8cbed60d3af782caa2269", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Since we should find out key variables that indicate the loop structure, we have to reorder\nall blocks of each function call in the IR to construct a control flow graph for loop blocks.\nTo reorder blocks and find out key variables at the same time, we design a special data structure\nto describe blocks in the IR. As shown in Figure 5, a block can be divided into four parts: the\nblock name, predecessors, the block content, and the branch statement. Each block has an unique\nname. The predecessors indicate which blocks (e.g., block1 and block2) can jump to this block.", "mimetype": "text/plain", "start_char_idx": 34342, "end_char_idx": 34912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9437fd62-8259-41e4-8953-57e2c81f337e": {"__data__": {"id_": "9437fd62-8259-41e4-8953-57e2c81f337e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ba2b0c0-174f-4b19-baeb-84ad56a59329", "node_type": "1", "metadata": {}, "hash": "8bbe70d0ba1543498be38c015896321409c4fce435bd705f130a2f76294434fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "745a34b2-9238-4ca9-9859-a19cb95e6d63", "node_type": "1", "metadata": {}, "hash": "8e3ad0d15902f52e6aa483f098366821d43ad198410938c95127848b7cc668a3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "ACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.\n47:12\nH. Jin et al.\nFig. 5. Self-defined data structure of IR blocks.\nFig. 6. Extracting key blocks in a single-level loop structure.\nThe block content contains a majority instructions of a program. The branch statement indicates\nwhich blocks (e.g., for.body and for.end blocks) can be reached from this block.\nFigure 6 illustrates an example of extracting loop blocks in a single-level loop structure.", "mimetype": "text/plain", "start_char_idx": 34913, "end_char_idx": 35431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "745a34b2-9238-4ca9-9859-a19cb95e6d63": {"__data__": {"id_": "745a34b2-9238-4ca9-9859-a19cb95e6d63", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9437fd62-8259-41e4-8953-57e2c81f337e", "node_type": "1", "metadata": {}, "hash": "dad3757b378021a7676673b8a166e77b892bbc51d8d8cbed60d3af782caa2269", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2f806ed-9172-4a16-a400-d4b6c18ebabc", "node_type": "1", "metadata": {}, "hash": "bdff2a9b18b12392a797c5a393f2feb8efd098e584b2a0af326a778e0c305796", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We use a\nlist to record all blocks involved in a function. The linked list maintains all blocks of a loop in order\nassuming all jump conditions are true. As shown in Figure 6, the successor of for.cond is for.body,\nwhile the successor of for.cond1 is for.body3. We first link all condition blocks and body blocks of\na loop in a list if all jump conditions are true, and then extract these loop blocks to construct a\ncontrol flow graph.", "mimetype": "text/plain", "start_char_idx": 35432, "end_char_idx": 35867, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2f806ed-9172-4a16-a400-d4b6c18ebabc": {"__data__": {"id_": "c2f806ed-9172-4a16-a400-d4b6c18ebabc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "745a34b2-9238-4ca9-9859-a19cb95e6d63", "node_type": "1", "metadata": {}, "hash": "8e3ad0d15902f52e6aa483f098366821d43ad198410938c95127848b7cc668a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51021f82-ef0a-4d15-8304-d82b3548a861", "node_type": "1", "metadata": {}, "hash": "aedb4b048b6d09b6299b07a87434e80b12caaca43c5f16f7b6da10b7cca4c4ed", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5.2\nRecognizing Key Variables\nWhen an accelerable pattern is found, we have to find out some key variables within it to complete\nthe code conversion. These variables are related to the interface provided by the CIM accelerator,\nincluding the starting address and the size of the matrix or the vector involved in the accelerable\npattern. Then, the CRE can read the data from main memory and map it to ReRAM crossbar ar-\nrays according to the starting address and the data size. When the CIM accelerator completes the\ncomputation, the CRE also writes the output result to main memory.", "mimetype": "text/plain", "start_char_idx": 35868, "end_char_idx": 36450, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51021f82-ef0a-4d15-8304-d82b3548a861": {"__data__": {"id_": "51021f82-ef0a-4d15-8304-d82b3548a861", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2f806ed-9172-4a16-a400-d4b6c18ebabc", "node_type": "1", "metadata": {}, "hash": "bdff2a9b18b12392a797c5a393f2feb8efd098e584b2a0af326a778e0c305796", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bb4d99d-65e9-4760-9542-7d74f8ee7545", "node_type": "1", "metadata": {}, "hash": "4fff7707b87d524e13ca994fa57459183e2f46bbe5795f885adeffa71edf2113", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For matrix-vector multiplications and matrix-matrix multiplications, we can format these oper-\nations uniformly as follows:\nM1[m \u00d7 n] \u00d7 M2[n \u00d7 k] = M3[m \u00d7 k].\n(4)\nFor an MVM, the variable m is 1, and thus we only need to find two other variables n and k.\nThe MVM is done in a two-level nested loop while the MMM is performed in a three-level nested\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.", "mimetype": "text/plain", "start_char_idx": 36451, "end_char_idx": 36915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bb4d99d-65e9-4760-9542-7d74f8ee7545": {"__data__": {"id_": "3bb4d99d-65e9-4760-9542-7d74f8ee7545", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51021f82-ef0a-4d15-8304-d82b3548a861", "node_type": "1", "metadata": {}, "hash": "aedb4b048b6d09b6299b07a87434e80b12caaca43c5f16f7b6da10b7cca4c4ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "173b70b7-1aac-4dcd-b90f-ac71891ce5c7", "node_type": "1", "metadata": {}, "hash": "105b9db57055a1c8196722ddc68ac332d35bb210c72bd55a5068d221481507ef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures\n47:13\nloop. For an MMM operation, the variable m can be found in the condition block of the outer\nloop.\nThe dimension of a vector (Vdim) can be easily found in loop condition blocks. In general, we\ncan perform a head-to-tail traversal or a tail-to-head traversal to infer the dimension of a vector. In\nthe loop control statement, the loop control variable (i) is initialized, tested, and changed when the\nloop executes.", "mimetype": "text/plain", "start_char_idx": 36916, "end_char_idx": 37414, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "173b70b7-1aac-4dcd-b90f-ac71891ce5c7": {"__data__": {"id_": "173b70b7-1aac-4dcd-b90f-ac71891ce5c7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bb4d99d-65e9-4760-9542-7d74f8ee7545", "node_type": "1", "metadata": {}, "hash": "4fff7707b87d524e13ca994fa57459183e2f46bbe5795f885adeffa71edf2113", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2764cd02-33d1-4750-9efd-8cdc99f927ad", "node_type": "1", "metadata": {}, "hash": "f65120f182368eb9250e89f01c3ee2d661ed2b269c05e52a4c6ec381e9963a25", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We denote its initial value as LoopVarInit and the range control value as RangeVar.\nAll of them appear in the loop.cond block definitely.\nFor the head-to-tail traversal, the initial value of i (LoopVarInit) is usually 0, and should be no\nmore than RangeVar. When the comparison symbol is < or \u2264, the dimension of the vectors should\nbe Ran\u0434eVar or Ran\u0434eVar +1, respectively. For the tail-to-head traversal, the initial value of Loop-\nVarInit is usually larger than or equal to the RangeVar.", "mimetype": "text/plain", "start_char_idx": 37415, "end_char_idx": 37904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2764cd02-33d1-4750-9efd-8cdc99f927ad": {"__data__": {"id_": "2764cd02-33d1-4750-9efd-8cdc99f927ad", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "173b70b7-1aac-4dcd-b90f-ac71891ce5c7", "node_type": "1", "metadata": {}, "hash": "105b9db57055a1c8196722ddc68ac332d35bb210c72bd55a5068d221481507ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2bbf3931-5239-4fdf-814f-e1b7cc3b3e80", "node_type": "1", "metadata": {}, "hash": "79d27d63260eb165fe977a5757db1d83c6ed11d113a3d95beaf4d6286ce5f2bd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Thus, the dimension of the vector becomes\nLoopVarInit + 1 regardless of the comparison symbol (> or \u2265).\nFortunately, the the loop condition block can be easily recognized in the IR. Although the IR can\nbe generated from source codes or a binary executable, the IR blocks corresponding to the loop\ncontrol statement are almost the same. By searching the \u201cicmp\u201d instruction, we can recognize the\njump condition and the RangeVar.", "mimetype": "text/plain", "start_char_idx": 37905, "end_char_idx": 38331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2bbf3931-5239-4fdf-814f-e1b7cc3b3e80": {"__data__": {"id_": "2bbf3931-5239-4fdf-814f-e1b7cc3b3e80", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2764cd02-33d1-4750-9efd-8cdc99f927ad", "node_type": "1", "metadata": {}, "hash": "f65120f182368eb9250e89f01c3ee2d661ed2b269c05e52a4c6ec381e9963a25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "023eaf60-3166-4103-a4f3-26084e42f378", "node_type": "1", "metadata": {}, "hash": "ae5d62b1a3b0f8144f7b5bc30d9b8e26bd27f2d541e82a788133eddcacdc600d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For example, in Pattern 1, \u201c<cmp_1>\u201d in the block loop.condA\nrepresents the comparison statement, and [v_1] represents the RangeVar. For LoopVarInit, we can\nfind it in the statement containing a load instruction, which use LoopVarInit to initialize the i, i.e.,\n[r_1] represents LoopVarInit.", "mimetype": "text/plain", "start_char_idx": 38332, "end_char_idx": 38623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "023eaf60-3166-4103-a4f3-26084e42f378": {"__data__": {"id_": "023eaf60-3166-4103-a4f3-26084e42f378", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2bbf3931-5239-4fdf-814f-e1b7cc3b3e80", "node_type": "1", "metadata": {}, "hash": "79d27d63260eb165fe977a5757db1d83c6ed11d113a3d95beaf4d6286ce5f2bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4348e3f2-7220-4f7a-aa3e-9ccf9abab9d9", "node_type": "1", "metadata": {}, "hash": "788686567019a9ce651058dfa12861a5e9571e5cf5c1205658dd459f01c130e3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Generally, the dimension calculated in the first loop condition block\n(e.g., loop.condA in Pattern 1) corresponds to the size of the input vector, while the dimension\ncalculated in the second loop condition block (e.g., loop.condC in Pattern 1) corresponds to the\nnumber of columns or rows in the matrix.\nTo complete an MVM, the element of the input vector and a row of the matrix should be accessed\nalternately. For convenience, we use the starting address of the vector/matrix as its identifier.", "mimetype": "text/plain", "start_char_idx": 38624, "end_char_idx": 39121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4348e3f2-7220-4f7a-aa3e-9ccf9abab9d9": {"__data__": {"id_": "4348e3f2-7220-4f7a-aa3e-9ccf9abab9d9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "023eaf60-3166-4103-a4f3-26084e42f378", "node_type": "1", "metadata": {}, "hash": "ae5d62b1a3b0f8144f7b5bc30d9b8e26bd27f2d541e82a788133eddcacdc600d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df172485-81b4-4ca1-aa82-75297446df18", "node_type": "1", "metadata": {}, "hash": "f8aa623da3e7a188c4d3e2c3d883779b919a8dd7c17b27e5bbfeec9916be757e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For a\nsingle element of a vector, its address can be represented by a[i] and calculated by startin\u0434_add +\nof f set, where starting_add is the starting address of the vector a, and offset equals to the loop\ncontrol variable (i) multiplied by the size of data.\nAs shown in Pattern 2, the variable %input_addr in the line 15 represents the starting address of\nthe input vector, and %a_1 represents the offset. However, the position of these two variables may\nbe not fixed in the IR.", "mimetype": "text/plain", "start_char_idx": 39122, "end_char_idx": 39601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df172485-81b4-4ca1-aa82-75297446df18": {"__data__": {"id_": "df172485-81b4-4ca1-aa82-75297446df18", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4348e3f2-7220-4f7a-aa3e-9ccf9abab9d9", "node_type": "1", "metadata": {}, "hash": "788686567019a9ce651058dfa12861a5e9571e5cf5c1205658dd459f01c130e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea8821e7-a465-479c-9ede-fcda348b98a2", "node_type": "1", "metadata": {}, "hash": "c8c8482118a48eea4e1ee12a1a31dfada3b8017c77380dfd48147c821126c4b6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Fortunately, we find that %input_addr only appears in the innermost loop\nbody block, while the loop control variable %a_1 is also involved in the loop control block. Thus,\nwe can still distinguish these two variables by tracking their positions in different blocks. Due to\nthe static single assignment form (SSA) property of the IR, a variable may correspond to several\nvariants with different names, but they have the same value.", "mimetype": "text/plain", "start_char_idx": 39602, "end_char_idx": 40032, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea8821e7-a465-479c-9ede-fcda348b98a2": {"__data__": {"id_": "ea8821e7-a465-479c-9ede-fcda348b98a2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df172485-81b4-4ca1-aa82-75297446df18", "node_type": "1", "metadata": {}, "hash": "f8aa623da3e7a188c4d3e2c3d883779b919a8dd7c17b27e5bbfeec9916be757e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63871b78-8b39-4786-a4b2-5743a0192158", "node_type": "1", "metadata": {}, "hash": "d51a1956b1481194f30b69ca608c0b9573d950a1b0d326dc5b62ca4e23511b38", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Thus, we construct an one-to-many\nmapping between the original loop control variable and its variants, and associate the starting\naddress of the vector with its dimension, which corresponds to the loop control variable.\nFor each element in a matrix, e.g., m[i][j], its address can be also calculated by startin\u0434_add +\nof f set. However, unlike the offset in an one-dimensional vector, an additional add operation is\nrequired to calculate the offset here, i.e., addr = f irst_addr +i \u00d7col_size \u00d7data_size +j\u00d7data_size.", "mimetype": "text/plain", "start_char_idx": 40033, "end_char_idx": 40550, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63871b78-8b39-4786-a4b2-5743a0192158": {"__data__": {"id_": "63871b78-8b39-4786-a4b2-5743a0192158", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea8821e7-a465-479c-9ede-fcda348b98a2", "node_type": "1", "metadata": {}, "hash": "c8c8482118a48eea4e1ee12a1a31dfada3b8017c77380dfd48147c821126c4b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4618e8f-9429-46ce-b8a2-aabb65355644", "node_type": "1", "metadata": {}, "hash": "2e3f3f6fc77d5fb8c3840b03f7660357eb88cb8013b650fa827b0c5dd89903ac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Since only two variables can be added in each addition operation, total two add instructions are\nrequired, as shown in the block_D of Pattern 2. Therefore, to find the starting address and the\ndimension of the matrix, we also need to identify and distinguish these three variables from each\nother by checking whether the variable appears in the loop control block. Since the loop control\nvariable (i) corresponding to the rows of the matrix is involved in one more multiplication than the\nloop control variable (j), it is possible to distinguish these two variables that correspond to the rows\nand columns of a matrix.", "mimetype": "text/plain", "start_char_idx": 40551, "end_char_idx": 41169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4618e8f-9429-46ce-b8a2-aabb65355644": {"__data__": {"id_": "d4618e8f-9429-46ce-b8a2-aabb65355644", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63871b78-8b39-4786-a4b2-5743a0192158", "node_type": "1", "metadata": {}, "hash": "d51a1956b1481194f30b69ca608c0b9573d950a1b0d326dc5b62ca4e23511b38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3386c002-adce-43cb-9c7d-552cfa71796b", "node_type": "1", "metadata": {}, "hash": "d7a41c999566dd47be0d4e26f4b620c88bb1808a8d78e4239f8638ff7019f266", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Once the dimensions of the input vector and the matrix are determined,\nwe can easily deduce the dimension of the output vector. However, its starting address still should\nbe determined in the same way as the above.\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.\n47:14\nH. Jin et al.\nFig. 7. APIs designed for an ReRAM-based CIM accelerator [38].", "mimetype": "text/plain", "start_char_idx": 41170, "end_char_idx": 41583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3386c002-adce-43cb-9c7d-552cfa71796b": {"__data__": {"id_": "3386c002-adce-43cb-9c7d-552cfa71796b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4618e8f-9429-46ce-b8a2-aabb65355644", "node_type": "1", "metadata": {}, "hash": "2e3f3f6fc77d5fb8c3840b03f7660357eb88cb8013b650fa827b0c5dd89903ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afde5e1f-9ae6-40f7-b8df-b9f44ec544c9", "node_type": "1", "metadata": {}, "hash": "d27d3601101f4196c8cbcd316b6e13fdd8b1496f18314575956fa7611c8dcb66", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To support general GEMV and GEMM, we need to further identify coefficients before matrices\nand vectors, such \u03b1 \u00d7a[m]\u00d7B[m \u00d7n]+\u03b2 \u00d7c[n]. The \u03b2 appears in the penultimate loop body block\nand multiplies each element of the output vector, and the \u03b1 appears in the innermost loop body\nblock and multiplies the result of the MAC.\nFor a bitmap logical operation, the variable of two bitmaps can be easily found from the loop\nbody of a single-level loop, as shown in Pattern 6.", "mimetype": "text/plain", "start_char_idx": 41584, "end_char_idx": 42051, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afde5e1f-9ae6-40f7-b8df-b9f44ec544c9": {"__data__": {"id_": "afde5e1f-9ae6-40f7-b8df-b9f44ec544c9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3386c002-adce-43cb-9c7d-552cfa71796b", "node_type": "1", "metadata": {}, "hash": "d7a41c999566dd47be0d4e26f4b620c88bb1808a8d78e4239f8638ff7019f266", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38bbec5d-8e77-4b46-8e10-2211408f8c2f", "node_type": "1", "metadata": {}, "hash": "02401536b901d623d0b47ad5f74b93b9beebd0632db151b2361c1a10687b8fcc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Then, we can infer the dimension of the bitmap\nfrom the loop condition block (i.e., the RangeVar).\nIn summary, we can recognize the dimensions of the input vector and the matrix from the\nloop control block, and recognize their starting addresses from the innermost loop body block.\nMoreover, the whole loop structure is also used to establish the mapping between the starting\naddress and the dimension of the input vector or the matrix. Although these variables can be\neasily identified from the IR code, the dimension of matrix or vector is usually only known at\nruntime.", "mimetype": "text/plain", "start_char_idx": 42052, "end_char_idx": 42624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38bbec5d-8e77-4b46-8e10-2211408f8c2f": {"__data__": {"id_": "38bbec5d-8e77-4b46-8e10-2211408f8c2f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afde5e1f-9ae6-40f7-b8df-b9f44ec544c9", "node_type": "1", "metadata": {}, "hash": "d27d3601101f4196c8cbcd316b6e13fdd8b1496f18314575956fa7611c8dcb66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f9b1487-2428-4aa3-9f47-be1d6b3a0d8d", "node_type": "1", "metadata": {}, "hash": "1b791a227ad33779276794751bc43709b168f74561a08a0a9f591560dc3523a5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Thus, we still need code instrumentation techniques to track these variables at runtime,\nand then determine whether the MVM/MMM operation should be run on the CIM accelerator.\n5.3\nConstructing CIM Function Calls\nWe provide three APIs for a typical ReRAM-based CIM simulator [38], as shown in Figure 7.\nCIM_MVM performs \u03b1 \u00d7 a[m] \u00d7 B[m \u00d7 n] + \u03b2 \u00d7 c[n], where \u03b1 and \u03b2 are coefficients, and m and\nn specify the numbers of rows and columns in the matrix, respectively.", "mimetype": "text/plain", "start_char_idx": 42625, "end_char_idx": 43088, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f9b1487-2428-4aa3-9f47-be1d6b3a0d8d": {"__data__": {"id_": "0f9b1487-2428-4aa3-9f47-be1d6b3a0d8d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38bbec5d-8e77-4b46-8e10-2211408f8c2f", "node_type": "1", "metadata": {}, "hash": "02401536b901d623d0b47ad5f74b93b9beebd0632db151b2361c1a10687b8fcc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "312ecd15-f42a-48b9-8a82-6600e5fa0e28", "node_type": "1", "metadata": {}, "hash": "c9399a27f7e969084247d9ceede554348ca4b2532d6a92eabde3a3bfbad464a0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Also, m and n reflect\nthe sizes of the input vector and the output vector, respectively. The symbols a, B, and c repre-\nsent the starting address of the input vector, the matrix, and the output vector, respectively. The\nnotation type represents the type of data in the MVM operation. Similarly, CIM_MMM performs\n\u03b1 \u00d7 A[m \u00d7 n] \u00d7 B[n \u00d7 k] + \u03b2 \u00d7 C[m \u00d7 k], where m, n, and k represent the numbers of rows and\ncolumns in the input/output matrices. \u03b1 and \u03b2 are also coefficients.", "mimetype": "text/plain", "start_char_idx": 43089, "end_char_idx": 43561, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "312ecd15-f42a-48b9-8a82-6600e5fa0e28": {"__data__": {"id_": "312ecd15-f42a-48b9-8a82-6600e5fa0e28", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f9b1487-2428-4aa3-9f47-be1d6b3a0d8d", "node_type": "1", "metadata": {}, "hash": "1b791a227ad33779276794751bc43709b168f74561a08a0a9f591560dc3523a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd24967b-5200-4fcd-8140-16e238331f6d", "node_type": "1", "metadata": {}, "hash": "1beaadbda37330fd6a3f7c1b9af320d2edb04c0394f2b267052a239caf71354a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A, B, and C represent the start-\ning address of the matrices. CIM_Bitmap_logic performs a[m] \u2297b[m] = c[m], where \u2297represents\nBoolean logic operators such as \u2228and \u2227, and a, b, and c represent the starting address of the three\nvectors.\nWhen the sizes of matrices and input vectors are determined, we can construct new function\ncalls according to these APIs defined for CIM accelerators, and then replace the original accelerable\npatterns in the IR.", "mimetype": "text/plain", "start_char_idx": 43562, "end_char_idx": 44008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd24967b-5200-4fcd-8140-16e238331f6d": {"__data__": {"id_": "bd24967b-5200-4fcd-8140-16e238331f6d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "312ecd15-f42a-48b9-8a82-6600e5fa0e28", "node_type": "1", "metadata": {}, "hash": "c9399a27f7e969084247d9ceede554348ca4b2532d6a92eabde3a3bfbad464a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13d5c9a8-541d-4f57-98c4-1dfb90d00fb4", "node_type": "1", "metadata": {}, "hash": "e0e1e7543c77d9af4f9a7a53e1bb42639dd70a38b7599abe83519794da452697", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Moreover, we also need to add the declaration of the newly inserted functions\nin the IR file.\n5.4\nCode Instrumentation\nTo accelerate IR patterns described in Section 4, we need to replace these code snippets with newly\ngenerated CIM function calls. For library functions of MVM and MMM, we can simply replace\nthe original function call. However, we can not do this for other accelerable patterns. Since the\nLLVM IR is based on the SSA form, some variables defined in an IR block may be referenced by\ninstructions in other blocks.", "mimetype": "text/plain", "start_char_idx": 44009, "end_char_idx": 44538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13d5c9a8-541d-4f57-98c4-1dfb90d00fb4": {"__data__": {"id_": "13d5c9a8-541d-4f57-98c4-1dfb90d00fb4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd24967b-5200-4fcd-8140-16e238331f6d", "node_type": "1", "metadata": {}, "hash": "1beaadbda37330fd6a3f7c1b9af320d2edb04c0394f2b267052a239caf71354a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af37f0aa-1182-4f6f-b2b3-2d18bb75f0f9", "node_type": "1", "metadata": {}, "hash": "a04e7da8ebee22d3b3eccaad7549bbf4b32d3af478d36256272e6e863a7d37ba", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Therefore, if we directly delete the accelerable IR blocks, then the\nprogram may not be compiled or gets incorrect results. In this case, to guarantee the correctness\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.\nA Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures\n47:15\nFig. 8. Example of code instrumentation in RCCT.\nof the program, we have to modify all instructions related to the deleted variables.", "mimetype": "text/plain", "start_char_idx": 44539, "end_char_idx": 45040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af37f0aa-1182-4f6f-b2b3-2d18bb75f0f9": {"__data__": {"id_": "af37f0aa-1182-4f6f-b2b3-2d18bb75f0f9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13d5c9a8-541d-4f57-98c4-1dfb90d00fb4", "node_type": "1", "metadata": {}, "hash": "e0e1e7543c77d9af4f9a7a53e1bb42639dd70a38b7599abe83519794da452697", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe7f1b9e-16c1-4b57-8361-5961c1311cd9", "node_type": "1", "metadata": {}, "hash": "9fed64fe13aeff7db71d61f2e84c70219a4a546b5074ee5cf58781f2c65d4e63", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This is a time-\nconsuming work and is difficult for automatic code conversion in the IR.\nOur compilation tool replaces the accelerable IR blocks using a more smart approach. Figure 8\nillustrates an example of code conversion in the IR. We construct and add a new block, and change\nthe original execution logic seamlessly. We do not delete any instructions in the accelerable IR\nblocks so that the following involved instructions can still execute without errors.", "mimetype": "text/plain", "start_char_idx": 45041, "end_char_idx": 45503, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe7f1b9e-16c1-4b57-8361-5961c1311cd9": {"__data__": {"id_": "fe7f1b9e-16c1-4b57-8361-5961c1311cd9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af37f0aa-1182-4f6f-b2b3-2d18bb75f0f9", "node_type": "1", "metadata": {}, "hash": "a04e7da8ebee22d3b3eccaad7549bbf4b32d3af478d36256272e6e863a7d37ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45830ee6-afa4-4456-8ebe-1b7b71ebf662", "node_type": "1", "metadata": {}, "hash": "c0c8159010a867f0abc8e6f2a44c7ef77fc220a7d6242ee62282bdd0afc50959", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "However, we\nchange the execution flow to skip the original accelerable block (i.e., for.body3), and jump to the\nnewly added block (i.e., block_mvm_0) in which the MVM operation is offloaded to the CIM accel-\nerator. We place the newly added block just before the innermost loop body block (i.e., for.body3),\nbecause the loop control variables have been defined and initialized at this time. Thus, we can fully\nutilize these original variables to determine the arguments of the cim_mvm function.", "mimetype": "text/plain", "start_char_idx": 45504, "end_char_idx": 45998, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45830ee6-afa4-4456-8ebe-1b7b71ebf662": {"__data__": {"id_": "45830ee6-afa4-4456-8ebe-1b7b71ebf662", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe7f1b9e-16c1-4b57-8361-5961c1311cd9", "node_type": "1", "metadata": {}, "hash": "9fed64fe13aeff7db71d61f2e84c70219a4a546b5074ee5cf58781f2c65d4e63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6edcc081-a266-4073-ac92-28cf9302839c", "node_type": "1", "metadata": {}, "hash": "2e1c39f2a5d29e9b99df8a00c92b848ca7be52f424dc6c4b616db41684091bc6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "When the\nfunction call returns, it directly jumps to the for. end12 block and the loop is finished.\nSince there may be more than one accelerable pattern of the same type in a single function,\nwe give each newly added block a unique name to avoid conflicts. Moreover, since some patterns\nmay be not accelerated by the CIM accelerator, we add instructions in the beginning of the newly\nadded block to calculate the net benefit according to the sizes of input vectors and matrices, and\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47.", "mimetype": "text/plain", "start_char_idx": 45999, "end_char_idx": 46565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6edcc081-a266-4073-ac92-28cf9302839c": {"__data__": {"id_": "6edcc081-a266-4073-ac92-28cf9302839c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45830ee6-afa4-4456-8ebe-1b7b71ebf662", "node_type": "1", "metadata": {}, "hash": "c0c8159010a867f0abc8e6f2a44c7ef77fc220a7d6242ee62282bdd0afc50959", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd50d53b-a36b-4f88-897b-1ecec7961fb4", "node_type": "1", "metadata": {}, "hash": "debe172ef115fbb4cdc846e871fa61cd899ebbaddc3dacded43048df58c5b723", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Publication date: October 2023.\n47:16\nH. Jin et al.\nFig. 9. Execution flow of the new binary executable.\nthen determine whether the program continues to execute the CIM function call or it jumps back\nto execute the loop on CPUs. In this way, we can achieve conditional computation offloading based\non the performance model.\nLimitation. Although our compilation tool can automatically transfer legacy programs to bi-\nnary executables that can offload some accelerable codes to the CIM accelerator, it still has some\nlimitations.", "mimetype": "text/plain", "start_char_idx": 46566, "end_char_idx": 47093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd50d53b-a36b-4f88-897b-1ecec7961fb4": {"__data__": {"id_": "cd50d53b-a36b-4f88-897b-1ecec7961fb4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6edcc081-a266-4073-ac92-28cf9302839c", "node_type": "1", "metadata": {}, "hash": "2e1c39f2a5d29e9b99df8a00c92b848ca7be52f424dc6c4b616db41684091bc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9afea05f-7150-406a-8608-a0b60157f561", "node_type": "1", "metadata": {}, "hash": "df3d0a41d045e7a3a1595192e28759bb498a03ff466e2efa4891d8fda0998aa5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For example, for several application domains such as graph processing and genome\nanalysis, the original algorithm usually needs some additional transformation to transfer to MVM,\nMMM, or bitmap logical operations. These cases cannot be easily supported by our compilation\ntool. However, since matrices and vectors are widely used in most deep learning and database ap-\nplications, our compilation tool naturally support these applications for automatic code migration.\n6\nEXECUTION OF THE BINARY CODE\nThe modified IR can be further compiled to a binary executable and then is executed in the CRE.\nFigure 9 shows the execution flow of the regenerated binary executable.", "mimetype": "text/plain", "start_char_idx": 47094, "end_char_idx": 47761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9afea05f-7150-406a-8608-a0b60157f561": {"__data__": {"id_": "9afea05f-7150-406a-8608-a0b60157f561", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd50d53b-a36b-4f88-897b-1ecec7961fb4", "node_type": "1", "metadata": {}, "hash": "debe172ef115fbb4cdc846e871fa61cd899ebbaddc3dacded43048df58c5b723", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c15efbc1-a2fb-4250-8b40-62fd9eaa2825", "node_type": "1", "metadata": {}, "hash": "238b93dc98ec2382738758497ffc318b31883b2c8e3fd28a437c2b568b38c2e4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In the beginning, all in-\nstructions and data are stored in main memory. When the program begins to run, the CPU is in\ncharge of dispatching instructions. A CIM instruction is offloaded to the ReRAM accelerator, and\nother instructions are still executed by the CPU. When the CIM instruction is finished, the result\nis returned to the CRE and finally is sent back to main memory. We use a synchronous execution\nmodel to synchronize the computation on CPU and ReRAM. The application calls a CIM API just\nlike a local function and does not continue until the result returns.", "mimetype": "text/plain", "start_char_idx": 47762, "end_char_idx": 48333, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c15efbc1-a2fb-4250-8b40-62fd9eaa2825": {"__data__": {"id_": "c15efbc1-a2fb-4250-8b40-62fd9eaa2825", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9afea05f-7150-406a-8608-a0b60157f561", "node_type": "1", "metadata": {}, "hash": "df3d0a41d045e7a3a1595192e28759bb498a03ff466e2efa4891d8fda0998aa5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "814aaeac-2eaf-4f8a-bd9d-03e74536fda0", "node_type": "1", "metadata": {}, "hash": "38f92b88a93f41dc30c754cf3ffcd85776831a9f51df032e73ac19fafeeb2262", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Thus, there is no data race be-\ntween the CPU and ReRAM-based CIM accelerator. We note that IR instructions associated with\nthe CIM function should be compiled according to the ISA of ReRAM. In our system, the CIM APIs\nare simulated by the ReRAM simulator. Here, we only validate the functional correctness of those\nAPIs. The simulator can recognize those CIM functions and simulate the execution of these APIs.\nHere, we take the CIM_MVM function as an example to illustrate the simulation of CIM APIs.", "mimetype": "text/plain", "start_char_idx": 48334, "end_char_idx": 48836, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "814aaeac-2eaf-4f8a-bd9d-03e74536fda0": {"__data__": {"id_": "814aaeac-2eaf-4f8a-bd9d-03e74536fda0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c15efbc1-a2fb-4250-8b40-62fd9eaa2825", "node_type": "1", "metadata": {}, "hash": "238b93dc98ec2382738758497ffc318b31883b2c8e3fd28a437c2b568b38c2e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "276029eb-491e-44af-b8fb-5090ddced02d", "node_type": "1", "metadata": {}, "hash": "e91dba5783596fbaab4aa455364ef7ff2fe2746793c428743188b66e17844d4f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "When a CIM instruction is recognized by the CRE, it is offloaded to the ReRAM-based CIM accel-\nerator. According to the arguments of CIM_MVM, such as the starting address and the size of the\ndata, the CIM accelerator loads the required data from main memory accordingly. At first, it should\nload the matrix into the ReRAM crossbar array row by row. Because the write latency of ReRAM is\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.", "mimetype": "text/plain", "start_char_idx": 48837, "end_char_idx": 49339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "276029eb-491e-44af-b8fb-5090ddced02d": {"__data__": {"id_": "276029eb-491e-44af-b8fb-5090ddced02d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "814aaeac-2eaf-4f8a-bd9d-03e74536fda0", "node_type": "1", "metadata": {}, "hash": "38f92b88a93f41dc30c754cf3ffcd85776831a9f51df032e73ac19fafeeb2262", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7cfe1085-38de-41b5-99cd-dfbfdb1fa048", "node_type": "1", "metadata": {}, "hash": "37b4f72e8e10da687391e8feaa4a8ad2a5a94ffaaa701c374cd8dfde2fffd108", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures\n47:17\nTable 1. Configuration of the Host CPU and the CIM Accelerator\nCIM Simulator Parameter\nConfiguration\nTechnology (128\u00d7128 @16-bit), 128 arrays per tile\nReRAM 8\u00d7 (128\u00d7128 @2-bit)\nCompute and Write latency/32-bit\n1.8 \u03bcs and 1.0 \u03bcs\nLoad/Store Latency (memory-to-buffer)\n0.", "mimetype": "text/plain", "start_char_idx": 49340, "end_char_idx": 49693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7cfe1085-38de-41b5-99cd-dfbfdb1fa048": {"__data__": {"id_": "7cfe1085-38de-41b5-99cd-dfbfdb1fa048", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "276029eb-491e-44af-b8fb-5090ddced02d", "node_type": "1", "metadata": {}, "hash": "e91dba5783596fbaab4aa455364ef7ff2fe2746793c428743188b66e17844d4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9d0931c-9237-4625-a349-323e48ff9912", "node_type": "1", "metadata": {}, "hash": "9a2dd93b8e52e5c27fc58ca9221a4bcf90780850d77ca3d7906d8253e68f43cc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1 \u03bcs\nCompute Energy/32-bit\n100 fJ\nWrite Energy/32-bit\n200 pJ\nHost CPU Specification\nConfiguration\nIntel-Xeon @2.3 GHz\n20 Cores\nCache\nL1-d/L1-i 32KB, L2 256KB, L3 25MB\nMemory\n64 GB DDR4\nCPU Power (TDP)\n105 W\nrather high, we use two input buffers to hide the latency of loading data from main memory.", "mimetype": "text/plain", "start_char_idx": 49693, "end_char_idx": 49991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9d0931c-9237-4625-a349-323e48ff9912": {"__data__": {"id_": "c9d0931c-9237-4625-a349-323e48ff9912", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7cfe1085-38de-41b5-99cd-dfbfdb1fa048", "node_type": "1", "metadata": {}, "hash": "37b4f72e8e10da687391e8feaa4a8ad2a5a94ffaaa701c374cd8dfde2fffd108", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e358112d-4b6c-4589-ab6c-13424a213b33", "node_type": "1", "metadata": {}, "hash": "cdaf17c45c3fe9dda871dbcba129a65beebe9c12b90af49fcaba4700917c790d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The\ntwo input buffers can be read and written alternately so that one buffer can be used to store the\ndata from main memory while another buffer can be used to write the data to the ReRAM array.\nWe note that a large matrix may be not perfectly fit for the size of crossbar arrays. They should be\npartitioned into multiple sub-matrices and mapped to different crossbar arrays. The CRE performs\nthe titling/blocking of matrices/vectors explicitly to fit the available capacity of crossbar arrays.", "mimetype": "text/plain", "start_char_idx": 49992, "end_char_idx": 50486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e358112d-4b6c-4589-ab6c-13424a213b33": {"__data__": {"id_": "e358112d-4b6c-4589-ab6c-13424a213b33", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9d0931c-9237-4625-a349-323e48ff9912", "node_type": "1", "metadata": {}, "hash": "9a2dd93b8e52e5c27fc58ca9221a4bcf90780850d77ca3d7906d8253e68f43cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "615beb3e-5614-4779-8bad-52087f6013ec", "node_type": "1", "metadata": {}, "hash": "ab7b18bbb8adeae5c406dda9a10fb4337a67637e53a54ef59c639e70cfeef1d7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "When the matrix has been mapped, the CRE loads the input vector from main memory to an input\nbuffer, and then the digital-to-analog converter converts the input vector into the input voltage.\nThe MVM operation can be completed in only one step. At last, the peripheral devices (ADC, S&A)\nare responsible for collecting and converting the output result to digital data. Finally, CRE writes\nback the final result to main memory.", "mimetype": "text/plain", "start_char_idx": 50487, "end_char_idx": 50913, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "615beb3e-5614-4779-8bad-52087f6013ec": {"__data__": {"id_": "615beb3e-5614-4779-8bad-52087f6013ec", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e358112d-4b6c-4589-ab6c-13424a213b33", "node_type": "1", "metadata": {}, "hash": "cdaf17c45c3fe9dda871dbcba129a65beebe9c12b90af49fcaba4700917c790d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99c84cf4-d116-4591-9175-4c43cf62995d", "node_type": "1", "metadata": {}, "hash": "aac7a004a37d21d173ebd110a083a91081b2188188d106b8ba72cb3246aaba99", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "7\nSIMULATION AND EVALUATION\nIn this section, we use the Polybench/C benchmark [42] and other micro-benchmarks to verify\nthe effectiveness of our complication tool\u2013RCCT. We also compare RCCT with the state-of-the-art\nTDO-CIM [47].\n7.1\nExperimental Setup\nWe use a simulation framework MHSim [38] to set up a heterogeneous computing system com-\nposed of CPUs and CIM accelerators. We simulate two Intel Xeon processors with total 20 cores\nbased on a cycle-accurate simulator ZSim [43].", "mimetype": "text/plain", "start_char_idx": 50914, "end_char_idx": 51396, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99c84cf4-d116-4591-9175-4c43cf62995d": {"__data__": {"id_": "99c84cf4-d116-4591-9175-4c43cf62995d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "615beb3e-5614-4779-8bad-52087f6013ec", "node_type": "1", "metadata": {}, "hash": "ab7b18bbb8adeae5c406dda9a10fb4337a67637e53a54ef59c639e70cfeef1d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c718b15-d4f3-49ea-9a12-924fcdaf6ce8", "node_type": "1", "metadata": {}, "hash": "d9af641aa209abcab87e590d0bed83fab60bcff86503078b0907ab75611f3fc7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The ReRAM accelerator is simulated by Neu-\nroSim [16] using a circuit-level macro model. Since Zsim can track specific instructions for function\ncalls, we can directly offload library functions to ReRAM crossbar arrays.\nTable 1 summarizes the configuration of CPUs and CIM accelerators. We use 128\u00d7128 ReRAM\ncrossbar arrays with 16-bit data accuracy. Each ReRAM cell can store a 2-bit number [51] with low\nlatency and high accuracy.", "mimetype": "text/plain", "start_char_idx": 51397, "end_char_idx": 51829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c718b15-d4f3-49ea-9a12-924fcdaf6ce8": {"__data__": {"id_": "7c718b15-d4f3-49ea-9a12-924fcdaf6ce8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99c84cf4-d116-4591-9175-4c43cf62995d", "node_type": "1", "metadata": {}, "hash": "aac7a004a37d21d173ebd110a083a91081b2188188d106b8ba72cb3246aaba99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28ded052-665f-45cb-a075-5352cda84962", "node_type": "1", "metadata": {}, "hash": "99dc13a448b4e5cd8498ec16434d1d24a07b30fb49e5c29ed4a23051ac9e38a2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Thus, 8 adjacent columns are used to represent a 16-bit number accord-\ning to IEEE-754 half-precision floating-point format. To reduce the die area of ReRAM, 8 columns\nshare a 9-bit ADC to convert the analog value to a digital value. The final result is computed by a\nweighted sum of 8 columns. For bitwise Boolean logic operations, we refer to a PIM architecture\ncalled Pinatubo [35] to simulate its computation latency and energy consumption.", "mimetype": "text/plain", "start_char_idx": 51830, "end_char_idx": 52274, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28ded052-665f-45cb-a075-5352cda84962": {"__data__": {"id_": "28ded052-665f-45cb-a075-5352cda84962", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c718b15-d4f3-49ea-9a12-924fcdaf6ce8", "node_type": "1", "metadata": {}, "hash": "d9af641aa209abcab87e590d0bed83fab60bcff86503078b0907ab75611f3fc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e762895a-426d-4d44-9a17-154b0151c079", "node_type": "1", "metadata": {}, "hash": "332e91a871a50865584a264fd2a608dfb6daa54df11ad7f404349f25892f644d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The energy and latency models of the crossbar array and peripheral circuits are based on Neu-\nrosim, as shown in Table 1. The simulator also takes into account the latency and energy consump-\ntion of data movement between main memory and CIM accelerators. We use thermal design power\n(TDP) to estimate the energy consumption of CPUs [30].\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.\n47:18\nH. Jin et al.\nTable 2.", "mimetype": "text/plain", "start_char_idx": 52275, "end_char_idx": 52758, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e762895a-426d-4d44-9a17-154b0151c079": {"__data__": {"id_": "e762895a-426d-4d44-9a17-154b0151c079", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28ded052-665f-45cb-a075-5352cda84962", "node_type": "1", "metadata": {}, "hash": "99dc13a448b4e5cd8498ec16434d1d24a07b30fb49e5c29ed4a23051ac9e38a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ba8ee52-d15a-4ce9-bec0-c8a43e165b4f", "node_type": "1", "metadata": {}, "hash": "8b0cab5f7114131fcd63ecb71dbc1b1e120d0b668f60abb5da79dd24cd5b56cf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Benchmarks\nCategory\nProgram\nDescription\nData Size\nPolybench/C\nmvt\nMatrix-Vector Product and Transpose\n4,000\u00d74,000\n3mm\n3 Matrix Multiplications (A\u00d7B)\u00d7(C\u00d7D)\n1,024\u00d71,024\ngemm\nMatrix-multiply ( C=\u03b1 \u00d7A\u00d7B+\u03b2 \u00d7C)\n1,024\u00d71,024\ngemver\nVector-multiply and Matrix-Vector Product\n1,024\u00d71,024\ngesummv\nScalar, Vector and Matrix Multiplication\n1,024\u00d71,", "mimetype": "text/plain", "start_char_idx": 52759, "end_char_idx": 53094, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ba8ee52-d15a-4ce9-bec0-c8a43e165b4f": {"__data__": {"id_": "5ba8ee52-d15a-4ce9-bec0-c8a43e165b4f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e762895a-426d-4d44-9a17-154b0151c079", "node_type": "1", "metadata": {}, "hash": "332e91a871a50865584a264fd2a608dfb6daa54df11ad7f404349f25892f644d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb1b4cf7-a033-4e60-aa94-ff4f9541db7b", "node_type": "1", "metadata": {}, "hash": "5db4c7852ab3e10f7427ca1f5b8f837ea5bcee90a611da9c78ec8436ba219026", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "024\nCblas\nsgemm\nMatrix-multiply using multi-threads\n1,024\u00d71,024\nBitmap\nbitmap_and\nlogical AND operation for two bitmaps\n220 bits\nMLP\nMLP_sort\nhandwritten digit classification\nMNIST\nCryptography\nMceliece\nasymmetric encryption algorithm\n1 MB\nSPEC CPU\n2017 (FP)\npovray\nRay tracing with matrix/vector operations\ntest\nlbm\nFluid dynamics with matrix/vector operations\ntest\nReal Bitwise\nOperations\nBFS [11]\nbitmap-based BFS for graph processing\nAmazon-2008\nFastbit", "mimetype": "text/plain", "start_char_idx": 53094, "end_char_idx": 53551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb1b4cf7-a033-4e60-aa94-ff4f9541db7b": {"__data__": {"id_": "cb1b4cf7-a033-4e60-aa94-ff4f9541db7b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ba8ee52-d15a-4ce9-bec0-c8a43e165b4f", "node_type": "1", "metadata": {}, "hash": "8b0cab5f7114131fcd63ecb71dbc1b1e120d0b668f60abb5da79dd24cd5b56cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37917e4f-cf59-47fd-aa5f-4e4db1ba43a5", "node_type": "1", "metadata": {}, "hash": "527dcc1ee8b4eefda348fa60fbd67b416e69604f36a8be7935c8d568b2839997", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[50]\nbitmap-based database operations\nSTAR [6]\nWe use several benchmarks to evaluate the benefit gained from CIM accelerators, as shown\nin Table 2. The Polybench/C benchmarks contain different combinations of MVM, MMM, and\nvector multiplication operations. Cblas_sgemm and Bitmap_and are particularly developed to\nverify the effectiveness of accelerating library functions, bitmap logical operations, respectively.\nCblas_sgemm performs the MMM operation using the CBLAS library. Bitmap_and performs\nthe logical \u201cAND\u201d operation for two bitmaps.", "mimetype": "text/plain", "start_char_idx": 53552, "end_char_idx": 54095, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37917e4f-cf59-47fd-aa5f-4e4db1ba43a5": {"__data__": {"id_": "37917e4f-cf59-47fd-aa5f-4e4db1ba43a5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb1b4cf7-a033-4e60-aa94-ff4f9541db7b", "node_type": "1", "metadata": {}, "hash": "5db4c7852ab3e10f7427ca1f5b8f837ea5bcee90a611da9c78ec8436ba219026", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef343a51-c03b-45ac-a28b-d9a6c8a0458f", "node_type": "1", "metadata": {}, "hash": "f54b8658f5b86152a124ebcc59c784d5c33424cad0918340c25aaebac9aa77bd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Apart from the above micro-benchmarks, we\nalso evaluate several real-world macro-benchmarks. MNIST_mlp_sort is a handwritten digit\nclassification application based on three-layer multi-layer perceptron (MLP) neural network.\nMceliece_decrypt is an asymmetric encryption algorithm involving in heavy matrix multiplica-\ntion operations. Povray and lbm are selected from SPEC CPU 2017 (floating point) benchmark suite,\nand they all contain heavy matrix/vector operations. BFS [11] and Fastbit [50] are bitmap-based\ngraph processing and database applications, respectively.", "mimetype": "text/plain", "start_char_idx": 54096, "end_char_idx": 54664, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef343a51-c03b-45ac-a28b-d9a6c8a0458f": {"__data__": {"id_": "ef343a51-c03b-45ac-a28b-d9a6c8a0458f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37917e4f-cf59-47fd-aa5f-4e4db1ba43a5", "node_type": "1", "metadata": {}, "hash": "527dcc1ee8b4eefda348fa60fbd67b416e69604f36a8be7935c8d568b2839997", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ce86b6c-571b-4270-815a-a28425a2eea7", "node_type": "1", "metadata": {}, "hash": "b0418406d4fa6e7104f9ce3768e45867a62f75345b2fc1909f329b7351c1978a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "These real-world applications are used\nto verify the effectiveness of our compilation tool for computation offloading in ReRAM-based\nCIM architectures.\n7.2\nAccuracy of RCCT\nIn this section, we evaluate the recognition accuracy of RCCT and TDO-CIM [47] for both com-\npiled and decompiled IR patterns in different benchmarks. At first, we analyze the source codes\nof programs in Table 2 manually, and count the total number of four accelerable IR patterns and\nuse it as a baseline.", "mimetype": "text/plain", "start_char_idx": 54665, "end_char_idx": 55144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ce86b6c-571b-4270-815a-a28425a2eea7": {"__data__": {"id_": "6ce86b6c-571b-4270-815a-a28425a2eea7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef343a51-c03b-45ac-a28b-d9a6c8a0458f", "node_type": "1", "metadata": {}, "hash": "f54b8658f5b86152a124ebcc59c784d5c33424cad0918340c25aaebac9aa77bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc4a02e8-2566-4e40-abb9-0ba20265675a", "node_type": "1", "metadata": {}, "hash": "9c0272ab1e881ad80e49770dfcacf58b482ed266ee6015777062e2314ce94d9b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For programs with source codes, we count the total number of accelerable\npatterns recognized by our compilation tool in the compiled IR, and then calculate the recognition\naccuracy, i.e., the ratio of recognized patterns in the compiled IR to the total number of accelerable\npatterns in the program. To evaluate the recognition accuracy of decompiled IR patterns, we first\ncompile the source code into a binary executable, and then decompile this binary executable into\nthe LLVM IR. At last, we calculate the recognition accuracy of decompiled IR patterns using the\nsame way similar to the compiled IR.", "mimetype": "text/plain", "start_char_idx": 55145, "end_char_idx": 55747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc4a02e8-2566-4e40-abb9-0ba20265675a": {"__data__": {"id_": "bc4a02e8-2566-4e40-abb9-0ba20265675a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ce86b6c-571b-4270-815a-a28425a2eea7", "node_type": "1", "metadata": {}, "hash": "b0418406d4fa6e7104f9ce3768e45867a62f75345b2fc1909f329b7351c1978a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "715d2610-c061-497e-bc9d-b973c002d5f7", "node_type": "1", "metadata": {}, "hash": "66444dd957d1369620a43dc1af0b7b2c6b4338df7bb1e51b675916c2906f08a4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Since a higher level optimization of the GCC compiler leads\nto a lower recognition accuracy for the decompiled IR, we adopt the O1 optimization to generate\nall executable binaries.\nTable 3 shows the recognition accuracy of RCCT and TDO-CIM. Both TDO-CIM and RCCT\nachieve 100% recognition accuracy for compiled IRs of Polybench/C benchmarks, because there\nare only a few and very simple computation kernels in each program\u2019s source code. However,\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47.", "mimetype": "text/plain", "start_char_idx": 55748, "end_char_idx": 56277, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "715d2610-c061-497e-bc9d-b973c002d5f7": {"__data__": {"id_": "715d2610-c061-497e-bc9d-b973c002d5f7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc4a02e8-2566-4e40-abb9-0ba20265675a", "node_type": "1", "metadata": {}, "hash": "9c0272ab1e881ad80e49770dfcacf58b482ed266ee6015777062e2314ce94d9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f51fcedb-a6e4-42b1-9a11-8b4f128b36f4", "node_type": "1", "metadata": {}, "hash": "545735be79fc6fa88823509a84f9ca754823db386ee4832bfc1434bfc09e6e1f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Publication date: October 2023.\nA Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures\n47:19\nTable 3. Recognition Accuracy of Accelerable IR Patterns\nBenchmarks\nCompiled IR\nDecompiled IR\nTDO-CIM\nRCCT\nTDO-CIM\nRCCT\nPolybench_mvt\n100%\n100%\nN.A.\n100%\nPolybench_3mm\n100%\n100%\nN.A.\n100%\nPolybench_gemm\n100%\n100%\nN.A.\n100%\nPolybench_gemver\n100%\n100%\nN.A.", "mimetype": "text/plain", "start_char_idx": 56278, "end_char_idx": 56649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f51fcedb-a6e4-42b1-9a11-8b4f128b36f4": {"__data__": {"id_": "f51fcedb-a6e4-42b1-9a11-8b4f128b36f4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "715d2610-c061-497e-bc9d-b973c002d5f7", "node_type": "1", "metadata": {}, "hash": "66444dd957d1369620a43dc1af0b7b2c6b4338df7bb1e51b675916c2906f08a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "627382b4-1d6a-4334-8028-bfe8ef264439", "node_type": "1", "metadata": {}, "hash": "6960907ac262ca3c4520f29682fbd84534feff217fe25e2fcfdad00c793edb40", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "100%\nPolybench_gesummv\n100%\n100%\nN.A.\n100%\nCblas_sgemm\nN.A.\n100%\nN.A.\n100%\nBitmap_and\nN.A.\n100%\nN.A.\n100%\nBFS\nN.A.\n100%\nN.A.\n100%\nFastbit\nN.A.\n100%\nN.A.\n100%\nMNIST_mlp_sort\n82.4%\n97.8%\nN.A.\n52.7%\nMceliece_decrypt\n87.5%\n98.5%\nN.A.", "mimetype": "text/plain", "start_char_idx": 56650, "end_char_idx": 56879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "627382b4-1d6a-4334-8028-bfe8ef264439": {"__data__": {"id_": "627382b4-1d6a-4334-8028-bfe8ef264439", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f51fcedb-a6e4-42b1-9a11-8b4f128b36f4", "node_type": "1", "metadata": {}, "hash": "545735be79fc6fa88823509a84f9ca754823db386ee4832bfc1434bfc09e6e1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4be65253-c5c1-49d7-a368-c4b338318f7b", "node_type": "1", "metadata": {}, "hash": "9953d0b97045b5c610540a0e033b1309f3d9edc43da9be0f2ff766a2f44f1cd5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "68.6%\nSPECfp_povary\n72.6%\n95.7%\nN.A.\n64.8%\nSPECfp_lbm\n62.6%\n96.4%\nN.A.\n54.3%\nTDO-CIM does not support the recognition of library functions and bitmap logical operations,\nsuch as Cblas_sgemm, bitmap_and, bitmap-based BFS and Fastbit, while RCCT can recognize\nthese accelerable patterns with 100% accuracy.", "mimetype": "text/plain", "start_char_idx": 56880, "end_char_idx": 57184, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4be65253-c5c1-49d7-a368-c4b338318f7b": {"__data__": {"id_": "4be65253-c5c1-49d7-a368-c4b338318f7b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "627382b4-1d6a-4334-8028-bfe8ef264439", "node_type": "1", "metadata": {}, "hash": "6960907ac262ca3c4520f29682fbd84534feff217fe25e2fcfdad00c793edb40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17ffb171-d7a1-4ff9-9478-ae0082cac502", "node_type": "1", "metadata": {}, "hash": "13b0e525eb5576fc4994609e2c55a0dcc62d988d076820a93eaf791b9455ca39", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For more complex real-world applications such\nas MNIST_mlp_sort, Mceliece_decrypt, SPECfp_povary, and SPECfp_lbm, TDO-CIM can only\nachieve 62.6%\u223c87.5% accuracy, because it omits some library functions and bitmap logical oper-\nations that can be offloaded to CIM accelerators. RCCT can recognize most accelerable patterns\nexcept that MVM or MMM operations are tightly coupled with other complex operations with\nhigh data dependency.", "mimetype": "text/plain", "start_char_idx": 57185, "end_char_idx": 57616, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17ffb171-d7a1-4ff9-9478-ae0082cac502": {"__data__": {"id_": "17ffb171-d7a1-4ff9-9478-ae0082cac502", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4be65253-c5c1-49d7-a368-c4b338318f7b", "node_type": "1", "metadata": {}, "hash": "9953d0b97045b5c610540a0e033b1309f3d9edc43da9be0f2ff766a2f44f1cd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c68b5b8b-e902-47cd-a80a-1fbc0110520b", "node_type": "1", "metadata": {}, "hash": "c48453d64857c81235cc467eaff240f251c73a0888e18e754398249d6570a811", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Since the decompiling technology is not full-blown, the decompiled IR gen-\nerated by the decompiling tool\u2013McSema [3] is usually significantly different with the compiled IR,\nincreasing the complexity of pattern recognition. Thus, RCCT can only achieve 52.7%\u223c68.6% ac-\ncuracy for decompiled IR of these real-world applications. Overall, TDO-CIM can only accelerate\nMVM and MMM operations from the source code, and cannot recognize the accelerable patterns\nfrom decompiled LLVM IR of executable binary.", "mimetype": "text/plain", "start_char_idx": 57617, "end_char_idx": 58117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c68b5b8b-e902-47cd-a80a-1fbc0110520b": {"__data__": {"id_": "c68b5b8b-e902-47cd-a80a-1fbc0110520b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17ffb171-d7a1-4ff9-9478-ae0082cac502", "node_type": "1", "metadata": {}, "hash": "13b0e525eb5576fc4994609e2c55a0dcc62d988d076820a93eaf791b9455ca39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc0512d4-eb35-4c5f-80cc-caec67869c9a", "node_type": "1", "metadata": {}, "hash": "3bacdd9db56119ffc147664d30c13c94ba8071536d58191ac64bf064615a2f5c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In contrast, RCCT is able to accelerate other typ-\nical patterns such as library functions and bitmap logic operations from both source codes and\nbinary executables.\n7.3\nValidation of Performance Models\nWe also evaluate the effectiveness of the performance model proposed in Section 4.6. We measure\nthe performance speedup of RCCT on MVM operations for different matrix sizes with/without the\ncomputation offloading model, as shown in Figure 10. Since all matrices are square arrays, we only\nuse the number of rows to represent their sizes.", "mimetype": "text/plain", "start_char_idx": 58118, "end_char_idx": 58658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc0512d4-eb35-4c5f-80cc-caec67869c9a": {"__data__": {"id_": "cc0512d4-eb35-4c5f-80cc-caec67869c9a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c68b5b8b-e902-47cd-a80a-1fbc0110520b", "node_type": "1", "metadata": {}, "hash": "c48453d64857c81235cc467eaff240f251c73a0888e18e754398249d6570a811", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c731faed-4857-4b22-97c8-a79b7fc5542b", "node_type": "1", "metadata": {}, "hash": "6b82ec72dfe50c9bdd7ae0f6d21251648bdf7f76b30137e7b288268785124cc6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Without our performance model, we can find that\nthe computation offloading even lead to a performance slowdown for small matrices compared\nwith the CPU-only system. For a larger matrix, our computation offloading model can achieve\nhigher performance speedup.\nTo verify the effectiveness of our performance models, we evaluate the performance speedup\nof RCCT (without models), RCCT (with models), and TDO-CIM for both micro and macro bench-\nmarks, as shown in Figure 11.", "mimetype": "text/plain", "start_char_idx": 58659, "end_char_idx": 59128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c731faed-4857-4b22-97c8-a79b7fc5542b": {"__data__": {"id_": "c731faed-4857-4b22-97c8-a79b7fc5542b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc0512d4-eb35-4c5f-80cc-caec67869c9a", "node_type": "1", "metadata": {}, "hash": "3bacdd9db56119ffc147664d30c13c94ba8071536d58191ac64bf064615a2f5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afdfe7a3-0f5f-49e7-bf57-11645500f841", "node_type": "1", "metadata": {}, "hash": "32c2cecc1c572d45a04197917f4d546f64160e6d52f838bd614a623154af7987", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Since the Polybench/C benchmark suite contains only very simple\ncomputation kernels such as MVM, MMM, and vector multiplications, and all involved matrices\nare very large, the CIM accelerators are well-suited to offload these kernels that involve heavy\nmatrix/vector operations. Thus, both TDO-CIM and RCCT achieve similar performance speedup\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.\n47:20\nH. Jin et al.\nFig. 10.", "mimetype": "text/plain", "start_char_idx": 59129, "end_char_idx": 59616, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afdfe7a3-0f5f-49e7-bf57-11645500f841": {"__data__": {"id_": "afdfe7a3-0f5f-49e7-bf57-11645500f841", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c731faed-4857-4b22-97c8-a79b7fc5542b", "node_type": "1", "metadata": {}, "hash": "6b82ec72dfe50c9bdd7ae0f6d21251648bdf7f76b30137e7b288268785124cc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13c63c38-4afd-4575-b896-d877cf9a9127", "node_type": "1", "metadata": {}, "hash": "08f458209d8ec35c2bd9b849385420144e8dd78fd12292b818c87c4cea3ac72f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Performance speedup of MVMs sensitive to the size of MVM.\nFig. 11. Performance speedup with/without performance models, all normalized to the CPU-only system.\neven without using our performance models. However, for other real-world applications such as\nSPECfp_povary, and SPECfp_lbm, RCCT with performance models deliver higher-performance\nspeedup, because it only offloads large-scale MVM/MMM operations to the CIM accelerators.", "mimetype": "text/plain", "start_char_idx": 59617, "end_char_idx": 60046, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13c63c38-4afd-4575-b896-d877cf9a9127": {"__data__": {"id_": "13c63c38-4afd-4575-b896-d877cf9a9127", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afdfe7a3-0f5f-49e7-bf57-11645500f841", "node_type": "1", "metadata": {}, "hash": "32c2cecc1c572d45a04197917f4d546f64160e6d52f838bd614a623154af7987", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbc28024-fbaf-4bd5-b331-9ffa2063d3d9", "node_type": "1", "metadata": {}, "hash": "62d8569e3516b8eb09658fff63ae988c9deb43db39431b29e631bc5f3a8d238f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "With-\nout our performance models, TDO-CIM and RCCT (w/o models) performs computation offloading\nfor all MVM, MMM, and bitmap logical operations indiscriminately, and thus may partially offset\nthe benefit of computation offloading for large-sized matrices/vectors.\n7.4\nPerformance Evaluation\nWe port all legacy programs in Table 2 to the simulated heterogeneous computing system using our\ncompilation tool. Both source codes and binary files of these programs are converted into the final\nbinary execuatables that can be run in CIM accelerators.", "mimetype": "text/plain", "start_char_idx": 60047, "end_char_idx": 60591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbc28024-fbaf-4bd5-b331-9ffa2063d3d9": {"__data__": {"id_": "bbc28024-fbaf-4bd5-b331-9ffa2063d3d9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13c63c38-4afd-4575-b896-d877cf9a9127", "node_type": "1", "metadata": {}, "hash": "08f458209d8ec35c2bd9b849385420144e8dd78fd12292b818c87c4cea3ac72f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d8995fe-d161-459b-8043-ef90a0bd76b2", "node_type": "1", "metadata": {}, "hash": "1be06d7d423598b4fe57cab4b95aebeee9099c3642695dd09a54aa5f027bf7f6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "All experimental results are normalized\nwith a baseline in which all original binary files execute in a CPU-only system.\nFigure 12(a) shows that the CIM accelerator can improve the application performance by\nup to 51\u00d7 compared with the CPU-only system. Polybench_3mm shows the highest speedup,\nbecause MMM operations spend the most of its execution time. Moreover, the decompilation\ncase shows similar performance speedup to the compilation case, implying that our compilation\ntool can effectively identify those MMM operations and offload them to CIM accelerators.", "mimetype": "text/plain", "start_char_idx": 60592, "end_char_idx": 61157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d8995fe-d161-459b-8043-ef90a0bd76b2": {"__data__": {"id_": "9d8995fe-d161-459b-8043-ef90a0bd76b2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbc28024-fbaf-4bd5-b331-9ffa2063d3d9", "node_type": "1", "metadata": {}, "hash": "62d8569e3516b8eb09658fff63ae988c9deb43db39431b29e631bc5f3a8d238f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52a7d3ee-c29b-446c-9f34-d35a5008d881", "node_type": "1", "metadata": {}, "hash": "c4fd856cdee7e543cb291da8afd21656b2b3ed6dc540086cf129141f2a339fa6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The\nCblas_sgemm shows the lowest speedup, because Clbas library functions can use multiple threads\nto take full advantage of the high-performance CPUs. For bitmap-based graph and database\napplications (BFS and Fastbit), because their codes are manually optimized to fully leverage the\nbitwise logical APIs of the CIM accelerator, RCCT can achieve similar performance speedup\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.", "mimetype": "text/plain", "start_char_idx": 61158, "end_char_idx": 61648, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52a7d3ee-c29b-446c-9f34-d35a5008d881": {"__data__": {"id_": "52a7d3ee-c29b-446c-9f34-d35a5008d881", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d8995fe-d161-459b-8043-ef90a0bd76b2", "node_type": "1", "metadata": {}, "hash": "1be06d7d423598b4fe57cab4b95aebeee9099c3642695dd09a54aa5f027bf7f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4dd37d7c-a5f5-4964-9439-703ead11d469", "node_type": "1", "metadata": {}, "hash": "9aa286b750892d8b6e7b3ddc83e8d97ee9eae7fdc74ef3b9b13fd7e33625d547", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures\n47:21\nFig. 12. Performance speedups and energy efficiency improved by CIM accelerators, all normalized to the\nCPU-only system.\nfor both compiled and decompiled IRs. Other programs show moderate performance speedup,\nbecause the number of accelerable patterns in those program are very limited. We also find that\nthe performance speedup of the decompiled IR is not as high as the compiled IR. The reason\nmainly stems from two-folds.", "mimetype": "text/plain", "start_char_idx": 61649, "end_char_idx": 62158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4dd37d7c-a5f5-4964-9439-703ead11d469": {"__data__": {"id_": "4dd37d7c-a5f5-4964-9439-703ead11d469", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52a7d3ee-c29b-446c-9f34-d35a5008d881", "node_type": "1", "metadata": {}, "hash": "c4fd856cdee7e543cb291da8afd21656b2b3ed6dc540086cf129141f2a339fa6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d9832d2-c9d8-4bf9-afa7-5b685f590008", "node_type": "1", "metadata": {}, "hash": "3d918bb57d708fea5d99e0ded56753f732644cdea74050d055f5c552d75086d1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "First, the reverse engineering tool\u2014McSema [3]\u2014can significantly\nincrease the number of instructions in the decompiled IR by several times and thus offset the\nbenefit from the CIM accelerator. Second, the recognition accuracy of accelerable patterns in the\ndecompiled IR also becomes low, especially for real-world applications, and thus cannot offload\nsufficient accelerable patterns to the CIM accelerator. We believe our approach can still achieve\nsufficient performance improvement when the reverse engineering tool is fully developed and\noptimized [2].", "mimetype": "text/plain", "start_char_idx": 62159, "end_char_idx": 62716, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d9832d2-c9d8-4bf9-afa7-5b685f590008": {"__data__": {"id_": "7d9832d2-c9d8-4bf9-afa7-5b685f590008", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dd37d7c-a5f5-4964-9439-703ead11d469", "node_type": "1", "metadata": {}, "hash": "9aa286b750892d8b6e7b3ddc83e8d97ee9eae7fdc74ef3b9b13fd7e33625d547", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffef8535-8176-45dd-a5b9-d413f0dda380", "node_type": "1", "metadata": {}, "hash": "e22d6a7ba9efa17ef8c14d87ea891b8b675357aa8e8602dbb084cdcd4b8ac111", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Figure 12(b) shows the energy efficiency of the CIM accelerator can be improved by up to 87\u00d7\non average compared with the CPU-only system. For Polybench_3mm, we can find the energy\nefficiency is significantly improved by 309 times, because the ReRAM-based CIM accelerator is\nparticular energy efficient for massive MMM operations. Overall, the energy efficiency shows a\nsimilar trend relative to the performance speedup.", "mimetype": "text/plain", "start_char_idx": 62717, "end_char_idx": 63137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffef8535-8176-45dd-a5b9-d413f0dda380": {"__data__": {"id_": "ffef8535-8176-45dd-a5b9-d413f0dda380", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d9832d2-c9d8-4bf9-afa7-5b685f590008", "node_type": "1", "metadata": {}, "hash": "3d918bb57d708fea5d99e0ded56753f732644cdea74050d055f5c552d75086d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc1b8cf2-ebd7-40c3-9605-af705de9b9b0", "node_type": "1", "metadata": {}, "hash": "69b33b57bcbb437332f1cfe74d2d043d0f65bb595f53b61df2d4b3bd54ab5c44", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The programs converted from executable binary\ncodes generally show lower energy efficiency than programs generated from source codes, because\nthe reverse engineering using McSema [3] significantly increases the number of instructions in the\ndecompiled IR and degrades the recognition accuracy of accelerable patterns, and thus it degrades\nthe energy efficiency.\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.\n47:22\nH. Jin et al.", "mimetype": "text/plain", "start_char_idx": 63138, "end_char_idx": 63635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc1b8cf2-ebd7-40c3-9605-af705de9b9b0": {"__data__": {"id_": "cc1b8cf2-ebd7-40c3-9605-af705de9b9b0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffef8535-8176-45dd-a5b9-d413f0dda380", "node_type": "1", "metadata": {}, "hash": "e22d6a7ba9efa17ef8c14d87ea891b8b675357aa8e8602dbb084cdcd4b8ac111", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9893d134-8dbb-4656-9e92-45ac7b4fc86e", "node_type": "1", "metadata": {}, "hash": "534caee18bc486da8bf7b9e3eeef3d9fdec76db8ea7f6fd88291d30f0a120336", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "8\nRELATED WORK\nThere have been only a few studies on the compilation tool for ReRAM-based CIM accelerators.\nTDO-CIM [47] uses the polyhedron tool to identify MVM and MMM operations from compiled\nLLVM IRs, and offloads them to CIM accelerators. However, it cannot handle the IR generated from\ndecompilation. It also does not recognize/offload bitmap logical operations and library functions.\nChakraborty et al. propose a similar work based on LLVM IR to migrate legacy programs to\nCIM accelerators [13].", "mimetype": "text/plain", "start_char_idx": 63636, "end_char_idx": 64138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9893d134-8dbb-4656-9e92-45ac7b4fc86e": {"__data__": {"id_": "9893d134-8dbb-4656-9e92-45ac7b4fc86e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc1b8cf2-ebd7-40c3-9605-af705de9b9b0", "node_type": "1", "metadata": {}, "hash": "69b33b57bcbb437332f1cfe74d2d043d0f65bb595f53b61df2d4b3bd54ab5c44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ee982fc-1e1e-4676-b822-26a01c688c4e", "node_type": "1", "metadata": {}, "hash": "23d699ee928d3087e49b6aaa611ab19be91db4f9e772a749b65eddb70cc639c2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "They transform accelerable computation kernels into a Boolean Decision\nDiagram (BDD), and then map the BDD to ReRAM crossbar array. However, the supported\nkernels are simple linear operations and can only be written in C language. Also, it cannot support\nnonlinear arithmetic operations such as multiplication and division. Ambrosi et al., implement\nan end-to-end software stack for a ReRAM-based accelerator [8]. The software stack consists of\ninterpreter, compiler, and driver, etc.", "mimetype": "text/plain", "start_char_idx": 64139, "end_char_idx": 64623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ee982fc-1e1e-4676-b822-26a01c688c4e": {"__data__": {"id_": "6ee982fc-1e1e-4676-b822-26a01c688c4e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9893d134-8dbb-4656-9e92-45ac7b4fc86e", "node_type": "1", "metadata": {}, "hash": "534caee18bc486da8bf7b9e3eeef3d9fdec76db8ea7f6fd88291d30f0a120336", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "517ef586-e04b-405f-a324-524868745a04", "node_type": "1", "metadata": {}, "hash": "67f3ca6cfe0cec398f08f8dcd42194c4b9fa486c1a006a40c2c97cea8f734734", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The software stack interprets and compiles computation\nmodels of neural network applications into a customized ReRAM instruction set. However, it can\nonly be used for applications developed with neural network frameworks such as TensorFlow.\nMoreover, it only supports neural network inference except neural network training. In contrast,\nRCCT does not have the above constraints. It supports a wide range of applications no matter the\nsource codes are available or not, and can recognize more computing patterns to fully exploit the\nadvantages of CIM accelerators.\nThere have been also a few proposals focusing on the programming interface for CIM accelera-\ntors.", "mimetype": "text/plain", "start_char_idx": 64624, "end_char_idx": 65287, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "517ef586-e04b-405f-a324-524868745a04": {"__data__": {"id_": "517ef586-e04b-405f-a324-524868745a04", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ee982fc-1e1e-4676-b822-26a01c688c4e", "node_type": "1", "metadata": {}, "hash": "23d699ee928d3087e49b6aaa611ab19be91db4f9e772a749b65eddb70cc639c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6b5e5df-534c-4e80-a530-35cc84be6677", "node_type": "1", "metadata": {}, "hash": "f683bc200f80422f95d325573d17e0305c9024f392f01ac7aa9b8529b28b582b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "PRIME [17] is a ReRAM-based crossbar architecture designed for accelerating neural network\ninference applications. It provides software/hardware APIs for developers to implement various\nneural network applications on CIM accelerators. PUMA [10] is also a ReRAM-based accelerator\ndesigned for accelerating neural network inference applications. It provides a customized ISA, an\noptimized compiler and high-level APIs for programmers to develop neural network applications\non the PUMA accelerator. However, PUMA does not support the migration of existing legacy\napplications to the PUMA accelerator.", "mimetype": "text/plain", "start_char_idx": 65288, "end_char_idx": 65885, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6b5e5df-534c-4e80-a530-35cc84be6677": {"__data__": {"id_": "f6b5e5df-534c-4e80-a530-35cc84be6677", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "517ef586-e04b-405f-a324-524868745a04", "node_type": "1", "metadata": {}, "hash": "67f3ca6cfe0cec398f08f8dcd42194c4b9fa486c1a006a40c2c97cea8f734734", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47cb6eea-6fdf-417d-9e32-62c7539645cb", "node_type": "1", "metadata": {}, "hash": "0a8e19809e1819a54f2a014c890057abe9eb409eb5a29c067b003b56cae785ed", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Pinatubo [35] is a ReRAM-based crossbar architecture.\nIt exploits sensor amplifiers for efficient bitmap Boolean logic operations, and also provides\nhigh-level APIs for developers. However, since it only supports bitmap logic operations, its\napplication scenario is very limited. Yu et al., design a domain specific language and APIs for\nReRAM-based accelerators [54], and offer a special compiler to translate computing patterns\ninto highly optimized CIM-executable circuits.", "mimetype": "text/plain", "start_char_idx": 65886, "end_char_idx": 66362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47cb6eea-6fdf-417d-9e32-62c7539645cb": {"__data__": {"id_": "47cb6eea-6fdf-417d-9e32-62c7539645cb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6b5e5df-534c-4e80-a530-35cc84be6677", "node_type": "1", "metadata": {}, "hash": "f683bc200f80422f95d325573d17e0305c9024f392f01ac7aa9b8529b28b582b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80d5b376-a62a-4664-be13-bc4b7fa94323", "node_type": "1", "metadata": {}, "hash": "5206ed50ced0c182bd56f73fd9b3009fbe512c2529a9a97e86d49613da793136", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "However, the language and compiler cannot be\napplied to migrate legacy applications, and developers still need to write codes from scratch to\naccelerate these specific computing patterns. Unlike these proposals, RCCT is a complication tool\nto automatically translate various legacy applications into CIM accelerable binary executables.\nRCCT is complementary to these existing CIM programming frameworks, and can efficiently\nmigrate various legacy applications to these CIM accelerators using their specialized APIs.\nPattern recognition has been comprehensively studied for decades.", "mimetype": "text/plain", "start_char_idx": 66363, "end_char_idx": 66944, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80d5b376-a62a-4664-be13-bc4b7fa94323": {"__data__": {"id_": "80d5b376-a62a-4664-be13-bc4b7fa94323", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47cb6eea-6fdf-417d-9e32-62c7539645cb", "node_type": "1", "metadata": {}, "hash": "0a8e19809e1819a54f2a014c890057abe9eb409eb5a29c067b003b56cae785ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58c87ddd-19e4-4729-8481-0bd4a49e897d", "node_type": "1", "metadata": {}, "hash": "0df601aa5ff66ddf71f25ba568f6be3358d88a24e32cf6672681c1897cb52139", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "There have many propos-\nals for code intelligence using machine learning techniques, e.g., detecting parallel patterns of\nmulti-threaded applications [19], binary code matching [23]. There are also many other studies\non retrieving design patterns with programs in embedded multicore systems and energy-efficient\nscenarios [36]. Recently, there have been many studies on computation offloading for CMOS-based\nPIM accelerators [7, 15, 20, 48].", "mimetype": "text/plain", "start_char_idx": 66945, "end_char_idx": 67386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58c87ddd-19e4-4729-8481-0bd4a49e897d": {"__data__": {"id_": "58c87ddd-19e4-4729-8481-0bd4a49e897d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80d5b376-a62a-4664-be13-bc4b7fa94323", "node_type": "1", "metadata": {}, "hash": "5206ed50ced0c182bd56f73fd9b3009fbe512c2529a9a97e86d49613da793136", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d2e6e11-b2bd-4d29-bed7-b13000ba1c97", "node_type": "1", "metadata": {}, "hash": "87d91a1e386de1c7437395910ec01c8f4e2dd238dd45e4c7eb50d2cff70c9581", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Although these schemes are not proposed for ReRAM-based PIM ar-\nchitectures, they offer other angles to model computation offloading in heterogeneous computing\narchitectures.\n9\nCONCLUSION\nIn this article, we first formulate four computing patterns that can be accelerated by ReRAM\ncrossbar arrays from the perspective of LLVM IR. We also propose a compilation tool to automat-\nACM Transactions on Architecture and Code Optimization, Vol. 20, No. 4, Article 47. Publication date: October 2023.", "mimetype": "text/plain", "start_char_idx": 67387, "end_char_idx": 67879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d2e6e11-b2bd-4d29-bed7-b13000ba1c97": {"__data__": {"id_": "1d2e6e11-b2bd-4d29-bed7-b13000ba1c97", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58c87ddd-19e4-4729-8481-0bd4a49e897d", "node_type": "1", "metadata": {}, "hash": "0df601aa5ff66ddf71f25ba568f6be3358d88a24e32cf6672681c1897cb52139", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab6d6655-7946-44f1-acab-626e95181823", "node_type": "1", "metadata": {}, "hash": "a8ba6d1cdda3fc37e0d16f8bfb71958d87b3e549a7e540de1f75cfd2fec1f706", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures\n47:23\nically identify and translate these patterns in the form of LLVM IR into special APIs that can be\naccelerated by ReRAM, without modifying the source codes. Moreover, our compilation tool also\nsupport the migration of legacy programs to CIM accelerators when the programs\u2019 source codes\nare not available.", "mimetype": "text/plain", "start_char_idx": 67880, "end_char_idx": 68268, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab6d6655-7946-44f1-acab-626e95181823": {"__data__": {"id_": "ab6d6655-7946-44f1-acab-626e95181823", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "957dd5328712fe0a1cb7cee727aed69b82e827ac39b0c3dd8bf8b4bd250a2e18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d2e6e11-b2bd-4d29-bed7-b13000ba1c97", "node_type": "1", "metadata": {}, "hash": "87d91a1e386de1c7437395910ec01c8f4e2dd238dd45e4c7eb50d2cff70c9581", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3617686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Experimental result shows that our compilation tool can effectively convert\nlegacy programs to CIM-accelerable binary executables, and the performance of programs can be\nsignificantly improved by 10\u00d7 on average, and energy consumption is reduced by 27\u00d7 on average.", "mimetype": "text/plain", "start_char_idx": 68269, "end_char_idx": 68533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10.1145/3617686": {"__data__": {"id_": "10.1145/3617686", "embedding": null, "metadata": {"node_type": "doi_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"4": {"node_id": "root_node", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "a00ab085-4161-40c6-8c30-e3ad607b89fd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a3096672-db53-482c-a009-95e6b036cef1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b6e222c2-08e1-434d-884b-ac775dc6bd98", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e3a6e9ad-0024-4a81-ad9c-73838f7602ac", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6cb7b791-e7c9-4c19-91b8-544737658d2f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a9525ac8-4de6-4526-a69a-ceaa039ec724", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b9c24d88-1cc9-4fe4-939c-050cedd3510b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2c98efe5-1ded-4666-a266-a0634c77a03e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "284a3228-e38d-4e17-80db-d78802d8f847", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e2f67fa7-b73b-4c7b-b178-d04c434b2f1a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f4517e9c-4f7f-47a6-817a-4197c697ca82", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f9e854b5-4acf-403a-b8ce-86ab39c90784", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d73297d6-32ce-4901-824c-8fce79c77571", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c89d7d84-6dda-4677-9adf-0cc318efdff9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9c0552e2-bfc9-4a44-9212-048333f21e01", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "90643c3e-7f32-4875-8584-2071d08b76cc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b5f4fba5-3d2f-4fcc-8de2-a1b3369330dc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2106144d-642e-4960-b425-558691a5c921", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c5b89651-6775-409f-a4cd-6b7e9b7335c3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e3cbb9b3-fb5c-43fa-85b6-ad6758f7041b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "59ce017a-b229-4751-ae99-2b35be85a5dd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "60db44b8-0543-4e04-a8e5-983e194d5060", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b91e7ef7-2c54-44fa-888d-bc700ea556d0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "21b8ae8b-bb04-436a-add1-104cb8dd909d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "777dcbac-c3e2-443f-91fd-d02d920f17e5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b86ae0c3-0e53-44ec-a9e8-8e2ca25b0a30", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "502ef66e-6dd2-4559-9c21-51caf7bca8c6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a248eb7d-7045-48f9-b481-3521d40ad33f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fe131852-5ce7-4d09-9faa-715544830cb0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c8e6dd4c-902a-489f-911f-fc016ac3b818", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "56afba75-0e7c-41f6-8310-43fb22925ca6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1e6022f1-114a-44b0-b4f8-070f850231cb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "dc7673ad-6e8a-41ab-86ff-a347ac735b2b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ce91620e-7eb7-4b78-9e15-53c97f6f9f5d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "54e970ed-7a06-43b4-bf52-ca552d9a7b8b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2b6aaf40-171d-4c09-bd70-a207e25cb551", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2c8d0d18-2d69-4ab7-9e31-420aea81192f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3c096f8d-52cd-48ab-8802-7364d73cafa2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c2df2b22-d8be-40e2-8dd1-90773fc1c113", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0f50e789-1e08-4641-8cd4-b28d5f6b3460", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "53e5906f-de74-465a-90c1-75e65dd77205", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f4629a79-99fd-4883-ae92-e054b85facc5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6222df82-1d98-47ff-a0c9-db189f8c6937", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8cfadd58-f42a-4802-b49e-3d3a2ed8aaea", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "04a5abff-9386-4cbd-b72d-85593cd205a3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2e8528e1-bf1b-4bf5-81da-28e65910d1b6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a76354e5-746f-4b01-becf-8353e6f0d923", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1b2e36ba-aa82-4011-8a43-ad89f8807cf4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8f57d00d-c394-42c0-a3e8-7b29cf00fd12", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "beddeded-41a9-41e2-b0d7-4d68fbef9c8a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fe4ff507-003c-478d-8d23-7c649516db2b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f2f79ecd-d00e-4320-bb3d-53171ac5ed78", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e5a01699-da84-4666-a2e6-f3696cc085bd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8af28542-ba25-47f0-ba2f-6bf41ab2e18e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7c04abd6-cef9-4c1c-8754-b992b75de0a9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f33879f1-570c-4f85-9e46-5e17602302a4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "49b1d941-ef85-4e88-8b11-d1834a0ced47", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "065d50ae-16a0-428e-95e2-756112bd9d65", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "518c0cd7-d67a-4f81-827c-d5d132d6ad34", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9fef4540-a8f1-45bb-98fd-1c5cfabc8776", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0bc940a3-bd3f-49df-adfa-b3977e4a8a35", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0ad79ff6-aeef-4ff5-a3bd-312024872057", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0f83664e-f4e5-4c94-a082-2a48f14fb158", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c83150f1-456a-4ea4-bf4b-7ded0d77cefa", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "28973d5e-e798-45b9-a244-8d8a2e33c7aa", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "73de1ec2-b29c-4505-8822-68e4e0215bdf", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "dcf1dd35-d48b-453a-a2a8-e0c57f97bffc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c62a6ac3-6952-44eb-97c7-9f869b5fa962", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "882bf157-6082-4418-a2d6-bdcd1b788533", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "95fdd86a-5a07-446b-ae20-0fb1ab62c1b2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "78eeca5b-10d3-4bd2-beba-6c6331f17a3c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "254e8095-883b-4a57-8435-68a9c5ab72ce", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "92dd26ee-30fe-4f33-92f6-5c8b12d4867a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "472c7f79-4cfc-43b2-9f2f-0827a8962d77", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7ba2b0c0-174f-4b19-baeb-84ad56a59329", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9437fd62-8259-41e4-8953-57e2c81f337e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "745a34b2-9238-4ca9-9859-a19cb95e6d63", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c2f806ed-9172-4a16-a400-d4b6c18ebabc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "51021f82-ef0a-4d15-8304-d82b3548a861", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3bb4d99d-65e9-4760-9542-7d74f8ee7545", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "173b70b7-1aac-4dcd-b90f-ac71891ce5c7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2764cd02-33d1-4750-9efd-8cdc99f927ad", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2bbf3931-5239-4fdf-814f-e1b7cc3b3e80", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "023eaf60-3166-4103-a4f3-26084e42f378", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4348e3f2-7220-4f7a-aa3e-9ccf9abab9d9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "df172485-81b4-4ca1-aa82-75297446df18", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ea8821e7-a465-479c-9ede-fcda348b98a2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "63871b78-8b39-4786-a4b2-5743a0192158", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d4618e8f-9429-46ce-b8a2-aabb65355644", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3386c002-adce-43cb-9c7d-552cfa71796b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "afde5e1f-9ae6-40f7-b8df-b9f44ec544c9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "38bbec5d-8e77-4b46-8e10-2211408f8c2f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0f9b1487-2428-4aa3-9f47-be1d6b3a0d8d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "312ecd15-f42a-48b9-8a82-6600e5fa0e28", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bd24967b-5200-4fcd-8140-16e238331f6d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "13d5c9a8-541d-4f57-98c4-1dfb90d00fb4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "af37f0aa-1182-4f6f-b2b3-2d18bb75f0f9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fe7f1b9e-16c1-4b57-8361-5961c1311cd9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "45830ee6-afa4-4456-8ebe-1b7b71ebf662", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6edcc081-a266-4073-ac92-28cf9302839c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cd50d53b-a36b-4f88-897b-1ecec7961fb4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9afea05f-7150-406a-8608-a0b60157f561", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c15efbc1-a2fb-4250-8b40-62fd9eaa2825", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "814aaeac-2eaf-4f8a-bd9d-03e74536fda0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "276029eb-491e-44af-b8fb-5090ddced02d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7cfe1085-38de-41b5-99cd-dfbfdb1fa048", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c9d0931c-9237-4625-a349-323e48ff9912", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e358112d-4b6c-4589-ab6c-13424a213b33", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "615beb3e-5614-4779-8bad-52087f6013ec", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "99c84cf4-d116-4591-9175-4c43cf62995d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7c718b15-d4f3-49ea-9a12-924fcdaf6ce8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "28ded052-665f-45cb-a075-5352cda84962", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e762895a-426d-4d44-9a17-154b0151c079", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5ba8ee52-d15a-4ce9-bec0-c8a43e165b4f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cb1b4cf7-a033-4e60-aa94-ff4f9541db7b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "37917e4f-cf59-47fd-aa5f-4e4db1ba43a5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ef343a51-c03b-45ac-a28b-d9a6c8a0458f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6ce86b6c-571b-4270-815a-a28425a2eea7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bc4a02e8-2566-4e40-abb9-0ba20265675a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "715d2610-c061-497e-bc9d-b973c002d5f7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f51fcedb-a6e4-42b1-9a11-8b4f128b36f4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "627382b4-1d6a-4334-8028-bfe8ef264439", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4be65253-c5c1-49d7-a368-c4b338318f7b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "17ffb171-d7a1-4ff9-9478-ae0082cac502", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c68b5b8b-e902-47cd-a80a-1fbc0110520b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cc0512d4-eb35-4c5f-80cc-caec67869c9a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c731faed-4857-4b22-97c8-a79b7fc5542b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "afdfe7a3-0f5f-49e7-bf57-11645500f841", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "13c63c38-4afd-4575-b896-d877cf9a9127", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bbc28024-fbaf-4bd5-b331-9ffa2063d3d9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9d8995fe-d161-459b-8043-ef90a0bd76b2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "52a7d3ee-c29b-446c-9f34-d35a5008d881", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4dd37d7c-a5f5-4964-9439-703ead11d469", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7d9832d2-c9d8-4bf9-afa7-5b685f590008", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ffef8535-8176-45dd-a5b9-d413f0dda380", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cc1b8cf2-ebd7-40c3-9605-af705de9b9b0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9893d134-8dbb-4656-9e92-45ac7b4fc86e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6ee982fc-1e1e-4676-b822-26a01c688c4e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "517ef586-e04b-405f-a324-524868745a04", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f6b5e5df-534c-4e80-a530-35cc84be6677", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "47cb6eea-6fdf-417d-9e32-62c7539645cb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "80d5b376-a62a-4664-be13-bc4b7fa94323", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "58c87ddd-19e4-4729-8481-0bd4a49e897d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1d2e6e11-b2bd-4d29-bed7-b13000ba1c97", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ab6d6655-7946-44f1-acab-626e95181823", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}]}, "text": "DOI: 10.1145/3617686", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac150157-944e-4376-ab03-a2adc0711b9e": {"__data__": {"id_": "ac150157-944e-4376-ab03-a2adc0711b9e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4036cd26-4a0c-47e6-a38c-5580571b84f2", "node_type": "1", "metadata": {}, "hash": "b9a996e9dc1f87fb807f3ed6684aa3e8ae7265ffd03f31f83f0d4ca4813a410b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "introduction to the modulator-free SRAM-CIM macro. The \nproposed compilation framework is illustrated in Section III. \nSection IV and Section V detailedly describe the proposed \nweight mapping strategies and error correction schemes. The \nexperimental evaluation is presented in Section VI. Finally, \nSection VII draws the conclusions. \nII. SRAM-CIM MACRO \nThe core components of SRAM-CIM-based accelerators are \nthe SRAM-CIM macros, and the charge-domain 6T SRAM-\nCIM macro is utilized as the deployment model [12] in this \npaper.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4036cd26-4a0c-47e6-a38c-5580571b84f2": {"__data__": {"id_": "4036cd26-4a0c-47e6-a38c-5580571b84f2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac150157-944e-4376-ab03-a2adc0711b9e", "node_type": "1", "metadata": {}, "hash": "67d5401092984e799a5cbc153bb43e09aa72b7d2020dd9e8d0a98743f61940ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddba2af4-55ac-4b35-bdd8-6b0cc7aa140c", "node_type": "1", "metadata": {}, "hash": "00bbb2b11fa87160383575af988842fa714170ed0424ba0ca4e2d52825e2280d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As depicted in Fig. 2, the binary inputs \ud835\udc4b\ufffd are applied to \nthe word lines (WL) connected to all memory cells in the \ncorresponding rows. The multiplication between binary \ud835\udc4b\ufffd and \nbinary weights \ud835\udc4a\n\ufffd\ufffd takes place within the memory cells in \ncharge domains. Subsequently, the product charge within the \nsame columns is shared along the bit lines (BL) to generate an \nanalog voltage value, denoted as \ud835\udc49\n\ufffd\ufffd,\ufffd, for each BL.", "mimetype": "text/plain", "start_char_idx": 532, "end_char_idx": 950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddba2af4-55ac-4b35-bdd8-6b0cc7aa140c": {"__data__": {"id_": "ddba2af4-55ac-4b35-bdd8-6b0cc7aa140c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4036cd26-4a0c-47e6-a38c-5580571b84f2", "node_type": "1", "metadata": {}, "hash": "b9a996e9dc1f87fb807f3ed6684aa3e8ae7265ffd03f31f83f0d4ca4813a410b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d69edfbc-1900-404e-85b4-19517f3146ec", "node_type": "1", "metadata": {}, "hash": "5662504fbcdbc2fefcff49e54671548b8541d90e5416fe55387bc42d97a9717b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Finally, the \nanalog-to-digital converter (ADC) circuits convert each \ud835\udc49\n\ufffd\ufffd \ninto digital outputs \ud835\udc4c\n\ufffd. CVLE and AQE occur during the \nobtainment of \ud835\udc49\n\ufffd\ufffd,\ufffd and \ud835\udc4c\n\ufffd, respectively.", "mimetype": "text/plain", "start_char_idx": 951, "end_char_idx": 1127, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d69edfbc-1900-404e-85b4-19517f3146ec": {"__data__": {"id_": "d69edfbc-1900-404e-85b4-19517f3146ec", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddba2af4-55ac-4b35-bdd8-6b0cc7aa140c", "node_type": "1", "metadata": {}, "hash": "00bbb2b11fa87160383575af988842fa714170ed0424ba0ca4e2d52825e2280d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74605057-79af-413a-aef5-6bdb141f3201", "node_type": "1", "metadata": {}, "hash": "f5e1f6262bc51d453eaf5124a40caa29016f36c69610f04a7e29beba79b795ef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The relationship \nbetween \ud835\udc4b\ufffd, \ud835\udc4a\n\ufffd\ufffd, \ud835\udc49\n\ufffd\ufffd,\ufffd, and \ud835\udc4c\n\ufffd is denoted in (1-1) and (1-2), \nwhere \ud835\udc49\n\ufffd\ufffd and \ud835\udc5a represent the supply voltage and the row \ncount of CIM macro, respectively.", "mimetype": "text/plain", "start_char_idx": 1128, "end_char_idx": 1304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74605057-79af-413a-aef5-6bdb141f3201": {"__data__": {"id_": "74605057-79af-413a-aef5-6bdb141f3201", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d69edfbc-1900-404e-85b4-19517f3146ec", "node_type": "1", "metadata": {}, "hash": "5662504fbcdbc2fefcff49e54671548b8541d90e5416fe55387bc42d97a9717b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "801ea761-ccc4-44d3-a49c-c99345bb67af", "node_type": "1", "metadata": {}, "hash": "cecca12e5c366872e88be5e510210d42e0c882a6bd9394e68d1adc037e2a569a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc49\n\ufffd\ufffd,\ufffd= \ud835\udc36\ud835\udc49\ud835\udc3f\ud835\udc38\ufffd\ud835\udc49\n\ufffd\ufffd\n\ud835\udc5a\ufffd\ud835\udc4b\ufffd\u2217\ud835\udc4a\n\ufffd\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\n\ufffd \n(1-1) \n\ud835\udc4c\n\ufffd= \ud835\udc34\ud835\udc44\ud835\udc38(\ud835\udc49\n\ufffd\ufffd,\ufffd) \n(1-2) \nThe input side and output side of the SRAM-CIM macro are \nalso annotated in Fig.", "mimetype": "text/plain", "start_char_idx": 1307, "end_char_idx": 1456, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "801ea761-ccc4-44d3-a49c-c99345bb67af": {"__data__": {"id_": "801ea761-ccc4-44d3-a49c-c99345bb67af", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74605057-79af-413a-aef5-6bdb141f3201", "node_type": "1", "metadata": {}, "hash": "f5e1f6262bc51d453eaf5124a40caa29016f36c69610f04a7e29beba79b795ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ee3e216-e1f3-450a-9106-c4afa90a077d", "node_type": "1", "metadata": {}, "hash": "97c5ad7ef1ab251561903da43c87362c45306c5e4ae7a15a4e51bb1877a2ff8c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2, and \ud835\udc3f\ufffd\ufffd\ufffd\ufffd\ufffd and \ud835\udc3f\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd represent the \nrow count and column count of the cell array, respectively. \nSince the SRAM-CIM structure is inherently suitable for \nbinary MAC operation, multi-bit algorithms, such as CNN, \nDSP, and DIP, encounter the challenge of mapping from single-\nbit to multi-bit for SRAM-CIM deployment. There are two \nwidely employed methods to address this issue: the modulator-\nbased methods [32, 33] and the modulator-free methods [34].", "mimetype": "text/plain", "start_char_idx": 1457, "end_char_idx": 1914, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ee3e216-e1f3-450a-9106-c4afa90a077d": {"__data__": {"id_": "7ee3e216-e1f3-450a-9106-c4afa90a077d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "801ea761-ccc4-44d3-a49c-c99345bb67af", "node_type": "1", "metadata": {}, "hash": "cecca12e5c366872e88be5e510210d42e0c882a6bd9394e68d1adc037e2a569a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a22e22a2-782d-48cf-9c42-1b77bc109742", "node_type": "1", "metadata": {}, "hash": "dc6eef2fe7a0e262032a230a0c11d990200fc2cbd61f8472e53efc438571417d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The modulator-based methods modulate the multi-bit digital-\ndomain data into a single-wire signal using digital-to-analog \nconverters (DACs) [33] or pulse modulators (PMs) [32]. \nDespite providing higher throughput than the modulator-free \nmethods with the same macro size, the modulator-based \nmethods introduce significant quantization errors. For instance, \nwhen utilizing 7-bit DACs, 256-cell columns, and 7-bit ADCs, \nthe lowest 8-bit output would be lost [35].", "mimetype": "text/plain", "start_char_idx": 1916, "end_char_idx": 2382, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a22e22a2-782d-48cf-9c42-1b77bc109742": {"__data__": {"id_": "a22e22a2-782d-48cf-9c42-1b77bc109742", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ee3e216-e1f3-450a-9106-c4afa90a077d", "node_type": "1", "metadata": {}, "hash": "97c5ad7ef1ab251561903da43c87362c45306c5e4ae7a15a4e51bb1877a2ff8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "263be616-50ef-4b9c-a9fe-364ebc3c4be4", "node_type": "1", "metadata": {}, "hash": "04d5778a6d537c2e8ae0096f09e41e97bd56e6ac869de8204a7579623a23fe20", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Furthermore, the \ninclusion of modulators negatively impacts area efficiency. \nStudies have demonstrated that incorporating a 7-bit DAC \nwould result in a 67% increase in chip area for a row of 64 cells \n[35].", "mimetype": "text/plain", "start_char_idx": 2383, "end_char_idx": 2592, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "263be616-50ef-4b9c-a9fe-364ebc3c4be4": {"__data__": {"id_": "263be616-50ef-4b9c-a9fe-364ebc3c4be4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a22e22a2-782d-48cf-9c42-1b77bc109742", "node_type": "1", "metadata": {}, "hash": "dc6eef2fe7a0e262032a230a0c11d990200fc2cbd61f8472e53efc438571417d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e4f548d-4156-4f8d-9ca3-7edc20df8b89", "node_type": "1", "metadata": {}, "hash": "29f5ff1e942eb0bc0a93e5b369429cc1575dd78017fe18cb3654c7458cd7df80", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The modulator-free methods feature more reliable \n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\nW11\nW12\nW1n\nW21\nW22\nW2n\nWm1\nWm2\nWmn\n\u2026\nX1(1bit)\nX2(1bit)\nXm(1bit)\nADC\nWord Line\nBit Line\nDigital Circuit\nSRAM-CIM Macro\n\u2026\n\u2026\nOutput Side\nInput Side\nLoutput\nLinput\nAccumulation\nY1\nY2\nYn\nVBL,1\nVBL,2\nVBL,", "mimetype": "text/plain", "start_char_idx": 2593, "end_char_idx": 2852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e4f548d-4156-4f8d-9ca3-7edc20df8b89": {"__data__": {"id_": "5e4f548d-4156-4f8d-9ca3-7edc20df8b89", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "263be616-50ef-4b9c-a9fe-364ebc3c4be4", "node_type": "1", "metadata": {}, "hash": "04d5778a6d537c2e8ae0096f09e41e97bd56e6ac869de8204a7579623a23fe20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "654b133a-b06b-419f-b9db-64712eb02a5c", "node_type": "1", "metadata": {}, "hash": "ed7f1b4a584e77bd1c15caa5ae50a078ec980942bf92c6db7a1c12c5084a28dc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "n\nCVLE\nAQE\n \nFig. 2. The structure of modulator-free SRAM-CIM macro. CVLE and AQE \nrepresent computation voltage linear error and ADC quantization error, \nrespectively. \nCIM runtime\nWeight \nscheduler\nQuantization Parameters\nError \nMitigator\nOperation Flow\nWeight Mapping Strategy\nMAQE Strategy\nAlgorithm Parameters\nAlgorithm Accuracy   (Exact)\nHardware Efficiency (Exact)\nMulti-layer IR\nCIM-aware Compiler\nCIM-aware Evaluator\nQuantization Analyser\nQuantizer\nEvaluator\nConvolution-based", "mimetype": "text/plain", "start_char_idx": 2852, "end_char_idx": 3337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "654b133a-b06b-419f-b9db-64712eb02a5c": {"__data__": {"id_": "654b133a-b06b-419f-b9db-64712eb02a5c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e4f548d-4156-4f8d-9ca3-7edc20df8b89", "node_type": "1", "metadata": {}, "hash": "29f5ff1e942eb0bc0a93e5b369429cc1575dd78017fe18cb3654c7458cd7df80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f7f2548-c016-49f9-bc8b-368200b3bfc4", "node_type": "1", "metadata": {}, "hash": "f872fae99d01ec68ac6327d616aaba5e7d519b1842b8b8ebcacb633d760aa79f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "algorithms\nCNN\nDSP\nDIP\nAlgorithm Accuracy  (Estimated)\nAlgorithm Accuracy (Demand)\nHardware Efficiency (Demand)\nLoadable\nFile\nSoftware\nHardware\nCPU\n(Controller)\nSRAM-CIM-based \nAccelerator\nDRAM (Main Memory)\nError Calibrator\nTest Case\nTest Data\nCalibration Parameter\nData Bus\nRun\nSRAM CIM System\n \nFig. 3. Overall diagram of the proposed compilation framework for SRAM-CIM systems. This framework support bit-flexibility and signed/unsigned \nconfigurability with optimized weight mapping and error correction schemes.", "mimetype": "text/plain", "start_char_idx": 3338, "end_char_idx": 3855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f7f2548-c016-49f9-bc8b-368200b3bfc4": {"__data__": {"id_": "9f7f2548-c016-49f9-bc8b-368200b3bfc4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "654b133a-b06b-419f-b9db-64712eb02a5c", "node_type": "1", "metadata": {}, "hash": "ed7f1b4a584e77bd1c15caa5ae50a078ec980942bf92c6db7a1c12c5084a28dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54372f18-4b51-4b04-b92c-018f74c8ddba", "node_type": "1", "metadata": {}, "hash": "0a519a5c6c52cc204420d25c3475a5d54ce65d6867b41b66e8327de8006ca8ce", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The MAQE represents mitigation of analog-to-digital quantization error. \nThis article has been accepted for publication in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. This is the author's version which has not been fully e\ncontent may change prior to final publication. Citation information: DOI 10.1109/TCAD.2024.3366025\n\u00a9 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\ufffd\ufffdSee https://www.ieee.org/publications/rights/index.html for more information.", "mimetype": "text/plain", "start_char_idx": 3856, "end_char_idx": 4390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54372f18-4b51-4b04-b92c-018f74c8ddba": {"__data__": {"id_": "54372f18-4b51-4b04-b92c-018f74c8ddba", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f7f2548-c016-49f9-bc8b-368200b3bfc4", "node_type": "1", "metadata": {}, "hash": "f872fae99d01ec68ac6327d616aaba5e7d519b1842b8b8ebcacb633d760aa79f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1e03c35-08e2-4e0f-8f85-0e108d31c967", "node_type": "1", "metadata": {}, "hash": "333b5aae438fb3d2840575c269357e48b66fd98f4860f2da4fe911066907f9fa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Authorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:46:33 UTC from IEEE Xplore.  Restrictions apply. \n4 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \ncalculation accuracy. However, its single-bit operator needs \ndedicated mapping strategies for efficient processing of \nconvolution-based algorithms.", "mimetype": "text/plain", "start_char_idx": 4391, "end_char_idx": 4766, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1e03c35-08e2-4e0f-8f85-0e108d31c967": {"__data__": {"id_": "c1e03c35-08e2-4e0f-8f85-0e108d31c967", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54372f18-4b51-4b04-b92c-018f74c8ddba", "node_type": "1", "metadata": {}, "hash": "0a519a5c6c52cc204420d25c3475a5d54ce65d6867b41b66e8327de8006ca8ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3737d0ab-d82a-4fa2-b916-7b6c2e2e57f0", "node_type": "1", "metadata": {}, "hash": "b1be5eec26ec81dce7647c5c7bfb618cd1073ae174405ce099ad294a4e0fee36", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This paper employs the modulator-free method to deploy \nconvolution-based algorithms without incurring additional \nhardware costs and too much AQE (over 2-bit) [34]. \nFurthermore, bit-level strategies for weight mapping are \nproposed or extended to support bit-flexibility and signed-\nunsigned reconfigurability with high utilization of SRAM-CIM \nmacros, as illustrated in Section IV. \nIII. COMPILATION FRAMEWORK \nThis section introduces a compilation framework to deploy \nconvolution-based algorithms into SRAM CIM systems. The \nworkflow of the framework is illustrated in Fig.", "mimetype": "text/plain", "start_char_idx": 4768, "end_char_idx": 5346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3737d0ab-d82a-4fa2-b916-7b6c2e2e57f0": {"__data__": {"id_": "3737d0ab-d82a-4fa2-b916-7b6c2e2e57f0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1e03c35-08e2-4e0f-8f85-0e108d31c967", "node_type": "1", "metadata": {}, "hash": "333b5aae438fb3d2840575c269357e48b66fd98f4860f2da4fe911066907f9fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a097ab96-b7e9-43da-b1d6-2c581306db21", "node_type": "1", "metadata": {}, "hash": "42e6e5c592b533392a1f6a64e9902b6e684df1c5b071d7c220fca505bfe2058b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "3, where the \ngreen boxes represent the demand or report of accuracy and \nhardware efficiency in different stages, and the yellow boxes \nrefer to the intermediate and output files. An algorithm model \nis applied to the compilation framework with a specific demand \nfor algorithm accuracy and hardware efficiency. First, the \nquantization analyzer performs fix-point quantization and \nprovides a preliminary estimation of algorithm accuracy with \ngiven bit widths of inputs and weights. Subsequently, the CIM-\naware compiler maps the quantized models into the target \nSRAM-CIM system with automatically generated weight \nmapping strategies and the MAQE strategy.", "mimetype": "text/plain", "start_char_idx": 5347, "end_char_idx": 6008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a097ab96-b7e9-43da-b1d6-2c581306db21": {"__data__": {"id_": "a097ab96-b7e9-43da-b1d6-2c581306db21", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3737d0ab-d82a-4fa2-b916-7b6c2e2e57f0", "node_type": "1", "metadata": {}, "hash": "b1be5eec26ec81dce7647c5c7bfb618cd1073ae174405ce099ad294a4e0fee36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a168720f-4e90-4475-9b55-2318ab043243", "node_type": "1", "metadata": {}, "hash": "8c0a3761adbfbd73116ff8e0edf973cfd7605e4da8ae41d1317da6b423158adc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Finally, the \ngenerated loadable file is executed by the CIM runtime in the \nSRAM-CIM system, utilizing CVLE calibration parameters \nobtained from the error calibrator. \nThe details of the workflow are described below. \nA. Quantization Analyzer \n(1) Quantizer \nThe quantizer aims to generally reduce the precision of \nweights and inputs from 32-bit floating-point representation to \nlow-bit signed or unsigned dynamic fixed-point representations \nwithout constraints of the specific computation platform. Thus \nlow power consumption [36] can be achieved with acceptable \naccuracy loss.", "mimetype": "text/plain", "start_char_idx": 6009, "end_char_idx": 6594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a168720f-4e90-4475-9b55-2318ab043243": {"__data__": {"id_": "a168720f-4e90-4475-9b55-2318ab043243", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a097ab96-b7e9-43da-b1d6-2c581306db21", "node_type": "1", "metadata": {}, "hash": "42e6e5c592b533392a1f6a64e9902b6e684df1c5b071d7c220fca505bfe2058b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a69f7089-ec4c-4cd8-ac38-91c8c79bea9d", "node_type": "1", "metadata": {}, "hash": "e3950f662c2406111236913b1b8317a733930d6af8920ec3579755d32bc06233", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This quantizer is based on the quantization \nalgorithm [25] proposed by Chen et al, which is a post-training \nquantization strategy with min-max symmetric scaling \nparameters. Additional unsigned quantization is supported to \nimprove algorithm accuracy if inputs or weights are positive in \nDSP, DIP, and CNN. Quantization parameters are optimized by \nanalyzing the statistical inference information and comparing \nthe impacts of different bit widths. The quantization process is \ndenoted in (2), where \ud835\udc4e represents the weight or input, and \ud835\udc4e\ufffd\ufffd\ufffd \nrepresents the quantized values.", "mimetype": "text/plain", "start_char_idx": 6595, "end_char_idx": 7174, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a69f7089-ec4c-4cd8-ac38-91c8c79bea9d": {"__data__": {"id_": "a69f7089-ec4c-4cd8-ac38-91c8c79bea9d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a168720f-4e90-4475-9b55-2318ab043243", "node_type": "1", "metadata": {}, "hash": "8c0a3761adbfbd73116ff8e0edf973cfd7605e4da8ae41d1317da6b423158adc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "043ede7b-f2b6-48b7-b4c8-0d01b515cef3", "node_type": "1", "metadata": {}, "hash": "a9dff767ccd417ca5bea10a0784add4e222220756eff134613f687f2e277995a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The parameter \ud835\udc46 denotes the \nfractional length and a larger \ud835\udc46 indicates higher quantization \nresolution.  \n\ud835\udc4e\ufffd\ufffd\ufffd= \u230a\ud835\udc4e\u22172\ufffd\u230b \n\uff082) \nThe value of \ud835\udc46 can be calculated by (3), where \ud835\udc35\ufffd denotes \nquantization bit-width, which is set by users. There is a slight \ndifference in the formula between the cases of unsigned \ud835\udc4e and \nsigned \ud835\udc4e. For signed \ud835\udc4e, the sign bit should be excluded from \ud835\udc35\ufffd.", "mimetype": "text/plain", "start_char_idx": 7175, "end_char_idx": 7553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "043ede7b-f2b6-48b7-b4c8-0d01b515cef3": {"__data__": {"id_": "043ede7b-f2b6-48b7-b4c8-0d01b515cef3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a69f7089-ec4c-4cd8-ac38-91c8c79bea9d", "node_type": "1", "metadata": {}, "hash": "e3950f662c2406111236913b1b8317a733930d6af8920ec3579755d32bc06233", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2c53940-fad3-4ef6-b6fb-6621fe630357", "node_type": "1", "metadata": {}, "hash": "d7fe517d5c6e8c60c4e807a3ad68d42c7a295b007ba6e63ca007240815af7b63", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc46= \ufffd  \ud835\udc35\ufffd\u2212\u2308log\ufffd\ud835\udc4e\ufffd\ufffd\ufffd\u2309,               \ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b\ud835\udc52\ud835\udc51 \ud835\udc4e\n\ud835\udc35\ufffd\u22121 \u2212\u2308log\ufffd|\ud835\udc4e|\ufffd\ufffd\ufffd\u2309,", "mimetype": "text/plain", "start_char_idx": 7555, "end_char_idx": 7620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2c53940-fad3-4ef6-b6fb-6621fe630357": {"__data__": {"id_": "f2c53940-fad3-4ef6-b6fb-6621fe630357", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "043ede7b-f2b6-48b7-b4c8-0d01b515cef3", "node_type": "1", "metadata": {}, "hash": "a9dff767ccd417ca5bea10a0784add4e222220756eff134613f687f2e277995a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af55d924-20e3-4c1a-b472-e94d5620e5bf", "node_type": "1", "metadata": {}, "hash": "2374ed4be762b499d89dba345592983dc08037da5bf42bc4e10fb10332350d91", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b\ud835\udc52\ud835\udc51 \ud835\udc4e  \n(3) \nThe weights within an output channel or layer are assigned \na common fractional length \ud835\udc46 based on the chosen quantization \ngranularity, such as layer-wise or channel-wise. Since the \ndynamic fixed-point representation is a hardware-friendly \nquantization method with the power-of-two scaling factor, the \nmultiplication of network parameters and scaling factors can be \nreplaced by shift operations, and the computational efficiency \nof the hardware accelerators is significantly improved.", "mimetype": "text/plain", "start_char_idx": 7629, "end_char_idx": 8134, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af55d924-20e3-4c1a-b472-e94d5620e5bf": {"__data__": {"id_": "af55d924-20e3-4c1a-b472-e94d5620e5bf", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2c53940-fad3-4ef6-b6fb-6621fe630357", "node_type": "1", "metadata": {}, "hash": "d7fe517d5c6e8c60c4e807a3ad68d42c7a295b007ba6e63ca007240815af7b63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d0e0363-8e64-47b2-876e-a4a2e7208c27", "node_type": "1", "metadata": {}, "hash": "fda24aa788a532eb09345365c1aaffc94e8c77f8ac0d64c4096342e8a4588fee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(2) Evaluator \nThe evaluator conducts the primary algorithm accuracy \nestimation based on the dynamic fixed-point representation. \nDifferent algorithms are evaluated based on specific criteria \nrelevant to each algorithm. Such as the top-1 classification \naccuracy, Signal-to-Noise Ratio (SNR), and structural \nsimilarity index measure (SSIM) for ResNet18, FIR filtering, \nand Gaussian image filtering, respectively. \nB. CIM-aware Compiler \nThe CIM-aware compiler transforms quantized algorithm \nmodels into loadable files for execution in SRAM CIM \nsystems.", "mimetype": "text/plain", "start_char_idx": 8136, "end_char_idx": 8694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d0e0363-8e64-47b2-876e-a4a2e7208c27": {"__data__": {"id_": "4d0e0363-8e64-47b2-876e-a4a2e7208c27", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af55d924-20e3-4c1a-b472-e94d5620e5bf", "node_type": "1", "metadata": {}, "hash": "2374ed4be762b499d89dba345592983dc08037da5bf42bc4e10fb10332350d91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2a448bd-c0fe-48a2-bd2e-9948218f8176", "node_type": "1", "metadata": {}, "hash": "1e9224e77a61cd0893523ade775444be144667948d83b5244f920d0389292b37", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Various convolution-based algorithms are supported, \nsuch as CNN models (ResNet, MobileNet, Yolo-series, etc.), \nFIR filtering, and image filtering. The workflow of the compiler \nis illustrated in Fig. 4, and it ultimately generates a loadable file \nfor runtime execution. \n(1) Intermediate Representations \nThe CIM-aware compiler includes front-end, middle-end, \nand back-end IRs to transform the quantized model into a \nloadable file.", "mimetype": "text/plain", "start_char_idx": 8695, "end_char_idx": 9131, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2a448bd-c0fe-48a2-bd2e-9948218f8176": {"__data__": {"id_": "b2a448bd-c0fe-48a2-bd2e-9948218f8176", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d0e0363-8e64-47b2-876e-a4a2e7208c27", "node_type": "1", "metadata": {}, "hash": "fda24aa788a532eb09345365c1aaffc94e8c77f8ac0d64c4096342e8a4588fee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77d6e95b-5746-409a-889a-f2b033a180c0", "node_type": "1", "metadata": {}, "hash": "90f897156c9300016e476c6aea2eceb0566c61cf11b4d69139c0d5eccca5b09f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "These graph-based IRs are instantiated utilizing \nFront-end\nMiddle-end\nGroup Assigner\nFeature Map Tiling\nBack-end\nWeight Bit \nMapping\nError \nMitigation\nOperation \nFlow\nCIM-aware Evaluator\nAccuracy Evaluator\n(CVLE, AQE)\nPerformance Evaluator\n(DDMA, WDMA, ...)\nOperator \nFusion\nConstant \nPrecompute\nNode \nScheduling\nComputational Graph\nFeedback\nCompilation Process\nFig. 4.", "mimetype": "text/plain", "start_char_idx": 9132, "end_char_idx": 9502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77d6e95b-5746-409a-889a-f2b033a180c0": {"__data__": {"id_": "77d6e95b-5746-409a-889a-f2b033a180c0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2a448bd-c0fe-48a2-bd2e-9948218f8176", "node_type": "1", "metadata": {}, "hash": "1e9224e77a61cd0893523ade775444be144667948d83b5244f920d0389292b37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b54e23e9-b27c-4647-8365-5473311fa6c2", "node_type": "1", "metadata": {}, "hash": "d8e64a8540b40d2bf2ff84c9c6a28e7e1d0f783da6a2c635ac00811476c2a078", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Structure of CIM-aware compiler, where CVLE and AQE represent \ncomputation voltage linear error and AQC quantization error, respectively. \nThis article has been accepted for publication in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. This is the author's version which has not been fully e\ncontent may change prior to final publication. Citation information: DOI 10.1109/TCAD.2024.3366025\n\u00a9 2024 IEEE.", "mimetype": "text/plain", "start_char_idx": 9503, "end_char_idx": 9938, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b54e23e9-b27c-4647-8365-5473311fa6c2": {"__data__": {"id_": "b54e23e9-b27c-4647-8365-5473311fa6c2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77d6e95b-5746-409a-889a-f2b033a180c0", "node_type": "1", "metadata": {}, "hash": "90f897156c9300016e476c6aea2eceb0566c61cf11b4d69139c0d5eccca5b09f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "125f6f54-7a86-4e57-b17f-c5d742dc864b", "node_type": "1", "metadata": {}, "hash": "d91ff91000218ac3c4b6279504017ae7e7b7c7d103fe6826c4d8f52e2c7f4c34", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Personal use is permitted, but republication/redistribution requires IEEE permission.\ufffd\ufffdSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:46:33 UTC from IEEE Xplore.  Restrictions apply. \n5 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nPython classes as the fundamental implementation units. In the \nfront-end IR, the model is abstracted into a general \ncomputational graph [37].", "mimetype": "text/plain", "start_char_idx": 9939, "end_char_idx": 10473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "125f6f54-7a86-4e57-b17f-c5d742dc864b": {"__data__": {"id_": "125f6f54-7a86-4e57-b17f-c5d742dc864b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b54e23e9-b27c-4647-8365-5473311fa6c2", "node_type": "1", "metadata": {}, "hash": "d8e64a8540b40d2bf2ff84c9c6a28e7e1d0f783da6a2c635ac00811476c2a078", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3aa1a0b9-c4cb-43f4-b481-6949a4976ed7", "node_type": "1", "metadata": {}, "hash": "2e195a317d4331b9d9593571de47f470af57ac8caca2d514d7806dc9edf5afa8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Constant values, such as fixed-point \nkernel weights and parameters of batch normalization (BN) or \nLeakyReLU, are pre-computed for the hardware operator, and \nfused operation nodes are scheduled into an execution list [37]. \nThe middle-end IR divides the computational graph into \nprocessing groups, where nodes are continuously processed in \nthe SRAM-CIM-based accelerator without the need for off-chip \ndata transmission [38]. Intermediate feature maps are tiled to fit \nthe on-chip memory size.", "mimetype": "text/plain", "start_char_idx": 10474, "end_char_idx": 10972, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3aa1a0b9-c4cb-43f4-b481-6949a4976ed7": {"__data__": {"id_": "3aa1a0b9-c4cb-43f4-b481-6949a4976ed7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "125f6f54-7a86-4e57-b17f-c5d742dc864b", "node_type": "1", "metadata": {}, "hash": "d91ff91000218ac3c4b6279504017ae7e7b7c7d103fe6826c4d8f52e2c7f4c34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4261e0fa-e9a9-4175-b22d-931e66dc3b3e", "node_type": "1", "metadata": {}, "hash": "6e88a3f1e89c9ace9005c26eee298ba8fc91b74da405c1032bcd773e0efbc8ca", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The convolution layers are mapped \ninto CIM macros in the back-end IR, and the corresponding \noperation flow information is generated. Bit-flexible and \nsigned/unsigned reconfigurable mapping are supported, as \ndescribed in Section IV. The ADC quantization error is also \nmitigated by the MAQE strategy to reduce the accuracy loss, as \nillustrated in Section V, Part B. For algorithms with a single \nconvolution layer, multi-layer-associated functions would be \nbypassed, such as operator fusion, node scheduling, and group \nassigner.", "mimetype": "text/plain", "start_char_idx": 10973, "end_char_idx": 11507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4261e0fa-e9a9-4175-b22d-931e66dc3b3e": {"__data__": {"id_": "4261e0fa-e9a9-4175-b22d-931e66dc3b3e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3aa1a0b9-c4cb-43f4-b481-6949a4976ed7", "node_type": "1", "metadata": {}, "hash": "2e195a317d4331b9d9593571de47f470af57ac8caca2d514d7806dc9edf5afa8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6a4388e-24df-4d21-9f4f-f14035297a6e", "node_type": "1", "metadata": {}, "hash": "19f0b1b31ec955b70123049e1f43bf9e943d42e629109d1088e3a7ea78cdec27", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(2) CIM-aware Evaluator \nThe CIM-aware evaluator comprises a weight-mapping-\naware accuracy evaluator and a cycle-precision performance \nevaluator. The accuracy evaluator is an operator-level \nemulation engine with consideration of weight mapping and \nintrinsic errors. The performance evaluator is a cost model to \ncalculate the total cycle count, including CIM calculation time, \nweight updating time, data direct-memory-access (DMA) time, \nand register configuration time. The evaluation results offer \nfeedback to the compiler for adjustment of weight mapping and \nMAQE strategy.", "mimetype": "text/plain", "start_char_idx": 11510, "end_char_idx": 12093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6a4388e-24df-4d21-9f4f-f14035297a6e": {"__data__": {"id_": "e6a4388e-24df-4d21-9f4f-f14035297a6e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4261e0fa-e9a9-4175-b22d-931e66dc3b3e", "node_type": "1", "metadata": {}, "hash": "6e88a3f1e89c9ace9005c26eee298ba8fc91b74da405c1032bcd773e0efbc8ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6303729d-8d4b-43f5-8b43-0a530b7e4445", "node_type": "1", "metadata": {}, "hash": "c212696dbcd6929db4e46f9552099d26108206d0afd56ef317f1e963e3419469", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "C. Error Calibrator \nThe SRAM-CIM macros inherently exhibit CVLE that \nvaries from chip to chip [17]. To address this issue, an error \ncalibrator is developed to automatically generate calibration \nparameters specific to each chip. These calculated parameters \nare then utilized by the CIM runtime to configure the dedicated \ncalibration module during the final execution. The working \nmechanism of the error calibrator is elaborated in Section V, \nPart A. \nD. CIM Runtime \nThe CIM runtime is executed within the controlling unit, \nsuch as the central processing unit (CPU).", "mimetype": "text/plain", "start_char_idx": 12096, "end_char_idx": 12670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6303729d-8d4b-43f5-8b43-0a530b7e4445": {"__data__": {"id_": "6303729d-8d4b-43f5-8b43-0a530b7e4445", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6a4388e-24df-4d21-9f4f-f14035297a6e", "node_type": "1", "metadata": {}, "hash": "19f0b1b31ec955b70123049e1f43bf9e943d42e629109d1088e3a7ea78cdec27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a293441b-a94c-445e-810e-1bbeea180301", "node_type": "1", "metadata": {}, "hash": "862c2783dd82aaffee430ff3966443328082b536f6e1d7ce80038ef1d668d66e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The workflow of \nCIM runtime is similar to that of NVDLA [39]. The loadable \nfile is parsed into an operation flow list, which includes Data \nDMA (DDMA), Weight DMA (WDMA), Register DMA \n(RDMA), and interrupt processing. DDMA and WDMA \nmanage the transfer of feature maps and weights between the \nSRAM-CIM-based accelerator and the main memory, \nrespectively. RDMA configures the working mode of the \nSRAM-CIM-based accelerator and parameters of the dedicated \ncalibration module. Interrupt processing communicates with \nthe accelerator.", "mimetype": "text/plain", "start_char_idx": 12671, "end_char_idx": 13208, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a293441b-a94c-445e-810e-1bbeea180301": {"__data__": {"id_": "a293441b-a94c-445e-810e-1bbeea180301", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6303729d-8d4b-43f5-8b43-0a530b7e4445", "node_type": "1", "metadata": {}, "hash": "c212696dbcd6929db4e46f9552099d26108206d0afd56ef317f1e963e3419469", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "155cdde0-2686-4bf5-addb-697f5a7f296a", "node_type": "1", "metadata": {}, "hash": "07ea4634f47b1c5080c57500095e3fe5b2c057b6b0b402ced45d825c48e70111", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Subsequently, the operation flow list is \nexecuted to fulfill the computation tasks. \nE. SRAM-CIM System \nThe SRAM-CIM system includes a controlling unit, an \nSRAM-CIM-based accelerator, and a main memory. All of \nthem are connected to a shared data bus, as shown in Fig. 3. The \ncontrolling unit parses and executes the operation flow within \nthe CIM system. The main memory, typically a DRAM, stores \nthe loadable file as well as the intermediate results of multi-\nlayer algorithms.", "mimetype": "text/plain", "start_char_idx": 13209, "end_char_idx": 13693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "155cdde0-2686-4bf5-addb-697f5a7f296a": {"__data__": {"id_": "155cdde0-2686-4bf5-addb-697f5a7f296a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a293441b-a94c-445e-810e-1bbeea180301", "node_type": "1", "metadata": {}, "hash": "862c2783dd82aaffee430ff3966443328082b536f6e1d7ce80038ef1d668d66e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "256e98f6-ecbb-45fc-b674-6f7f2eba332e", "node_type": "1", "metadata": {}, "hash": "becdf586766b5090ab05da3173c007460b4b71ee005fedfb6187ee42b78d8f42", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The SRAM-CIM-based accelerator performs \nthe convolution operations with support for both bit-flexibility \nand signed-unsigned reconfigurability. \nIV. WEIGHT MAPPING STRATEGIES \nWeight mapping strategies play a crucial role in deploying \nData\nKernel\n&\nICH\n3\n4\n1\n2\nOCH_0\nIFM\nH\nW\nICH\nWeight\nw3 w4\nw1", "mimetype": "text/plain", "start_char_idx": 13694, "end_char_idx": 13991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "256e98f6-ecbb-45fc-b674-6f7f2eba332e": {"__data__": {"id_": "256e98f6-ecbb-45fc-b674-6f7f2eba332e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "155cdde0-2686-4bf5-addb-697f5a7f296a", "node_type": "1", "metadata": {}, "hash": "07ea4634f47b1c5080c57500095e3fe5b2c057b6b0b402ced45d825c48e70111", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29084e2f-3623-4886-aa67-cb7f33768cae", "node_type": "1", "metadata": {}, "hash": "615b1e9a0e338b5b9daf9e8fe8df748fd3ea5698f2118122df3730e7185fd498", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "w2\n\u2026\nKernel1\nKernel2\nKernel3\nKernel4\nx1\nx2\nx3\nx5\nx6\nx7\nx9\nx10\nx11\nx1\nx2\nx3\nx5\nx6\nx7\nx9\nx10\nx11\nx1\nx5\nx2\nx6\nx1\nx5\nx2\nx6\n\u2026\n\u2026\n\u2026\nOCH\n3\n4\n1\n2\nOCH_1\nw3 w4\nw1", "mimetype": "text/plain", "start_char_idx": 13992, "end_char_idx": 14143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29084e2f-3623-4886-aa67-cb7f33768cae": {"__data__": {"id_": "29084e2f-3623-4886-aa67-cb7f33768cae", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "256e98f6-ecbb-45fc-b674-6f7f2eba332e", "node_type": "1", "metadata": {}, "hash": "becdf586766b5090ab05da3173c007460b4b71ee005fedfb6187ee42b78d8f42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "049171c9-1891-471d-ba6e-4e172cb9c0e9", "node_type": "1", "metadata": {}, "hash": "67ae842f5996cc76ddf5cfb91dbbc00b07e1bbcc208ec8171507e03c5059b7c3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "w2\n3\n4\n1\n2\nOCH_2\nw3 w4\nw1 w2\n3\n4\n1\n2\nOCH_3\nw3 w4\nw1 w2\nICH1\nICH2\nICH3\nICH4\n21 22 23\n16 17 18 19\n11 12 13 14\n6\n7\n8\n9\n1\n2\n3\n4\nx13 x14 x15 x16\nx9 x10 x11 x12\nx5 x6 x7 x8\nx1", "mimetype": "text/plain", "start_char_idx": 14144, "end_char_idx": 14313, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "049171c9-1891-471d-ba6e-4e172cb9c0e9": {"__data__": {"id_": "049171c9-1891-471d-ba6e-4e172cb9c0e9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29084e2f-3623-4886-aa67-cb7f33768cae", "node_type": "1", "metadata": {}, "hash": "615b1e9a0e338b5b9daf9e8fe8df748fd3ea5698f2118122df3730e7185fd498", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "077a973e-2710-4c6a-9f9d-b31c3b511395", "node_type": "1", "metadata": {}, "hash": "b7a885f5ec790c2f186b736bb91b7cc6c7cc5f0d9ef6af960d7ccc4efabeb67f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "x2 x3 x4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4", "mimetype": "text/plain", "start_char_idx": 14314, "end_char_idx": 14442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "077a973e-2710-4c6a-9f9d-b31c3b511395": {"__data__": {"id_": "077a973e-2710-4c6a-9f9d-b31c3b511395", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "049171c9-1891-471d-ba6e-4e172cb9c0e9", "node_type": "1", "metadata": {}, "hash": "67ae842f5996cc76ddf5cfb91dbbc00b07e1bbcc208ec8171507e03c5059b7c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c03a63e1-25ee-4a68-9138-2502329a1305", "node_type": "1", "metadata": {}, "hash": "05b0947865d92e7b78cd1268f87add108ce6a7e048accd6b35555e4c9e6c6ef9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "w1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3", "mimetype": "text/plain", "start_char_idx": 14323, "end_char_idx": 14451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c03a63e1-25ee-4a68-9138-2502329a1305": {"__data__": {"id_": "c03a63e1-25ee-4a68-9138-2502329a1305", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "077a973e-2710-4c6a-9f9d-b31c3b511395", "node_type": "1", "metadata": {}, "hash": "b7a885f5ec790c2f186b736bb91b7cc6c7cc5f0d9ef6af960d7ccc4efabeb67f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d41c218d-7d89-456e-b33c-a780722c64b2", "node_type": "1", "metadata": {}, "hash": "54d7dba98f24eb8271b6a16900516e550158907fe61348536aaae2884dfc1616", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "w4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw", "mimetype": "text/plain", "start_char_idx": 14332, "end_char_idx": 14459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d41c218d-7d89-456e-b33c-a780722c64b2": {"__data__": {"id_": "d41c218d-7d89-456e-b33c-a780722c64b2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c03a63e1-25ee-4a68-9138-2502329a1305", "node_type": "1", "metadata": {}, "hash": "05b0947865d92e7b78cd1268f87add108ce6a7e048accd6b35555e4c9e6c6ef9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "267ca3fe-b251-49b2-b6fb-b90c06fb40ca", "node_type": "1", "metadata": {}, "hash": "b7a885f5ec790c2f186b736bb91b7cc6c7cc5f0d9ef6af960d7ccc4efabeb67f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4", "mimetype": "text/plain", "start_char_idx": 14327, "end_char_idx": 14454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "267ca3fe-b251-49b2-b6fb-b90c06fb40ca": {"__data__": {"id_": "267ca3fe-b251-49b2-b6fb-b90c06fb40ca", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d41c218d-7d89-456e-b33c-a780722c64b2", "node_type": "1", "metadata": {}, "hash": "54d7dba98f24eb8271b6a16900516e550158907fe61348536aaae2884dfc1616", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4892b2a5-8ca4-4033-9314-0ef9038f7ef0", "node_type": "1", "metadata": {}, "hash": "05b0947865d92e7b78cd1268f87add108ce6a7e048accd6b35555e4c9e6c6ef9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "w1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3", "mimetype": "text/plain", "start_char_idx": 14323, "end_char_idx": 14451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4892b2a5-8ca4-4033-9314-0ef9038f7ef0": {"__data__": {"id_": "4892b2a5-8ca4-4033-9314-0ef9038f7ef0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "267ca3fe-b251-49b2-b6fb-b90c06fb40ca", "node_type": "1", "metadata": {}, "hash": "b7a885f5ec790c2f186b736bb91b7cc6c7cc5f0d9ef6af960d7ccc4efabeb67f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd6b9d04-df7f-423f-988a-3e8e33a760c0", "node_type": "1", "metadata": {}, "hash": "5294cc346bc0a26f47044ca0805e5edc1c130a175444a3d19798445123680296", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "w4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw", "mimetype": "text/plain", "start_char_idx": 14332, "end_char_idx": 14459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd6b9d04-df7f-423f-988a-3e8e33a760c0": {"__data__": {"id_": "cd6b9d04-df7f-423f-988a-3e8e33a760c0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4892b2a5-8ca4-4033-9314-0ef9038f7ef0", "node_type": "1", "metadata": {}, "hash": "05b0947865d92e7b78cd1268f87add108ce6a7e048accd6b35555e4c9e6c6ef9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adaec0f7-23ee-46ed-ab15-0d23cb8255f5", "node_type": "1", "metadata": {}, "hash": "71a513f4d7a9685544b71038aee0740b79d305601716f0cd812ec6ca446f81c9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\nw1\nw2\nw3\nw4\n\u2026\nMacro_1\nMacro_2\nMacro_", "mimetype": "text/plain", "start_char_idx": 15083, "end_char_idx": 15211, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adaec0f7-23ee-46ed-ab15-0d23cb8255f5": {"__data__": {"id_": "adaec0f7-23ee-46ed-ab15-0d23cb8255f5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd6b9d04-df7f-423f-988a-3e8e33a760c0", "node_type": "1", "metadata": {}, "hash": "5294cc346bc0a26f47044ca0805e5edc1c130a175444a3d19798445123680296", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "844e2005-5def-4c53-9586-cffa04a20dfd", "node_type": "1", "metadata": {}, "hash": "320df487d4a7388a771d5c5cfe5ac8904e115a8d78d1430691a86a0b3e010b46", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "3\nMacro_4\nx1\nx5\nx2\nx6\nx1\nx5\nx2\nx6\n\u2026\n \nFig. 5. Macro-level strategy for weight mapping. Each solid black box \nrepresents a multi-bit input or weight. \nw1\nw2\n3\nw3\nw4\n7\n9\n10\n11\n1\nw1\nw2\n5\nw3\nw4\n9\n10\n11\n1\n2\n3\nw1\nw2\n7", "mimetype": "text/plain", "start_char_idx": 15211, "end_char_idx": 15422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "844e2005-5def-4c53-9586-cffa04a20dfd": {"__data__": {"id_": "844e2005-5def-4c53-9586-cffa04a20dfd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "adaec0f7-23ee-46ed-ab15-0d23cb8255f5", "node_type": "1", "metadata": {}, "hash": "71a513f4d7a9685544b71038aee0740b79d305601716f0cd812ec6ca446f81c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "890450b4-411e-4a3c-ae4a-3c6e9ce1fdbb", "node_type": "1", "metadata": {}, "hash": "588b67f8bc45a5b5efeaf95cd98845f155e7d5b43253412e68f55417322d7e75", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "w3\nw4\n11\n1\n2\n3\n5\nw1\nw2\n9\nw3\nw4\nw1\nw2\n3\nw3\nw4\n7\n9\n10\n11\n1\nw1\nw2\n5\nw3\nw4\n9\n10\n11\n1\n2\n3\nw1\nw2\n7\nw3\nw4\n11\n1\n2\n3\n5\nw1\nw2\n9\nw3\nw4\n\u2026\nDa", "mimetype": "text/plain", "start_char_idx": 15423, "end_char_idx": 15551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "890450b4-411e-4a3c-ae4a-3c6e9ce1fdbb": {"__data__": {"id_": "890450b4-411e-4a3c-ae4a-3c6e9ce1fdbb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "844e2005-5def-4c53-9586-cffa04a20dfd", "node_type": "1", "metadata": {}, "hash": "320df487d4a7388a771d5c5cfe5ac8904e115a8d78d1430691a86a0b3e010b46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80cd8183-88a1-4478-be77-a6c1be7349f4", "node_type": "1", "metadata": {}, "hash": "f0317b7b71ea7a796a10d4828d91070237811423db0c726873e60d861e2983b2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "ta\nKernel\n&\nICH\nIFM\nH\nW\nICH\n21 22 23\n16 17 18\n11 12 13\n6\n7\n8\nx13 x14 x15 x16\nx9 x10 x11 x12\nx5 x6 x7 x8\nx1 x2 x3 x4\nx1\nx2\nx3\nx5\nx6\nx7\nx9\nx10\nx11\n\u2026\nw1\nw2\n3", "mimetype": "text/plain", "start_char_idx": 15551, "end_char_idx": 15705, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80cd8183-88a1-4478-be77-a6c1be7349f4": {"__data__": {"id_": "80cd8183-88a1-4478-be77-a6c1be7349f4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "890450b4-411e-4a3c-ae4a-3c6e9ce1fdbb", "node_type": "1", "metadata": {}, "hash": "588b67f8bc45a5b5efeaf95cd98845f155e7d5b43253412e68f55417322d7e75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37c68533-1723-4951-8ade-fd6962c4e9be", "node_type": "1", "metadata": {}, "hash": "474e5653bf008d698e323e35a7da9e3f04854b23b4c6071991c1c5730eb08e05", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "w3\nw4\n7\n9\n10\n11\n1\nw1\nw2\n5\nw3\nw4\n9\n10\n11\n1\n2\n3\nw1\nw2\n7\nw3\nw4\n11\n1\n2\n3\n5\nw1\nw2\n9\nw3\nw4\nw1\nw2\n3\nw3\nw4\n7\n9\n10\n11\n1\nw1\nw2\n5\nw3\nw4\n9", "mimetype": "text/plain", "start_char_idx": 15369, "end_char_idx": 15495, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37c68533-1723-4951-8ade-fd6962c4e9be": {"__data__": {"id_": "37c68533-1723-4951-8ade-fd6962c4e9be", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80cd8183-88a1-4478-be77-a6c1be7349f4", "node_type": "1", "metadata": {}, "hash": "f0317b7b71ea7a796a10d4828d91070237811423db0c726873e60d861e2983b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c17b12a3-9751-4a27-b1ec-35fb1ec0f714", "node_type": "1", "metadata": {}, "hash": "e19b657e79d4d9e181c6dec01224c57eb2421746c21397194afd3c6ab15b8eb6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "10\n11\n1\n2\n3\nw1\nw2\n7\nw3\nw4\n11\n1\n2\n3\n5\nw1\nw2\n9\nw3\nw4\nw1\nw2\n3\nw3\nw4\n7\n9\n10\n11\n1\nw1\nw2\n5\nw3\nw4\n9\n10\n11\n1\n2\n3\nw1\nw2\n7\nw3\nw4\n11\n1\n2\n3", "mimetype": "text/plain", "start_char_idx": 15403, "end_char_idx": 15530, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c17b12a3-9751-4a27-b1ec-35fb1ec0f714": {"__data__": {"id_": "c17b12a3-9751-4a27-b1ec-35fb1ec0f714", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37c68533-1723-4951-8ade-fd6962c4e9be", "node_type": "1", "metadata": {}, "hash": "474e5653bf008d698e323e35a7da9e3f04854b23b4c6071991c1c5730eb08e05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8fa2f47f-fd5c-4362-b80b-afc4e654d3b3", "node_type": "1", "metadata": {}, "hash": "99b1c4097488b00bf42754d256b2d291d89115ba49062d08821a52dd05df7cf1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5\nw1\nw2\n9\nw3\nw4\nw1\nw2\n3\nw3\nw4\n7\n9\n10\n11\n1\nw1\nw2\n5\nw3\nw4\n9\n10\n11\n1\n2\n3\nw1\nw2\n7\nw3\nw4\n11\n1\n2\n3\n5\nw1\nw2\n9\nw3\nw4\nWeight\n\u2026\n3\n4\n1\n2\nOC", "mimetype": "text/plain", "start_char_idx": 15961, "end_char_idx": 16089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8fa2f47f-fd5c-4362-b80b-afc4e654d3b3": {"__data__": {"id_": "8fa2f47f-fd5c-4362-b80b-afc4e654d3b3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c17b12a3-9751-4a27-b1ec-35fb1ec0f714", "node_type": "1", "metadata": {}, "hash": "e19b657e79d4d9e181c6dec01224c57eb2421746c21397194afd3c6ab15b8eb6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53bd0054-6806-4be8-a421-7b4beb57ed1a", "node_type": "1", "metadata": {}, "hash": "ab6c1910cbc234e9c9dbc055340959a584a72d9afd18c85a70033d2aa621a166", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "H_0\nw3 w4\nw1 w2\n3\n4\n1\n2\nOCH_1\nw3 w4\nw1 w2\n3\n4\n1\n2\nOCH_2\nw3 w4\nw1 w2\n3\n4\n1\n2\nOCH_3\nw3 w4\nw1 w2\nx1\nx2\nx3\nx5\nx6\nx7\nx9\nx10\nx11\n \nFig. 6.", "mimetype": "text/plain", "start_char_idx": 16089, "end_char_idx": 16221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53bd0054-6806-4be8-a421-7b4beb57ed1a": {"__data__": {"id_": "53bd0054-6806-4be8-a421-7b4beb57ed1a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8fa2f47f-fd5c-4362-b80b-afc4e654d3b3", "node_type": "1", "metadata": {}, "hash": "99b1c4097488b00bf42754d256b2d291d89115ba49062d08821a52dd05df7cf1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2effd1bf-74a9-4806-a3f6-b91a1df0c5fb", "node_type": "1", "metadata": {}, "hash": "da9c190440af6325540f1e0fb206934b1867138d6608ffe7c8b3ac1f6c2244c7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The 2\u00d72 weight-level strategy for weight mapping. Each solid black \nbox represents a multi-bit input or weight. \nThis article has been accepted for publication in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. This is the author's version which has not been fully e\ncontent may change prior to final publication. Citation information: DOI 10.1109/TCAD.2024.3366025\n\u00a9 2024 IEEE.", "mimetype": "text/plain", "start_char_idx": 16222, "end_char_idx": 16631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2effd1bf-74a9-4806-a3f6-b91a1df0c5fb": {"__data__": {"id_": "2effd1bf-74a9-4806-a3f6-b91a1df0c5fb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53bd0054-6806-4be8-a421-7b4beb57ed1a", "node_type": "1", "metadata": {}, "hash": "ab6c1910cbc234e9c9dbc055340959a584a72d9afd18c85a70033d2aa621a166", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca9a43d5-c69b-4419-acf9-a743c4b5ae60", "node_type": "1", "metadata": {}, "hash": "e3550e8b114bc660e193e231aa1c4774567f1e7be6b9b1514682aaba13850ce9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Personal use is permitted, but republication/redistribution requires IEEE permission.\ufffd\ufffdSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:46:33 UTC from IEEE Xplore.  Restrictions apply. \n6 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nconvolution-based algorithms into multi-macro SRAM-CIM-\nbased accelerators.", "mimetype": "text/plain", "start_char_idx": 16632, "end_char_idx": 17098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca9a43d5-c69b-4419-acf9-a743c4b5ae60": {"__data__": {"id_": "ca9a43d5-c69b-4419-acf9-a743c4b5ae60", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2effd1bf-74a9-4806-a3f6-b91a1df0c5fb", "node_type": "1", "metadata": {}, "hash": "da9c190440af6325540f1e0fb206934b1867138d6608ffe7c8b3ac1f6c2244c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dca40fa6-6512-4762-ad13-a124d1620665", "node_type": "1", "metadata": {}, "hash": "e2e8d7143eeff1ec47b978a95165bcb0570b799e98dfe045d827a8b42771c91d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This section divides weight mapping \nstrategies into three levels: Macro level, weight level, and bit \nlevel, and the relationship between the multi-bit MAC operation \nand single-bit MAC operation is carefully discussed.  \nA. Macro-level Strategy \nThe macro-level strategy controls the weight mapping rule \nacross different SRAM-CIM macros. There are two widely \nused strategies: the spatial strategy and the temporal strategy \n[16]. Both strategies first segment the weight kernels into \nmultiple chunks according to the size of a single macro.", "mimetype": "text/plain", "start_char_idx": 17099, "end_char_idx": 17644, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dca40fa6-6512-4762-ad13-a124d1620665": {"__data__": {"id_": "dca40fa6-6512-4762-ad13-a124d1620665", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca9a43d5-c69b-4419-acf9-a743c4b5ae60", "node_type": "1", "metadata": {}, "hash": "e3550e8b114bc660e193e231aa1c4774567f1e7be6b9b1514682aaba13850ce9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c84aa64e-14b3-49c6-b5cf-01054923401e", "node_type": "1", "metadata": {}, "hash": "6a5cc020c061dd059ba850bea9da181b6cc359aa890fba167a95df4b66317734", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Then, \nthe spatial strategy allocates all the weight chunks into multiple \nmacros at once to eliminate weight updates during computation \n[40]. While the temporal strategy simultaneously puts one \nchunk into all the macros, a switching mechanism is used to \nalternate between different weight chunks during the operation \n[16]. The proposed framework chooses the temporal strategy to \nstrike a balance between performance and efficiency, \nconsidering the efficient weight-writing operation of SRAM \nmacro and its limited memory size [17]. Fig.", "mimetype": "text/plain", "start_char_idx": 17645, "end_char_idx": 18188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c84aa64e-14b3-49c6-b5cf-01054923401e": {"__data__": {"id_": "c84aa64e-14b3-49c6-b5cf-01054923401e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dca40fa6-6512-4762-ad13-a124d1620665", "node_type": "1", "metadata": {}, "hash": "e2e8d7143eeff1ec47b978a95165bcb0570b799e98dfe045d827a8b42771c91d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c987d570-0b06-46f4-9d5b-9962c930be68", "node_type": "1", "metadata": {}, "hash": "945cea08581560aac9ec734d6c71f9942a898bc1760e91dbf04391d91b5f6c27", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5 shows a four-\nmacro example with duplicated weight chunks to calculate 2 \u00d7\n2  computational kernels concurrently. Four computational \nkernels enclosed in colored dashed lines are calculated by the \nfour macros, respectively. \nB. Weight-level Strategy \nThe WLS focuses on maximizing utilization by exploiting \nweight-level parallelism within a single macro. Fig. 6 illustrates \nthe concept of a 2\u00d72 WLS, where data reuse between adjacent \ncomputational kernels is efficiently employed to achieve higher \nutilization. Each grid in Fig.", "mimetype": "text/plain", "start_char_idx": 18189, "end_char_idx": 18724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c987d570-0b06-46f4-9d5b-9962c930be68": {"__data__": {"id_": "c987d570-0b06-46f4-9d5b-9962c930be68", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c84aa64e-14b3-49c6-b5cf-01054923401e", "node_type": "1", "metadata": {}, "hash": "6a5cc020c061dd059ba850bea9da181b6cc359aa890fba167a95df4b66317734", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd5f8cd2-3162-4683-ad90-f573a3974d1f", "node_type": "1", "metadata": {}, "hash": "5ca603cdf954a2df7fbd34f8a771aba894cd20311bf4640dde218c7c7d1698e3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "6 represents a multi-bit weight, \nwhich occupies multiple memory cells in the SRAM-CIM \nmacro. However, it is worth noting that the WLS only improves \nthe utilization along the output side direction, and it may result \nin a large area of spare cells and affect the overall utilization, \nespecially when high parallelism is employed [15]. \nC. Bit-level Strategy \nThe BLS complements the WLS to improve overall \nutilization.", "mimetype": "text/plain", "start_char_idx": 18725, "end_char_idx": 19147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd5f8cd2-3162-4683-ad90-f573a3974d1f": {"__data__": {"id_": "dd5f8cd2-3162-4683-ad90-f573a3974d1f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c987d570-0b06-46f4-9d5b-9962c930be68", "node_type": "1", "metadata": {}, "hash": "945cea08581560aac9ec734d6c71f9942a898bc1760e91dbf04391d91b5f6c27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50cd358d-c2be-4dcb-a444-a9c0e2bd10ec", "node_type": "1", "metadata": {}, "hash": "75323b04bdeb7408b8ddb3e4fa966cd13608a732b1814d55aa79fde2494bc196", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Since modulator-free SRAM macros are fabricated \nfor single-bit MAC operation, multi-bit MAC operations should \nbe converted into single-bit MAC operations for deployment. \nThe multi-bit MAC operation is shown in (4), where \ud835\udc65\ufffd, \ud835\udc64\ufffd, and \n\ud835\udc66 are the values of multi-bit input, multi-bit weight, and MAC \nresult, respectively.", "mimetype": "text/plain", "start_char_idx": 19148, "end_char_idx": 19470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50cd358d-c2be-4dcb-a444-a9c0e2bd10ec": {"__data__": {"id_": "50cd358d-c2be-4dcb-a444-a9c0e2bd10ec", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd5f8cd2-3162-4683-ad90-f573a3974d1f", "node_type": "1", "metadata": {}, "hash": "5ca603cdf954a2df7fbd34f8a771aba894cd20311bf4640dde218c7c7d1698e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60b4552f-9495-4a77-be55-4c9610b2048c", "node_type": "1", "metadata": {}, "hash": "486a653910a5ac9a089e5b7ed275261b3c807bd7933552422b7df6fb083019c2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc66= \ufffd\ud835\udc65\ufffd\u00d7 \ud835\udc64\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\n \n(4) \nFirst, each \ud835\udc65\ufffd and \ud835\udc64\ufffd is decomposed into 1-bit elements, as \nshown in (5) and (6). \ud835\udc65\ufffd,\ufffd and  \ud835\udc64\ufffd,\ufffd are the 1-bit elements of \ud835\udc65\ufffd \nand \ud835\udc64\ufffd, respectively. \ud835\udc54\ufffd and \u210e\ufffd are the sign coefficients of \ud835\udc65\ufffd,\ufffd \nand \ud835\udc64\ufffd,\ufffd, respectively.", "mimetype": "text/plain", "start_char_idx": 19472, "end_char_idx": 19714, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60b4552f-9495-4a77-be55-4c9610b2048c": {"__data__": {"id_": "60b4552f-9495-4a77-be55-4c9610b2048c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50cd358d-c2be-4dcb-a444-a9c0e2bd10ec", "node_type": "1", "metadata": {}, "hash": "75323b04bdeb7408b8ddb3e4fa966cd13608a732b1814d55aa79fde2494bc196", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83edf9cc-2c56-4e3e-a783-8a4cc9602629", "node_type": "1", "metadata": {}, "hash": "67b7738ee86bd20fd8e07f1540a84eec53184d80c5d8478ccc8f423c8d596a7d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For Unsigned elements, all of the sign \ncoefficients are equal to 1. For signed complement elements, \nthe highest sign coefficients are equal to -1, and others are equal \nto 1 . \ud835\udc41\ufffd and \ud835\udc41\ufffd are the bit-width of each \ud835\udc65\ufffd and \ud835\udc64\ufffd, \nrespectively.", "mimetype": "text/plain", "start_char_idx": 19715, "end_char_idx": 19954, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83edf9cc-2c56-4e3e-a783-8a4cc9602629": {"__data__": {"id_": "83edf9cc-2c56-4e3e-a783-8a4cc9602629", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60b4552f-9495-4a77-be55-4c9610b2048c", "node_type": "1", "metadata": {}, "hash": "486a653910a5ac9a089e5b7ed275261b3c807bd7933552422b7df6fb083019c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3d1e384-1766-40ca-a943-7e3fafb8ee76", "node_type": "1", "metadata": {}, "hash": "0c743f601d1f98d03a1779934733c7491ef69f84def34590ad012ab29bf18000", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc65\ufffd= \ufffd\ud835\udc54\ufffd\u00d7 2\ufffd\u00d7 \ud835\udc65\ufffd,\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\n \n(5) \n\ud835\udc64\ufffd= \ufffd\u210e\ufffd\u00d7 2\ufffd\u00d7 \ud835\udc64\ufffd,\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\n \n(6) \nThen, the multi-bit MAC operation can be transformed into \nthe sum of the single-bit MAC operation, as shown in (7).", "mimetype": "text/plain", "start_char_idx": 19956, "end_char_idx": 20140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3d1e384-1766-40ca-a943-7e3fafb8ee76": {"__data__": {"id_": "a3d1e384-1766-40ca-a943-7e3fafb8ee76", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83edf9cc-2c56-4e3e-a783-8a4cc9602629", "node_type": "1", "metadata": {}, "hash": "67b7738ee86bd20fd8e07f1540a84eec53184d80c5d8478ccc8f423c8d596a7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebd14e5a-6aec-4cdc-a6d5-f59340b15933", "node_type": "1", "metadata": {}, "hash": "a147754c1b8c92157a76c617536256a99e5d0d3863b097aaf2863ba1a1bfdfef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2\n1\n0\n2\n1\n0\n2\n1\n0\n2\n1\n0\n2\n1\n0\n2\n1\n0\n2\n1\n0\n2\n1\n0\nInput Cycle\n0\n1\n2\n.\n.\n.\n.\n.\n.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebd14e5a-6aec-4cdc-a6d5-f59340b15933": {"__data__": {"id_": "ebd14e5a-6aec-4cdc-a6d5-f59340b15933", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3d1e384-1766-40ca-a943-7e3fafb8ee76", "node_type": "1", "metadata": {}, "hash": "0c743f601d1f98d03a1779934733c7491ef69f84def34590ad012ab29bf18000", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4237b3b6-8ee3-4caf-acf1-a5fb12003dc2", "node_type": "1", "metadata": {}, "hash": "f02f5704706c18013dc8f7ecfb6b07f1ebb0072ae505661a7e66da5734db6958", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "p22\np21\np20\np12\np11\np10\np02\np01\np00\n0\n1\n2\nScale&Sum\nx_bitwidth=3\n2\n1\n0\nw_bitwidth=3\n2\n1\n0\n.\n.\n.\n.\n.\n.\n.\n.\nx1\nx2\nx3\nx4\n0\n1\n2\n0\n1\n2\n.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4237b3b6-8ee3-4caf-acf1-a5fb12003dc2": {"__data__": {"id_": "4237b3b6-8ee3-4caf-acf1-a5fb12003dc2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebd14e5a-6aec-4cdc-a6d5-f59340b15933", "node_type": "1", "metadata": {}, "hash": "a147754c1b8c92157a76c617536256a99e5d0d3863b097aaf2863ba1a1bfdfef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8d51b4c-d422-48ec-9835-5c92c0db20fd", "node_type": "1", "metadata": {}, "hash": "2b83b8f7c9d3ba72f1d601e5311572f2b1aa3d70ee57607534e52ad23a47aef9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2\n1\n0\n2\n1\n0\n2\n1\n0\n2\n1\n0\n2\n1\n0\n2\n1\n0\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n0\n1\n2\n3\n4\nq2\nq1\nq0\nq4\nq3\nScale&Sum\n0\n1\n2\n3\n4\nx_bitwidth=3\n2\n1\n0\nw_bit", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8d51b4c-d422-48ec-9835-5c92c0db20fd": {"__data__": {"id_": "f8d51b4c-d422-48ec-9835-5c92c0db20fd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4237b3b6-8ee3-4caf-acf1-a5fb12003dc2", "node_type": "1", "metadata": {}, "hash": "f02f5704706c18013dc8f7ecfb6b07f1ebb0072ae505661a7e66da5734db6958", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad9507f7-2aca-40ca-ba72-114064ecbd2a", "node_type": "1", "metadata": {}, "hash": "8c99f42c44b7675c4062ca8c1298a30c83236b2127cc362a0237a8d309d1b1d2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "width=3\n2\n1\n0\n2\n1\n0\n2\n1\n0\n2\n1\n0\n2\n1\n0\n2\n1\n0\n2\n1\n0\n0\n1\n2\n0\n1\n2\nr4\nr3\nr2\nr1\nr0\nScale&Sum\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad9507f7-2aca-40ca-ba72-114064ecbd2a": {"__data__": {"id_": "ad9507f7-2aca-40ca-ba72-114064ecbd2a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8d51b4c-d422-48ec-9835-5c92c0db20fd", "node_type": "1", "metadata": {}, "hash": "2b83b8f7c9d3ba72f1d601e5311572f2b1aa3d70ee57607534e52ad23a47aef9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2f4e56a-c637-4a1c-8f86-dde4cad30142", "node_type": "1", "metadata": {}, "hash": "8d16075736fb796dc9facbe0db1b697060ea37362d2156d73988b469de3e4d80", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "x_bitwidth=3\n2\n1\n0\nw_bitwidth=3\nOutput Cycle\n2\n1\n0\n2\n1\n0\n2\n1\n0\n2\n1\n0\n2\n1\n0\n.\n.\n.\n.\n.\n.\ny\nInput Cycle\nOutput Cycle\ny\ny\nx1\nx2\nx1\nx2\n.\n.\n \n(a)                                                                                            (b)                                                                              (c) \nFig. 7.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2f4e56a-c637-4a1c-8f86-dde4cad30142": {"__data__": {"id_": "c2f4e56a-c637-4a1c-8f86-dde4cad30142", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad9507f7-2aca-40ca-ba72-114064ecbd2a", "node_type": "1", "metadata": {}, "hash": "8c99f42c44b7675c4062ca8c1298a30c83236b2127cc362a0237a8d309d1b1d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed8f540f-4642-434c-b8e9-4dac06d8df1e", "node_type": "1", "metadata": {}, "hash": "344a37f29dd57edec760cc206ba9b4701bd2bb9ca5e09399406c8a4797d9c017", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The bit-level strategies for weight mapping: (a) serial bit input parallel weight (SBIPW) strategy, (b) input side parallel (ISP) strategy, and (c) input \nand output side parallel (IOSP) strategy. Each solid black box represents a bit of multi-bit input or weight. \nThis article has been accepted for publication in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. This is the author's version which has not been fully e\ncontent may change prior to final publication.", "mimetype": "text/plain", "start_char_idx": 21007, "end_char_idx": 21504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed8f540f-4642-434c-b8e9-4dac06d8df1e": {"__data__": {"id_": "ed8f540f-4642-434c-b8e9-4dac06d8df1e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2f4e56a-c637-4a1c-8f86-dde4cad30142", "node_type": "1", "metadata": {}, "hash": "8d16075736fb796dc9facbe0db1b697060ea37362d2156d73988b469de3e4d80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26815d5a-2cdd-4aba-9fce-c1382d153986", "node_type": "1", "metadata": {}, "hash": "158676e46bd9c7464e506836d0a1f0141120e4c04fea093511258c5c871d24fc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Citation information: DOI 10.1109/TCAD.2024.3366025\n\u00a9 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\ufffd\ufffdSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:46:33 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 4161, "end_char_idx": 4528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26815d5a-2cdd-4aba-9fce-c1382d153986": {"__data__": {"id_": "26815d5a-2cdd-4aba-9fce-c1382d153986", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed8f540f-4642-434c-b8e9-4dac06d8df1e", "node_type": "1", "metadata": {}, "hash": "344a37f29dd57edec760cc206ba9b4701bd2bb9ca5e09399406c8a4797d9c017", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5c95d48-ac60-47f5-a723-76937988cbaa", "node_type": "1", "metadata": {}, "hash": "fc7d1a1bb135ce0eca52775687ef8ea16c9448bf76c88c2054a08ea0477f0be3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "7 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \n\ud835\udc66= \ufffd\ufffd\ufffd\ud835\udc54\ufffd\u00d7 \u210e\ufffd\u00d7 2\ufffd\ufffd\ufffd\u00d7 \ud835\udc65\ufffd,\ufffd\u2219\ud835\udc64\ufffd,\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\n \n(7) \nEquation (7) can be adjusted into (8) for better illustration.", "mimetype": "text/plain", "start_char_idx": 21874, "end_char_idx": 22084, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5c95d48-ac60-47f5-a723-76937988cbaa": {"__data__": {"id_": "e5c95d48-ac60-47f5-a723-76937988cbaa", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26815d5a-2cdd-4aba-9fce-c1382d153986", "node_type": "1", "metadata": {}, "hash": "158676e46bd9c7464e506836d0a1f0141120e4c04fea093511258c5c871d24fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5651519-d1d2-4289-aa68-a907b739f8b1", "node_type": "1", "metadata": {}, "hash": "f24ae8a19b0fda3cd4379a3f8537186f9e6a598ead296be77743f94cfefc0098", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc66= \ufffd\ufffd\ud835\udc54\ufffd\u00d7 \u210e\ufffd\u00d7 2\ufffd\ufffd\ufffd\u00d7 \ufffd\ud835\udc65\ufffd,\ufffd\u2219\ud835\udc64\ufffd,\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\n \n(8) \nThe simplified formula is shown in (9), where \ud835\udc52\n\ufffd,\ufffd and \ud835\udc66\ufffd,\ufffd \nare calculated as (10) and (11), respectively. \n\ud835\udc66= \ufffd\ufffd\ud835\udc52\n\ufffd,\ufffd\u00d7 \ud835\udc66\ufffd,", "mimetype": "text/plain", "start_char_idx": 22086, "end_char_idx": 22272, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5651519-d1d2-4289-aa68-a907b739f8b1": {"__data__": {"id_": "a5651519-d1d2-4289-aa68-a907b739f8b1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5c95d48-ac60-47f5-a723-76937988cbaa", "node_type": "1", "metadata": {}, "hash": "fc7d1a1bb135ce0eca52775687ef8ea16c9448bf76c88c2054a08ea0477f0be3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ed8b7a2-5dab-4cbc-bb07-6c279e5b7229", "node_type": "1", "metadata": {}, "hash": "6734c6db93238efd91e7c28fb7e828bce512697e6503043701155876f27e0051", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\n \n(9) \n\ud835\udc52\n\ufffd,\ufffd= \ud835\udc54\ufffd\u00d7 \u210e\ufffd\u00d7 2\ufffd\ufffd\ufffd \n(10) \n\ud835\udc66\ufffd,\ufffd= \ufffd\ud835\udc65\ufffd,\ufffd\u2219\ud835\udc64\ufffd,\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\n \n(11) \nWhen the MAC operation is deployed into SRAM-CIM-\nbased accelerators, the scaling and sign are implemented by the \nshift-add digital module, and \ud835\udc66\ufffd,\ufffd is calculated by CIM macros.", "mimetype": "text/plain", "start_char_idx": 22272, "end_char_idx": 22535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ed8b7a2-5dab-4cbc-bb07-6c279e5b7229": {"__data__": {"id_": "8ed8b7a2-5dab-4cbc-bb07-6c279e5b7229", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5651519-d1d2-4289-aa68-a907b739f8b1", "node_type": "1", "metadata": {}, "hash": "f24ae8a19b0fda3cd4379a3f8537186f9e6a598ead296be77743f94cfefc0098", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38b243ee-b354-41ee-a49b-72f4869e1cf5", "node_type": "1", "metadata": {}, "hash": "82fd21c781a1aaeb969d7f1ac234e2634d2c89d59e00299025da76dd73d9dd17", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Three bit-level strategies for weight mapping are included in \nthis work. \n(1) Serial Bit Input Parallel Weight \nA straightforward strategy is to strictly follow the \ncomputational flow of (9). Fig. 7(a) shows the SBIPW strategy \nwith 3-bit \ud835\udc65\ufffd and 3-bit \ud835\udc64\ufffd. Each \ud835\udc64\ufffd is flattened along the output \nside direction, and each \ud835\udc65\ufffd is serially applied to the input port.", "mimetype": "text/plain", "start_char_idx": 22537, "end_char_idx": 22900, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38b243ee-b354-41ee-a49b-72f4869e1cf5": {"__data__": {"id_": "38b243ee-b354-41ee-a49b-72f4869e1cf5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ed8b7a2-5dab-4cbc-bb07-6c279e5b7229", "node_type": "1", "metadata": {}, "hash": "6734c6db93238efd91e7c28fb7e828bce512697e6503043701155876f27e0051", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bb8374c-399e-4665-84cd-c6f38e477818", "node_type": "1", "metadata": {}, "hash": "a8356044320db6c48b653b6f591aefe86d419e5cf265df69d74db714c17679a8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The consumed cycle count \ud835\udc36\ufffd\ufffd\ufffd is equal to the bit-width of \ud835\udc65\ufffd \nand an array of \ud835\udc5d\ufffd,\ufffd is obtained. Each \ud835\udc5d\ufffd,\ufffd is equivalent to \ud835\udc66\ufffd,\ufffd. \nThen, the multi-bit MAC result can be calculated by (9). \nPrevious works have adopted the SBIPW strategy for \ncomputation mapping [22, 34, 41]. But most of them use \nunsigned \ud835\udc65\ufffd and signed \ud835\udc64\n\ufffd [42].", "mimetype": "text/plain", "start_char_idx": 22902, "end_char_idx": 23231, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bb8374c-399e-4665-84cd-c6f38e477818": {"__data__": {"id_": "9bb8374c-399e-4665-84cd-c6f38e477818", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38b243ee-b354-41ee-a49b-72f4869e1cf5", "node_type": "1", "metadata": {}, "hash": "82fd21c781a1aaeb969d7f1ac234e2634d2c89d59e00299025da76dd73d9dd17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9117ab4-073c-4c3f-b876-d8d2bf49bfc4", "node_type": "1", "metadata": {}, "hash": "45c8370aebc5e248f8996d61ac498a23141e4803acbee9893cff1235ee67ca37", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This paper extends SBIPW \nstrategy to support signed/unsigned x\ufffd and signed/unsigned w\ufffd \nwith dedicated +/\u2212 scaling modules instead of shifting \nmodules only, as shown in (9) and (10). So that a wider range \nof DSP applications can be deployed. \nSBIPW strategy is fit for convolution layers with enough \ninput channel and low output channel count because it arranges \nweight bits along the output side direction. However, SBIPW \nwould cause a low macro utilization for convolution layers with \na small input channel count.", "mimetype": "text/plain", "start_char_idx": 23232, "end_char_idx": 23754, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9117ab4-073c-4c3f-b876-d8d2bf49bfc4": {"__data__": {"id_": "e9117ab4-073c-4c3f-b876-d8d2bf49bfc4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bb8374c-399e-4665-84cd-c6f38e477818", "node_type": "1", "metadata": {}, "hash": "a8356044320db6c48b653b6f591aefe86d419e5cf265df69d74db714c17679a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27ffa399-15fc-453b-8b47-1cfc7d90d073", "node_type": "1", "metadata": {}, "hash": "09201acfdb2b454f38db0a5021f7ad3f625dc2dd4e330d2f5955a878933037b1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(2) Input Side Parallel \nThe input side parallel (ISP) strategy is proposed to extend \nweights along the input side direction. As shown in Fig. 7(b),  \nweight bits are arranged along the input side direction, and \nmultiple copies of each \ud835\udc65\ufffd are serially applied to the input port \nwith a time-domain shift between adjacent rows. The cycle \ncount \ud835\udc36\ufffd\ufffd\ufffd is formulated in (12).", "mimetype": "text/plain", "start_char_idx": 23756, "end_char_idx": 24129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27ffa399-15fc-453b-8b47-1cfc7d90d073": {"__data__": {"id_": "27ffa399-15fc-453b-8b47-1cfc7d90d073", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9117ab4-073c-4c3f-b876-d8d2bf49bfc4", "node_type": "1", "metadata": {}, "hash": "45c8370aebc5e248f8996d61ac498a23141e4803acbee9893cff1235ee67ca37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a7a68e6-722c-43a3-b2a6-f3486f5bb9c1", "node_type": "1", "metadata": {}, "hash": "bdbb451c80cae2b0daf7cd5c6c7101446c744768afa430de727c4a9766c39422", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc36\ufffd\ufffd\ufffd= \ud835\udc41\ufffd+ \ud835\udc41\ufffd\u22121 \n(12) \nDifferent from the SBIPW strategy, each \ud835\udc5e\ufffd is the \ncombination of one or several \ud835\udc66\ufffd,\ufffd. The relationship is shown in \n(13).", "mimetype": "text/plain", "start_char_idx": 24131, "end_char_idx": 24275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a7a68e6-722c-43a3-b2a6-f3486f5bb9c1": {"__data__": {"id_": "8a7a68e6-722c-43a3-b2a6-f3486f5bb9c1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27ffa399-15fc-453b-8b47-1cfc7d90d073", "node_type": "1", "metadata": {}, "hash": "09201acfdb2b454f38db0a5021f7ad3f625dc2dd4e330d2f5955a878933037b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7c207e8-5bab-403c-ab59-bd4f9581a268", "node_type": "1", "metadata": {}, "hash": "908199b2ca36d288fae97f1a8f757ea31c4e756c19855af47f967848c86ecebe", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc5e\ufffd= \ufffd\ud835\udc66\ufffd,\ufffd, \ud835\udc57+ \ud835\udc58= \ud835\udc5b, 0 \u2264\ud835\udc57< \ud835\udc41\ufffd, 0 \u2264\ud835\udc58< \ud835\udc41\ufffd \n(13) \nSince all \ud835\udc66\ufffd,\ufffd belonging to the same \ud835\udc5e\ufffd share the same scale, \nthe multi-bit MAC result can be calculated by (14).", "mimetype": "text/plain", "start_char_idx": 24277, "end_char_idx": 24437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7c207e8-5bab-403c-ab59-bd4f9581a268": {"__data__": {"id_": "f7c207e8-5bab-403c-ab59-bd4f9581a268", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a7a68e6-722c-43a3-b2a6-f3486f5bb9c1", "node_type": "1", "metadata": {}, "hash": "bdbb451c80cae2b0daf7cd5c6c7101446c744768afa430de727c4a9766c39422", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a0bdd21-90b0-4e60-926a-a08a9836e870", "node_type": "1", "metadata": {}, "hash": "dca6ec87ddf5a0688d9c8d23c84cfd7af1f41c0a80387e96c61074ae7aade2c2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc66= \ufffd2\ufffd\ud835\udc5e\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\n \n(14) \nAlthough the ISP strategy intrinsically introduces invalid \ncalculations, it can increase the overall utilization of \nconvolution layers with a small input channel count. It is worth \nnoting that the ISP strategy is only fit for unsigned \ud835\udc65\ufffd and \nunsigned \ud835\udc64\ufffd, because the sign bit is not separately processed.", "mimetype": "text/plain", "start_char_idx": 24439, "end_char_idx": 24773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a0bdd21-90b0-4e60-926a-a08a9836e870": {"__data__": {"id_": "3a0bdd21-90b0-4e60-926a-a08a9836e870", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7c207e8-5bab-403c-ab59-bd4f9581a268", "node_type": "1", "metadata": {}, "hash": "908199b2ca36d288fae97f1a8f757ea31c4e756c19855af47f967848c86ecebe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6dcb5e8e-536e-4500-8dfc-32a0304e4445", "node_type": "1", "metadata": {}, "hash": "b6678fcf1b4f3538ca79f4b959a3be7e999258f23417616123e3eeccab40c3fb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(3) Input and Output Side Parallel \nFor convolution layers with both small input and output \nchannel count, weights can be extended with both input and \noutput side parallel (IOSP) strategy, as shown in Fig. 7(c). Each \n\ud835\udc65\ufffd is flattened along the input side direction and parallelly \napplied to the input port. Multiple copies of each \ud835\udc64\ufffd are \narranged in different rows with a right shift between the \nadjacent rows.", "mimetype": "text/plain", "start_char_idx": 24775, "end_char_idx": 25190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6dcb5e8e-536e-4500-8dfc-32a0304e4445": {"__data__": {"id_": "6dcb5e8e-536e-4500-8dfc-32a0304e4445", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a0bdd21-90b0-4e60-926a-a08a9836e870", "node_type": "1", "metadata": {}, "hash": "dca6ec87ddf5a0688d9c8d23c84cfd7af1f41c0a80387e96c61074ae7aade2c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec6dfe0e-ca07-4b2f-ad12-06ff8686adeb", "node_type": "1", "metadata": {}, "hash": "1511275a76e137a9e40d0a9ed0b884a3062ca58c40d13c41eb60d86221a5f01a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "All the \ud835\udc5f\n\ufffd are obtained with only one-cycle \ncalculation, and the multi-bit MAC result can be calculated by \nEquation (15). \n\ud835\udc66=\n\ufffd\n2\ufffd\ud835\udc5f\n\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\n \n(15) \nThe calculation flow of the IOSP strategy is similar to the \nISP strategy, and the value of \ud835\udc5f\n\ufffd is equal to \ud835\udc5e\ufffd. IOSP is also for \nunsigned \ud835\udc65\ufffd and unsigned \ud835\udc64\ufffd, because the calculation of the \nsign bit is mixed with other bits.", "mimetype": "text/plain", "start_char_idx": 25191, "end_char_idx": 25573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec6dfe0e-ca07-4b2f-ad12-06ff8686adeb": {"__data__": {"id_": "ec6dfe0e-ca07-4b2f-ad12-06ff8686adeb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6dcb5e8e-536e-4500-8dfc-32a0304e4445", "node_type": "1", "metadata": {}, "hash": "b6678fcf1b4f3538ca79f4b959a3be7e999258f23417616123e3eeccab40c3fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8da8d61-95ab-44e7-9ece-1096e1c2176a", "node_type": "1", "metadata": {}, "hash": "87d7a4424cb74e02c240d6ac7eb61040bda7970e197ca1ed7b4d501c356acb00", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "D. Selection of Optimal Parameters for Weight Mapping \nFor weight mapping strategies, the CIM-aware compiler \nautomatically decides the parallelism (\ud835\udc43\ud835\udc64 and \ud835\udc43\u210e) of weight \nlevel strategies (WLS) and the selection of BLS. First, a hash \ntable is constructed offline consisting of frequently-used \nconvolution kernel shapes, their optimal parameters \ud835\udc43\ud835\udc64, \ud835\udc43\u210e, \nand BLS selection.", "mimetype": "text/plain", "start_char_idx": 25575, "end_char_idx": 25949, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8da8d61-95ab-44e7-9ece-1096e1c2176a": {"__data__": {"id_": "b8da8d61-95ab-44e7-9ece-1096e1c2176a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec6dfe0e-ca07-4b2f-ad12-06ff8686adeb", "node_type": "1", "metadata": {}, "hash": "1511275a76e137a9e40d0a9ed0b884a3062ca58c40d13c41eb60d86221a5f01a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44eae16a-3d54-4de7-9b43-b5e341e9797a", "node_type": "1", "metadata": {}, "hash": "0c36a725a06675e299d06569fa065f5aed0b628f6f91149cfacd3616901c6a73", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Then, during the compilation of algorithm \nmodels, the optimal parameters are obtained by searching \nTest Point\nFitted Curve\nv  = \u03b1 \u00b7 n + \u03b2 \nuncalibrated\ncalibrated\ny = n \ny = g  \u00b7 v + d \nError Calibrator\nCIM Macro\nError Extraction\nError Calibration\nMAC Result (n)\nMAC Result (n)\n\u0394V\n(v)\nCIM Runtime\nCalibration Parameters\nParameter Calculation\nStep 1\nStep 2\nStep 3\n\u0394V\n(v)\n \nFig. 8.", "mimetype": "text/plain", "start_char_idx": 25950, "end_char_idx": 26331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44eae16a-3d54-4de7-9b43-b5e341e9797a": {"__data__": {"id_": "44eae16a-3d54-4de7-9b43-b5e341e9797a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8da8d61-95ab-44e7-9ece-1096e1c2176a", "node_type": "1", "metadata": {}, "hash": "87d7a4424cb74e02c240d6ac7eb61040bda7970e197ca1ed7b4d501c356acb00", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fd9e821-624f-4ddc-9838-b6379884f26d", "node_type": "1", "metadata": {}, "hash": "28de02c6f13c36da66e309a044a6189ddac746ed4eaf46060937af5a069a93d0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The work flow of error calibrator. \nThis article has been accepted for publication in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. This is the author's version which has not been fully e\ncontent may change prior to final publication. Citation information: DOI 10.1109/TCAD.2024.3366025\n\u00a9 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\ufffd\ufffdSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Nanjing University.", "mimetype": "text/plain", "start_char_idx": 26332, "end_char_idx": 26885, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fd9e821-624f-4ddc-9838-b6379884f26d": {"__data__": {"id_": "4fd9e821-624f-4ddc-9838-b6379884f26d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44eae16a-3d54-4de7-9b43-b5e341e9797a", "node_type": "1", "metadata": {}, "hash": "0c36a725a06675e299d06569fa065f5aed0b628f6f91149cfacd3616901c6a73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3264cdaf-7145-44f1-b2d6-db6d76037556", "node_type": "1", "metadata": {}, "hash": "0cf31b3ac4503827499732d7a211bc3d4d56860307bef2c14ed758ec3855a85c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Downloaded on June 02,2024 at 08:46:33 UTC from IEEE Xplore.  Restrictions apply. \n8 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nthrough the hash table. If some convolution kernel shapes are \nnot in the hash table, a limited traversal will be used to search \nfor optimal parameters.", "mimetype": "text/plain", "start_char_idx": 26886, "end_char_idx": 27209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3264cdaf-7145-44f1-b2d6-db6d76037556": {"__data__": {"id_": "3264cdaf-7145-44f1-b2d6-db6d76037556", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fd9e821-624f-4ddc-9838-b6379884f26d", "node_type": "1", "metadata": {}, "hash": "28de02c6f13c36da66e309a044a6189ddac746ed4eaf46060937af5a069a93d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a247cb7-fe2e-4f23-ac34-ad7fe134f255", "node_type": "1", "metadata": {}, "hash": "ddf1fe4f45e0c5a78a4087938998fd301ecb4da0095998fabfd81edb4d7b1131", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "V. ERROR CORRECTION SCHEMES \nAlthough SRAM-CIM systems can achieve high power \nefficiency, the deployment of the widely used convolution-\nbased algorithms is hindered by the intrinsic errors of SRAM-\nCIM macros [17, 18]. These intrinsic errors include CVLE and \nAQE, both existing in the single-bit MAC operation mentioned \nin Section IV.C. This Section introduces an automatic \ncalibration method (Part. A) and a mitigation strategy (Part. B) \nfor CVLE and AQE, respectively.", "mimetype": "text/plain", "start_char_idx": 27211, "end_char_idx": 27687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a247cb7-fe2e-4f23-ac34-ad7fe134f255": {"__data__": {"id_": "5a247cb7-fe2e-4f23-ac34-ad7fe134f255", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3264cdaf-7145-44f1-b2d6-db6d76037556", "node_type": "1", "metadata": {}, "hash": "0cf31b3ac4503827499732d7a211bc3d4d56860307bef2c14ed758ec3855a85c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb1ad426-cc9c-42f7-a25d-356300a24c76", "node_type": "1", "metadata": {}, "hash": "c99656e02ba0bba78c96f53da1491289e9492b5c64433526f421f8ba3f50bdd6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A. Calibration of Computation Voltage Linearity Error \nThe CVLE is attributed to the intrinsic non-ideal macro [17]. \nThis type of error causes the variation of analog computing \nvoltage. Specifically, it changes the slope and zero-point offset \nin the MAC-result-voltage curve. \nTo address this issue, a dedicated error calibrator is \nproposed for automatic linearity error calibration. Fig. 8 \nillustrates the calibration process, which consists of three steps: \nerror extraction, parameter calculation, and error calibration.", "mimetype": "text/plain", "start_char_idx": 27690, "end_char_idx": 28218, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb1ad426-cc9c-42f7-a25d-356300a24c76": {"__data__": {"id_": "fb1ad426-cc9c-42f7-a25d-356300a24c76", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a247cb7-fe2e-4f23-ac34-ad7fe134f255", "node_type": "1", "metadata": {}, "hash": "ddf1fe4f45e0c5a78a4087938998fd301ecb4da0095998fabfd81edb4d7b1131", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7595f677-646e-498f-b541-1330db2293e2", "node_type": "1", "metadata": {}, "hash": "a11819280ecd18076b6537020354a7ad3420de732702b4605e1b7b246a35a6c5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(1) Error extraction: The linear error is extracted by obtaining \nspecific test points with pre-set MAC results.  \n(2) Parameter calculation: The calibration parameters are \ncalculated using linear fitting of the test points. The least square \nmethod [43] is adopted to estimate the slope \ud835\udefc and zero point \n\ud835\udefd, as shown in (16) and (17), where (\ud835\udc5b\ufffd, \ud835\udc63\ufffd) represent one of the \n\ud835\udc44 testpoints. \n\ud835\udefc= \ud835\udc44\u00d7 \u2211\n\ud835\udc5b\ufffd\u00d7", "mimetype": "text/plain", "start_char_idx": 28220, "end_char_idx": 28620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7595f677-646e-498f-b541-1330db2293e2": {"__data__": {"id_": "7595f677-646e-498f-b541-1330db2293e2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb1ad426-cc9c-42f7-a25d-356300a24c76", "node_type": "1", "metadata": {}, "hash": "c99656e02ba0bba78c96f53da1491289e9492b5c64433526f421f8ba3f50bdd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15b9f45f-fef0-41c7-a1f0-c3e87fde8cf8", "node_type": "1", "metadata": {}, "hash": "90d4b859dc503b9d59c38eb431be99f2aabe186f8b1a30b8bd3c683ea59b3a39", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc63\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\n\u2212\u2211\n\ud835\udc63\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\n\u00d7 \u2211\n\ud835\udc5b\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\n\ud835\udc44\u00d7 \u2211\n\ud835\udc5b\ufffd\n\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\n\u2212\ufffd\u2211\n\ud835\udc5b\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\n\ufffd\n\ufffd\n \n(16) \n\ud835\udefd=\n\u2211\n\ud835\udc63\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\n\u2212\ud835\udefc\u00d7 \u2211\n\ud835\udc5b\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\n\ud835\udc44\n \n(17) \nThen,", "mimetype": "text/plain", "start_char_idx": 28621, "end_char_idx": 28740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15b9f45f-fef0-41c7-a1f0-c3e87fde8cf8": {"__data__": {"id_": "15b9f45f-fef0-41c7-a1f0-c3e87fde8cf8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7595f677-646e-498f-b541-1330db2293e2", "node_type": "1", "metadata": {}, "hash": "a11819280ecd18076b6537020354a7ad3420de732702b4605e1b7b246a35a6c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37d6843f-52a3-4903-a090-347780b80ca1", "node_type": "1", "metadata": {}, "hash": "2720018e3a60199ada1f30fecc586dd37772e363fbe4d8837d9c96328b212480", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "the calibration parameters \ud835\udefe= 1/\ud835\udefc and \ud835\udeff= \u2212\ud835\udefd/\ud835\udefc \nare obtained for subsequent deployment. \n (3) Error calibration: The calculated calibration parameters \nare applied to CIM runtime for register configuration of the \ndedicated calibration module, and the outputs are calibrated as \n(18). \n\ud835\udc66= \ud835\udefe\u00d7 \ud835\udc63+ \ud835\udeff \n(18) \nB. Mitigation Strategies of ADC Quantization Error \nThe ADC quantization error arises from insufficient \nprecision of the ADC.", "mimetype": "text/plain", "start_char_idx": 28741, "end_char_idx": 29170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37d6843f-52a3-4903-a090-347780b80ca1": {"__data__": {"id_": "37d6843f-52a3-4903-a090-347780b80ca1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15b9f45f-fef0-41c7-a1f0-c3e87fde8cf8", "node_type": "1", "metadata": {}, "hash": "90d4b859dc503b9d59c38eb431be99f2aabe186f8b1a30b8bd3c683ea59b3a39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70259326-07a8-4f9d-b3b5-6b8a038b4d94", "node_type": "1", "metadata": {}, "hash": "6920b4e1a0099069bc45138fa16f043ba05b4d8416d661ee5195558e83c8cf24", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Theoretically, an SRAM-CIM-based \naccelerator can achieve digital-level accuracy when the ADC \nbit-width is equal to log\ufffd\ud835\udc3f\ufffd\ufffd\ufffd\ufffd\ufffd [44]. Since high-precision \nADCs result in significant area and power overhead [45]. \nEmploying ADC with inadequate precision has become a \ncompromise solution [12]. This paper proposes a mitigation \nstrategy that is aware of both the quantization error and \nhardware efficiency. \nThe mitigation is realized through the row-duplication-\nbased MAC result scaling.", "mimetype": "text/plain", "start_char_idx": 29171, "end_char_idx": 29661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70259326-07a8-4f9d-b3b5-6b8a038b4d94": {"__data__": {"id_": "70259326-07a8-4f9d-b3b5-6b8a038b4d94", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37d6843f-52a3-4903-a090-347780b80ca1", "node_type": "1", "metadata": {}, "hash": "2720018e3a60199ada1f30fecc586dd37772e363fbe4d8837d9c96328b212480", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eeab61f0-8040-4c5d-9e00-63191c35abbc", "node_type": "1", "metadata": {}, "hash": "8f69155d9572b8778168a92a48b0792d30aab53ead660723d0a1546a63843733", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "An \ud835\udc45 time duplication is able to \nreduce the quantization error by log\ufffd\ud835\udc45 bits. Fig. 9 shows the \nquantization error with two-time duplication and without row \nduplication. However, this would also cause performance loss \nwith \ud835\udc45-time increase in processing time.  \nTo alleviate the aforementioned performance-loss issue, this \npaper proposes a partition-based MAQE strategy. It separately \nconfigures the weight copy counts of the high-bit and low-bit \ncalculations to balance the hardware efficiency and algorithm \naccuracy. Fig.", "mimetype": "text/plain", "start_char_idx": 29662, "end_char_idx": 30191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eeab61f0-8040-4c5d-9e00-63191c35abbc": {"__data__": {"id_": "eeab61f0-8040-4c5d-9e00-63191c35abbc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70259326-07a8-4f9d-b3b5-6b8a038b4d94", "node_type": "1", "metadata": {}, "hash": "6920b4e1a0099069bc45138fa16f043ba05b4d8416d661ee5195558e83c8cf24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11e11d42-a0a5-4559-bc69-40a12e88d381", "node_type": "1", "metadata": {}, "hash": "df4c8c08f4d55294e481cd893693433e713a375635c29486d0ad10f6b8da324b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "10 showcases an example of MAQE on a \n128\u00d7 64 CIM macro with 5-bit ADCs. \ud835\udc65\ufffd and \ud835\udc64\ufffd are both \ndivided into high-bit parts and low-bit parts by the dividers \n\ud835\udc4b\ufffd\ufffd\ufffd and \ud835\udc4a\n\ufffd\ufffd\ufffd. Then, the origin full-bit MAC can be \ndecomposed into the calculation of four partial sums, denoted \nas PSUM1-PSUM4.", "mimetype": "text/plain", "start_char_idx": 30192, "end_char_idx": 30480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11e11d42-a0a5-4559-bc69-40a12e88d381": {"__data__": {"id_": "11e11d42-a0a5-4559-bc69-40a12e88d381", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eeab61f0-8040-4c5d-9e00-63191c35abbc", "node_type": "1", "metadata": {}, "hash": "8f69155d9572b8778168a92a48b0792d30aab53ead660723d0a1546a63843733", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26b8f05a-8164-4d6f-8157-1b5ad68dcf2e", "node_type": "1", "metadata": {}, "hash": "652a41758a008ce0f986f408d20a195873598d96d1d54d66400bd01af2a97c35", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "PSUM1 is calculated by the low-bit \ud835\udc65\ufffd + \nlow-bit \ud835\udc64\ufffd without weight duplication (2-bit ADC quantization \nerror). PSUM3 is calculated by the high-bit \ud835\udc65\ufffd + high-bit \ud835\udc64\ufffd \nwith four-time duplication(without ADC quantization error).", "mimetype": "text/plain", "start_char_idx": 30481, "end_char_idx": 30706, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26b8f05a-8164-4d6f-8157-1b5ad68dcf2e": {"__data__": {"id_": "26b8f05a-8164-4d6f-8157-1b5ad68dcf2e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11e11d42-a0a5-4559-bc69-40a12e88d381", "node_type": "1", "metadata": {}, "hash": "df4c8c08f4d55294e481cd893693433e713a375635c29486d0ad10f6b8da324b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9066fafd-7d42-4ca2-84e0-e812f25906cc", "node_type": "1", "metadata": {}, "hash": "40a1436e5bb27d60f89cf7d18c1609c3df60fbc0739b92fdf6506c3ac08c1519", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The calculation of PSUM2 and PSUM4 adopt the moderate \nstrategy with two-time duplication(1-bit ADC quantization \nerror) for high-bit \ud835\udc65\ufffd + low-bit \ud835\udc64\ufffd and low-bit \ud835\udc65\ufffd + high-bit \ud835\udc64\ufffd, \nrespectively. Finally, the ultimate MAC result is calculated \nthrough the accumulation of PSUM1-PSUM4. The detailed \nanalysis of algorithm accuracy and hardware efficiency is \nshown in Section VI.", "mimetype": "text/plain", "start_char_idx": 30708, "end_char_idx": 31085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9066fafd-7d42-4ca2-84e0-e812f25906cc": {"__data__": {"id_": "9066fafd-7d42-4ca2-84e0-e812f25906cc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26b8f05a-8164-4d6f-8157-1b5ad68dcf2e", "node_type": "1", "metadata": {}, "hash": "652a41758a008ce0f986f408d20a195873598d96d1d54d66400bd01af2a97c35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b74c8e48-aee3-4149-8714-ba22a73f79bc", "node_type": "1", "metadata": {}, "hash": "1d76876cb6bf839fd3bb26bf7f28c2609e8bfe39ca5791aea8a3e985ebb69188", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "C. Generation of Error Correction Parameters \nFor error correction schemes of specific SRAM-CIM \nsystems, the parameters of CCVLE are calculated offline. The \nparameters \ud835\udc4b\ud835\udc51\ud835\udc63\ud835\udc51 and \ud835\udc4a\ud835\udc51\ud835\udc63\ud835\udc51 are generated through binary search \nwith a time complexity of \ud835\udc42(log(\ud835\udc41\ufffd\u2217\ud835\udc41\ufffd)) to achieve the \nhighest hardware throughput with qualified algorithm accuracy. \nVI. EXPERIMENTAL EVALUATION \nIn this section, the deployment results of convolution-based \nalgorithms are analyzed.", "mimetype": "text/plain", "start_char_idx": 31087, "end_char_idx": 31543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b74c8e48-aee3-4149-8714-ba22a73f79bc": {"__data__": {"id_": "b74c8e48-aee3-4149-8714-ba22a73f79bc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9066fafd-7d42-4ca2-84e0-e812f25906cc", "node_type": "1", "metadata": {}, "hash": "40a1436e5bb27d60f89cf7d18c1609c3df60fbc0739b92fdf6506c3ac08c1519", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ff4f179-74dd-4833-a931-8a6894dfa8bf", "node_type": "1", "metadata": {}, "hash": "dd09a7862232829b036d6bbfab9c39a60790ded8538c92a3bed220788f9580c5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The evaluation is conducted based on \na cost model of an SRAM-CIM system [28-30]. First, the \nproposed weight mapping strategies are evaluated across \n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n1\n2\n3\n4\n5\n6\n7", "mimetype": "text/plain", "start_char_idx": 31544, "end_char_idx": 31786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ff4f179-74dd-4833-a931-8a6894dfa8bf": {"__data__": {"id_": "5ff4f179-74dd-4833-a931-8a6894dfa8bf", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b74c8e48-aee3-4149-8714-ba22a73f79bc", "node_type": "1", "metadata": {}, "hash": "1d76876cb6bf839fd3bb26bf7f28c2609e8bfe39ca5791aea8a3e985ebb69188", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2cf9ce0-82d6-4603-ad98-90e8ec39bfc5", "node_type": "1", "metadata": {}, "hash": "e1b1fe20e0e95edeb8a39b82122225772490b81a2d4d011abe0cf338f7ac8641", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "8\n9\n10\n11\n12\n13\n14\n15\n16\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11", "mimetype": "text/plain", "start_char_idx": 31709, "end_char_idx": 31835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2cf9ce0-82d6-4603-ad98-90e8ec39bfc5": {"__data__": {"id_": "d2cf9ce0-82d6-4603-ad98-90e8ec39bfc5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ff4f179-74dd-4833-a931-8a6894dfa8bf", "node_type": "1", "metadata": {}, "hash": "dd09a7862232829b036d6bbfab9c39a60790ded8538c92a3bed220788f9580c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a30f9db2-6708-4fcb-a526-55f612fda45e", "node_type": "1", "metadata": {}, "hash": "f28e2984bae75e0c34fde09fc166a6a8d91e5f356885dcaccdd0d69e91b8b3bc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "12\n13\n14\n15\n16\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n2-bit ADC\n2-bit ADC\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7\n8\n8\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7", "mimetype": "text/plain", "start_char_idx": 31914, "end_char_idx": 32047, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a30f9db2-6708-4fcb-a526-55f612fda45e": {"__data__": {"id_": "a30f9db2-6708-4fcb-a526-55f612fda45e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2cf9ce0-82d6-4603-ad98-90e8ec39bfc5", "node_type": "1", "metadata": {}, "hash": "e1b1fe20e0e95edeb8a39b82122225772490b81a2d4d011abe0cf338f7ac8641", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1b26f3c-da54-4f18-8805-6e51a829b7dd", "node_type": "1", "metadata": {}, "hash": "48474f0311c1a4073d2dcf1cbcec6616c20983e1b1de98a587110c4c98d6ba8a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "8\n8\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7\n8\n8\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7\n8\n8\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7\n8\n8\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7", "mimetype": "text/plain", "start_char_idx": 32016, "end_char_idx": 32143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1b26f3c-da54-4f18-8805-6e51a829b7dd": {"__data__": {"id_": "d1b26f3c-da54-4f18-8805-6e51a829b7dd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a30f9db2-6708-4fcb-a526-55f612fda45e", "node_type": "1", "metadata": {}, "hash": "f28e2984bae75e0c34fde09fc166a6a8d91e5f356885dcaccdd0d69e91b8b3bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "627a737d-8c86-4f12-a92e-c8350b4bdc8a", "node_type": "1", "metadata": {}, "hash": "ed6aa8c5c684be850e6ce9f907f5b67dc4e2b3c9b5177323b7b709b93416a01d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "8\n8\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7\n8\n8\nTwo-time Duplication\n\ud835\udc9a= \ud835\udfd0\ud835\udfd0\u2217\ud835\udc9a\ud835\udfce\n\ud835\udfd0\ud835\udfd0\n\ud835\udc6c\ud835\udc93\ud835\udc93\ud835\udc98/\ud835\udc90 \ud835\udc74\ud835\udc68\ud835\udc78\ud835\udc6c=", "mimetype": "text/plain", "start_char_idx": 32176, "end_char_idx": 32257, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "627a737d-8c86-4f12-a92e-c8350b4bdc8a": {"__data__": {"id_": "627a737d-8c86-4f12-a92e-c8350b4bdc8a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1b26f3c-da54-4f18-8805-6e51a829b7dd", "node_type": "1", "metadata": {}, "hash": "48474f0311c1a4073d2dcf1cbcec6616c20983e1b1de98a587110c4c98d6ba8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8c3c6bc-c8c5-4664-8ac3-f0fa2ddb285f", "node_type": "1", "metadata": {}, "hash": "5ee76beb9801bde011fdd5dc0abfe3283e33d541866280ebd4728c9ac7b4ae37", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc9a\ud835\udfce\u2212\ud835\udfd0\ud835\udfd0\u2217\ud835\udc9a\ud835\udfce\n\ud835\udfd0\ud835\udfd0\n\ud835\udc9a= \ud835\udfd0\ud835\udfcf\u2217\ud835\udfd0\ud835\udfcf\u2217\ud835\udc9a\ud835\udfce\n\ud835\udfd0\ud835\udfd0\n\ud835\udc6c\ud835\udc93\ud835\udc93\ud835\udc98\ud835\udc8a\ud835\udc95\ud835\udc89 \ud835\udc74\ud835\udc68\ud835\udc78\ud835\udc6c=", "mimetype": "text/plain", "start_char_idx": 32258, "end_char_idx": 32298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8c3c6bc-c8c5-4664-8ac3-f0fa2ddb285f": {"__data__": {"id_": "d8c3c6bc-c8c5-4664-8ac3-f0fa2ddb285f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "627a737d-8c86-4f12-a92e-c8350b4bdc8a", "node_type": "1", "metadata": {}, "hash": "ed6aa8c5c684be850e6ce9f907f5b67dc4e2b3c9b5177323b7b709b93416a01d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7515ce6f-3c1c-4493-8410-2e21e71ac14a", "node_type": "1", "metadata": {}, "hash": "e1fe1be8f1b0a6d5d8dc645046f435de49a201c6720573835f58c0efb19d3614", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc9a\ud835\udfce\u2212\ud835\udfd0\ud835\udfcf\u2217\ud835\udc9a\ud835\udfce\n\ud835\udfd0\ud835\udfcf\nMAQE\n \n(a)                                                         (b) \nFig. 9. The quantization error with and without row duplication: (a) \nwithout duplication, (b) with two-time duplication.", "mimetype": "text/plain", "start_char_idx": 32299, "end_char_idx": 32504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7515ce6f-3c1c-4493-8410-2e21e71ac14a": {"__data__": {"id_": "7515ce6f-3c1c-4493-8410-2e21e71ac14a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8c3c6bc-c8c5-4664-8ac3-f0fa2ddb285f", "node_type": "1", "metadata": {}, "hash": "5ee76beb9801bde011fdd5dc0abfe3283e33d541866280ebd4728c9ac7b4ae37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5c31810-4758-4042-b44e-3d21c6c3336e", "node_type": "1", "metadata": {}, "hash": "da898bf8d94dba32b3ae6bc08bbeef84baa7a9b55849875949fb19f6db9c093b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Solid black boxes \nrepresents single-bit intputs and weights of SRAM-CIM operator. \nThis article has been accepted for publication in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. This is the author's version which has not been fully e\ncontent may change prior to final publication. Citation information: DOI 10.1109/TCAD.2024.3366025\n\u00a9 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\ufffd\ufffdSee https://www.ieee.org/publications/rights/index.html for more information.", "mimetype": "text/plain", "start_char_idx": 32505, "end_char_idx": 33050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5c31810-4758-4042-b44e-3d21c6c3336e": {"__data__": {"id_": "d5c31810-4758-4042-b44e-3d21c6c3336e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7515ce6f-3c1c-4493-8410-2e21e71ac14a", "node_type": "1", "metadata": {}, "hash": "e1fe1be8f1b0a6d5d8dc645046f435de49a201c6720573835f58c0efb19d3614", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f3cabe7-7a02-459e-adc1-fb69edb96957", "node_type": "1", "metadata": {}, "hash": "d5f082679a60ed25fc25488a31423c9ac71d0e634ecf44b04b48c40200d90669", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Authorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:46:33 UTC from IEEE Xplore.  Restrictions apply. \n9 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \ndifferent convolution layers. Then, we show the effectiveness \nof the introduced error correction schemes by analysing the \ndeployment results of several commonly used convolution-\nbased algorithms, such as ResNet18, FIR filtering, and \nGaussian image filtering.", "mimetype": "text/plain", "start_char_idx": 33051, "end_char_idx": 33539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f3cabe7-7a02-459e-adc1-fb69edb96957": {"__data__": {"id_": "5f3cabe7-7a02-459e-adc1-fb69edb96957", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5c31810-4758-4042-b44e-3d21c6c3336e", "node_type": "1", "metadata": {}, "hash": "da898bf8d94dba32b3ae6bc08bbeef84baa7a9b55849875949fb19f6db9c093b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51b5d905-ece4-4529-92cd-1094dbc2f55a", "node_type": "1", "metadata": {}, "hash": "a23e189e120821aad3253be4ed43797f4df4044d0a39d24d098ec84abc0d35ef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The evaluation results demonstrate \nthat the proposed framework provides enhanced flexibility and \nenables the effective deployment of convolution-based \nalgorithms into SRAM-CIM systems. The compilation time is \nalso evaluated for the typical CNN model. \nA. Experimental Setup \n(1) SRAM-CIM-based accelerator \nThe top view of the SRAM-CIM-based accelerator is \ndepicted in Fig. 11. This accelerator comprises an instruction \nregister file, a top-level controller, and the datapath.", "mimetype": "text/plain", "start_char_idx": 33540, "end_char_idx": 34022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51b5d905-ece4-4529-92cd-1094dbc2f55a": {"__data__": {"id_": "51b5d905-ece4-4529-92cd-1094dbc2f55a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f3cabe7-7a02-459e-adc1-fb69edb96957", "node_type": "1", "metadata": {}, "hash": "d5f082679a60ed25fc25488a31423c9ac71d0e634ecf44b04b48c40200d90669", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d9fec61-97c6-4ac5-90a1-49185a038e62", "node_type": "1", "metadata": {}, "hash": "43d141175c5a29afa036ec19871285aec1b56ec5432ee451314b31f5de68803b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The data \ninterface is connected to Bank A, Bank B, and the instruction \nregister file with a bandwidth of 128 bits per cycle. The weight \ninterface is connected to a weight loader with 64 bits per cycle \nbandwidth. The system includes four processing tiles, each \nconsisting of a 128 \u00d7 64 SRAM-CIM macro, a calibration \nmodel, scale-add logic, a scratch pad, an activation module, and \na pooling module. The scale-add logic includes a finite state \nmachine to achieve a cycle-level control of +/\u2212 scaling.", "mimetype": "text/plain", "start_char_idx": 34023, "end_char_idx": 34529, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d9fec61-97c6-4ac5-90a1-49185a038e62": {"__data__": {"id_": "3d9fec61-97c6-4ac5-90a1-49185a038e62", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51b5d905-ece4-4529-92cd-1094dbc2f55a", "node_type": "1", "metadata": {}, "hash": "a23e189e120821aad3253be4ed43797f4df4044d0a39d24d098ec84abc0d35ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc7b3abf-c180-4a8d-ba3c-81d30bb8a299", "node_type": "1", "metadata": {}, "hash": "d6e35c35ed3a9cecc02d0bc190f7bef8e3d0f36dc93980e2b8e94044c1e45a40", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The \nerror model of computation voltage is derived from a previous \nwork [17], which is a charge-domain SRAM-CIM macro. \nKernel weights are updated using a 64-bit weight loader, which \nbroadcasts the same weight into all four macros. The size of \nBank A and Bank B are both 1024KB. For multi-layer \nprocessing, the data flow operates in a ping-pong manner \nbetween Bank A and Bank B [38].", "mimetype": "text/plain", "start_char_idx": 34530, "end_char_idx": 34918, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc7b3abf-c180-4a8d-ba3c-81d30bb8a299": {"__data__": {"id_": "cc7b3abf-c180-4a8d-ba3c-81d30bb8a299", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d9fec61-97c6-4ac5-90a1-49185a038e62", "node_type": "1", "metadata": {}, "hash": "43d141175c5a29afa036ec19871285aec1b56ec5432ee451314b31f5de68803b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b02f5eee-462b-412a-aea0-d3b4d7ab5d07", "node_type": "1", "metadata": {}, "hash": "cea5ec9c618239416d96db51e03f1c3f1670b9b3c1148cf8236eb887b63314be", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(2) Evaluation Matrics \nThis paper evaluates both hardware efficiency and \nalgorithm accuracy to illustrate the optimization and flexibility \nof the proposed weight mapping strategies and error correction \nschemes. In addition, the compilation time is also evaluated \nbased on Intel Core i9-9900X CPU and GeForce RTX 2080 Ti. \nFor hardware efficiency, two key metrics are considered: \nmacro utilization [15] and normalized throughput [21]. The \neffectiveness of weight mapping strategies is evaluated based \non the macro utilization, denoted as \ud835\udc48\ufffd\ufffd\ufffd\ufffd\ufffd, which is calculated \nas (19).", "mimetype": "text/plain", "start_char_idx": 34920, "end_char_idx": 35502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b02f5eee-462b-412a-aea0-d3b4d7ab5d07": {"__data__": {"id_": "b02f5eee-462b-412a-aea0-d3b4d7ab5d07", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc7b3abf-c180-4a8d-ba3c-81d30bb8a299", "node_type": "1", "metadata": {}, "hash": "d6e35c35ed3a9cecc02d0bc190f7bef8e3d0f36dc93980e2b8e94044c1e45a40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4c3a51d-8bf1-4de0-ad46-83c549db4ba9", "node_type": "1", "metadata": {}, "hash": "85d9d26204f0d2d1b7fddb76ca0b800fe6ed2a04feb0bab21b06d6b32d05d1d3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The values of \ud835\udc36\ufffd, \ud835\udc48\ufffd, and \ud835\udc37 represent the macro \ncomputing cycle count, the number of used memory cells, and \nthe number of total memory cells in an SRAM-CIM macro, \nrespectively.  \n\ud835\udc48\ufffd\ufffd\ufffd\ufffd\ufffd(%) = 1\n\ud835\udc36\ufffd\n\ufffd\ufffd\ud835\udc48\ufffd\n\ud835\udc37\n\ufffd\ufffd\n\ufffd\ufffd\ufffd\n\ufffd\u00d7 100 \n(19) \nSince the employment of MAQE has a significant impact on \nhardware throughput but a minor effect on macro utilization.", "mimetype": "text/plain", "start_char_idx": 35503, "end_char_idx": 35848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4c3a51d-8bf1-4de0-ad46-83c549db4ba9": {"__data__": {"id_": "b4c3a51d-8bf1-4de0-ad46-83c549db4ba9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b02f5eee-462b-412a-aea0-d3b4d7ab5d07", "node_type": "1", "metadata": {}, "hash": "cea5ec9c618239416d96db51e03f1c3f1670b9b3c1148cf8236eb887b63314be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c1b0741-1cac-4b04-89ea-a0ca8640b1d0", "node_type": "1", "metadata": {}, "hash": "5dc7b275fbe88339654bdcc10d8044a189150988c5113c33c319b7a8cb9f30f8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The evaluation of different MAQE strategies focuses on the \nnormalized throughput, demoted as \ud835\udc47\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd. The calculation of \n\ud835\udc47\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd is illustrated in (20), where \ud835\udc36\ufffd, \ud835\udc36\ufffd, \ud835\udc36\ufffd, and \ud835\udc36\ufffd, \nrepresent the cycle count of DDMA, WDMA, RDMA, and \nmacro computing, respectively. \ud835\udc3e\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd is the normalization \nfactor.", "mimetype": "text/plain", "start_char_idx": 35850, "end_char_idx": 36154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c1b0741-1cac-4b04-89ea-a0ca8640b1d0": {"__data__": {"id_": "3c1b0741-1cac-4b04-89ea-a0ca8640b1d0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4c3a51d-8bf1-4de0-ad46-83c549db4ba9", "node_type": "1", "metadata": {}, "hash": "85d9d26204f0d2d1b7fddb76ca0b800fe6ed2a04feb0bab21b06d6b32d05d1d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2617f1c9-9be0-4bb8-abb9-111ee1b6d31d", "node_type": "1", "metadata": {}, "hash": "447d8ed29e8fb87c237e479aef1a4ee5904486b1d93498b14f6a9836e61ec9fd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc47\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd=\n\ud835\udc3e\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ud835\udc36\ufffd+ \ud835\udc36\ufffd+ \ud835\udc36\ufffd+ \ud835\udc36\ufffd\n \n(20) \nFor algorithm performance, top-1 classification accuracy, \nsignal-to-noise ratio (SNR), and structural similarity index \nmeasure (SSIM) are employed for ResNet18, FIR filtering, and \nGaussian image filtering, respectively. \nDuring the experimental evaluation, convolution-based \nalgorithm models are applied to the compilation framework to \nobtain evaluation results by the CIM-aware evaluator. \nSpecifically, in Part.", "mimetype": "text/plain", "start_char_idx": 36156, "end_char_idx": 36615, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2617f1c9-9be0-4bb8-abb9-111ee1b6d31d": {"__data__": {"id_": "2617f1c9-9be0-4bb8-abb9-111ee1b6d31d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c1b0741-1cac-4b04-89ea-a0ca8640b1d0", "node_type": "1", "metadata": {}, "hash": "5dc7b275fbe88339654bdcc10d8044a189150988c5113c33c319b7a8cb9f30f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f45badec-6d97-433d-9f1b-de4bf4dc146f", "node_type": "1", "metadata": {}, "hash": "01254d7bd68015efa3e97532942c77ce05f3d076b950906b395d77c5e78de427", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "B, the comparison between different BLSs \nis conducted by manually selected BLSs with automatically \ngenerated WLS parameters. MAQE are uniformly disabled for \nfair comparisons. In Part. C, weight mapping strategies are \nselected automatically, and the MAQE parameters are manually \nconfigured to show the tradeoff between accuracy and hardware \nLow-Bit X\nLow-Bit W\nx2\nx1\n\u2026\nw2\nw1\n\u2026\n\u2026\n\u2026\nLow-Bit X + Low-Bit W\n\u2026\n\u2026\n\u2026\nHigh-bit X + High-bit W\n\u2026\n\u2026\nLow-Bit X + High-bit", "mimetype": "text/plain", "start_char_idx": 36616, "end_char_idx": 37078, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f45badec-6d97-433d-9f1b-de4bf4dc146f": {"__data__": {"id_": "f45badec-6d97-433d-9f1b-de4bf4dc146f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2617f1c9-9be0-4bb8-abb9-111ee1b6d31d", "node_type": "1", "metadata": {}, "hash": "447d8ed29e8fb87c237e479aef1a4ee5904486b1d93498b14f6a9836e61ec9fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a51bb02-7264-4564-9e6a-a0ddac7f5fbc", "node_type": "1", "metadata": {}, "hash": "bed88c842ac689b7efbe1d8240c9525f75001d942bffb2d95b3c9602ef8acb4c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "W\n\u2026\n\u2026\n\u2026\nHigh-bit X + Low-Bit W\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n4 times\nHigh-bit X\nHigh-bit W\nPSUM1\nPSUM2\nPSUM4\nPSUM3\nXdvd\nWdvd\n2 times\n2 times\n \nFig. 10. An example of MAQE on a 128\u00d764 CIM macro with 5-bit ADCs.", "mimetype": "text/plain", "start_char_idx": 37079, "end_char_idx": 37268, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a51bb02-7264-4564-9e6a-a0ddac7f5fbc": {"__data__": {"id_": "5a51bb02-7264-4564-9e6a-a0ddac7f5fbc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f45badec-6d97-433d-9f1b-de4bf4dc146f", "node_type": "1", "metadata": {}, "hash": "01254d7bd68015efa3e97532942c77ce05f3d076b950906b395d77c5e78de427", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5eae40bc-aea7-454f-9be5-ff9065e10b2a", "node_type": "1", "metadata": {}, "hash": "4647af8d6c96b6021b610587b6fe7006988fe2ef3c584b4074d38fcb47b514e7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc65\ufffd and \ud835\udc64\ufffd are both divided into high-bit parts and low-bit parts by the \ndividers \ud835\udc4b\ufffd\ufffd\ufffd and \ud835\udc4a\n\ufffd\ufffd\ufffd to obtain PSUM1-PSUM4 with different AQE. \nTop-Level Controller\nInstruction Register File\nOff-\nchip\nDRAM\nScale&Add\nADC\n128*64\nCIM Macro\nActivation\nPooling\nScratch \nPad\nBuffer\nManager\n BANK A\n(1024KB)\n BANK B\n(1024KB)\nIFM\nOFM\nWeight\nFIFO\nWeight\nCalib.\n \nFig.", "mimetype": "text/plain", "start_char_idx": 37270, "end_char_idx": 37624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5eae40bc-aea7-454f-9be5-ff9065e10b2a": {"__data__": {"id_": "5eae40bc-aea7-454f-9be5-ff9065e10b2a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a51bb02-7264-4564-9e6a-a0ddac7f5fbc", "node_type": "1", "metadata": {}, "hash": "bed88c842ac689b7efbe1d8240c9525f75001d942bffb2d95b3c9602ef8acb4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b18e431-1ae6-4bd9-b530-6f3149c864b3", "node_type": "1", "metadata": {}, "hash": "d3e85adbfbe745bc24dd91e8bbe4a2a33ed0f457cc751c96375e6a111282a50a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "11. The top-level architecture of the SRAM-CIM-based accelerator. \nThis article has been accepted for publication in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. This is the author's version which has not been fully e\ncontent may change prior to final publication. Citation information: DOI 10.1109/TCAD.2024.3366025\n\u00a9 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\ufffd\ufffdSee https://www.ieee.org/publications/rights/index.html for more information.", "mimetype": "text/plain", "start_char_idx": 37625, "end_char_idx": 38153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b18e431-1ae6-4bd9-b530-6f3149c864b3": {"__data__": {"id_": "7b18e431-1ae6-4bd9-b530-6f3149c864b3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5eae40bc-aea7-454f-9be5-ff9065e10b2a", "node_type": "1", "metadata": {}, "hash": "4647af8d6c96b6021b610587b6fe7006988fe2ef3c584b4074d38fcb47b514e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5104dde-612b-4da9-a3e2-5ae3eab0b0a0", "node_type": "1", "metadata": {}, "hash": "a3b02d8992b515ab8b4cafb0f6f2cdb735979a5b6d2ebc1b2cd9550e048ae3fa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Authorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:46:33 UTC from IEEE Xplore.  Restrictions apply. \n10 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nthroughput. In Part. D, both weight mapping strategies and \nMAQE parameters are selected or generated automatically to \nevaluate the overall compilation time. \nB. Evaluation of Weight Mapping Strategies \nSeveral frequently-used types of convolution layers are \nselected to demonstrate the optimization of the proposed weight \nmapping strategies.", "mimetype": "text/plain", "start_char_idx": 38154, "end_char_idx": 38726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5104dde-612b-4da9-a3e2-5ae3eab0b0a0": {"__data__": {"id_": "f5104dde-612b-4da9-a3e2-5ae3eab0b0a0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b18e431-1ae6-4bd9-b530-6f3149c864b3", "node_type": "1", "metadata": {}, "hash": "d3e85adbfbe745bc24dd91e8bbe4a2a33ed0f457cc751c96375e6a111282a50a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de1bba2e-8038-4056-82af-ab21c2bb3533", "node_type": "1", "metadata": {}, "hash": "56d61fc5e0cfe75faed5d271c688268f52f54565f4564bd20d44a6f484770950", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "All convolution layers are configured with \n8-bit inputs, 8-bit weights, and \ud835\udc60\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc51\ud835\udc52= 1  for uniform \nevaluation. Table I provides comparisons of the utilization \nachieved by the traditional and proposed strategies. The \ntraditional SBIPW [22] achieves the same 100% utilization as \nISP in Case 1. In Case 2, similarly, SBIPW records the highest \nutilization. However, in Case 3 and Case 4, SBIPW yields the \nlowest utilization.", "mimetype": "text/plain", "start_char_idx": 38727, "end_char_idx": 39154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de1bba2e-8038-4056-82af-ab21c2bb3533": {"__data__": {"id_": "de1bba2e-8038-4056-82af-ab21c2bb3533", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5104dde-612b-4da9-a3e2-5ae3eab0b0a0", "node_type": "1", "metadata": {}, "hash": "a3b02d8992b515ab8b4cafb0f6f2cdb735979a5b6d2ebc1b2cd9550e048ae3fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4a5a5c0-d280-44c5-8c28-017aac86330d", "node_type": "1", "metadata": {}, "hash": "ee58258122cd01886bcf3fe629ffac31c94ff93c7825cf56ef9f41d4b67da30e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This is attributed to SBIPW\u2019s inability to \naddress the constraint posed by the shortage of input channel \ncount.  \nCase1 represents the hidden layers of deep CNNs [46] and \nhas both large input and output channel count. Since the input \nchannel count is the same as \ud835\udc3f\ufffd\ufffd\ufffd\ufffd\ufffd, and the output channel \ncount is two times of \ud835\udc3f\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd. A 1\u00d71 WLS is sufficient for all \nthree strategies. Both SBIPW and ISP ensure the complete \nutilization of the SRAM-CIM macro.", "mimetype": "text/plain", "start_char_idx": 39155, "end_char_idx": 39610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4a5a5c0-d280-44c5-8c28-017aac86330d": {"__data__": {"id_": "c4a5a5c0-d280-44c5-8c28-017aac86330d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de1bba2e-8038-4056-82af-ab21c2bb3533", "node_type": "1", "metadata": {}, "hash": "56d61fc5e0cfe75faed5d271c688268f52f54565f4564bd20d44a6f484770950", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d0711fd-4ece-475d-8540-0452799f2e9f", "node_type": "1", "metadata": {}, "hash": "43c6046acc79861c07f0a6f7a7b6c8be74cd721ddcdc59df2864bab56f61143b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "IOSP results in a 50.00% \nutilization because shifted weight copies results in additional \nspare cells.  \nCase 2 corresponds to the last convolution layer of image-\ndenoising CNNs with a limited output channel count. The \ntraditional SBIPW achieves the highest utilization, which is \nattributed to SBIPW\u2019s specialization in improving coverage \nalong the output side direction. \nCase 3 represents the first convolution layer of image \nclassification CNNs with a limited input channel count [47].", "mimetype": "text/plain", "start_char_idx": 39611, "end_char_idx": 40105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d0711fd-4ece-475d-8540-0452799f2e9f": {"__data__": {"id_": "3d0711fd-4ece-475d-8540-0452799f2e9f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4a5a5c0-d280-44c5-8c28-017aac86330d", "node_type": "1", "metadata": {}, "hash": "ee58258122cd01886bcf3fe629ffac31c94ff93c7825cf56ef9f41d4b67da30e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8526dd5b-3d8e-442e-b2c2-ce6e2f50482a", "node_type": "1", "metadata": {}, "hash": "9a8eaf714fb1c5ab974a9fc9a558b6bc51d2589e9bd1ab5cbf2ae2bf34c6a5a2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Since the output channel count is sufficiently large compared \nwith \ud835\udc3f\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd, both WLS and SBIPW fails to improve the overall \nmacro utilization. While ISP can efficiently complement the \nshortage of input channel count, and a 63.29% utilization \npromotion is achieved compared to SBIPW. As shown in Fig. \n12(a), the employment of SBIPW extends the weight to eight \nmacro chunks along the output side direction, but each macro \nchunk has a considerable among of spare cells.", "mimetype": "text/plain", "start_char_idx": 40107, "end_char_idx": 40580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8526dd5b-3d8e-442e-b2c2-ce6e2f50482a": {"__data__": {"id_": "8526dd5b-3d8e-442e-b2c2-ce6e2f50482a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d0711fd-4ece-475d-8540-0452799f2e9f", "node_type": "1", "metadata": {}, "hash": "43c6046acc79861c07f0a6f7a7b6c8be74cd721ddcdc59df2864bab56f61143b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2694119-f476-4d3f-8a21-89fff41abcb3", "node_type": "1", "metadata": {}, "hash": "b5a1ff1ff29bd811335871f6b5924d6d636900299fd80e3ba8420bc88bc682b1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In contrast, ISP \ninvolves two macro chunks along the input side direction, with \nthe majority of memory cells being effectively utilized. \nCase 4 corresponds to the depth-wise (DW) convolution \n \nTABLE I \nUTILIZATION OF FREQUENTLY-USED CONVOLUTION LAYERS \n \nConvolution Layers \nTraditional \nProposed \nSBIPW \nJSSC\u20192020 \nISP \nIOSP \nCase1: \nCONV3\u00d73 \nICH=128,OCH=128 \n1\u00d71 WLS \n1\u00d71 WLS \n1\u00d71 WLS \n100.", "mimetype": "text/plain", "start_char_idx": 40581, "end_char_idx": 40977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2694119-f476-4d3f-8a21-89fff41abcb3": {"__data__": {"id_": "b2694119-f476-4d3f-8a21-89fff41abcb3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8526dd5b-3d8e-442e-b2c2-ce6e2f50482a", "node_type": "1", "metadata": {}, "hash": "9a8eaf714fb1c5ab974a9fc9a558b6bc51d2589e9bd1ab5cbf2ae2bf34c6a5a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5254579-2d07-4115-ae95-88338a412a36", "node_type": "1", "metadata": {}, "hash": "49e9e72945aab860c1748e58976d4cd3c68f5a58b87c18a8154a068610d15529", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "00% \n100.00% \n50.00% \nCase2: \nCONV3\u00d73 \nICH=32,OCH=3 \n1\u00d72 WLS \n3\u00d77 WLS \n1\u00d71 WLS \n56.25% \n19.69% \n37.50% \nCase3: \nCONV3\u00d73 \nICH=3,OCH=64 \n1\u00d71 WLS \n1\u00d71 WLS \n1\u00d71 WLS \n21.09% \n84.38% \n42.", "mimetype": "text/plain", "start_char_idx": 40977, "end_char_idx": 41158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5254579-2d07-4115-ae95-88338a412a36": {"__data__": {"id_": "f5254579-2d07-4115-ae95-88338a412a36", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2694119-f476-4d3f-8a21-89fff41abcb3", "node_type": "1", "metadata": {}, "hash": "b5a1ff1ff29bd811335871f6b5924d6d636900299fd80e3ba8420bc88bc682b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1c5d6c1-5384-4f7f-a260-9a0749afe0bc", "node_type": "1", "metadata": {}, "hash": "06e28ade75ae0c8b96fa400b390ebf58e601c0b655beef4ca2cfe6e8679fc60c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "19% \nCase4: \nCONVDW3\u00d73 \n4\u00d72 WLS \n4\u00d73 WLS \n2\u00d72 WLS \n7.03% \n8.04% \n28.13% \n \nICH 2\n\u2026\nChunk 1\nChunk 8\n\u2026\n3 ICH\nOCH\n61-64\nOCH 2\nKernel Weight\n1\u00d71 WLS + SBIPW : 21.1% Utilization\n1\u00d71 WLS + ISP : 84.38%", "mimetype": "text/plain", "start_char_idx": 41158, "end_char_idx": 41353, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1c5d6c1-5384-4f7f-a260-9a0749afe0bc": {"__data__": {"id_": "e1c5d6c1-5384-4f7f-a260-9a0749afe0bc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5254579-2d07-4115-ae95-88338a412a36", "node_type": "1", "metadata": {}, "hash": "49e9e72945aab860c1748e58976d4cd3c68f5a58b87c18a8154a068610d15529", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1194fa25-d58a-43c0-8848-9a8ceec7d985", "node_type": "1", "metadata": {}, "hash": "1ac2ebe7738e01d49568c54030d660dc094bf5165357a57cdcd4ffe2c9a3ff1a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Utilization\n5\n5\n4\n4\n3\n3\n2\n2\n1\n1\n8-bit w\nMSB\nLSB\n\u2026\n\u2026\nChunk 2\nChunk 1\n\u2026\n\u2026\n\u2026\n8-bit w\nLSB\nMSB\nSpare\nSpare\nSpare\nOCH 1\n1\n1\n1\n7\n1\n1\n1\n8\n1\n1\n9\n1\n1\n1\n4\n1\n1\n1\n5\n1\n1\n6\n1\n1\n1\n1\n1\n2\n1", "mimetype": "text/plain", "start_char_idx": 41354, "end_char_idx": 41525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1194fa25-d58a-43c0-8848-9a8ceec7d985": {"__data__": {"id_": "1194fa25-d58a-43c0-8848-9a8ceec7d985", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1c5d6c1-5384-4f7f-a260-9a0749afe0bc", "node_type": "1", "metadata": {}, "hash": "06e28ade75ae0c8b96fa400b390ebf58e601c0b655beef4ca2cfe6e8679fc60c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19097887-91eb-4dbe-ab29-1d0fe3e46dae", "node_type": "1", "metadata": {}, "hash": "79eb038f0078a1572253a59e9dd649cd81aad31178016e385ebde7a37a253229", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1\n3\n\u2026\n1\n1\n1\n7\n1\n1\n1\n8\n1\n1\n9\n1\n1\n1\n4\n1\n1\n1\n5\n1\n1\n6\n1\n1\n1\n1\n1\n2\n1\n1\n3\n1\n1\n1\n7\n1\n1\n1\n8\n1\n1\n9\n1\n1\n1\n4\n1\n1\n1\n5\n1\n1\n6\n1\n1\n1\n1\n1\n2\n1\n1", "mimetype": "text/plain", "start_char_idx": 41526, "end_char_idx": 41653, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19097887-91eb-4dbe-ab29-1d0fe3e46dae": {"__data__": {"id_": "19097887-91eb-4dbe-ab29-1d0fe3e46dae", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1194fa25-d58a-43c0-8848-9a8ceec7d985", "node_type": "1", "metadata": {}, "hash": "1ac2ebe7738e01d49568c54030d660dc094bf5165357a57cdcd4ffe2c9a3ff1a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7e05c07-469e-40b9-a258-1ca79e16000e", "node_type": "1", "metadata": {}, "hash": "2e0c57650b3a4f4ea4044f927376e958833d996ee61767e09f8989a63b2cee66", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "3\n1\n1\n1\n7\n1\n1\n1\n8\n1\n1\n9\n1\n1\n1\n4\n1\n1\n1\n5\n1\n1\n6\n1\n1\n1\n1\n1\n2\n1\n1\n3\n\u2026\nOCH 1\nOCH 2\nOCH 63\nOCH 64\nICH 1\nICH 3\n \n(a) \n8-bit w\nMSB\nLSB\nChunk 1\n3\u00d73 Weights\nKernel \nWeight\n4\u00d72", "mimetype": "text/plain", "start_char_idx": 41654, "end_char_idx": 41819, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7e05c07-469e-40b9-a258-1ca79e16000e": {"__data__": {"id_": "f7e05c07-469e-40b9-a258-1ca79e16000e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19097887-91eb-4dbe-ab29-1d0fe3e46dae", "node_type": "1", "metadata": {}, "hash": "79eb038f0078a1572253a59e9dd649cd81aad31178016e385ebde7a37a253229", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "499ac069-0437-4725-af5d-f45920be32df", "node_type": "1", "metadata": {}, "hash": "98650f6bfcaf13882d7fbef5112decd4ffeaefb5c8ad4051cb400429b58688b7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "WLS+SBIPW : 7.03% Utilization\n2\u00d72 WLS + IOSP : 28.13% Utilization\nChunk 1\n8-bit w\nMSB\nLSB\n8 shift copies\nSpare\n1\n1\n1\n7\n1\n1\n8\n9\n1\n1\n4\n1\n1\n5\n6\n1\n2\n3\n7-9\n1-3\nSpare\nSpare\n4-6\n3\n2\n1\n \n(b) \nFig. 12.", "mimetype": "text/plain", "start_char_idx": 41820, "end_char_idx": 42012, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "499ac069-0437-4725-af5d-f45920be32df": {"__data__": {"id_": "499ac069-0437-4725-af5d-f45920be32df", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7e05c07-469e-40b9-a258-1ca79e16000e", "node_type": "1", "metadata": {}, "hash": "2e0c57650b3a4f4ea4044f927376e958833d996ee61767e09f8989a63b2cee66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e05b0377-7383-4dff-9094-2b7774acd686", "node_type": "1", "metadata": {}, "hash": "344a37f29dd57edec760cc206ba9b4701bd2bb9ca5e09399406c8a4797d9c017", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The mapping results of traditional SBIPW and proposed ISP and IOSP strategies on diferent cases. (a) the 3\u00d73 convolution layer with 3 input channels \nand 64 output channels, (b) the 3\u00d73 depth-wise convolution layer. \nThis article has been accepted for publication in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. This is the author's version which has not been fully e\ncontent may change prior to final publication.", "mimetype": "text/plain", "start_char_idx": 42013, "end_char_idx": 42461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e05b0377-7383-4dff-9094-2b7774acd686": {"__data__": {"id_": "e05b0377-7383-4dff-9094-2b7774acd686", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "499ac069-0437-4725-af5d-f45920be32df", "node_type": "1", "metadata": {}, "hash": "98650f6bfcaf13882d7fbef5112decd4ffeaefb5c8ad4051cb400429b58688b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f3c33e7-0c47-429b-9568-262f51a01c4f", "node_type": "1", "metadata": {}, "hash": "d3bd4d26d618bf3d2e8feb39ec2f696b7df5c6b579b1673dbbb34eade11fd4df", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Citation information: DOI 10.1109/TCAD.2024.3366025\n\u00a9 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\ufffd\ufffdSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:46:33 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 4161, "end_char_idx": 4528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f3c33e7-0c47-429b-9568-262f51a01c4f": {"__data__": {"id_": "2f3c33e7-0c47-429b-9568-262f51a01c4f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e05b0377-7383-4dff-9094-2b7774acd686", "node_type": "1", "metadata": {}, "hash": "344a37f29dd57edec760cc206ba9b4701bd2bb9ca5e09399406c8a4797d9c017", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31a66546-a080-4035-93f0-68f7fe5d625a", "node_type": "1", "metadata": {}, "hash": "729e2194e1c67dc89c1bc64fc394157288adc5e49f87b56770e8bc2945bce02f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "11 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nlayer commonly found in lightweight CNNs and Gaussian \nimage filtering [6]. Since each output channel is calculated \nbased on a single channel of input and weight, multi-channel \nDW convolution equates to performing multiple 1-channel DW \nconvolutions. Thus the DW convolution exhibits a deficiency \nin both input and output channel count. While the employment \nof SBIPW can enlarge the coverage of the output side, it fails \nto address the coverage insufficiency along the input side \ndirection.", "mimetype": "text/plain", "start_char_idx": 42831, "end_char_idx": 43415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31a66546-a080-4035-93f0-68f7fe5d625a": {"__data__": {"id_": "31a66546-a080-4035-93f0-68f7fe5d625a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f3c33e7-0c47-429b-9568-262f51a01c4f", "node_type": "1", "metadata": {}, "hash": "d3bd4d26d618bf3d2e8feb39ec2f696b7df5c6b579b1673dbbb34eade11fd4df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7fef8d0-5273-4882-a0e9-1ef09fcce340", "node_type": "1", "metadata": {}, "hash": "dcbbd1f85bc667cfeb61d46919d0bfd36dbf4a3d91a97736c935cfb324d812b1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "IOSP strategy proves more appropriate because of the \ncapability to improve the coverage along both the input and \noutput side directions. Fig. 12(b) shows a 1-channel 3\u00d7 3 \ndepth-wise convolution layer as a basic illustration. The SBIPW \ncan completely cover the macro along the output side direction, \nbut there exists a significant area of spare cells along the input \nside direction. In contrast, the IOSP improves the utilization by \n21.10% compared to SBIPW because of heightened coverage \nalong the input side direction.", "mimetype": "text/plain", "start_char_idx": 43416, "end_char_idx": 43943, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7fef8d0-5273-4882-a0e9-1ef09fcce340": {"__data__": {"id_": "d7fef8d0-5273-4882-a0e9-1ef09fcce340", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31a66546-a080-4035-93f0-68f7fe5d625a", "node_type": "1", "metadata": {}, "hash": "729e2194e1c67dc89c1bc64fc394157288adc5e49f87b56770e8bc2945bce02f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d62b6d8c-c24b-4fc0-8007-9bed085ff6c8", "node_type": "1", "metadata": {}, "hash": "c121286f161afce8c29f08425d771eececcb53b5910878b997485914766c3418", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "C. Evaluation of MAQE strategy \nThree convolution-based algorithms are chosen to showcase \nthe effectiveness of the proposed error correction schemes: \nResNet18, FIR filtering, and Gaussian image filtering. \n(1) Convolution Neural Network \nResNet18 is chosen as an example of the CNN model for \nimage classification. The pre-trained floating network model is \ndirectly downloaded from Pytorch without pruning or fine-\ntuning. Subsequently, the model is fed to the proposed \nframework for quantization and compiling.", "mimetype": "text/plain", "start_char_idx": 43946, "end_char_idx": 44461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d62b6d8c-c24b-4fc0-8007-9bed085ff6c8": {"__data__": {"id_": "d62b6d8c-c24b-4fc0-8007-9bed085ff6c8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7fef8d0-5273-4882-a0e9-1ef09fcce340", "node_type": "1", "metadata": {}, "hash": "dcbbd1f85bc667cfeb61d46919d0bfd36dbf4a3d91a97736c935cfb324d812b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e60ef1b1-dc9f-42ec-a0f2-4250c3800f17", "node_type": "1", "metadata": {}, "hash": "9046eb09993122c726a01d896d48fd9242f383aefa57e12eef1efdbd812f5345", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The accuracy is \nobtained using 1000 randomly selected images from the \nImageNet validation set. The top-1 classification accuracy of \n1000 classes is adopted to represent the algorithm accuracy.  \nFor SRAM-CIM deployment, the combination of 16-bit \ninputs and 8-bit weights is selected to achieve a comparable \nlevel of accuracy to the floating point result (70.1%). The 1\u00d71 \nWLS + SBIPW is employed for all layers to support signed \nweights. In Fig.", "mimetype": "text/plain", "start_char_idx": 44462, "end_char_idx": 44913, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e60ef1b1-dc9f-42ec-a0f2-4250c3800f17": {"__data__": {"id_": "e60ef1b1-dc9f-42ec-a0f2-4250c3800f17", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d62b6d8c-c24b-4fc0-8007-9bed085ff6c8", "node_type": "1", "metadata": {}, "hash": "c121286f161afce8c29f08425d771eececcb53b5910878b997485914766c3418", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b5123d1-1b2a-4c30-aad7-d7903ad76254", "node_type": "1", "metadata": {}, "hash": "7109c85a15544449644f743ba59880d58126df367af185d38fcc9289b9880986", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "13(a), the obtained top-1 classification \naccuracy employing the MAQE strategy is presented with \nvarying values of of \ud835\udc4b\ufffd\ufffd\ufffd and \ud835\udc4a\n\ufffd\ufffd\ufffd. The accuracy exhibits a \ndecreasing trend as both \ud835\udc4b\ufffd\ufffd\ufffd and \ud835\udc4a\n\ufffd\ufffd\ufffd increase. This is \nbecause a higher proportion of partial sums would be \nconfigured with larger ADC quantization error with elevated \nvalues of \ud835\udc4b\ufffd\ufffd\ufffd or \ud835\udc4a\n\ufffd\ufffd\ufffd.", "mimetype": "text/plain", "start_char_idx": 44914, "end_char_idx": 45272, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b5123d1-1b2a-4c30-aad7-d7903ad76254": {"__data__": {"id_": "4b5123d1-1b2a-4c30-aad7-d7903ad76254", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e60ef1b1-dc9f-42ec-a0f2-4250c3800f17", "node_type": "1", "metadata": {}, "hash": "9046eb09993122c726a01d896d48fd9242f383aefa57e12eef1efdbd812f5345", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16a852c8-932a-4fcf-9060-7b8178f45567", "node_type": "1", "metadata": {}, "hash": "0b3f39c7a7d73b9bf31e9bc5af77d7f7c4f39b634f091bd5a6e8344632e933af", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Notably, the accuracy drops to zero \nwhen \ud835\udc4a\n\ufffd\ufffd\ufffd reaches or exceeds 4, while for \ud835\udc4b\ufffd\ufffd\ufffd this value is \n14. This indicates that the weight causes a more significant \nimpact on the ADC quantization error. \nFig. 14(a) diagrams the accuracy-throughput tradeoff under \ndifferent configurations. For the points with the same color, the \nleftmost points refer to \ud835\udc4b\ufffd\ufffd\ufffd= 0, and \ud835\udc4b\ufffd\ufffd\ufffd of the right adjacent \npoint is incremented by two compared to each point.", "mimetype": "text/plain", "start_char_idx": 45273, "end_char_idx": 45718, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16a852c8-932a-4fcf-9060-7b8178f45567": {"__data__": {"id_": "16a852c8-932a-4fcf-9060-7b8178f45567", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b5123d1-1b2a-4c30-aad7-d7903ad76254", "node_type": "1", "metadata": {}, "hash": "7109c85a15544449644f743ba59880d58126df367af185d38fcc9289b9880986", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6695cee5-9101-4c5d-9f44-600f88f1a617", "node_type": "1", "metadata": {}, "hash": "c7a7bfd2d17b743e03b1cd2ca1253b5ee02bd2593773bc5d5efe2961aa0c0648", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The direct \ndeployment without CCVLE and MAQE achieves the highest \nthroughput but yields zero accuracy. Sole employment of \nCCVLE makes no difference in accuracy, indicating the error-\nsensitivity of the ImageNet-based classification. The rest points \nrepresent the evaluation results of different configurations of \nthe MAQE strategy. The circled points show top-1 accuracy \nranging from 66.3% to 70.1%, which is similar to the floating \npoint result.", "mimetype": "text/plain", "start_char_idx": 45719, "end_char_idx": 46172, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6695cee5-9101-4c5d-9f44-600f88f1a617": {"__data__": {"id_": "6695cee5-9101-4c5d-9f44-600f88f1a617", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16a852c8-932a-4fcf-9060-7b8178f45567", "node_type": "1", "metadata": {}, "hash": "0b3f39c7a7d73b9bf31e9bc5af77d7f7c4f39b634f091bd5a6e8344632e933af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd3d8818-768b-45c0-9802-ded4460d491a", "node_type": "1", "metadata": {}, "hash": "09504baa180c1819d114ab1c0dfa7e5ff36e95c946da3d981d104f1d551ef596", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Notably, a 33.3% throughput promotion is \nachieved by \ud835\udc64\ufffd\ufffd\ufffd= 0, \ud835\udc65\ufffd\ufffd\ufffd= 8 with only a 0.4% drop in top-\n1 accuracy compared to \ud835\udc4a\n\ufffd\ufffd\ufffd= 0, \ud835\udc4b\ufffd\ufffd\ufffd= 0. \n(2) Digital Signal Processing \nA 64-tap band-pass FIR filtering is employed as an example \nof DSP to filter the 500Hz and 100Hz noises from the 250Hz \nsignal with an input size of 1200 points.", "mimetype": "text/plain", "start_char_idx": 46173, "end_char_idx": 46509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd3d8818-768b-45c0-9802-ded4460d491a": {"__data__": {"id_": "cd3d8818-768b-45c0-9802-ded4460d491a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6695cee5-9101-4c5d-9f44-600f88f1a617", "node_type": "1", "metadata": {}, "hash": "c7a7bfd2d17b743e03b1cd2ca1253b5ee02bd2593773bc5d5efe2961aa0c0648", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c7d91c3-d464-4a7e-a0ca-c16305069142", "node_type": "1", "metadata": {}, "hash": "91d53735d79bf9d3f379e599bc82ad24b510e8ad83621fb750b73ff7b2b11168", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The lower/upper \nstopband edge frequencies and lower/upper passband edge \nfrequencies are 100Hz/400Hz and 200/300Hz, respectively \n[48]. The effectiveness of the algorithm is represented by the \nsignal-to-noise ratio (SNR) between the filtered and ideal \nwaveform.  \nThe combination of an 8-bit input signal and 10-bit filter \nweight is selected for CIM deployment to attain a comparable \nSNR to the floating point result (44.67dB).", "mimetype": "text/plain", "start_char_idx": 46510, "end_char_idx": 46942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c7d91c3-d464-4a7e-a0ca-c16305069142": {"__data__": {"id_": "5c7d91c3-d464-4a7e-a0ca-c16305069142", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd3d8818-768b-45c0-9802-ded4460d491a", "node_type": "1", "metadata": {}, "hash": "09504baa180c1819d114ab1c0dfa7e5ff36e95c946da3d981d104f1d551ef596", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c8d574b-2f2d-4fc8-b83f-1456a9739b5d", "node_type": "1", "metadata": {}, "hash": "de72aaeec2d92b5587a2782e77392aab5f9512fe30c68fcf9d3db2fac48448e2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The 1\u00d732 weight-\nlevel strategy and SBIPW bit-level strategy are employed to \nmaximize the macro utilization. Fig. 13(b) provides a \nvisualization of the attained SNR using the MAQE strategy \nunder various combinations of \ud835\udc4b\ufffd\ufffd\ufffd and \ud835\udc4a\n\ufffd\ufffd\ufffd values. The \naccuracy pattern also follows a decreasing trend as both \ud835\udc4b\ufffd\ufffd\ufffd \nand \ud835\udc4a\n\ufffd\ufffd\ufffd increase because a higher proportion of partial sums \nconfigured with larger ADC quantization error. \nFig. 14(b) diagrams the tradeoff between SNR and \nthroughput across diverse configurations.", "mimetype": "text/plain", "start_char_idx": 46943, "end_char_idx": 47459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c8d574b-2f2d-4fc8-b83f-1456a9739b5d": {"__data__": {"id_": "1c8d574b-2f2d-4fc8-b83f-1456a9739b5d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c7d91c3-d464-4a7e-a0ca-c16305069142", "node_type": "1", "metadata": {}, "hash": "91d53735d79bf9d3f379e599bc82ad24b510e8ad83621fb750b73ff7b2b11168", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "091dd910-213b-41e4-9c53-9dd6ff0bb16f", "node_type": "1", "metadata": {}, "hash": "9a8957b877b952c8c2fe13a45f3d3f2259cca69cbb0ba58e25857810ae7448b6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A direct deployment \nhas the highest throughput but the lowest SNR value. A mere \nCCVLE increases SNR to 13.89dB, which is still far from the \nXdvd\nTop-1 Accuracy(%)\nResnet18\nXdvd\nSNR(dB)\nFIR Filtering\nXdvd\nSSIM\nGaussian Image Filtering\n \n(a)                                                                                  (b)                                                                                 (c) \nFig. 13.", "mimetype": "text/plain", "start_char_idx": 47460, "end_char_idx": 47881, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "091dd910-213b-41e4-9c53-9dd6ff0bb16f": {"__data__": {"id_": "091dd910-213b-41e4-9c53-9dd6ff0bb16f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c8d574b-2f2d-4fc8-b83f-1456a9739b5d", "node_type": "1", "metadata": {}, "hash": "de72aaeec2d92b5587a2782e77392aab5f9512fe30c68fcf9d3db2fac48448e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08ee27fc-2100-4d2f-8070-e497f1a44c9c", "node_type": "1", "metadata": {}, "hash": "344a37f29dd57edec760cc206ba9b4701bd2bb9ca5e09399406c8a4797d9c017", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The algorithm accuracy of MAQE strategy with different \ud835\udc4a\n\ufffd\ufffd\ufffd and \ud835\udc4b\ufffd\ufffd\ufffd : (a) the top-1 classification accuracy of ResNet18, (b) the Signal-to-Noise \nRatio (SNR) of FIR filtering, (c) the structural similarity index measure (SSIM) of Gaussian image filtering. \nThis article has been accepted for publication in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. This is the author's version which has not been fully e\ncontent may change prior to final publication.", "mimetype": "text/plain", "start_char_idx": 47882, "end_char_idx": 48372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08ee27fc-2100-4d2f-8070-e497f1a44c9c": {"__data__": {"id_": "08ee27fc-2100-4d2f-8070-e497f1a44c9c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "091dd910-213b-41e4-9c53-9dd6ff0bb16f", "node_type": "1", "metadata": {}, "hash": "9a8957b877b952c8c2fe13a45f3d3f2259cca69cbb0ba58e25857810ae7448b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad0cfb61-29a4-4154-8e66-5a1b6feeb3e5", "node_type": "1", "metadata": {}, "hash": "dc4a30abecb07b657b91f9c5bb34d329de72c290e39d51108eb8cbb45c2510ff", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Citation information: DOI 10.1109/TCAD.2024.3366025\n\u00a9 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\ufffd\ufffdSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:46:33 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 4161, "end_char_idx": 4528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad0cfb61-29a4-4154-8e66-5a1b6feeb3e5": {"__data__": {"id_": "ad0cfb61-29a4-4154-8e66-5a1b6feeb3e5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08ee27fc-2100-4d2f-8070-e497f1a44c9c", "node_type": "1", "metadata": {}, "hash": "344a37f29dd57edec760cc206ba9b4701bd2bb9ca5e09399406c8a4797d9c017", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e666599-af44-4415-8492-d39946c5b04b", "node_type": "1", "metadata": {}, "hash": "e5c3c7a5bbd905592dfe58c55ac4f02b8ce8b38de0108e9591b76d82103b1617", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "12 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nfloating point result. The MAQE strategy depicted in the result \npoints exhibits a wide range of SNR to fulfill the SNR-\nthroughput requirement in various application scenarios. \n(3) Digital Image Processing \nA Gaussian image filtering with a kernel size of 3\u00d73 is used \nas an example of DIP to remove Gaussian noise from the Lena \nimage [49]. The weights follow a Gaussian distribution with a \nmean of 0 and a variance of 0.5 [6].", "mimetype": "text/plain", "start_char_idx": 48742, "end_char_idx": 49261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e666599-af44-4415-8492-d39946c5b04b": {"__data__": {"id_": "4e666599-af44-4415-8492-d39946c5b04b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad0cfb61-29a4-4154-8e66-5a1b6feeb3e5", "node_type": "1", "metadata": {}, "hash": "dc4a30abecb07b657b91f9c5bb34d329de72c290e39d51108eb8cbb45c2510ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32a52e5c-1165-42fe-b110-41d1114fde62", "node_type": "1", "metadata": {}, "hash": "9c873287cea50c1615d5b6e8a8ee74c446fe6c1dc7c03c634744a3956305ebee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The input is a 256\u00d7256 \nLena image with Gaussian noise, which has a mean of 0 and a \nvariance of 9. The quality of the filtered images is assessed by \ncalculating the SSIM between the filtered and ideal images. \nThe 24-bit RGB image format and 8-bit filter weight are \nadopted for SRAM-CIM deployment to achieve a comparable \nSSIM to the floating point result (82.85%). The 2\u00d72 weight-\nlevel strategy and IOSP bit-level strategy are employed to \nmaximize the macro utilization. Fig.", "mimetype": "text/plain", "start_char_idx": 49262, "end_char_idx": 49744, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32a52e5c-1165-42fe-b110-41d1114fde62": {"__data__": {"id_": "32a52e5c-1165-42fe-b110-41d1114fde62", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e666599-af44-4415-8492-d39946c5b04b", "node_type": "1", "metadata": {}, "hash": "e5c3c7a5bbd905592dfe58c55ac4f02b8ce8b38de0108e9591b76d82103b1617", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2aa4a14d-3ee6-4e45-aa3f-f6adebc04a09", "node_type": "1", "metadata": {}, "hash": "c7eb2b4f1d36b089509a25dd4207a84dee334239020d64cbd4ae1c06da6d55a3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "13(c) illustrates the \nachieved SSIM with MAQE strategy with different values of \n\ud835\udc4b\ufffd\ufffd\ufffd and \ud835\udc4a\n\ufffd\ufffd\ufffd. The accuracy decreases with the increase of \n\ud835\udc4b\ufffd\ufffd\ufffd and \ud835\udc4a\n\ufffd\ufffd\ufffd because larger ADC quantization error.  \nFig. 14(c) diagrams the tradeoff between SSIM and \nthroughput under different configurations of MAQE strategy. A \nstraightforward deployment without calibration has the highest \nthroughput but zero SSIM value.", "mimetype": "text/plain", "start_char_idx": 49745, "end_char_idx": 50153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2aa4a14d-3ee6-4e45-aa3f-f6adebc04a09": {"__data__": {"id_": "2aa4a14d-3ee6-4e45-aa3f-f6adebc04a09", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32a52e5c-1165-42fe-b110-41d1114fde62", "node_type": "1", "metadata": {}, "hash": "9c873287cea50c1615d5b6e8a8ee74c446fe6c1dc7c03c634744a3956305ebee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bf2ba76-3186-4756-9394-bb8b65249acc", "node_type": "1", "metadata": {}, "hash": "0d9de487076115942991a87a74f5a4c300716a17f16fbffeb9207a5c1e65cbda", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A mere CCVLE increases \nSSIM to 25.54%, which is still much lower than the floating \npoint result. The MAQE strategy depicted in the result points \ncan fulfill the SSIM-throughput requirement in diverse \napplications. \nD. Compilation Time \nThe compilation time is closely related to the specific \nalgorithms. We use ResNet18 with 16-bit inputs, 8-bit weights, \nand a 68% accuracy demand as a representative. As shown in \nTable II, it takes 1432.82 seconds to generate the loadable file \nautomatically.", "mimetype": "text/plain", "start_char_idx": 50154, "end_char_idx": 50655, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8bf2ba76-3186-4756-9394-bb8b65249acc": {"__data__": {"id_": "8bf2ba76-3186-4756-9394-bb8b65249acc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2aa4a14d-3ee6-4e45-aa3f-f6adebc04a09", "node_type": "1", "metadata": {}, "hash": "c7eb2b4f1d36b089509a25dd4207a84dee334239020d64cbd4ae1c06da6d55a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffb775fe-762e-48c1-a3f6-b8f628bcfade", "node_type": "1", "metadata": {}, "hash": "1ce1eadbb6d3ba1262db62df738c4d6a3017b9eab5bc90a0dbcdf8d7c311eb35", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Quantization and obtainment of MAQE \nparameters occupy the major portion of the overall times due to \nthe intensive computation requirement for accuracy evaluation. \nE. Comparison with Previous Works \nTable III provides a comparison between the proposed \nframework \nand \nprevious \nCIM-oriented \ncompilation \nframeworks [28-30].", "mimetype": "text/plain", "start_char_idx": 50656, "end_char_idx": 50983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffb775fe-762e-48c1-a3f6-b8f628bcfade": {"__data__": {"id_": "ffb775fe-762e-48c1-a3f6-b8f628bcfade", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8bf2ba76-3186-4756-9394-bb8b65249acc", "node_type": "1", "metadata": {}, "hash": "0d9de487076115942991a87a74f5a4c300716a17f16fbffeb9207a5c1e65cbda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a8ecd39-d4bc-4bd7-9b42-b78260d5efad", "node_type": "1", "metadata": {}, "hash": "80cd0acdf18f798e703b911c698944d94826654162e78d733257c3c3535494d7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "It is important to note that the previous \nframeworks, such as TC-CIM, TDO-CIM, and OCC, were \ndeveloped for PCM-CIM systems [28-30] which have a \nResnet18\nFIR Filtering\nGaussian Image Filtering\nTop-1 Accuracy(%)\nNormalized Throughtput\nNormalized Throughtput\nNormalized Throughtput\nSNR(dB)\nSSIM\nSimilar accuracy to \nfloating-point result\nHigher Xdvd\n \n(a)                                                                                  (b)                                                                                 (c) \nFig. 14. The tradeoff between algorithm accuracy and normalized throughput.", "mimetype": "text/plain", "start_char_idx": 50984, "end_char_idx": 51585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a8ecd39-d4bc-4bd7-9b42-b78260d5efad": {"__data__": {"id_": "2a8ecd39-d4bc-4bd7-9b42-b78260d5efad", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffb775fe-762e-48c1-a3f6-b8f628bcfade", "node_type": "1", "metadata": {}, "hash": "1ce1eadbb6d3ba1262db62df738c4d6a3017b9eab5bc90a0dbcdf8d7c311eb35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "623d66f6-5aec-438a-8453-1f23249323e9", "node_type": "1", "metadata": {}, "hash": "843a8c9859657c0387fed04a8e86a4b518f42e878c48796c7b6040f0a71aacfb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(a), (b), and (c) represent ResNet18, FIR filtering, and Gaussian image filtering, \nrespectively. \nTABLE II \nCOMPILATION TIME OF RESNET18 \n \nProcessing Steps \nTime (s) \nQuantization Analyser \n192.3 \nFrontend IR \n1.2 \nMiddleend IR \n<0.1 \nBackend IR \nWeight Mapping \n<0.1 \nMAQE \n1396.8 \nOthers \n34.6 \nTotal \n1432.82 \n \nTABLE III \nCOMPARISON WITH CIM-ORIENTED COMPILATION FRAMEWORK \n \nFramework", "mimetype": "text/plain", "start_char_idx": 51586, "end_char_idx": 51977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "623d66f6-5aec-438a-8453-1f23249323e9": {"__data__": {"id_": "623d66f6-5aec-438a-8453-1f23249323e9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a8ecd39-d4bc-4bd7-9b42-b78260d5efad", "node_type": "1", "metadata": {}, "hash": "80cd0acdf18f798e703b911c698944d94826654162e78d733257c3c3535494d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63cf1a5a-40f9-49a7-83ea-c5e34e109fbe", "node_type": "1", "metadata": {}, "hash": "969bbe5371d485d08cf1b856ded4515c7afb157b611307aa59998c0bd7bc8dd5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "TC-CIM \nIMPACT \n\u20192020 \nTDO-CIM \nDATE \n\u20192020 \nOCC \nTCAD \n\u20192022 \nProposed \nMacro Type \nPCM \nPCM \nPCM \nSRAM \nQuantization \nSupport \n8-bit \n8-bit \n8-bit \nS/U X:1-16bit* \nS/U W:1-16bit \nCompilation \nIR \nHalide +  \nPolyhedral \nLLVM \nMLIR \nCustomized IR \nWeight \nMapping \nStrategy \nIm2Col \nIm2Col \nIm2Col \nWLS+BLS \nError", "mimetype": "text/plain", "start_char_idx": 51979, "end_char_idx": 52292, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63cf1a5a-40f9-49a7-83ea-c5e34e109fbe": {"__data__": {"id_": "63cf1a5a-40f9-49a7-83ea-c5e34e109fbe", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "623d66f6-5aec-438a-8453-1f23249323e9", "node_type": "1", "metadata": {}, "hash": "843a8c9859657c0387fed04a8e86a4b518f42e878c48796c7b6040f0a71aacfb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0ea5197-4f30-4338-b42f-60e76036bcf4", "node_type": "1", "metadata": {}, "hash": "c00eff749dc3846c8c597a8a7254e8288a88d6efc36ea9e89674317875b200b3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Correction \nSchemes \n\u00d7  \n\u00d7  \n\u00d7  \n\u221a \nCCVLE+ \nMAQE \n*S/U represents signed-unsigned reconfigurable. \nTABLE IV \nCOMPARISON WITH ERROR CORRECTION SCHEMES \n \nWork \nCalibration of Voltage Linear \nError \nMitigation of ADC \nQuantization Error \nCAP-RAM \nJSSC\u20192021 \n\u221a \nWithout detailed description to \nobtain calibration parameters \n\u00d7  \nSAMBA \nTCAS-I\u20192023 \n\u221a \nNeed preliminary CNN \ninference to obtained \ncalibration parameters \n\u00d7", "mimetype": "text/plain", "start_char_idx": 52294, "end_char_idx": 52714, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0ea5197-4f30-4338-b42f-60e76036bcf4": {"__data__": {"id_": "f0ea5197-4f30-4338-b42f-60e76036bcf4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63cf1a5a-40f9-49a7-83ea-c5e34e109fbe", "node_type": "1", "metadata": {}, "hash": "969bbe5371d485d08cf1b856ded4515c7afb157b611307aa59998c0bd7bc8dd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eefdc475-5bd6-478f-a30b-37e39b61f8f2", "node_type": "1", "metadata": {}, "hash": "f0da0e6e2463df095b7f60315e6b99be03d42c05a0263f87428795685af5490a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Proposed \n\u221a \nAn efficient calibration flow \nwithout need for knowledge of \ndeployed algorithm \n\u221a \nFlexible MAQE for \na broad range of \ntradeoff \n \nThis article has been accepted for publication in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. This is the author's version which has not been fully e\ncontent may change prior to final publication. Citation information: DOI 10.1109/TCAD.2024.3366025\n\u00a9 2024 IEEE.", "mimetype": "text/plain", "start_char_idx": 52717, "end_char_idx": 53160, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eefdc475-5bd6-478f-a30b-37e39b61f8f2": {"__data__": {"id_": "eefdc475-5bd6-478f-a30b-37e39b61f8f2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0ea5197-4f30-4338-b42f-60e76036bcf4", "node_type": "1", "metadata": {}, "hash": "c00eff749dc3846c8c597a8a7254e8288a88d6efc36ea9e89674317875b200b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c42c74a7-e693-4486-a231-1d1d2a35c1f1", "node_type": "1", "metadata": {}, "hash": "defe7c466849e55d00247a13605633a12efd842e23ae8722c2cb93abbb4f03a4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Personal use is permitted, but republication/redistribution requires IEEE permission.\ufffd\ufffdSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:46:33 UTC from IEEE Xplore.  Restrictions apply. \n13 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \ndifferent computational model compared to SRAM-CIM \nsystems.", "mimetype": "text/plain", "start_char_idx": 53161, "end_char_idx": 53613, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c42c74a7-e693-4486-a231-1d1d2a35c1f1": {"__data__": {"id_": "c42c74a7-e693-4486-a231-1d1d2a35c1f1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eefdc475-5bd6-478f-a30b-37e39b61f8f2", "node_type": "1", "metadata": {}, "hash": "f0da0e6e2463df095b7f60315e6b99be03d42c05a0263f87428795685af5490a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d440ec05-fa83-4ff2-b9a3-18b2df8b026d", "node_type": "1", "metadata": {}, "hash": "d5d1fe8a9bd42a53baada566a3b9950c7379c8838f01d28fd7ba45093ad541bb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This paper introduces a dedicated framework \nspecifically designed for SRAM-CIM systems, supporting both \nbit-flexible quantization and error correction schemes. One of \nthe key contributions of the proposed framework is its \nflexibility in weight mapping strategies, including WLS and \nBLS. This flexibility enables the framework to achieve higher \nhardware efficiency in SRAM-CIM systems.  \nTable IV presents a comparison of the correction of intrinsic \nerrors in the SRAM-CIM system [17, 18].", "mimetype": "text/plain", "start_char_idx": 53614, "end_char_idx": 54109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d440ec05-fa83-4ff2-b9a3-18b2df8b026d": {"__data__": {"id_": "d440ec05-fa83-4ff2-b9a3-18b2df8b026d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c42c74a7-e693-4486-a231-1d1d2a35c1f1", "node_type": "1", "metadata": {}, "hash": "defe7c466849e55d00247a13605633a12efd842e23ae8722c2cb93abbb4f03a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b5f4007-0919-4914-83ba-f9b4037275eb", "node_type": "1", "metadata": {}, "hash": "0822f7e135990b4b458b2424af7ff91474986a77c2dec205e3510f1f2e1ec7e5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The proposed work \ndistinguishes itself by providing a comprehensive mitigation \nstrategy for the quantization error that arises from the ADC\u2019s \ninadequate precision. In contrast, previous works primarily \nfocus on addressing the CVLE and often consider the AQE as \nan unavoidable drawback [17]. Importantly, the proposed \nframework also offers a sophisticated calibration strategy for \nthe CVLE without the need for knowledge of the deployed \nalgorithms [18].", "mimetype": "text/plain", "start_char_idx": 54110, "end_char_idx": 54570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b5f4007-0919-4914-83ba-f9b4037275eb": {"__data__": {"id_": "8b5f4007-0919-4914-83ba-f9b4037275eb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d440ec05-fa83-4ff2-b9a3-18b2df8b026d", "node_type": "1", "metadata": {}, "hash": "d5d1fe8a9bd42a53baada566a3b9950c7379c8838f01d28fd7ba45093ad541bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f3910e6-60a5-4249-8af3-12fa10bb4fbd", "node_type": "1", "metadata": {}, "hash": "d4b00f4bb57685e9a10ed4de76a02c28c2b905af8b2b61a6937d61588b8d29f1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In comparison, the CAP-RAM approach \nprovides a rough description of calibration without a detailed \nexplanation of the linear fitting process used to obtain \ncalibration parameters [17]. Another work, named SAMBA, \noffers a practical method to obtain calibration parameters but \nrequires a preliminary CNN inference to acquire these \nparameters [18].  \nVII. CONCLUSION \nThis paper introduces a compilation framework for SRAM-\nCIM systems, which supports bit-flexibility and signed-\nunsigned reconfigurability. The framework introduces BLS for \nweight mapping to improve hardware efficiency.", "mimetype": "text/plain", "start_char_idx": 54571, "end_char_idx": 55162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f3910e6-60a5-4249-8af3-12fa10bb4fbd": {"__data__": {"id_": "0f3910e6-60a5-4249-8af3-12fa10bb4fbd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b5f4007-0919-4914-83ba-f9b4037275eb", "node_type": "1", "metadata": {}, "hash": "0822f7e135990b4b458b2424af7ff91474986a77c2dec205e3510f1f2e1ec7e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd5c6df7-f609-4148-9f9e-990e52ae6611", "node_type": "1", "metadata": {}, "hash": "61ba2f5ccf76e5c64cc95339dd3f7a862be033ba37d0316cfd894bc3a6b0a24a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Additionally, \ntwo error correction schemes are proposed, including CCVLE \nfor automatic calibration parameter generation and MAQE to \neliminate or reduce the impact of ADC with insufficient \nprecision. Compared with the traditional weight mapping \nstrategy, the proposed strategy achieves utilization promotions \nof 63.29% and 21.10% for two types of frequently used \nconvolution layers. ResNet18, FIR filtering, and Gaussian \nimage filtering are deployed and analyzed to showcase the \nversatility of the proposed framework.", "mimetype": "text/plain", "start_char_idx": 55163, "end_char_idx": 55688, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd5c6df7-f609-4148-9f9e-990e52ae6611": {"__data__": {"id_": "cd5c6df7-f609-4148-9f9e-990e52ae6611", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "67fd357fa02c501b0838345584bd2d584b8251b5dcbf0e61aa373764030304d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f3910e6-60a5-4249-8af3-12fa10bb4fbd", "node_type": "1", "metadata": {}, "hash": "d4b00f4bb57685e9a10ed4de76a02c28c2b905af8b2b61a6937d61588b8d29f1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.36227/techrxiv.170629582.25144576/v1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The proposed error \ncorrection schemes achieve similar algorithm accuracy to the \nfloating point results, and the deployment of the ResNet18 \nmodel achieves the top-1 accuracy ranging from 66.3% to 70.1% \non the ImageNet dataset with different throughput tradeoffs.", "mimetype": "text/plain", "start_char_idx": 55689, "end_char_idx": 55954, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10.36227/techrxiv.170629582.25144576/v1": {"__data__": {"id_": "10.36227/techrxiv.170629582.25144576/v1", "embedding": null, "metadata": {"node_type": "doi_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"4": {"node_id": "root_node", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "ac150157-944e-4376-ab03-a2adc0711b9e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4036cd26-4a0c-47e6-a38c-5580571b84f2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ddba2af4-55ac-4b35-bdd8-6b0cc7aa140c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d69edfbc-1900-404e-85b4-19517f3146ec", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "74605057-79af-413a-aef5-6bdb141f3201", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "801ea761-ccc4-44d3-a49c-c99345bb67af", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7ee3e216-e1f3-450a-9106-c4afa90a077d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a22e22a2-782d-48cf-9c42-1b77bc109742", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "263be616-50ef-4b9c-a9fe-364ebc3c4be4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5e4f548d-4156-4f8d-9ca3-7edc20df8b89", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "654b133a-b06b-419f-b9db-64712eb02a5c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9f7f2548-c016-49f9-bc8b-368200b3bfc4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "54372f18-4b51-4b04-b92c-018f74c8ddba", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c1e03c35-08e2-4e0f-8f85-0e108d31c967", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3737d0ab-d82a-4fa2-b916-7b6c2e2e57f0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a097ab96-b7e9-43da-b1d6-2c581306db21", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a168720f-4e90-4475-9b55-2318ab043243", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a69f7089-ec4c-4cd8-ac38-91c8c79bea9d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "043ede7b-f2b6-48b7-b4c8-0d01b515cef3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f2c53940-fad3-4ef6-b6fb-6621fe630357", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "af55d924-20e3-4c1a-b472-e94d5620e5bf", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4d0e0363-8e64-47b2-876e-a4a2e7208c27", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b2a448bd-c0fe-48a2-bd2e-9948218f8176", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "77d6e95b-5746-409a-889a-f2b033a180c0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b54e23e9-b27c-4647-8365-5473311fa6c2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "125f6f54-7a86-4e57-b17f-c5d742dc864b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3aa1a0b9-c4cb-43f4-b481-6949a4976ed7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4261e0fa-e9a9-4175-b22d-931e66dc3b3e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e6a4388e-24df-4d21-9f4f-f14035297a6e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6303729d-8d4b-43f5-8b43-0a530b7e4445", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a293441b-a94c-445e-810e-1bbeea180301", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "155cdde0-2686-4bf5-addb-697f5a7f296a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "256e98f6-ecbb-45fc-b674-6f7f2eba332e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "29084e2f-3623-4886-aa67-cb7f33768cae", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "049171c9-1891-471d-ba6e-4e172cb9c0e9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "077a973e-2710-4c6a-9f9d-b31c3b511395", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c03a63e1-25ee-4a68-9138-2502329a1305", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d41c218d-7d89-456e-b33c-a780722c64b2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "267ca3fe-b251-49b2-b6fb-b90c06fb40ca", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4892b2a5-8ca4-4033-9314-0ef9038f7ef0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cd6b9d04-df7f-423f-988a-3e8e33a760c0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "adaec0f7-23ee-46ed-ab15-0d23cb8255f5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "844e2005-5def-4c53-9586-cffa04a20dfd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "890450b4-411e-4a3c-ae4a-3c6e9ce1fdbb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "80cd8183-88a1-4478-be77-a6c1be7349f4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "37c68533-1723-4951-8ade-fd6962c4e9be", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c17b12a3-9751-4a27-b1ec-35fb1ec0f714", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8fa2f47f-fd5c-4362-b80b-afc4e654d3b3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "53bd0054-6806-4be8-a421-7b4beb57ed1a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2effd1bf-74a9-4806-a3f6-b91a1df0c5fb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ca9a43d5-c69b-4419-acf9-a743c4b5ae60", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "dca40fa6-6512-4762-ad13-a124d1620665", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c84aa64e-14b3-49c6-b5cf-01054923401e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c987d570-0b06-46f4-9d5b-9962c930be68", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "dd5f8cd2-3162-4683-ad90-f573a3974d1f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "50cd358d-c2be-4dcb-a444-a9c0e2bd10ec", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "60b4552f-9495-4a77-be55-4c9610b2048c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "83edf9cc-2c56-4e3e-a783-8a4cc9602629", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a3d1e384-1766-40ca-a943-7e3fafb8ee76", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ebd14e5a-6aec-4cdc-a6d5-f59340b15933", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4237b3b6-8ee3-4caf-acf1-a5fb12003dc2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f8d51b4c-d422-48ec-9835-5c92c0db20fd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ad9507f7-2aca-40ca-ba72-114064ecbd2a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c2f4e56a-c637-4a1c-8f86-dde4cad30142", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ed8f540f-4642-434c-b8e9-4dac06d8df1e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "26815d5a-2cdd-4aba-9fce-c1382d153986", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e5c95d48-ac60-47f5-a723-76937988cbaa", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a5651519-d1d2-4289-aa68-a907b739f8b1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8ed8b7a2-5dab-4cbc-bb07-6c279e5b7229", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "38b243ee-b354-41ee-a49b-72f4869e1cf5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9bb8374c-399e-4665-84cd-c6f38e477818", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e9117ab4-073c-4c3f-b876-d8d2bf49bfc4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "27ffa399-15fc-453b-8b47-1cfc7d90d073", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8a7a68e6-722c-43a3-b2a6-f3486f5bb9c1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f7c207e8-5bab-403c-ab59-bd4f9581a268", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3a0bdd21-90b0-4e60-926a-a08a9836e870", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6dcb5e8e-536e-4500-8dfc-32a0304e4445", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ec6dfe0e-ca07-4b2f-ad12-06ff8686adeb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b8da8d61-95ab-44e7-9ece-1096e1c2176a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "44eae16a-3d54-4de7-9b43-b5e341e9797a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4fd9e821-624f-4ddc-9838-b6379884f26d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3264cdaf-7145-44f1-b2d6-db6d76037556", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5a247cb7-fe2e-4f23-ac34-ad7fe134f255", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fb1ad426-cc9c-42f7-a25d-356300a24c76", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7595f677-646e-498f-b541-1330db2293e2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "15b9f45f-fef0-41c7-a1f0-c3e87fde8cf8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "37d6843f-52a3-4903-a090-347780b80ca1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "70259326-07a8-4f9d-b3b5-6b8a038b4d94", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "eeab61f0-8040-4c5d-9e00-63191c35abbc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "11e11d42-a0a5-4559-bc69-40a12e88d381", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "26b8f05a-8164-4d6f-8157-1b5ad68dcf2e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9066fafd-7d42-4ca2-84e0-e812f25906cc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b74c8e48-aee3-4149-8714-ba22a73f79bc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5ff4f179-74dd-4833-a931-8a6894dfa8bf", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d2cf9ce0-82d6-4603-ad98-90e8ec39bfc5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a30f9db2-6708-4fcb-a526-55f612fda45e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d1b26f3c-da54-4f18-8805-6e51a829b7dd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "627a737d-8c86-4f12-a92e-c8350b4bdc8a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d8c3c6bc-c8c5-4664-8ac3-f0fa2ddb285f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7515ce6f-3c1c-4493-8410-2e21e71ac14a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d5c31810-4758-4042-b44e-3d21c6c3336e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5f3cabe7-7a02-459e-adc1-fb69edb96957", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "51b5d905-ece4-4529-92cd-1094dbc2f55a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3d9fec61-97c6-4ac5-90a1-49185a038e62", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cc7b3abf-c180-4a8d-ba3c-81d30bb8a299", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b02f5eee-462b-412a-aea0-d3b4d7ab5d07", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b4c3a51d-8bf1-4de0-ad46-83c549db4ba9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3c1b0741-1cac-4b04-89ea-a0ca8640b1d0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2617f1c9-9be0-4bb8-abb9-111ee1b6d31d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f45badec-6d97-433d-9f1b-de4bf4dc146f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5a51bb02-7264-4564-9e6a-a0ddac7f5fbc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5eae40bc-aea7-454f-9be5-ff9065e10b2a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7b18e431-1ae6-4bd9-b530-6f3149c864b3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f5104dde-612b-4da9-a3e2-5ae3eab0b0a0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "de1bba2e-8038-4056-82af-ab21c2bb3533", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c4a5a5c0-d280-44c5-8c28-017aac86330d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3d0711fd-4ece-475d-8540-0452799f2e9f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8526dd5b-3d8e-442e-b2c2-ce6e2f50482a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b2694119-f476-4d3f-8a21-89fff41abcb3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f5254579-2d07-4115-ae95-88338a412a36", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e1c5d6c1-5384-4f7f-a260-9a0749afe0bc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1194fa25-d58a-43c0-8848-9a8ceec7d985", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "19097887-91eb-4dbe-ab29-1d0fe3e46dae", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f7e05c07-469e-40b9-a258-1ca79e16000e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "499ac069-0437-4725-af5d-f45920be32df", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e05b0377-7383-4dff-9094-2b7774acd686", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2f3c33e7-0c47-429b-9568-262f51a01c4f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "31a66546-a080-4035-93f0-68f7fe5d625a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d7fef8d0-5273-4882-a0e9-1ef09fcce340", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d62b6d8c-c24b-4fc0-8007-9bed085ff6c8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e60ef1b1-dc9f-42ec-a0f2-4250c3800f17", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4b5123d1-1b2a-4c30-aad7-d7903ad76254", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "16a852c8-932a-4fcf-9060-7b8178f45567", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6695cee5-9101-4c5d-9f44-600f88f1a617", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cd3d8818-768b-45c0-9802-ded4460d491a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5c7d91c3-d464-4a7e-a0ca-c16305069142", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1c8d574b-2f2d-4fc8-b83f-1456a9739b5d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "091dd910-213b-41e4-9c53-9dd6ff0bb16f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "08ee27fc-2100-4d2f-8070-e497f1a44c9c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ad0cfb61-29a4-4154-8e66-5a1b6feeb3e5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4e666599-af44-4415-8492-d39946c5b04b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "32a52e5c-1165-42fe-b110-41d1114fde62", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2aa4a14d-3ee6-4e45-aa3f-f6adebc04a09", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8bf2ba76-3186-4756-9394-bb8b65249acc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ffb775fe-762e-48c1-a3f6-b8f628bcfade", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2a8ecd39-d4bc-4bd7-9b42-b78260d5efad", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "623d66f6-5aec-438a-8453-1f23249323e9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "63cf1a5a-40f9-49a7-83ea-c5e34e109fbe", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f0ea5197-4f30-4338-b42f-60e76036bcf4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "eefdc475-5bd6-478f-a30b-37e39b61f8f2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c42c74a7-e693-4486-a231-1d1d2a35c1f1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d440ec05-fa83-4ff2-b9a3-18b2df8b026d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8b5f4007-0919-4914-83ba-f9b4037275eb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0f3910e6-60a5-4249-8af3-12fa10bb4fbd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cd5c6df7-f609-4148-9f9e-990e52ae6611", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}]}, "text": "DOI: 10.36227/techrxiv.170629582.25144576/v1", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce6beca6-91b1-4a1e-bb9e-5e2aef65f7bf": {"__data__": {"id_": "ce6beca6-91b1-4a1e-bb9e-5e2aef65f7bf", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a34a36d-0ff6-43b9-9b6f-8fdc4497a8dc", "node_type": "1", "metadata": {}, "hash": "62ec279085aeade4a903cbd0b4751f5d1c61ed182040673bf5a3d6015057e234", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1\nIntroduction\nSearch operations come in numerous forms at the heart of many\ncomparison-intensive applications. In the past decade, the rev-\nolution in machine learning, data analytics, and bioinformatics\nhas played a significant role in driving the demand for efficient\nhardware acceleration of these operations. Domains such as\nnetwork security [12], bioinformatics [45], data mining, and\ndata analytics [53] heavily rely on exact matching of the query\npattern with pre-stored patterns.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a34a36d-0ff6-43b9-9b6f-8fdc4497a8dc": {"__data__": {"id_": "8a34a36d-0ff6-43b9-9b6f-8fdc4497a8dc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce6beca6-91b1-4a1e-bb9e-5e2aef65f7bf", "node_type": "1", "metadata": {}, "hash": "174a37d200ed4ea759c1c302aa86eb56c2b8cc3e0004c9203a806eb042fb6ea7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e2ebfad-89b9-4d83-b74b-d5746082b6a2", "node_type": "1", "metadata": {}, "hash": "2bf35e04f87bb6879c143a911d42e0c5b0a5d930e87f944d6bfb0532e261d455", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In other applications, such\nas K-nearest neighbors (KNN) and genome analysis [15, 39],\nthe emphasis lies on identifying similarities rather than exact\npattern matching. In approximate search, when the dissimilar-\nity between a stored pattern and the query pattern is within\na predefined threshold, the stored pattern is regarded as a\n\"match\". From the computational standpoint, both exact and\napproximate search operations are time-consuming and are\noften bottlenecks in comparison-intensive kernels [50].", "mimetype": "text/plain", "start_char_idx": 489, "end_char_idx": 994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e2ebfad-89b9-4d83-b74b-d5746082b6a2": {"__data__": {"id_": "1e2ebfad-89b9-4d83-b74b-d5746082b6a2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a34a36d-0ff6-43b9-9b6f-8fdc4497a8dc", "node_type": "1", "metadata": {}, "hash": "62ec279085aeade4a903cbd0b4751f5d1c61ed182040673bf5a3d6015057e234", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3e3932a-b0bb-4749-99ea-edf00ee93a5f", "node_type": "1", "metadata": {}, "hash": "84bd072727276d54b88beca26f88f9918755fc3653d4c42f21c59e3cee0fe560", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Recently, there has been a surge in the adoption of content\naddressable memories CAM-based system designs for efficient\nsearch operations. Originally employed in network routing\nand CPU caching [40], CAMs have now found applications in\na wider range of data-intensive domains [15, 32, 39]. CAMs\nallow massively parallel search operations for an input query,\nenabling the search to be performed across the entire memory\nwith a single operation.", "mimetype": "text/plain", "start_char_idx": 995, "end_char_idx": 1438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3e3932a-b0bb-4749-99ea-edf00ee93a5f": {"__data__": {"id_": "b3e3932a-b0bb-4749-99ea-edf00ee93a5f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e2ebfad-89b9-4d83-b74b-d5746082b6a2", "node_type": "1", "metadata": {}, "hash": "2bf35e04f87bb6879c143a911d42e0c5b0a5d930e87f944d6bfb0532e261d455", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33178490-e400-4113-bbf7-7f978ef19fe2", "node_type": "1", "metadata": {}, "hash": "8aac493d17514801681ba356f34a8c4b95842a41738b23e69d319de1c372fe92", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "CAM\u2019s high-speed parallel search\nmakes it a popular component for constructing cutting-edge\ncompute-in-memory (CIM) systems, aiming to provide an\nenergy-efficient alternative to the von Neumann bottleneck in\nterms of both latency and energy consumption.\nCAM designs are broadly classified into binary, ternary,\nmulti-state, and analog CAMs (BCAM, TCAM, MCAM,\nACAM, respectively), with implementations based on either\nconventional CMOS or emerging non-volatile memory (NVM)\ntechnologies [6, 34, 39, 50].", "mimetype": "text/plain", "start_char_idx": 1439, "end_char_idx": 1941, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33178490-e400-4113-bbf7-7f978ef19fe2": {"__data__": {"id_": "33178490-e400-4113-bbf7-7f978ef19fe2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3e3932a-b0bb-4749-99ea-edf00ee93a5f", "node_type": "1", "metadata": {}, "hash": "84bd072727276d54b88beca26f88f9918755fc3653d4c42f21c59e3cee0fe560", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29978223-f6ac-44b6-b30c-c94c15f525e0", "node_type": "1", "metadata": {}, "hash": "75fb0025c25d9421ae805df5ed903a1ada5ee53f3c141b1c6685db3f5e5001ca", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Compared to CMOS technologies,\nNVM technologies, like magnetic RAM (MRAM), resistive\nRAM (ReRAM), or ferroelectric (FeFET), are denser and\n164\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nFarzaneh et al.\nmore energy efficient, yielding more efficient CAM arrays\n[14, 16, 37].", "mimetype": "text/plain", "start_char_idx": 1942, "end_char_idx": 2224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29978223-f6ac-44b6-b30c-c94c15f525e0": {"__data__": {"id_": "29978223-f6ac-44b6-b30c-c94c15f525e0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33178490-e400-4113-bbf7-7f978ef19fe2", "node_type": "1", "metadata": {}, "hash": "8aac493d17514801681ba356f34a8c4b95842a41738b23e69d319de1c372fe92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58c0c755-97ee-4e50-b41f-8395cf490ebd", "node_type": "1", "metadata": {}, "hash": "b553b5b3cba24e2c6c970210316ffc82dbea2e19f052aefbd069ed48dfee4b62", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "BCAMs and TCAMs use a bit-wise Hamming\ndistance (HD) to compare the query and stored data, whereas\nMCAMs and ACAMs apply a specific distance metric to\ncompare the query with memory entries and determine which\nmemory entries match the query based on the distance metric.\nIn terms of match types, CAMs can be classified into the exact\nmatch (EX), best match (BE), and threshold match (TH) [16].", "mimetype": "text/plain", "start_char_idx": 2225, "end_char_idx": 2617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58c0c755-97ee-4e50-b41f-8395cf490ebd": {"__data__": {"id_": "58c0c755-97ee-4e50-b41f-8395cf490ebd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29978223-f6ac-44b6-b30c-c94c15f525e0", "node_type": "1", "metadata": {}, "hash": "75fb0025c25d9421ae805df5ed903a1ada5ee53f3c141b1c6685db3f5e5001ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c20a560a-e946-4202-bc5d-d7eafc0748e5", "node_type": "1", "metadata": {}, "hash": "1ce77ebb5eecec24940fc792e86247b7597d7b3cf6253ec4049d4d28027eba19", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Although CAM designs have shown better performance\nthan traditional methods for computing similarity in many\ndomains [15, 32, 39], effectively mapping applications writ-\nten in high-level programming languages onto CAM-based\naccelerators remains a challenge. This is due to the disparity\nin the abstractions of the applications (high-level) and the\n(low-level) set of commands needed to program the CAM\narrays. Presently, CAM arrays are programmed manually with\nlow-level code that only the device experts understand.", "mimetype": "text/plain", "start_char_idx": 2618, "end_char_idx": 3135, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c20a560a-e946-4202-bc5d-d7eafc0748e5": {"__data__": {"id_": "c20a560a-e946-4202-bc5d-d7eafc0748e5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58c0c755-97ee-4e50-b41f-8395cf490ebd", "node_type": "1", "metadata": {}, "hash": "b553b5b3cba24e2c6c970210316ffc82dbea2e19f052aefbd069ed48dfee4b62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4db71d11-1e03-4ced-9a5a-ecdbf4b8d893", "node_type": "1", "metadata": {}, "hash": "a2e560e14025dac3052f6ae44807a6140d9be68f9f9b52c069bb2673e45980b7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Exist-\ning design automation and compilation tools for in-memory\ncomputing [46, 47] do not provide support for CAM prim-\nitives, highlighting the need for solutions that can support\nmapping a wider range of applications and accelerate the\ndesign process.\nThis paper proposes C4CAM, the first end-to-end auto-\nmated retargetable framework that enables efficient mapping\nof applications from a higher TorchScript program onto CAM\narrays. C4CAM leverages the multi-level intermediate rep-\nresentation (MLIR) framework to seamlessly optimize and\noffload comparison-intensive kernels to CAM-enabled sys-\ntems.", "mimetype": "text/plain", "start_char_idx": 3136, "end_char_idx": 3740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4db71d11-1e03-4ced-9a5a-ecdbf4b8d893": {"__data__": {"id_": "4db71d11-1e03-4ced-9a5a-ecdbf4b8d893", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c20a560a-e946-4202-bc5d-d7eafc0748e5", "node_type": "1", "metadata": {}, "hash": "1ce77ebb5eecec24940fc792e86247b7597d7b3cf6253ec4049d4d28027eba19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c256a51b-751d-4338-a3cf-f82f72e51766", "node_type": "1", "metadata": {}, "hash": "1f6b451fe3cb086a953f65c786f65a53294ee9bc0244788ad59c82d8c3502951", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Concretely, we make the following contributions:\n\u2022 An automated end-to-end compilation flow that (i)\nmakes CAM accelerators accessible to non-experts\nand (ii) enables device/circuit/architecture experts to\nexplore design trade-offs. C4CAM takes applications\nwritten in TorchScript along an architectural model\nfor retargetability and generates code for the given\narchitecture (see Section 4).\n\u2022 An extension to the MLIR front-end to express search\noperations in PyTorch applications (see Section 4.3).", "mimetype": "text/plain", "start_char_idx": 3741, "end_char_idx": 4242, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c256a51b-751d-4338-a3cf-f82f72e51766": {"__data__": {"id_": "c256a51b-751d-4338-a3cf-f82f72e51766", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4db71d11-1e03-4ced-9a5a-ecdbf4b8d893", "node_type": "1", "metadata": {}, "hash": "a2e560e14025dac3052f6ae44807a6140d9be68f9f9b52c069bb2673e45980b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3877773d-f394-4ada-bd5f-612912b7546e", "node_type": "1", "metadata": {}, "hash": "c31ca6083c2af3c4611d5bfff139f228b4bb613ce152b7bf50203c96f7db1dc7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\u2022 Extension to the CIM abstraction from [26] to cater to\nCAM accelerators. Specifically, we propose analyses to\ndetect computational primitives in applications that can\nbe rewritten as search operations (see Section 4.4.1).\n\u2022 A novel CAM abstraction that supports different CAMs\ntypes and search operations (see Section 4.4.2).\n\u2022 Transformation passes to optimize for latency, power,\nand array utilization (see Section 4.4.2).\n\u2022 A comprehensive evaluation of the generated code,\nincluding validation and comparison to a GPU target\nand the hand-crafted implementations (see Section 5).", "mimetype": "text/plain", "start_char_idx": 4243, "end_char_idx": 4827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3877773d-f394-4ada-bd5f-612912b7546e": {"__data__": {"id_": "3877773d-f394-4ada-bd5f-612912b7546e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c256a51b-751d-4338-a3cf-f82f72e51766", "node_type": "1", "metadata": {}, "hash": "1f6b451fe3cb086a953f65c786f65a53294ee9bc0244788ad59c82d8c3502951", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dde9786d-e7bf-4fc1-baaa-639a4161b487", "node_type": "1", "metadata": {}, "hash": "38323eeb95948b88b75dc64c84dc3a289c03c15e1628881bdafab15c397b0d06", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Our evaluation of C4CAM demonstrates that the generated\ncode achieves comparable results to hand-crafted designs.\nWe also showcase the capabilities of C4CAM in performing\ndesign space exploration on different CAM architectures.", "mimetype": "text/plain", "start_char_idx": 4828, "end_char_idx": 5055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dde9786d-e7bf-4fc1-baaa-639a4161b487": {"__data__": {"id_": "dde9786d-e7bf-4fc1-baaa-639a4161b487", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3877773d-f394-4ada-bd5f-612912b7546e", "node_type": "1", "metadata": {}, "hash": "c31ca6083c2af3c4611d5bfff139f228b4bb613ce152b7bf50203c96f7db1dc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d1f07fb-d417-425b-bacd-2e2091174c99", "node_type": "1", "metadata": {}, "hash": "0aeec913ff1bc2be5d2a52ebf81514b57e964222bc92060d0556679466bd2b8e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "SLF1\nSLT1\nCAM cell\nCAM cell\nCAM cell\nEncoder\nML1\nML2\nMLN\nScL1\nScL2\nScLN\nSLF2\nSLT2\nCAM cell\nCAM cell\nCAM cell\nSLFM\nSLTM\nCAM cell\nCAM cell\nCAM cell\n...\n...\n...\nSense Amplifiers\nMatch Line driver\nSearch Data\nML\nScL\nSLF\nSLT\nFigure 1.", "mimetype": "text/plain", "start_char_idx": 5056, "end_char_idx": 5285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d1f07fb-d417-425b-bacd-2e2091174c99": {"__data__": {"id_": "9d1f07fb-d417-425b-bacd-2e2091174c99", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dde9786d-e7bf-4fc1-baaa-639a4161b487", "node_type": "1", "metadata": {}, "hash": "38323eeb95948b88b75dc64c84dc3a289c03c15e1628881bdafab15c397b0d06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a24779c4-6c4b-4e32-930d-c687149ca7f6", "node_type": "1", "metadata": {}, "hash": "0710ed398f9038c75ab4491286dcb62aa0557ade57595bccef2fe70b010226c3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Structure of a FeFET-based CAM array [51]\n2\nBackground\nThis section presents background on the MLIR framework\nand CAM-based structures and describes our proposed archi-\ntecture. It also motivates the need for automatic compilation\ntools by explaining the challenges in the state-of-the-art pro-\ngramming models for CAMs.\n2.1\nMLIR compiler infrastructure\nMLIR is a framework that enables representing and transform-\ning intermediate representations (IR) at various abstraction\nlevels, catering to diverse application domains and heteroge-\nneous hardware targets [30].", "mimetype": "text/plain", "start_char_idx": 5286, "end_char_idx": 5852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a24779c4-6c4b-4e32-930d-c687149ca7f6": {"__data__": {"id_": "a24779c4-6c4b-4e32-930d-c687149ca7f6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d1f07fb-d417-425b-bacd-2e2091174c99", "node_type": "1", "metadata": {}, "hash": "0aeec913ff1bc2be5d2a52ebf81514b57e964222bc92060d0556679466bd2b8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa55ac90-ed41-41d5-aceb-1afdaa0a4b8a", "node_type": "1", "metadata": {}, "hash": "a618a68b5b48afb042dcb97b36ffa50ad9cab84f32eef22a438c073a5fae2506", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "It offers a customizable IR with\nminimal built-in features, enabling compiler developers to\nincorporate their own abstractions. This empowers them to op-\ntimize for specific domains or targets by leveraging matching\ntechniques at the appropriate levels of abstraction.\nMLIR consists of a collection of reusable abstractions\norganized into dialects. Each dialect incorporates custom\ntypes, operations, and attributes, which serve as fundamental\nbuilding blocks of the IR. In MLIR, values are associated\nwith compile-time known types, while attributes provide\ncompile-time information linked to operations.", "mimetype": "text/plain", "start_char_idx": 5853, "end_char_idx": 6457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa55ac90-ed41-41d5-aceb-1afdaa0a4b8a": {"__data__": {"id_": "aa55ac90-ed41-41d5-aceb-1afdaa0a4b8a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a24779c4-6c4b-4e32-930d-c687149ca7f6", "node_type": "1", "metadata": {}, "hash": "0710ed398f9038c75ab4491286dcb62aa0557ade57595bccef2fe70b010226c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02119eb9-8504-4083-ba41-22f37e599f29", "node_type": "1", "metadata": {}, "hash": "c2eba71a2fd324acbf6987f68d5f33c9366a86a994028b41b9bb7cdb2e9d909b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Dialects in\nMLIR maintain preconditions for transformation validity\nwithin their IR, reducing the complexity and cost of analysis\npasses. Dialects are typically designed for specific domains\n(e.g., linalg for linear algebra, TOSA for tensor operations),\nrepresentations (e.g., affine for the polyhedral model, scf\nfor control flow), or targets (e.g., gpu, cim). The llvm dialect\nmodels LLVM IR constructs.", "mimetype": "text/plain", "start_char_idx": 6458, "end_char_idx": 6863, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02119eb9-8504-4083-ba41-22f37e599f29": {"__data__": {"id_": "02119eb9-8504-4083-ba41-22f37e599f29", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa55ac90-ed41-41d5-aceb-1afdaa0a4b8a", "node_type": "1", "metadata": {}, "hash": "a618a68b5b48afb042dcb97b36ffa50ad9cab84f32eef22a438c073a5fae2506", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7cfcfb5-ed7f-4c5f-9d78-ccfa5bed2ccc", "node_type": "1", "metadata": {}, "hash": "d038530d4f170c32dfde4ae1b15966e3e4b9355d39ff83b9b8e456b5700bd25f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Abstractions in MLIR can\nbe progressively lowered (from high-level domain-specific\ndialects to low-level platform-specific dialects) and raised [9].\n2.2\nContent addressable memories\nCIM fabrics are generally categorized into three classes: CIM-\ncrossbars, renowned for their ability to compute matrix-vector\nmultiplications in constant time; CIM-logic, facilitating the\nacceleration of bulk bitwise logic operations; and content\naddressable memories (CAMs), enabling fast and energy-\nefficient search operations [25].", "mimetype": "text/plain", "start_char_idx": 6864, "end_char_idx": 7381, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7cfcfb5-ed7f-4c5f-9d78-ccfa5bed2ccc": {"__data__": {"id_": "f7cfcfb5-ed7f-4c5f-9d78-ccfa5bed2ccc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02119eb9-8504-4083-ba41-22f37e599f29", "node_type": "1", "metadata": {}, "hash": "c2eba71a2fd324acbf6987f68d5f33c9366a86a994028b41b9bb7cdb2e9d909b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9318ad85-4d09-4acc-8bb5-04e4182061b8", "node_type": "1", "metadata": {}, "hash": "89bdbdda6774c1b85664c9aa6215d6f9cf1c090f72f578d440e6807f2e3d5ae2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "CAMs support two main functions: search, which identifies\nthe memory entries that match the input query, and write,\nwhich stores data entries in the memory cells. With CAMs,\nparallel searches can be performed on all stored data in\n165\nC4CAM: A Compiler for CAM-based In-memory Accelerators\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nFigure 2. Hierarchical structure of a CAM-based accelerator\nmemory in constant time (O(1)).", "mimetype": "text/plain", "start_char_idx": 7382, "end_char_idx": 7815, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9318ad85-4d09-4acc-8bb5-04e4182061b8": {"__data__": {"id_": "9318ad85-4d09-4acc-8bb5-04e4182061b8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7cfcfb5-ed7f-4c5f-9d78-ccfa5bed2ccc", "node_type": "1", "metadata": {}, "hash": "d038530d4f170c32dfde4ae1b15966e3e4b9355d39ff83b9b8e456b5700bd25f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78ea12e7-82c1-4cb3-8726-745b61e8741e", "node_type": "1", "metadata": {}, "hash": "27d236a497d8b01496b7fa233c47108b8ad17192afce770ec634991d9fc331e7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The most common type of\nCAM is the ternary CAM (TCAM), where the data elements\ncan be either 0, 1, or don\u2019t care (\u2018x\u2019), which is a wildcard state\nmatching both 0 and 1. Figure 1 illustrates a TCAM array\nwith \ud835\udc45rows and \ud835\udc36columns. Each cell in a row is connected to\na common match line (ML) and stores one of the tree states.", "mimetype": "text/plain", "start_char_idx": 7816, "end_char_idx": 8138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78ea12e7-82c1-4cb3-8726-745b61e8741e": {"__data__": {"id_": "78ea12e7-82c1-4cb3-8726-745b61e8741e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9318ad85-4d09-4acc-8bb5-04e4182061b8", "node_type": "1", "metadata": {}, "hash": "89bdbdda6774c1b85664c9aa6215d6f9cf1c090f72f578d440e6807f2e3d5ae2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5859417b-2dd1-4b3c-8fa7-7cd5cd3ae3e6", "node_type": "1", "metadata": {}, "hash": "095e5cbaa83cc034dc79da9cfc4f9b08df9d10468d8bb62a8137c3e919ce7d93", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "During a search operation, each cell \ud835\udc36\ud835\udc56\ud835\udc57in row \ud835\udc56performs an\nXNOR operation between its content and the query element\n\ud835\udc5e\ud835\udc57. The ML implements a logic OR operation of all the cells\nin the row to determine the result for that row.\nDifferent sensing circuits can be designed to realize different\nmatch schemes, such as EX, BE, and TH.", "mimetype": "text/plain", "start_char_idx": 8139, "end_char_idx": 8467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5859417b-2dd1-4b3c-8fa7-7cd5cd3ae3e6": {"__data__": {"id_": "5859417b-2dd1-4b3c-8fa7-7cd5cd3ae3e6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78ea12e7-82c1-4cb3-8726-745b61e8741e", "node_type": "1", "metadata": {}, "hash": "27d236a497d8b01496b7fa233c47108b8ad17192afce770ec634991d9fc331e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2996cee2-bdba-4ffc-b43f-b558d59696e4", "node_type": "1", "metadata": {}, "hash": "e5d75dda50481519c4246087eedd4acdf097fd51d575a6caff26424b767814a1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "EX search is the\nfastest search type due to its simple sensing requirement,\nwhereas best match search reports the row with the least\nnumber of mismatching cells and is widely used for nearest\nneighbor search. To find the best match, more sophisticated\nsensing circuits are needed, e.g., analog-digital-converters or\na winner-take-all circuit, with the latter being more energy\nand area-efficient but limited to finding the best matches only\nwithin a certain number of mismatch cells [20].", "mimetype": "text/plain", "start_char_idx": 8468, "end_char_idx": 8956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2996cee2-bdba-4ffc-b43f-b558d59696e4": {"__data__": {"id_": "2996cee2-bdba-4ffc-b43f-b558d59696e4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5859417b-2dd1-4b3c-8fa7-7cd5cd3ae3e6", "node_type": "1", "metadata": {}, "hash": "095e5cbaa83cc034dc79da9cfc4f9b08df9d10468d8bb62a8137c3e919ce7d93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8714f23-6cd1-4440-b8cd-7fad4cd6d068", "node_type": "1", "metadata": {}, "hash": "2b1cff12dae7b01e466e0129ca4be76939ea0be4cce3dc952cceb4b72705c869", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2.3\nAccelerator architecture\nFor this work, we consider a general CAM-accelerator design\nbased on the state-of-the-art [23]. As illustrated in Figure 2,\nthe CAM structure is organized into a four-level hierarchy\ncomprising \ud835\udc35banks, each bank containing \ud835\udc47mats where each\nmat consists of \ud835\udc34CAM arrays which are further partitioned\ninto \ud835\udc46subarrays. The subarrays can be operated and accessed\nindependently.", "mimetype": "text/plain", "start_char_idx": 8957, "end_char_idx": 9358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8714f23-6cd1-4440-b8cd-7fad4cd6d068": {"__data__": {"id_": "e8714f23-6cd1-4440-b8cd-7fad4cd6d068", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2996cee2-bdba-4ffc-b43f-b558d59696e4", "node_type": "1", "metadata": {}, "hash": "e5d75dda50481519c4246087eedd4acdf097fd51d575a6caff26424b767814a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "baef1291-a654-4c00-8f4c-63c65a6d71ab", "node_type": "1", "metadata": {}, "hash": "45599a21d38e8df144fb0d81b3caf25cb107ab3b17ca7e2519037b697829bd07", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This hierarchical organization allows for scal-\nable and flexible computation, as the number of banks, mats,\nand arrays can be allocated according to the computational\nrequirements of each application. Within each bank, all mats\nand arrays can perform parallel search operations using the\n\ud835\udc46CAM subarrays either in a sequential or parallel manner,\nproviding further granularity for parallel processing and re-\nsource allocation. \ud835\udc35banks operate independently to allow for\ntask-level parallelism.", "mimetype": "text/plain", "start_char_idx": 9359, "end_char_idx": 9852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baef1291-a654-4c00-8f4c-63c65a6d71ab": {"__data__": {"id_": "baef1291-a654-4c00-8f4c-63c65a6d71ab", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8714f23-6cd1-4440-b8cd-7fad4cd6d068", "node_type": "1", "metadata": {}, "hash": "2b1cff12dae7b01e466e0129ca4be76939ea0be4cce3dc952cceb4b72705c869", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78a77e59-3b3c-4b1b-8ac3-6758df3750e1", "node_type": "1", "metadata": {}, "hash": "142d68710dd9cac7acb1a2baf8e7d75e817d64cb593bc17196dca0541865aeaf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "RecSys [32], for instance, can profit\nfrom CAMs in both filtering and ranking stages, where each\nstage executes different tasks on different banks in parallel.\n3\nRelated work\nCAM\u2019s efficient data retrieval capabilities make it highly suit-\nable for applications that rely heavily on large-scale matching\nor search operations. CAMs have been proposed based on\nvarious memory technologies including SRAM [5], resistive\nRAM [17], racetrack memory [11], and FeFET [39].", "mimetype": "text/plain", "start_char_idx": 9853, "end_char_idx": 10318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78a77e59-3b3c-4b1b-8ac3-6758df3750e1": {"__data__": {"id_": "78a77e59-3b3c-4b1b-8ac3-6758df3750e1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "baef1291-a654-4c00-8f4c-63c65a6d71ab", "node_type": "1", "metadata": {}, "hash": "45599a21d38e8df144fb0d81b3caf25cb107ab3b17ca7e2519037b697829bd07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f2f15cc-42dc-4bea-9899-0221f9286888", "node_type": "1", "metadata": {}, "hash": "2dd67a16d671552e2c756c58e8fa1bc334f5b26be03ec919011d5393f4a93a9c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Recent\nworks have demonstrated the use of CAMs in various fields,\ne.g., bioinformatics [8], high dimensional computing [23],\nreinforcement learning [31], few-shot learning [28] and rec-\nommender systems [32].\nIn terms of programmability, CAMs have received relatively\nlittle attention compared to other CIM paradigms. Prominent\nframeworks like TVM [10] and EXO [18] offer high-level\nprogramming abstractions and optimization passes for kernels,\nprimarily from the machine learning domain, for CPU/GPU\nsystems and traditional HW accelerators.", "mimetype": "text/plain", "start_char_idx": 10319, "end_char_idx": 10860, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f2f15cc-42dc-4bea-9899-0221f9286888": {"__data__": {"id_": "7f2f15cc-42dc-4bea-9899-0221f9286888", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78a77e59-3b3c-4b1b-8ac3-6758df3750e1", "node_type": "1", "metadata": {}, "hash": "142d68710dd9cac7acb1a2baf8e7d75e817d64cb593bc17196dca0541865aeaf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a864e6a5-4b5a-46fa-98ee-4e060bead9a7", "node_type": "1", "metadata": {}, "hash": "e28c40de7f06d2d23a6c2a363c1ce748523c509b750b5c34e5acba4bebd24b3b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Today, however,\nthese high-level frameworks offer no support for CIM systems.\nIn fact, CIM support in compilers and programming frame-\nworks is scarce, with most approaches focused on accelerating\nneural networks on CIM crossbars. For example, OCC [46]\nautomatically identifies the matrix multiplication (matmul)\npattern, transforms kernels to match it, and offloads them to\na PCM-crossbar. However, it does not support the diversity\nin crossbar technologies and architectures.", "mimetype": "text/plain", "start_char_idx": 10861, "end_char_idx": 11338, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a864e6a5-4b5a-46fa-98ee-4e060bead9a7": {"__data__": {"id_": "a864e6a5-4b5a-46fa-98ee-4e060bead9a7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f2f15cc-42dc-4bea-9899-0221f9286888", "node_type": "1", "metadata": {}, "hash": "2dd67a16d671552e2c756c58e8fa1bc334f5b26be03ec919011d5393f4a93a9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e5f7931-ee12-496a-a66c-02b52df3449c", "node_type": "1", "metadata": {}, "hash": "5cb387d6d1fc3979cb34d628a92b65cdcfb3585af76664d2310d5af5407d3484", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To address this,\nCIM-MLC [43] is proposed, which considers the hierarchical\nstructure of the CIM hardware and generates efficient code\nfor it. For CIM-logic, Soeken et al. [47] introduced a com-\npiler leveraging majority inverter graphs to enable operation\nrewriting and optimization for a given RRAM CIM accel-\nerator. Recently, Jin et al.[21] proposed a compilation flow\nsupporting both RRAM-based crossbar and logic architec-\ntures.", "mimetype": "text/plain", "start_char_idx": 11339, "end_char_idx": 11774, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e5f7931-ee12-496a-a66c-02b52df3449c": {"__data__": {"id_": "1e5f7931-ee12-496a-a66c-02b52df3449c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a864e6a5-4b5a-46fa-98ee-4e060bead9a7", "node_type": "1", "metadata": {}, "hash": "e28c40de7f06d2d23a6c2a363c1ce748523c509b750b5c34e5acba4bebd24b3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "216abd8a-aff8-4768-b247-11f3f384b8b7", "node_type": "1", "metadata": {}, "hash": "17131880c61d5a77ecadefebceeeb2e9cfec39d1251b303724594825847865e0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Unlike CIM-logic and crossbars, there is currently no\nautomatic general-purpose compilation framework targeting\nCIM-CAMs and generating efficient code for them.\nThe fundamental programming model of the CIM abstrac-\ntion introduced in CINM [26] is generic and designed to be\neasily retargetable to different CIM architectures. CINM, how-\never, primarily focuses on arithmetic and logic operations and\ndoes not support the search-based operations of CAMs (e.g.,\nfor computing distances, similarities, or comparisons).", "mimetype": "text/plain", "start_char_idx": 11775, "end_char_idx": 12290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "216abd8a-aff8-4768-b247-11f3f384b8b7": {"__data__": {"id_": "216abd8a-aff8-4768-b247-11f3f384b8b7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e5f7931-ee12-496a-a66c-02b52df3449c", "node_type": "1", "metadata": {}, "hash": "5cb387d6d1fc3979cb34d628a92b65cdcfb3585af76664d2310d5af5407d3484", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d8da926-8381-4901-b1d2-9719080a6dc9", "node_type": "1", "metadata": {}, "hash": "542fc7e20144c71186c32dbe1093de4830c3d26bf14e9ddebd7e12b568fb3381", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For\nCAM-based accelerators, frameworks such as DT2CAM [44]\nand X-TIME [41] exist to map and simulate decision trees onto\nTCAMs and ACAMs, respectively. However, these mapping\ntools do not generalize to other comparison-intensive kernels\nand require programmers to deeply understand both the appli-\ncation and the accelerator architecture. Therefore, there is a\nconsiderable demand for a generalized framework capable of\nefficiently lowering high-level language programs for diverse\ninput applications.", "mimetype": "text/plain", "start_char_idx": 12291, "end_char_idx": 12792, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d8da926-8381-4901-b1d2-9719080a6dc9": {"__data__": {"id_": "5d8da926-8381-4901-b1d2-9719080a6dc9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "216abd8a-aff8-4768-b247-11f3f384b8b7", "node_type": "1", "metadata": {}, "hash": "17131880c61d5a77ecadefebceeeb2e9cfec39d1251b303724594825847865e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c54d84a-6815-4859-8152-c971e049b6e2", "node_type": "1", "metadata": {}, "hash": "8748a991a9aaba5c129bdbc1bd185df81cebd90bb892ac86e4a3cd78c0af808a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This framework should also incorporate\nCAM array optimizations, e.g., selective row pre-charging,\nto generate optimized code for the underlying architecture.\nIn the following section, we introduce how the hierarchical\nC4CAM framework effectively addresses this gap.\n4\nThe C4CAM framework\nThis section presents C4CAM, including the abstractions,\nlowerings, analysis, and optimization passes.\n166\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nFarzaneh et al.\nFigure 3.", "mimetype": "text/plain", "start_char_idx": 12793, "end_char_idx": 13265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c54d84a-6815-4859-8152-c971e049b6e2": {"__data__": {"id_": "1c54d84a-6815-4859-8152-c971e049b6e2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d8da926-8381-4901-b1d2-9719080a6dc9", "node_type": "1", "metadata": {}, "hash": "542fc7e20144c71186c32dbe1093de4830c3d26bf14e9ddebd7e12b568fb3381", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fbc1f57-49fe-4f17-90db-b249ba235f4a", "node_type": "1", "metadata": {}, "hash": "1dc5820563197860e0ce6d425b11bf6cfc16099dd86c38f26f788faff04d99e1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A high-level overview of C4CAM flow\n4.1\nAn overview of the compilation flow\nFigure 3 shows a high-level overview of C4CAM. The Torch-\nScript functions chosen for offloading to the CAM accelerators\nare transformed into MLIR\u2019s representation using the PyTorch\nMLIR converter (see Section 4.3). This produces the Torch\nIR, which is the entry point into C4CAM, which includes\nATen tensor library operations.", "mimetype": "text/plain", "start_char_idx": 13266, "end_char_idx": 13669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7fbc1f57-49fe-4f17-90db-b249ba235f4a": {"__data__": {"id_": "7fbc1f57-49fe-4f17-90db-b249ba235f4a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c54d84a-6815-4859-8152-c971e049b6e2", "node_type": "1", "metadata": {}, "hash": "8748a991a9aaba5c129bdbc1bd185df81cebd90bb892ac86e4a3cd78c0af808a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56631314-cff8-4964-9c57-d6ab1019aa40", "node_type": "1", "metadata": {}, "hash": "4f30196892552751ca7f664def05b7e7b1219e654f8a1172992d13cf5ae36070", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Torch MLIR is then lowered\nto the cim abstraction, which is a comprehensive dialect for\nvarious CIM technologies, taking over the shared responsi-\nbilities of host-device interfacing and device mapping (see\nSection 4.4.1). The cim abstraction has been previously inves-\ntigated in [46] and [26], where a programming model for CIM\ndevices was introduced. C4CAM extends this abstraction by\nincorporating the necessary analysis for CAM devices.\nTo enable application mapping, cim supports partitioning,\nrewriting, and kernel modifications.", "mimetype": "text/plain", "start_char_idx": 13670, "end_char_idx": 14206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56631314-cff8-4964-9c57-d6ab1019aa40": {"__data__": {"id_": "56631314-cff8-4964-9c57-d6ab1019aa40", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fbc1f57-49fe-4f17-90db-b249ba235f4a", "node_type": "1", "metadata": {}, "hash": "1dc5820563197860e0ce6d425b11bf6cfc16099dd86c38f26f788faff04d99e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4f5f59c-eab0-4eff-9506-716203d706d4", "node_type": "1", "metadata": {}, "hash": "f328500897b299847513cdea5f2b7c610aeab9e0bf52336c79e0936563e93b5c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The latter transforms the\ncode to use device-compatible sizes and operations, which\nthe low-level dialects can then further process. Subsequently,\nthe cim dialect is either lowered to cam, loop, or other\ndevice dialects. The cam dialect and other device dialects at\nthe same level, such as crossbar, offer an abstraction for\nprogramming and executing functions on the target device (see\nSection 4.4.2). The cam dialect also provides transformation\npasses that enable mapping and optimization of the selected\nkernel while accounting for the concrete hierarchy and other\ncharacteristics of CAM-based architectures.", "mimetype": "text/plain", "start_char_idx": 14207, "end_char_idx": 14819, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4f5f59c-eab0-4eff-9506-716203d706d4": {"__data__": {"id_": "f4f5f59c-eab0-4eff-9506-716203d706d4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56631314-cff8-4964-9c57-d6ab1019aa40", "node_type": "1", "metadata": {}, "hash": "4f30196892552751ca7f664def05b7e7b1219e654f8a1172992d13cf5ae36070", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ded2ef5f-9f03-4ddf-adf9-3c0cdb3065d1", "node_type": "1", "metadata": {}, "hash": "c70dad70de01fc598e5cc346a935e8cb6217a31927a677e781eee11edf928d78", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4.2\nArchitecture specification\nIn addition to the input application, C4CAM takes the architec-\ntural configuration as input, as shown in Figure 3. This outlines\nthe hierarchy of the proposed architecture (as discussed in\nSection 2.3), as well as the access mode for each level of the\nhierarchy, whether it supports sequential or parallel accesses.\nNote that all active rows within a subarray are accessed in\nparallel. However, through selective row accessing [55], it\nis possible to activate and pre-charge only a subset of rows\nwithin a subarray.", "mimetype": "text/plain", "start_char_idx": 14820, "end_char_idx": 15367, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ded2ef5f-9f03-4ddf-adf9-3c0cdb3065d1": {"__data__": {"id_": "ded2ef5f-9f03-4ddf-adf9-3c0cdb3065d1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4f5f59c-eab0-4eff-9506-716203d706d4", "node_type": "1", "metadata": {}, "hash": "f328500897b299847513cdea5f2b7c610aeab9e0bf52336c79e0936563e93b5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09de1bb1-ddc6-4c63-8556-bc80420029d0", "node_type": "1", "metadata": {}, "hash": "1e958f51206aab7c8c1d14929bc7e9b9a2ecef8bf6b40b8054d56463d3cd3423", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This specification makes it easy to retarget\nC4CAM for different CAM accelerators. Apart from the archi-\ntecture description, this input file specifies the optimization\ntarget, which can be set to latency, power, or array utilization.\n4.3\nC4CAM front end\nThe PyTorch MLIR converter [13] is responsible for con-\nverting Python code written in TorchScript. However, certain\noperations from the ATen library, particularly those used in\nsearch-based applications such as norm and topk, are not\nsupported.", "mimetype": "text/plain", "start_char_idx": 15368, "end_char_idx": 15868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09de1bb1-ddc6-4c63-8556-bc80420029d0": {"__data__": {"id_": "09de1bb1-ddc6-4c63-8556-bc80420029d0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ded2ef5f-9f03-4ddf-adf9-3c0cdb3065d1", "node_type": "1", "metadata": {}, "hash": "c70dad70de01fc598e5cc346a935e8cb6217a31927a677e781eee11edf928d78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71da9476-65fd-44b0-bc1a-253d08c47b3a", "node_type": "1", "metadata": {}, "hash": "9f1d5bd7f80636c34469570061ab8b4df36e02f9ba02d3e010222118cbeb9ee1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "From the CAM perspective, these are essential\nprimitives in any input application. Since C4CAM is built\nupon the MLIR framework and this is the only available front-\nend that enables lowering TorchScript input to the MLIR torch\ndialect, we extend the front-end to support the norm and topk\nprimitives that are commonly accelerated on CAM arrays.\nTo implement the benchmarks in Section 5, TorchScript was\nused. However, C4CAM is not confined only to TorchScript\nand the PyTorch MLIR Converter as its front-end.", "mimetype": "text/plain", "start_char_idx": 15869, "end_char_idx": 16378, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71da9476-65fd-44b0-bc1a-253d08c47b3a": {"__data__": {"id_": "71da9476-65fd-44b0-bc1a-253d08c47b3a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09de1bb1-ddc6-4c63-8556-bc80420029d0", "node_type": "1", "metadata": {}, "hash": "1e958f51206aab7c8c1d14929bc7e9b9a2ecef8bf6b40b8054d56463d3cd3423", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "854008a0-4e99-4b8e-b495-b88b9dc090a1", "node_type": "1", "metadata": {}, "hash": "59be0a0085ed7b51aa710e083861e27b72070e92b09b244f66b2f2ac3d346e03", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Represent-\ning every CAM-suitable application in TorchScript may not\nbe straightforward (such as DNA mapping). For that, it is also\npossible to use alternative flows, such as ONNX-MLIR [4] and\nsupported importers in IREE [3]. Such flows can also serve\nthis purpose by implementing necessary conversions from\ntheir corresponding dialect to cim.\n4.4\nC4CAM progressive lowering\nThe compilation flow begins with the Torch dialect, as de-\npicted in Figure 3. This dialect includes most of the ATen\ntensor library operations.", "mimetype": "text/plain", "start_char_idx": 16379, "end_char_idx": 16898, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "854008a0-4e99-4b8e-b495-b88b9dc090a1": {"__data__": {"id_": "854008a0-4e99-4b8e-b495-b88b9dc090a1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71da9476-65fd-44b0-bc1a-253d08c47b3a", "node_type": "1", "metadata": {}, "hash": "9f1d5bd7f80636c34469570061ab8b4df36e02f9ba02d3e010222118cbeb9ee1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "816881c4-a614-43c0-8d14-984014afd457", "node_type": "1", "metadata": {}, "hash": "d06d0298a60f476b9afa8db59ea6395d8f3a1e4f259e8e6493739bfe7f48df78", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To enable support for the cim ab-\nstraction, we have introduced a torch-to-cim conversion\npass. This pass lowers the operations that are compatible\nwith the cim abstraction. Examples of these operations in-\nclude topk, norm, sub, and matmul, which can be executed\nindividually or as part of a kernel on a CIM device.\nTo demonstrate how the IR of the application is transformed\nat each hierarchy level, we use the similarity kernel in hyperdi-\nmensional computing (HDC) as a running example.", "mimetype": "text/plain", "start_char_idx": 16899, "end_char_idx": 17389, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "816881c4-a614-43c0-8d14-984014afd457": {"__data__": {"id_": "816881c4-a614-43c0-8d14-984014afd457", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "854008a0-4e99-4b8e-b495-b88b9dc090a1", "node_type": "1", "metadata": {}, "hash": "59be0a0085ed7b51aa710e083861e27b72070e92b09b244f66b2f2ac3d346e03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce83d2a3-2280-4a71-93a1-e25407d1b548", "node_type": "1", "metadata": {}, "hash": "bb7ffca94cc009d4e5e8b30ba9b08482c97cfdb9559b56b6b9153b8600c75c6a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Figure 4a\nshows the TorchScript code of the input kernel. Figure 4b\npresents its MLIR representation at the Torch abstraction\nas produced by the MLIR PyTorch front-end. The conver-\nsion from Torch to cim is accomplished with target-agnostic\ntransformations. The outcome of the conversion primarily\nshowcases the interface with a generic CIM device.\n4.4.1\nThe extended cim abstraction.", "mimetype": "text/plain", "start_char_idx": 17390, "end_char_idx": 17774, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce83d2a3-2280-4a71-93a1-e25407d1b548": {"__data__": {"id_": "ce83d2a3-2280-4a71-93a1-e25407d1b548", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "816881c4-a614-43c0-8d14-984014afd457", "node_type": "1", "metadata": {}, "hash": "d06d0298a60f476b9afa8db59ea6395d8f3a1e4f259e8e6493739bfe7f48df78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc9faad3-0c6a-48fb-be64-4e8b32de0891", "node_type": "1", "metadata": {}, "hash": "1953d9dcb3db96dc656d976d5be683f26fb55291f7d672b546994c8e77005242", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "cim is a high-level\ndialect for device-supported operations that includes essential\ntransformations required to prepare kernels to run on target\n167\nC4CAM: A Compiler for CAM-based In-memory Accelerators\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\ndef forward(self, input: Tensor, dot: bool = False)\n\u21a9\u2192-> Tensor:\nothers = self.weight.transpose(-2, -1)\nmatmul = torch.matmul(input, (others))\nvalues, indices = torch.ops.aten.topk(matmul ,", "mimetype": "text/plain", "start_char_idx": 17775, "end_char_idx": 18220, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc9faad3-0c6a-48fb-be64-4e8b32de0891": {"__data__": {"id_": "dc9faad3-0c6a-48fb-be64-4e8b32de0891", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce83d2a3-2280-4a71-93a1-e25407d1b548", "node_type": "1", "metadata": {}, "hash": "bb7ffca94cc009d4e5e8b30ba9b08482c97cfdb9559b56b6b9153b8600c75c6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54417010-3505-47e3-be62-a9740807383b", "node_type": "1", "metadata": {}, "hash": "35a31310237aaec4403a8522b9788b7203d1014e37dcd26ef74ed79feabdd4aa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1,\n\u21a9\u2192largest=False)\nreturn indices\n(a) PyTorchScript code for HDC dot similarity\n%1 = torch.aten.transpose.int %0, %int-2, %int-1 : !\n\u21a9\u2192torch.vtensor <[10,8192],f32>, !torch.int, !\n\u21a9\u2192torch.int -> !torch.vtensor <[8192,10],f32>\n%2 = torch.aten.mm %arg0, %1 : !torch.vtensor\n\u21a9\u2192<[10,8192],f32>,", "mimetype": "text/plain", "start_char_idx": 18221, "end_char_idx": 18512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54417010-3505-47e3-be62-a9740807383b": {"__data__": {"id_": "54417010-3505-47e3-be62-a9740807383b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc9faad3-0c6a-48fb-be64-4e8b32de0891", "node_type": "1", "metadata": {}, "hash": "1953d9dcb3db96dc656d976d5be683f26fb55291f7d672b546994c8e77005242", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "670d4244-7cc5-4a33-95ab-5e03a994198c", "node_type": "1", "metadata": {}, "hash": "b6f1c6a3a4657aae8e1ce7188e59456bccc7c3c0b40a7c4b29e98ca0f19c2457", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "!torch.vtensor <[8192,10],f32\n\u21a9\u2192> -> !torch.vtensor <[10,10],f32>\n%values , %indices = torch.aten.topk %2, %int1, %int\n\u21a9\u2192-1, %false , %true : !torch.vtensor <[10,10],\n\u21a9\u2192f32>, !torch.int, !torch.int, !torch.bool, !\n\u21a9\u2192torch.bool -> !torch.vtensor <[10,1],f32>, !", "mimetype": "text/plain", "start_char_idx": 18513, "end_char_idx": 18773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "670d4244-7cc5-4a33-95ab-5e03a994198c": {"__data__": {"id_": "670d4244-7cc5-4a33-95ab-5e03a994198c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54417010-3505-47e3-be62-a9740807383b", "node_type": "1", "metadata": {}, "hash": "35a31310237aaec4403a8522b9788b7203d1014e37dcd26ef74ed79feabdd4aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7409d33a-3652-4002-8c97-87fbdc4a7bfb", "node_type": "1", "metadata": {}, "hash": "2f7a105855bf06287df3dc9cadf00c8e150a4103560fcff91c1e95fe7687c5e7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\u21a9\u2192torch.vtensor <[10,1],f32>\n(b) Torch IR for HDC dot similarity\nFigure 4. Python and MLIR representations of HDC similarity\ndevices.", "mimetype": "text/plain", "start_char_idx": 18774, "end_char_idx": 18907, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7409d33a-3652-4002-8c97-87fbdc4a7bfb": {"__data__": {"id_": "7409d33a-3652-4002-8c97-87fbdc4a7bfb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "670d4244-7cc5-4a33-95ab-5e03a994198c", "node_type": "1", "metadata": {}, "hash": "b6f1c6a3a4657aae8e1ce7188e59456bccc7c3c0b40a7c4b29e98ca0f19c2457", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce6c6eb7-e015-4572-94fe-81884a1c0a6b", "node_type": "1", "metadata": {}, "hash": "9d0fff641096115d6e5549309cec71a266be287a471bc46f2dace540e3010299", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This abstraction is mainly responsible for: (i) analyz-\ning the input code to identify CIM-amenable primitives that\ncan be offloaded to the accelerator, (ii) if a CIM-executable\npattern is identified but the operand sizes exceed the array\nsizes specified in the given architecture, dividing the input into\nsmaller partitions to ensure compatibility with the accelerator,\nand (iii) providing an abstract programming model to enable\nthe execution of kernels on a device.\nThe programming model of the cim abstraction is based on\nthree main functions.", "mimetype": "text/plain", "start_char_idx": 18908, "end_char_idx": 19455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce6c6eb7-e015-4572-94fe-81884a1c0a6b": {"__data__": {"id_": "ce6c6eb7-e015-4572-94fe-81884a1c0a6b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7409d33a-3652-4002-8c97-87fbdc4a7bfb", "node_type": "1", "metadata": {}, "hash": "2f7a105855bf06287df3dc9cadf00c8e150a4103560fcff91c1e95fe7687c5e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "471fb5ad-4434-49db-a405-c75619cbc511", "node_type": "1", "metadata": {}, "hash": "e090f2991001420af545c5142f15a7e1dac33271e724d3b27f2a707036a96b08", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To allocate an accelerator, cim uses the\ncim.acquire function that returns a handle to the device. The\ncim.execute function uses this handle and specifies the op-\nerations that are to be executed on this accelerator. Finally, the\ndevice is released using the cim.release function. For CAM\narchitectures, we show how these functions are lowered to\ndifferent CAM functions in Section 4.4.2.", "mimetype": "text/plain", "start_char_idx": 19456, "end_char_idx": 19844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "471fb5ad-4434-49db-a405-c75619cbc511": {"__data__": {"id_": "471fb5ad-4434-49db-a405-c75619cbc511", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce6c6eb7-e015-4572-94fe-81884a1c0a6b", "node_type": "1", "metadata": {}, "hash": "9d0fff641096115d6e5549309cec71a266be287a471bc46f2dace540e3010299", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "537b0ab5-144d-4d5f-b014-b48212fc1c44", "node_type": "1", "metadata": {}, "hash": "8a8f5def647695fce70932ad7bcbba6dc9bdc896cc65475c0e3b95e6c3b4fa95", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Figure 5a shows the\nIR for the running example at the cim abstraction, which can\nbe produced by running the conversion pass torch-to-cim\nat the Torch abstraction. As the Torch abstraction does not,\nand is not supposed to, specify the kernel type, the fundamen-\ntal assumption of the torch-to-cim conversion is that each\nsupported operation can be executed on a separate (non-)CIM\ndevice. Since all the torch operations are supported by the\ncim dialect (as they are part of the dot similarity), they are\nlowered to their corresponding cim versions.", "mimetype": "text/plain", "start_char_idx": 19845, "end_char_idx": 20392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "537b0ab5-144d-4d5f-b014-b48212fc1c44": {"__data__": {"id_": "537b0ab5-144d-4d5f-b014-b48212fc1c44", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "471fb5ad-4434-49db-a405-c75619cbc511", "node_type": "1", "metadata": {}, "hash": "e090f2991001420af545c5142f15a7e1dac33271e724d3b27f2a707036a96b08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b4bdbc7-f17b-4754-88a2-2704af18b3db", "node_type": "1", "metadata": {}, "hash": "0c7b7315c28c246451c5982b86ea2042e6e2214b96de9f1a871a6802d163d6e0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Pattern matching and fusing: cim implements analysis and\noptimization passes to recover patterns that can be offloaded\nto a CIM accelerator and, when possible, optimizes them\nfor the target. The analysis pass identifies blocks containing\noperations that cannot be directly lowered to the accelerator\nand fuses them. Once the code analysis is completed, the\nexecution blocks can be transformed and offloaded to CIM\naccelerators, or they can follow the standard MLIR pipeline\nto generate llvm code for execution on the host processor.\nAlgorithm 1: Similarity search detection.", "mimetype": "text/plain", "start_char_idx": 20393, "end_char_idx": 20967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b4bdbc7-f17b-4754-88a2-2704af18b3db": {"__data__": {"id_": "4b4bdbc7-f17b-4754-88a2-2704af18b3db", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "537b0ab5-144d-4d5f-b014-b48212fc1c44", "node_type": "1", "metadata": {}, "hash": "8a8f5def647695fce70932ad7bcbba6dc9bdc896cc65475c0e3b95e6c3b4fa95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7ab454b-d73e-4558-a4b3-3ab76b473001", "node_type": "1", "metadata": {}, "hash": "7453bb150e6c2bf4f37bf80b6f6daac5a45e46d01458d8de137c52678d605b98", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1 /* Pattern matching for dot product similarity\n*/\n2 Replace op<topk> (op<matmul> (arg2, op<transpose> (arg1)), arg3)\n3\nwith op <similarity> (dot, arg1, arg2, arg3);\n4 /* Pattern matching for Euclidean distance\n*/\n5 Replace op <topk>(op<norm> (op<sub> (arg1, arg2)), arg3)\n6\nwith op <similarity> (euc, arg1, arg2, arg3);", "mimetype": "text/plain", "start_char_idx": 20968, "end_char_idx": 21289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7ab454b-d73e-4558-a4b3-3ab76b473001": {"__data__": {"id_": "d7ab454b-d73e-4558-a4b3-3ab76b473001", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b4bdbc7-f17b-4754-88a2-2704af18b3db", "node_type": "1", "metadata": {}, "hash": "0c7b7315c28c246451c5982b86ea2042e6e2214b96de9f1a871a6802d163d6e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "557e8d1e-f6bf-4c69-8f84-8cf0e0762a34", "node_type": "1", "metadata": {}, "hash": "8a593d549a1b39a4121beb3e7034c9744c7ccc8e4a99736e4f71d623e9a9d730", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "7 /* Pattern matching for cosine similarity\n*/\n8 Replace op <smulmat>(op <div> (cons1, op<mul>(op<norm>(arg1),\nop<norm> (arg2))), op<matmul>(arg1, op<transpose>(arg2)))\n9\nwith op <similarity> (cos, arg1, arg2);\n10 /* Pattern matching for Hamming distance\n*/\n11 Replace op <nonzero>(op <cmp>(lt, op <popcount>(op <xor>(arg1,\narg2),", "mimetype": "text/plain", "start_char_idx": 21290, "end_char_idx": 21620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "557e8d1e-f6bf-4c69-8f84-8cf0e0762a34": {"__data__": {"id_": "557e8d1e-f6bf-4c69-8f84-8cf0e0762a34", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7ab454b-d73e-4558-a4b3-3ab76b473001", "node_type": "1", "metadata": {}, "hash": "7453bb150e6c2bf4f37bf80b6f6daac5a45e46d01458d8de137c52678d605b98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f934dfee-396b-4f19-a869-1f5c3ed52fad", "node_type": "1", "metadata": {}, "hash": "ecf96ef7f67a9a8b9e2c94a4d104e4ee6e536ab13173fc3f7f0d74b33b2d6472", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "arg3)\n12\nwith op <similarity> (ham, arg1, arg2, arg3);\nAlgorithm 1 illustrates the function designed for pattern\nmatching within an execution block, tailored explicitly for\nidentifying various similarity search operations. This func-\ntion assesses whether a given data flow graph aligns with\npre-defined supported patterns and replaces it with the corre-\nsponding similarity search operation.\nThe pattern matching for dot product, Euclidean norm, co-\nsine, and Hamming similarity are defined in Lines 2, 5, 8, and\n11, respectively.", "mimetype": "text/plain", "start_char_idx": 21621, "end_char_idx": 22152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f934dfee-396b-4f19-a869-1f5c3ed52fad": {"__data__": {"id_": "f934dfee-396b-4f19-a869-1f5c3ed52fad", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "557e8d1e-f6bf-4c69-8f84-8cf0e0762a34", "node_type": "1", "metadata": {}, "hash": "8a593d549a1b39a4121beb3e7034c9744c7ccc8e4a99736e4f71d623e9a9d730", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01253bd9-b089-49f8-a571-9b1a9b8ee380", "node_type": "1", "metadata": {}, "hash": "b54dc787bbccebf82bf231bf2266e58ac9c2b5eb18d2855e16f46525487cf12d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Algorithm 1 is the simplified illustration of\ncim-fuse-ops pass, which, when enabled with the similarity\nflag indicating the type of search, identifies code blocks that\nmatch the criteria and subsequently replace their operations\nwith the cim.similarity operation. Figure 5a shows the\nbase cim IR produced by the torch-to-cim conversion pass,\nwhile Figure 5c showcases the result obtained after applying\nthe cim-fuse-ops pass to Figure 5b.", "mimetype": "text/plain", "start_char_idx": 22153, "end_char_idx": 22592, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01253bd9-b089-49f8-a571-9b1a9b8ee380": {"__data__": {"id_": "01253bd9-b089-49f8-a571-9b1a9b8ee380", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f934dfee-396b-4f19-a869-1f5c3ed52fad", "node_type": "1", "metadata": {}, "hash": "ecf96ef7f67a9a8b9e2c94a4d104e4ee6e536ab13173fc3f7f0d74b33b2d6472", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c58947c-bfd2-4a53-9c22-0df6255f21e5", "node_type": "1", "metadata": {}, "hash": "cf6b4a50644128eafa97dd675aaae2a089a5736d9c64c61cf0fdda80d6515425", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Compulsory partitioning: Kernels often require more space\nthan what the processing elements (PE) of the target can\nsupport. To overcome this limitation, the kernel is partitioned\naccording to the size supported by a PE. In a CAM system, the\nsmallest block within the system is the subarray. Therefore,\nwhen partitioning the application, it is important to consider\nthis level of granularity and divide it accordingly. To support\nthis, C4CAM includes a partitioning transformation within the\ncim abstraction.", "mimetype": "text/plain", "start_char_idx": 22593, "end_char_idx": 23100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c58947c-bfd2-4a53-9c22-0df6255f21e5": {"__data__": {"id_": "4c58947c-bfd2-4a53-9c22-0df6255f21e5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01253bd9-b089-49f8-a571-9b1a9b8ee380", "node_type": "1", "metadata": {}, "hash": "b54dc787bbccebf82bf231bf2266e58ac9c2b5eb18d2855e16f46525487cf12d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec38f990-1634-4209-a465-676674df7c22", "node_type": "1", "metadata": {}, "hash": "610ed138cffa467633b4901e19bc3db2cd74205d6086f66761159de9597e7de3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This transformation can be likened to tiling in\ncompiler terminology, with hardware-specific considerations.\nIt enables the efficient partitioning of kernels to facilitate\ntheir execution on the device(s), but requires an abstraction\nto accumulate partial results. To this end, the cim dialect\nwas extended with the cim.merge_partial operation. It\nconsiders both the type of operation for which partial results\nare generated and the direction in which these results are\naccumulated. The partitioned version of Figure 5c for a device\nof size 32x32 is shown in Figure 5d.", "mimetype": "text/plain", "start_char_idx": 23101, "end_char_idx": 23670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec38f990-1634-4209-a465-676674df7c22": {"__data__": {"id_": "ec38f990-1634-4209-a465-676674df7c22", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c58947c-bfd2-4a53-9c22-0df6255f21e5", "node_type": "1", "metadata": {}, "hash": "cf6b4a50644128eafa97dd675aaae2a089a5736d9c64c61cf0fdda80d6515425", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8c18495-ad01-4e85-81e4-d93ef23795b2", "node_type": "1", "metadata": {}, "hash": "b08116ef0864630b276f211d9e4ebeac7d79b03bff01e40cd30ab48ad09d42ca", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Note that this partitioned\ncode and the surrounding loop (scf.for) is still serial.\nOur extension to the cim abstraction focuses on identifying\noperations that can be offloaded to the CAM accelerator and\n168\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nFarzaneh et al.\n\n%4 = cim.acquire : index\n%5 = cim.execute(%4, %2) ({\n%11= cim.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8c18495-ad01-4e85-81e4-d93ef23795b2": {"__data__": {"id_": "d8c18495-ad01-4e85-81e4-d93ef23795b2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec38f990-1634-4209-a465-676674df7c22", "node_type": "1", "metadata": {}, "hash": "610ed138cffa467633b4901e19bc3db2cd74205d6086f66761159de9597e7de3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17a8dc1d-2514-4bd0-bb6e-6ea070567a47", "node_type": "1", "metadata": {}, "hash": "8566f31aba4481bb740c31cd08e6702a6c7d47b4910ba94ae19de3c1c2b43a31", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "transpose %2: tensor <10x8192xf32 >\n-> tensor <8192x10xf32 >\ncim.yield %11 : tensor <8192x10xf32 >\n}) : (index, tensor <10x8192xf32 >) -> tensor <8192x10xf32 >\ncim.release %4 : index\n%6 = cim.acquire : index\n%7 = cim.execute(%6, %0, %5) ({\n%11 = cim.matmul %0, %5 : tensor <10x8192xf32 >,", "mimetype": "text/plain", "start_char_idx": 24013, "end_char_idx": 24301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17a8dc1d-2514-4bd0-bb6e-6ea070567a47": {"__data__": {"id_": "17a8dc1d-2514-4bd0-bb6e-6ea070567a47", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8c18495-ad01-4e85-81e4-d93ef23795b2", "node_type": "1", "metadata": {}, "hash": "b08116ef0864630b276f211d9e4ebeac7d79b03bff01e40cd30ab48ad09d42ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c796ebd-8863-4ae2-a86d-409a8ec7f21f", "node_type": "1", "metadata": {}, "hash": "79d143cb587f17e068eba86450565497c271d31526c2a7fef79c60247d4a638f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "tensor <8192x10xf32 > -> tensor <10x10xf32 >\ncim.yield %11 : tensor <10x10xf32 >\n}) : (index, tensor <10x8192xf32 >, tensor <8192x10xf32 >) -> tensor\n\u21a9\u2192<10x10xf32 >\n.\n(a) cim IR\n.\n%4 = cim.acquire : index\n%5:2 = cim.execute(%4, %2, %0, %3) ({\n%7 = cim.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c796ebd-8863-4ae2-a86d-409a8ec7f21f": {"__data__": {"id_": "5c796ebd-8863-4ae2-a86d-409a8ec7f21f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17a8dc1d-2514-4bd0-bb6e-6ea070567a47", "node_type": "1", "metadata": {}, "hash": "8566f31aba4481bb740c31cd08e6702a6c7d47b4910ba94ae19de3c1c2b43a31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5eb85767-dbf4-48fd-a118-9b08e1bc849a", "node_type": "1", "metadata": {}, "hash": "fd6b3f0c20690f908e993e59f5c448e71a0eabc618b1f90c7eae8112da352d62", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "transpose %2 : tensor <10x8192xf32 >\n-> tensor <8192x10xf32 >\n%8 = cim.matmul %0, %7 : tensor <10x8192xf32 >,\ntensor <8192x10xf32 >\n-> tensor <10x10xf32 >\n%values , %indices = cim.topk %8, %3 : tensor <10x10xf32 >\n, i64 -> tensor <10x1xf32 >, tensor <10x1xf32 >\ncim.yield %values ,", "mimetype": "text/plain", "start_char_idx": 24558, "end_char_idx": 24839, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5eb85767-dbf4-48fd-a118-9b08e1bc849a": {"__data__": {"id_": "5eb85767-dbf4-48fd-a118-9b08e1bc849a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c796ebd-8863-4ae2-a86d-409a8ec7f21f", "node_type": "1", "metadata": {}, "hash": "79d143cb587f17e068eba86450565497c271d31526c2a7fef79c60247d4a638f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fb7aa01-72d7-4552-abb3-a30e88ba1c8a", "node_type": "1", "metadata": {}, "hash": "db4436e239ce188c59b65885f55bcca5291fd4a51d8f61950bbde739509c0ba3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "%indices : tensor <10x1xf32>, tensor <10x1xf32>\n}) : (index, tensor <10x8192xf32 >, tensor <10x8192xf32 >, i64)\n-> (tensor <10x1xf32>, tensor <10x1xf32 >)\ncim.release %4 : index\n.\n(b) cim IR after fusing execution blocks\n.\n%4 = cim.acquire : index\n%5:2 = cim.execute(%4, %2, %0, %3) ({\n%values ,", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fb7aa01-72d7-4552-abb3-a30e88ba1c8a": {"__data__": {"id_": "3fb7aa01-72d7-4552-abb3-a30e88ba1c8a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5eb85767-dbf4-48fd-a118-9b08e1bc849a", "node_type": "1", "metadata": {}, "hash": "fd6b3f0c20690f908e993e59f5c448e71a0eabc618b1f90c7eae8112da352d62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00794754-fc63-45d5-9d6d-73a1d041162c", "node_type": "1", "metadata": {}, "hash": "1f2533e252575b642e6648527b803b332424201b137fccfffdbca6705bdf0e8a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "%indices = cim.similarity dot %2,\n%0, %3 : tensor <10x8192xf32 >, tensor <10x8192xf32 >,\ni64 -> tensor <10x1xf32>, tensor <10x1xf32>\ncim.yield %values , %indices : tensor <10x1xf32>, tensor <10x1xf32>\n}) : (index, tensor <10x8192xf32 >, tensor <10x8192xf32 >, i64)\n-> (tensor <10x1xf32>,", "mimetype": "text/plain", "start_char_idx": 25140, "end_char_idx": 25427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00794754-fc63-45d5-9d6d-73a1d041162c": {"__data__": {"id_": "00794754-fc63-45d5-9d6d-73a1d041162c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fb7aa01-72d7-4552-abb3-a30e88ba1c8a", "node_type": "1", "metadata": {}, "hash": "db4436e239ce188c59b65885f55bcca5291fd4a51d8f61950bbde739509c0ba3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "413c7c40-c43d-44ea-828d-1348a3b97ea4", "node_type": "1", "metadata": {}, "hash": "bee77439f026238356938ec89c700fbb870fb59430a75b0ee90ce72fa8a2d9f2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "tensor <10x1xf32 >)\ncim.release %4 : index\n.\n(c) cim IR with rewrites for CAM lowering\nscf.for %arg1 = %c0 to %c8192 step %c32 {\n%extr_slice = tensor.extract_slice %2[0, %arg1] [10, 32]\n[1, 1] : tensor <10x8192xf32 > to tensor <10x32xf32 >\n%extr_slice_0 = tensor.extract_slice %0[0, %arg1] [10,", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "413c7c40-c43d-44ea-828d-1348a3b97ea4": {"__data__": {"id_": "413c7c40-c43d-44ea-828d-1348a3b97ea4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00794754-fc63-45d5-9d6d-73a1d041162c", "node_type": "1", "metadata": {}, "hash": "1f2533e252575b642e6648527b803b332424201b137fccfffdbca6705bdf0e8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c184e13-a2e1-4b6d-80a0-f96d5e819a63", "node_type": "1", "metadata": {}, "hash": "d6206e410f2a5dc7af3e817bdf0c6d86cdce38c51db19563f9e7846fbb540dc6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "32]\n[1, 1] : tensor <10x8192xf32 > to tensor <10x32xf32 >\n%7 = cim.acquire : index\n%8:2 = cim.execute(%7, %extr_slice , %extr_slice_0 , %3) ({\n%values , %indices = cim.similarity dot\n%extr_slice ,\n%extr_slice_0 , %3 : tensor <10x32xf32 >, tensor <10x32xf32 >,\ni64 -> tensor <10x1xf32>,", "mimetype": "text/plain", "start_char_idx": 25725, "end_char_idx": 26010, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c184e13-a2e1-4b6d-80a0-f96d5e819a63": {"__data__": {"id_": "2c184e13-a2e1-4b6d-80a0-f96d5e819a63", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "413c7c40-c43d-44ea-828d-1348a3b97ea4", "node_type": "1", "metadata": {}, "hash": "bee77439f026238356938ec89c700fbb870fb59430a75b0ee90ce72fa8a2d9f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49e9d009-c89d-496a-beb5-dc968978421f", "node_type": "1", "metadata": {}, "hash": "8eefc3f2a24a88bbe1fd8fcc1a25ce7eea0228b52e6fb5b63f29401d292c7ba5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "tensor <10x1xf32>\ncim.yield %values , %indices : tensor <10x1xf32>,\ntensor <10x1xf32 >}) : (index, tensor <10x32xf32 >,\ntensor <10x32xf32 >, i64) -> (tensor <10x1xf32>,\ntensor <10x1xf32 >)\n%9 = cim.merge_partial values\nsimilarity dot\nhorizontal\n%7, %4, %8#0 : index, tensor <10x1xf32>,", "mimetype": "text/plain", "start_char_idx": 26011, "end_char_idx": 26296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49e9d009-c89d-496a-beb5-dc968978421f": {"__data__": {"id_": "49e9d009-c89d-496a-beb5-dc968978421f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c184e13-a2e1-4b6d-80a0-f96d5e819a63", "node_type": "1", "metadata": {}, "hash": "d6206e410f2a5dc7af3e817bdf0c6d86cdce38c51db19563f9e7846fbb540dc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "500f8d9b-1a0e-4bea-8443-34eb023678bb", "node_type": "1", "metadata": {}, "hash": "fa0d46245fb314c58a024fbd6e507b8543973848c1b1925d5c71775b475eca63", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "tensor <10x1xf32>\n-> tensor <10x1xf32>\n.\ncim.release %7 : index\n}\n(d) cim IR after partitioning\nFigure 5. cim IR of the HDC similarity function, after dif-\nferent analysis and optimization passes. Similar operations at\ndifferent stages are highlighted using the same color.\non preparing the code via partitioning for further lowering to\nCAMs. It does not address the mapping of an input applica-\ntion or its partitions onto the CAM accelerator, nor does it\nincorporate any device-specific optimizations.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "500f8d9b-1a0e-4bea-8443-34eb023678bb": {"__data__": {"id_": "500f8d9b-1a0e-4bea-8443-34eb023678bb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49e9d009-c89d-496a-beb5-dc968978421f", "node_type": "1", "metadata": {}, "hash": "8eefc3f2a24a88bbe1fd8fcc1a25ce7eea0228b52e6fb5b63f29401d292c7ba5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "761432ee-0a5c-4331-96e3-98fccc17f028", "node_type": "1", "metadata": {}, "hash": "5ef0867ece72cfa13313e61f0fb282a1f5cdba4ef6240c857acb7b28c4e43baa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Our novel cam\nabstraction takes on these responsibilities.\n4.4.2\nThe cam abstraction. To convert the cim IR into\nthe cam IR, C4CAM introduces the cim-to-cam conversion\npass. This pass requires specifying the target CAM device\ntype (e.g., ACAM, TCAM, or MCAM) in the architecture\nspecification, which also determines the search type (EX, BE\nor TH) and the metric to be utilized during the conversion\nprocess. Additionally, it requires specifying the method for\naccumulating partial results.", "mimetype": "text/plain", "start_char_idx": 26803, "end_char_idx": 27292, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "761432ee-0a5c-4331-96e3-98fccc17f028": {"__data__": {"id_": "761432ee-0a5c-4331-96e3-98fccc17f028", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "500f8d9b-1a0e-4bea-8443-34eb023678bb", "node_type": "1", "metadata": {}, "hash": "fa0d46245fb314c58a024fbd6e507b8543973848c1b1925d5c71775b475eca63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3efb12e8-48aa-42b1-b8a0-7c17b031051b", "node_type": "1", "metadata": {}, "hash": "adda5251987bf88a90a625fef499ca214aff6732b1dde5215f7c833b8c50a1e3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "By default, this is handled by the\nCPU. Alternatively, a specialized method with HW support\ncan be specified. This accounts for CAM arrays that include\ndedicated circuitry (e.g., an adder tree) or an extra CIM module\nto support typical accumulation functions. Depending on the\nmerging circuitry and the mapping to the CAM arrays, the\nsearch and merge operations can be pipelined or executed\nsequentially. For example, in BioHD [54], search operations\nand the accumulation of partial Hamming Distance values are\npipelined to improve throughput.", "mimetype": "text/plain", "start_char_idx": 27293, "end_char_idx": 27836, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3efb12e8-48aa-42b1-b8a0-7c17b031051b": {"__data__": {"id_": "3efb12e8-48aa-42b1-b8a0-7c17b031051b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "761432ee-0a5c-4331-96e3-98fccc17f028", "node_type": "1", "metadata": {}, "hash": "5ef0867ece72cfa13313e61f0fb282a1f5cdba4ef6240c857acb7b28c4e43baa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cce391b7-00ab-4456-9032-fc4bd361911c", "node_type": "1", "metadata": {}, "hash": "88f6cc4a39d008650eb88b76dd2190fe7f33fdc6ac47e6aa694fbcf5bcd18358", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The cam dialect is responsible for mapping the high-level\nfunctions from the cim dialect to the CAM-device calls. After\napplying this conversion pass, occurrences of a sequence of\ncim.acquire, cim.execute, and cim.release working\non the same device handle are substituted with allocation\ncalls at bank, mat, and array-level. The allocated modules, i.e.,\nbanks, mats, arrays, and subarrays, then execute the search\noperations in parallel.", "mimetype": "text/plain", "start_char_idx": 27837, "end_char_idx": 28274, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cce391b7-00ab-4456-9032-fc4bd361911c": {"__data__": {"id_": "cce391b7-00ab-4456-9032-fc4bd361911c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3efb12e8-48aa-42b1-b8a0-7c17b031051b", "node_type": "1", "metadata": {}, "hash": "adda5251987bf88a90a625fef499ca214aff6732b1dde5215f7c833b8c50a1e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12e75cd0-66b2-4c7e-b5fa-90ed1949b5da", "node_type": "1", "metadata": {}, "hash": "f65d18f2d1551b8d1d555adb90ef98a5e2eacd644aeb1b94e8f9858d052ea68e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "More concretely, the cam.alloc_bank function is used to\nallocate a CAM bank, taking the row and column sizes of the\ndesired CAM size as parameters. Furthermore, allocating a\nmat from the bank, and a CAM array from the mat, and a subar-\nray from an array is accomplished using the cam.alloc_mat,\ncam.alloc_array, and cam.alloc_subarray functions,\nrespectively. Similarly, the cim.execute function is low-\nered into three CAM function calls: cam.write_value,\ncam.search, and cam.read_value.", "mimetype": "text/plain", "start_char_idx": 28275, "end_char_idx": 28763, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12e75cd0-66b2-4c7e-b5fa-90ed1949b5da": {"__data__": {"id_": "12e75cd0-66b2-4c7e-b5fa-90ed1949b5da", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cce391b7-00ab-4456-9032-fc4bd361911c", "node_type": "1", "metadata": {}, "hash": "88f6cc4a39d008650eb88b76dd2190fe7f33fdc6ac47e6aa694fbcf5bcd18358", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18cd6ee6-e7c7-4314-8c31-f39a431205db", "node_type": "1", "metadata": {}, "hash": "94d23ea28c8dc5762cf09f1e54bf289c4623df4f17412bdc952ded0ff8febdac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The write operation\n(line 16) programs the CAM arrays with the input data. The\nsearch operation (line 18) performs the actual search on the\ndata based on the specified search type and metric. The sup-\nported search types include exact match, best match, and range\nmatch, while the available distance metrics are Euclidean and\nHamming. The read operation (line 20) reads the values and\nindices of the search results from the device.\nThe original program underwent partitioning at the CIM\ndialect without considering the hierarchy.", "mimetype": "text/plain", "start_char_idx": 28764, "end_char_idx": 29293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18cd6ee6-e7c7-4314-8c31-f39a431205db": {"__data__": {"id_": "18cd6ee6-e7c7-4314-8c31-f39a431205db", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12e75cd0-66b2-4c7e-b5fa-90ed1949b5da", "node_type": "1", "metadata": {}, "hash": "f65d18f2d1551b8d1d555adb90ef98a5e2eacd644aeb1b94e8f9858d052ea68e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dabff7c6-6f21-43ca-b1a1-dcc8b8c4e843", "node_type": "1", "metadata": {}, "hash": "f3baf9b8b2f44f5ddd7b6e36fe27590761246945dba6e7025bc612c09f5054ff", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This approach was\nchosen because dealing with synchronization and accumula-\ntion of partial results across different levels of the hierarchy\noften requires hardware-specific information, which goes\nagainst the principles of the cim dialect. To map an applica-\ntion onto the CAM abstraction, the cam-map pass within the\n169\nC4CAM: A Compiler for CAM-based In-memory Accelerators\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\n1\n%bank_values_buffer = memref.", "mimetype": "text/plain", "start_char_idx": 29294, "end_char_idx": 29755, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dabff7c6-6f21-43ca-b1a1-dcc8b8c4e843": {"__data__": {"id_": "dabff7c6-6f21-43ca-b1a1-dcc8b8c4e843", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18cd6ee6-e7c7-4314-8c31-f39a431205db", "node_type": "1", "metadata": {}, "hash": "94d23ea28c8dc5762cf09f1e54bf289c4623df4f17412bdc952ded0ff8febdac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7dcfcdbe-c07c-4039-acb1-d192252a1b50", "node_type": "1", "metadata": {}, "hash": "3bfaa96c759944f8a46ad496f3091b3434144324ad86fab910b0a1de9c186b76", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "alloc() : memref <2x10x1xf32 >\n2\n%bank_indices_buffer = memref.alloc() : memref <2x10x1xf32 >\n3\n.\n4\nscf.parallel (%arg1) = (%c0) to (%c8192) step (%c4096) {\n5\n%1 = cam.alloc_bank %c32, %c32 : index, index -> cam.bank_id\n6\n.\n7\nscf.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7dcfcdbe-c07c-4039-acb1-d192252a1b50": {"__data__": {"id_": "7dcfcdbe-c07c-4039-acb1-d192252a1b50", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dabff7c6-6f21-43ca-b1a1-dcc8b8c4e843", "node_type": "1", "metadata": {}, "hash": "f3baf9b8b2f44f5ddd7b6e36fe27590761246945dba6e7025bc612c09f5054ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb7f4a98-93be-46a4-b4ee-fe82bb1f2349", "node_type": "1", "metadata": {}, "hash": "fb874603fcf198b1f6ae78aeecc2eecca0f68a667bc0c58c769d4ac0e6699374", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "parallel (%arg2) = (%c0) to (%c4096) step (%c1024) {\n8\n%4 = cam.alloc_mat %1 : cam.bank_id -> cam.mat_id\n9\n.\n10\nscf.parallel (%arg3) = (%c0) to (%c1024) step (%c256) {\n11\n%7 = cam.alloc_array %4 : cam.mat_id -> cam.array_id\n12\n.\n13\nscf.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb7f4a98-93be-46a4-b4ee-fe82bb1f2349": {"__data__": {"id_": "fb7f4a98-93be-46a4-b4ee-fe82bb1f2349", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7dcfcdbe-c07c-4039-acb1-d192252a1b50", "node_type": "1", "metadata": {}, "hash": "3bfaa96c759944f8a46ad496f3091b3434144324ad86fab910b0a1de9c186b76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a49871ef-1677-4bde-97d9-abc11fa8d738", "node_type": "1", "metadata": {}, "hash": "350bc8a10423f186ea28c45d4b969d0d243db853344b2aea81af7e18176e0224", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "parallel (%arg4) = (%c0) to (%c256) step (%c32) {\n14\n.\n15\n%12 = cam.alloc_subarray %7 : cam.array_id -> cam.\n\u21a9\u2192subarray_id\n16\ncam.write_value %12, %subarray_data_buffer :\n17\ncam.subarray_id , memref <10x32xf32 >\n18\ncam.search\nexact\neucl %12, %subarray_query_buffer :\n19\ncam.subarray_id ,", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a49871ef-1677-4bde-97d9-abc11fa8d738": {"__data__": {"id_": "a49871ef-1677-4bde-97d9-abc11fa8d738", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb7f4a98-93be-46a4-b4ee-fe82bb1f2349", "node_type": "1", "metadata": {}, "hash": "fb874603fcf198b1f6ae78aeecc2eecca0f68a667bc0c58c769d4ac0e6699374", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "971a6b47-4916-45c6-9d97-2f2a1eb16643", "node_type": "1", "metadata": {}, "hash": "f4739ede7c9e1f38ac7522a134e737adce1c97a626343129f3e25ba534ce3fa2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "memref <1x32xf32 >\n20\n%13:2 = cam.read exact %12 : cam.subarray_id\n21\n-> memref <10x1xf32>, memref <10x1xf32>\n22\n. }\n23\n.}\n24\n.}\n25\n.}\n26\n.\n27\n%value_res = cam.merge_partial bank values horizontal %1,\n28\n%bank_values_buffer :cam.bank_id , memref <2x10x1xf32 >\n29\n-> memref <10x1xf32>\n30\n.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "971a6b47-4916-45c6-9d97-2f2a1eb16643": {"__data__": {"id_": "971a6b47-4916-45c6-9d97-2f2a1eb16643", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a49871ef-1677-4bde-97d9-abc11fa8d738", "node_type": "1", "metadata": {}, "hash": "350bc8a10423f186ea28c45d4b969d0d243db853344b2aea81af7e18176e0224", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72ce1635-7d22-40d0-aee9-7f73daecc27f", "node_type": "1", "metadata": {}, "hash": "a982ae050a00cd72a21c569e52303691440bd5396d6c5b4f535475d8303e4a5e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Figure 6. cam IR after mapping. Similar operations at cim\nand cam stages are highlighted using the same color.\ncam dialect can be employed. This pass transforms the appli-\ncation into a nested loop structure according to the provided\nspecifications, incorporating the required hardware calls at\neach loop level.\nThe code in Figure 6 shows the mapping of the code in\nFigure 5d to a system with two banks, four mats per bank, four\narrays per bank, and eight subarrays per array.", "mimetype": "text/plain", "start_char_idx": 30820, "end_char_idx": 31296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72ce1635-7d22-40d0-aee9-7f73daecc27f": {"__data__": {"id_": "72ce1635-7d22-40d0-aee9-7f73daecc27f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "971a6b47-4916-45c6-9d97-2f2a1eb16643", "node_type": "1", "metadata": {}, "hash": "f4739ede7c9e1f38ac7522a134e737adce1c97a626343129f3e25ba534ce3fa2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "001eb75c-bc35-4e70-9854-219ce8474aae", "node_type": "1", "metadata": {}, "hash": "95435591aa13eac3ee283e83196498c54f443de4745df0c6085469e0e3e01cf8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In this example\nwe assume the method for merging partial results from [54],\nenabling parallel execution of operations. Consequently, the\nserial scf.for loop is substituted with a scf.parallel loop,\nand the cim.merge_partial operation is replaced with a\ncorresponding cam operation. The required components of\nthe CAM accelerator (bank, tile, array, subarray) and buffers\nfor storing the results (values and indices) are allocated at\neach level of the loop.", "mimetype": "text/plain", "start_char_idx": 31297, "end_char_idx": 31753, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "001eb75c-bc35-4e70-9854-219ce8474aae": {"__data__": {"id_": "001eb75c-bc35-4e70-9854-219ce8474aae", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72ce1635-7d22-40d0-aee9-7f73daecc27f", "node_type": "1", "metadata": {}, "hash": "a982ae050a00cd72a21c569e52303691440bd5396d6c5b4f535475d8303e4a5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "245911d7-bde2-4736-954a-5e097feae088", "node_type": "1", "metadata": {}, "hash": "a1e6de75f84f18c65003c689705ba1498e7c8208996e851500ea7f9443e1c835", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In cases where the system size precisely\nmatches the data size, the levels of the nested loop, starting\nfrom the outermost level, iterate over the banks, mats within\neach bank, arrays within each bank, and subarrays within each\narray. However, if the data size exceeds the system\u2019s capacity,\nan additional loop is introduced. This loop includes iteration\nover banks, allowing the system to be called multiple times to\nprocess the data effectively.\nThe cim-to-cam conversion pass also performs bufferiza-\ntion of tensors involved in executing a kernel on the CAM.", "mimetype": "text/plain", "start_char_idx": 31754, "end_char_idx": 32316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "245911d7-bde2-4736-954a-5e097feae088": {"__data__": {"id_": "245911d7-bde2-4736-954a-5e097feae088", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "001eb75c-bc35-4e70-9854-219ce8474aae", "node_type": "1", "metadata": {}, "hash": "95435591aa13eac3ee283e83196498c54f443de4745df0c6085469e0e3e01cf8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df27660a-ffba-4c72-adad-509aa46c096a", "node_type": "1", "metadata": {}, "hash": "5a727c01e78ff7fde3f4c796d3f79b844ca43df6fa2c0376170d9dd24245d255", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This process determines how the memory is handled between\nthe host and the device. During the process of lowering from\ncam to scf and subsequently to llvm, the cam operations are\nmapped to function calls corresponding to the low-level API\nto access the CAM accelerator.\nBuilt-in optimizations: C4CAM provides an extensible\nand flexible framework that enables future research in code\noptimizations and auto-tuning. Currently, the framework uses\nsimple heuristics to optimize for different metrics, namely,\nfor latency/performance, power consumption and device uti-\nlization.", "mimetype": "text/plain", "start_char_idx": 32317, "end_char_idx": 32890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df27660a-ffba-4c72-adad-509aa46c096a": {"__data__": {"id_": "df27660a-ffba-4c72-adad-509aa46c096a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "245911d7-bde2-4736-954a-5e097feae088", "node_type": "1", "metadata": {}, "hash": "a1e6de75f84f18c65003c689705ba1498e7c8208996e851500ea7f9443e1c835", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec8798bc-3283-40e5-b2d1-9ddee2b41f2a", "node_type": "1", "metadata": {}, "hash": "2b3433f0302c2eedc57737448c9a13d53dcdfbc114f44237a1d87469b3318438", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This is enabled by device-specific transformations\nthat can be further composed by performance engineers. For\nexample, in order to minimize latency, C4CAM prioritizes\nmaximizing the utilization of parallel-executing arrays in the\nsystem. In contrast, C4CAM reduces the number of enabled\nsubarrays at a time inside an array to minimize power con-\nsumption.", "mimetype": "text/plain", "start_char_idx": 32891, "end_char_idx": 33246, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec8798bc-3283-40e5-b2d1-9ddee2b41f2a": {"__data__": {"id_": "ec8798bc-3283-40e5-b2d1-9ddee2b41f2a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df27660a-ffba-4c72-adad-509aa46c096a", "node_type": "1", "metadata": {}, "hash": "5a727c01e78ff7fde3f4c796d3f79b844ca43df6fa2c0376170d9dd24245d255", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85c869f0-a5d1-4785-a5b9-0cec9c18b045", "node_type": "1", "metadata": {}, "hash": "2ca60f28b34f3da1c25ad7a7faf64f40fd74ddbcb05b626a983ed2e4862eef21", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For devices that support selective search and in\ncases where the standard data placement underutilizes an\narray due to the number of data entries being smaller than the\nnumber of rows in the memory, it is possible to place multiple\nbatches of data on the same array. By utilizing selective search,\ndifferent queries can be searched on corresponding rows of\nthe same array in multiple cycles.\nAs demonstrated in Section 5.6.1, employing the same\nhierarchy specification (mat, array, and subarray sizes) con-\nsistently leads to longer latencies.", "mimetype": "text/plain", "start_char_idx": 33247, "end_char_idx": 33790, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85c869f0-a5d1-4785-a5b9-0cec9c18b045": {"__data__": {"id_": "85c869f0-a5d1-4785-a5b9-0cec9c18b045", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec8798bc-3283-40e5-b2d1-9ddee2b41f2a", "node_type": "1", "metadata": {}, "hash": "2b3433f0302c2eedc57737448c9a13d53dcdfbc114f44237a1d87469b3318438", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03b403a4-e9bf-4301-8ef6-ef0ab9e5b204", "node_type": "1", "metadata": {}, "hash": "c640439cceb2e6b6110bc6d38f488d326c937a7547ed3cbeef91ab0f7dba5fee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "However, the impact on\nlatency varies depending on the dimensions of the subarrays,\nwhich subsequently alters the number of banks. These de-\nsign choices significantly impact energy consumption and are\nchallenging to predict without an integrated framework that\nfacilitates such optimizations and is supported by simulation.\nThus, C4CAM enables quickly identifying the configurations\nthat best meet workload requirements, while considering scal-\nability and favorable compromise between latency and energy\nconsumption.\n5\nEvaluation\nThis section presents our experimental setup and gives a\ndetailed analysis of the code generated with C4CAM.", "mimetype": "text/plain", "start_char_idx": 33791, "end_char_idx": 34431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03b403a4-e9bf-4301-8ef6-ef0ab9e5b204": {"__data__": {"id_": "03b403a4-e9bf-4301-8ef6-ef0ab9e5b204", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85c869f0-a5d1-4785-a5b9-0cec9c18b045", "node_type": "1", "metadata": {}, "hash": "2ca60f28b34f3da1c25ad7a7faf64f40fd74ddbcb05b626a983ed2e4862eef21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a4f4cdb-c95c-4498-ab84-aacb1c5c9b4d", "node_type": "1", "metadata": {}, "hash": "36ae550737b8d59de1dc664213a040c9e360d46afe3bfa6ad0636faa6589bad5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5.1\nExperimental setup\n5.1.1\nSystem setup and technology parameters. For the\nCAM technology parameters, we consider the 2FeFET CAM\ndesign proposed in [51] at the 45 nm technology node. Energy\nand latency numbers for TCAM and MCAM operations were\nextracted from Eva-CAM [36], which is backed by experi-\nmental demonstrations of manufactured FeFET CAMs.", "mimetype": "text/plain", "start_char_idx": 34432, "end_char_idx": 34783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a4f4cdb-c95c-4498-ab84-aacb1c5c9b4d": {"__data__": {"id_": "6a4f4cdb-c95c-4498-ab84-aacb1c5c9b4d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03b403a4-e9bf-4301-8ef6-ef0ab9e5b204", "node_type": "1", "metadata": {}, "hash": "c640439cceb2e6b6110bc6d38f488d326c937a7547ed3cbeef91ab0f7dba5fee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "778be731-0acb-41b8-a061-bb1e22ddca5e", "node_type": "1", "metadata": {}, "hash": "c5d15de3296fbd5c43f4f51986626060e5a694437ee780899f3c00a5e02f8344", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Since\nwe are varying the array size for design space exploration, the\nsearch latency can vary from 860 ps to 7.5 ns for array sizes\nof 16 \u00d7 16 and 256 \u00d7 256, respectively. For the GPU results,\nwe use the NVIDIA RTX 3090 GPU (8 nm process) with a\nbase clock speed of 1395 MHz. The power consumption is\nmeasured using the NVIDIA System Management Interface\nnvidia-smi, and energy is derived thereof.\n5.1.2\nSimulation infrastructure.", "mimetype": "text/plain", "start_char_idx": 34784, "end_char_idx": 35214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "778be731-0acb-41b8-a061-bb1e22ddca5e": {"__data__": {"id_": "778be731-0acb-41b8-a061-bb1e22ddca5e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a4f4cdb-c95c-4498-ab84-aacb1c5c9b4d", "node_type": "1", "metadata": {}, "hash": "36ae550737b8d59de1dc664213a040c9e360d46afe3bfa6ad0636faa6589bad5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18ac7c6d-a6af-4c41-bfc0-bf41eeed3cd8", "node_type": "1", "metadata": {}, "hash": "c14fe03efc6f7545512f5c532bc527d77ae9d2d968a583b676bc83eedbe22024", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For evaluation, we use\nthe open-source CAM simulator CAMASim [33] and extend\nit with an interface to connect it to C4CAM. Table 1 lists our\nconfiguration parameters for all experiments in this section.\n170\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nFarzaneh et al.\nTable 1.", "mimetype": "text/plain", "start_char_idx": 35215, "end_char_idx": 35497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18ac7c6d-a6af-4c41-bfc0-bf41eeed3cd8": {"__data__": {"id_": "18ac7c6d-a6af-4c41-bfc0-bf41eeed3cd8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "778be731-0acb-41b8-a061-bb1e22ddca5e", "node_type": "1", "metadata": {}, "hash": "c5d15de3296fbd5c43f4f51986626060e5a694437ee780899f3c00a5e02f8344", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12d254d7-14aa-4063-8d3a-a6b4155d3750", "node_type": "1", "metadata": {}, "hash": "3a49f713311671ba6824d853897bb3f409c9d1f893042b2edc2fbe2ef795ac95", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Simulator configuration\nArchitecture & Circuit configuration\nType\nHDC\nKNN\nDNA\nHorizontal merge\nVoting\nVoting\nCounter\nVertical merge\nComparator\nComparator\nGather\nCell\nTCAM\nT/MCAM\nTCAM\nSensing circuit\nBE\nBE\nTH\nCost of additional circuits\nType\nLatency\nEnergy\nAdder\n0.25 ns\n1.3 fJ/bit\nRegister\n0.5 ns\n4.5 fJ/bit\nComparator\n0.25 ns\n0.4 fJ/bit\nDecoder/Encoder\n0.", "mimetype": "text/plain", "start_char_idx": 35498, "end_char_idx": 35854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12d254d7-14aa-4063-8d3a-a6b4155d3750": {"__data__": {"id_": "12d254d7-14aa-4063-8d3a-a6b4155d3750", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18ac7c6d-a6af-4c41-bfc0-bf41eeed3cd8", "node_type": "1", "metadata": {}, "hash": "c14fe03efc6f7545512f5c532bc527d77ae9d2d968a583b676bc83eedbe22024", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a172d1b-4b1b-492d-b96d-57e7810f3a6e", "node_type": "1", "metadata": {}, "hash": "fc9a2c4bf287da2bb443dd850892879bfcd9ea63a8cd088f8de41c0a117ba648", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "25 ns\n29 fJ\nThe simulator consists of two main modules: the Functional\nSimulator and the Performance Evaluator. The functional\nsimulator module performs quantization and mapping to\nthe subarrays of the simulated CAM. It also executes the\nsearch operation on each subarray and merges the partial\nresults. The performance evaluator estimates device specifics,\nincluding the size of peripheral circuits at each hierarchy level,\npredicts the circuit types and sizes depending on the merge\nscheme for estimating merging costs and adjusts buffer size\naccordingly.", "mimetype": "text/plain", "start_char_idx": 35854, "end_char_idx": 36411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a172d1b-4b1b-492d-b96d-57e7810f3a6e": {"__data__": {"id_": "4a172d1b-4b1b-492d-b96d-57e7810f3a6e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12d254d7-14aa-4063-8d3a-a6b4155d3750", "node_type": "1", "metadata": {}, "hash": "3a49f713311671ba6824d853897bb3f409c9d1f893042b2edc2fbe2ef795ac95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85b7c7f5-dff1-4eea-a6b4-7b5ad7493abc", "node_type": "1", "metadata": {}, "hash": "b8130364e23bf4713c99d7bad5a413f423683f623714cef10d4d34609640e4cc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The peripheral estimator manages the latency of\nmerge operations generated at the CAM abstraction through\nthe compiler flow. At the subarray level, CAMASim integrates\nexternal circuit-level CAM modeling tools like EvaCAM [36]\nor SPICE simulation results to generate performance values,\nensuring compatibility with various CAM cell designs. To\nhandle large data dimensions and entry sizes, the extended\nsimulator allows for fine-grain control of the hierarchy, and\nmodels CAM queries to obtain energy and latency based on\nreal hardware behavior.", "mimetype": "text/plain", "start_char_idx": 36412, "end_char_idx": 36956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85b7c7f5-dff1-4eea-a6b4-7b5ad7493abc": {"__data__": {"id_": "85b7c7f5-dff1-4eea-a6b4-7b5ad7493abc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a172d1b-4b1b-492d-b96d-57e7810f3a6e", "node_type": "1", "metadata": {}, "hash": "fc9a2c4bf287da2bb443dd850892879bfcd9ea63a8cd088f8de41c0a117ba648", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9929cc7-ed3d-421d-bf6b-0cfcfaa3041a", "node_type": "1", "metadata": {}, "hash": "5f6538b27d8a1857df675707c070199e45eebe293b5bba22bcfc905c86aa4c66", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5.2\nEvaluated applications\nIn a real-world scenario, the data transfer for chosen bench-\nmarks on an accelerator, like a GPU or CAM, would be\namortized in the long run. For that, unless specified, reported\nlatency and energy exclude preprocessing (e.g., hashing and\nquantization) and transfer of training data or reference genome,\nfocusing only on the query processing for all applications.\n5.2.1\nK-nearest neighbors. KNN is a popular classification,\nregression, and anomaly detection algorithm.", "mimetype": "text/plain", "start_char_idx": 36957, "end_char_idx": 37452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9929cc7-ed3d-421d-bf6b-0cfcfaa3041a": {"__data__": {"id_": "c9929cc7-ed3d-421d-bf6b-0cfcfaa3041a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85b7c7f5-dff1-4eea-a6b4-7b5ad7493abc", "node_type": "1", "metadata": {}, "hash": "b8130364e23bf4713c99d7bad5a413f423683f623714cef10d4d34609640e4cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37427d17-0b90-47e1-96db-dac275939524", "node_type": "1", "metadata": {}, "hash": "0c46f545c13d0370e105430f258772e5b6d684a8b60770049885df8abb473a33", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "It identifies the\n\ud835\udc3eclosest training examples in the feature space to a given test\nsample. It is especially interesting because of its versatility\nand explainability, with no training required. KNNs are both\nmemory and computationally-intensive, making their scalabil-\nity and performance strongly limited on conventional systems.\nTwo versions of KNN were implemented, utilizing cosine\nsimilarity with matmul and Euclidean distance with norm,\nboth followed by a topk operation.", "mimetype": "text/plain", "start_char_idx": 37453, "end_char_idx": 37929, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37427d17-0b90-47e1-96db-dac275939524": {"__data__": {"id_": "37427d17-0b90-47e1-96db-dac275939524", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9929cc7-ed3d-421d-bf6b-0cfcfaa3041a", "node_type": "1", "metadata": {}, "hash": "5f6538b27d8a1857df675707c070199e45eebe293b5bba22bcfc905c86aa4c66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1379cd16-9541-4653-93bb-b076d7d15e11", "node_type": "1", "metadata": {}, "hash": "a361232d8ce14c21493518585a901373286b1a973c820fe490d8b4fcb6368508", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For a direct comparison\nwith [24] in Section 5.3, we classify the top 4 most popular\ndatasets: Iris, Wine, Breast Cancer, and Wine Quality [1],\nusing an 80/20 training/test split on them. We also evaluated\nKNN on chest X-Ray images from the Pneumonia dataset [2]\nin Section 5.6, Table 2.\n5.2.2\nHyperdimensional Computing. HDC is a framework\ninspired by the human brain\u2019s ability to process information.", "mimetype": "text/plain", "start_char_idx": 37930, "end_char_idx": 38332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1379cd16-9541-4653-93bb-b076d7d15e11": {"__data__": {"id_": "1379cd16-9541-4653-93bb-b076d7d15e11", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37427d17-0b90-47e1-96db-dac275939524", "node_type": "1", "metadata": {}, "hash": "0c46f545c13d0370e105430f258772e5b6d684a8b60770049885df8abb473a33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c935663-660d-4652-b0d5-49ad7c2ab4ac", "node_type": "1", "metadata": {}, "hash": "bfc3c4056c655a91736b7f79f207881bef7e8b861a772095c23bf42f8ec2d129", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "It utilizes high-dimensional vectors known as hypervectors\nas a fundamental building block. Hypervectors are large\nbinary vectors with thousands of dimensions. We evaluated\nbinary and multi-bit HDC models with 8k dimensions on the\nMNIST dataset following the approach outlined in [52]. The\nTorchScript implementation of the main kernel employs the\nsame operations shown in Figure 4a.\n5.2.3\nDNA read mapping.", "mimetype": "text/plain", "start_char_idx": 38333, "end_char_idx": 38740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c935663-660d-4652-b0d5-49ad7c2ab4ac": {"__data__": {"id_": "9c935663-660d-4652-b0d5-49ad7c2ab4ac", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1379cd16-9541-4653-93bb-b076d7d15e11", "node_type": "1", "metadata": {}, "hash": "a361232d8ce14c21493518585a901373286b1a973c820fe490d8b4fcb6368508", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9db3837b-faf5-46fc-b985-cdb443f76f25", "node_type": "1", "metadata": {}, "hash": "63b224ebbfa44cbf3424a29b811ccf5d8270a235803399e44d863ba9e7b2f07f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Read mapping is a fundamental\nstep in genome analysis, aiming to identify the origin of each\nread \ud835\udc45from an input genome in a reference genome \ud835\udc3a. This\ntask is computationally challenging due to the very large\ngenome sizes and permissible minor differences in the form\nof insertions, deletions, and substitutions. One algorithm used\nfor this purpose is known as the fast seed-and-vote algorithm\n(FVSA) [35]. This approach performs a read-by-read compar-\nison with the reference genome and counts matching seeds\n(smaller substrings in a read).", "mimetype": "text/plain", "start_char_idx": 38741, "end_char_idx": 39281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9db3837b-faf5-46fc-b985-cdb443f76f25": {"__data__": {"id_": "9db3837b-faf5-46fc-b985-cdb443f76f25", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c935663-660d-4652-b0d5-49ad7c2ab4ac", "node_type": "1", "metadata": {}, "hash": "bfc3c4056c655a91736b7f79f207881bef7e8b861a772095c23bf42f8ec2d129", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f4aa01c-f415-4fda-a8c9-bb332a4b6c1d", "node_type": "1", "metadata": {}, "hash": "65eb30dcb2e9ef6e73e477456462e21a13269a9a1be430a4fbc122fa241d65b2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The block with the highest vote\ncount in the reference genome is selected as the most likely\nlocation of a read \ud835\udc45in the reference genome \ud835\udc3a. Unlike KNN\nand HDC, which is based on one-shot similarity search, read\nmapping finds high-similarity sections based on the Hamming\ndistance of overlap positions of reads against \ud835\udc3a. Hamming\ndistance is often used in prior CAM-based designs for read\nmapping [22, 27, 29].", "mimetype": "text/plain", "start_char_idx": 39282, "end_char_idx": 39691, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f4aa01c-f415-4fda-a8c9-bb332a4b6c1d": {"__data__": {"id_": "0f4aa01c-f415-4fda-a8c9-bb332a4b6c1d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9db3837b-faf5-46fc-b985-cdb443f76f25", "node_type": "1", "metadata": {}, "hash": "63b224ebbfa44cbf3424a29b811ccf5d8270a235803399e44d863ba9e7b2f07f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f154dde-95ef-4afe-b861-10abfd744e38", "node_type": "1", "metadata": {}, "hash": "058d81e1491c45fd133c646578aab0707671d7c5854a6570918c66b765e39f87", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We implemented the FVSA algorithm in TorchScript,\ndrawing inspiration from SaVI [29] (seed-and-vote-based\nin-memory accelerator), where the seed-reference lookup\nand voting steps are solely performed by TCAM subarrays\nand shift-register counters, respectively. We encode base\npairs using 4-bit encoding as in [29], which allows for rep-\nresenting low-quality bases (N), transversions, and transi-\ntions.", "mimetype": "text/plain", "start_char_idx": 39692, "end_char_idx": 40095, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f154dde-95ef-4afe-b861-10abfd744e38": {"__data__": {"id_": "5f154dde-95ef-4afe-b861-10abfd744e38", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f4aa01c-f415-4fda-a8c9-bb332a4b6c1d", "node_type": "1", "metadata": {}, "hash": "65eb30dcb2e9ef6e73e477456462e21a13269a9a1be430a4fbc122fa241d65b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "327e205d-63cc-452f-9378-810c133eeabd", "node_type": "1", "metadata": {}, "hash": "d4d28bb3195d8949105329ffd75f6da90d4d3f9fb0414ace69e82b1e671c1a71", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Figure 7 illustrates the seed-reference lookup based on\nthe Hamming distance between the reference genome and\nseeds of a read sequence implemented at the cim abstrac-\ntion. The Hamming distance of two words, A and B, is\ncalculated as A xor B (line 11) followed by a population\ncount (line 14). The next step in FVSA involves counting\nvotes based on the location of matched seeds in the refer-\nence genome through a threshold operation. This threshold\naccounts for partial matches, allowing for substitutions in\nthe genome sequence.", "mimetype": "text/plain", "start_char_idx": 40096, "end_char_idx": 40627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "327e205d-63cc-452f-9378-810c133eeabd": {"__data__": {"id_": "327e205d-63cc-452f-9378-810c133eeabd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f154dde-95ef-4afe-b861-10abfd744e38", "node_type": "1", "metadata": {}, "hash": "058d81e1491c45fd133c646578aab0707671d7c5854a6570918c66b765e39f87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1701c0e8-06f6-4410-9cb4-ec66139cfacf", "node_type": "1", "metadata": {}, "hash": "0812ef5dcfc11cbb4661836fcd6c94e7a35f8930dd099202e0e681a27a152c4b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The below_threshold operation is\nachieved through a cim.cmp (compare) operation with lt\n(less than) argument (line 16), followed by a nonzero op-\neration (line 18) to return the indices of the matched rows.\nWe evaluated the FVSA use-case using the publicly available\nreal human genome GRCh38 build (3 GB) [38] and the read\nsequence NA12878 (59 GB). This allowed us to stress-test\nour compilation framework and mapping algorithm for CAM\narrays.", "mimetype": "text/plain", "start_char_idx": 40628, "end_char_idx": 41071, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1701c0e8-06f6-4410-9cb4-ec66139cfacf": {"__data__": {"id_": "1701c0e8-06f6-4410-9cb4-ec66139cfacf", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "327e205d-63cc-452f-9378-810c133eeabd", "node_type": "1", "metadata": {}, "hash": "d4d28bb3195d8949105329ffd75f6da90d4d3f9fb0414ace69e82b1e671c1a71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e333101-0b0a-418d-a9f2-ee9546ef71af", "node_type": "1", "metadata": {}, "hash": "42956dbffa99191901e4d0a841a11c5b696921ed6d5dd8f0abc06937c38f16a7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We evaluated the accuracy, throughput and energy\n171\nC4CAM: A Compiler for CAM-based In-memory Accelerators\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\n1\n...\n2\nscf.for %arg1 = %c0 to %read_count step %c1 {\n3\n...\n4\n// 1.", "mimetype": "text/plain", "start_char_idx": 41072, "end_char_idx": 41299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e333101-0b0a-418d-a9f2-ee9546ef71af": {"__data__": {"id_": "8e333101-0b0a-418d-a9f2-ee9546ef71af", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1701c0e8-06f6-4410-9cb4-ec66139cfacf", "node_type": "1", "metadata": {}, "hash": "0812ef5dcfc11cbb4661836fcd6c94e7a35f8930dd099202e0e681a27a152c4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95f9778e-5e99-404d-be88-5fc8e59e695d", "node_type": "1", "metadata": {}, "hash": "c6a7bd92a2f890b95d3b1ae040fa6e600278fa13b639a53c3e7009ddf74bb4d4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Seed generation\n5\n%seeds = func.call @seed_generation(%read, %cst16) :\n6\n(tensor <70xi8>, i64) -> tensor <1x110x16xi8 >\n7\n...\n8\n// 2. Seed-reference lookup\n9\n%0 = cim.acquire : index\n10\n%1 = cim.execute(%0, %ref_genome , %seeds , %cst3) ({\n11\n%2 = cim.xor %ref_genome ,", "mimetype": "text/plain", "start_char_idx": 41300, "end_char_idx": 41569, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95f9778e-5e99-404d-be88-5fc8e59e695d": {"__data__": {"id_": "95f9778e-5e99-404d-be88-5fc8e59e695d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e333101-0b0a-418d-a9f2-ee9546ef71af", "node_type": "1", "metadata": {}, "hash": "42956dbffa99191901e4d0a841a11c5b696921ed6d5dd8f0abc06937c38f16a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "277d09d8-91ee-47d8-af25-8d2f3c892b86", "node_type": "1", "metadata": {}, "hash": "97fa50f83f73f22afabc1383f252f5f80d1383f4f5ecc7bad27d4acc1db862a0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "%seeds :\n12\ntensor <8192x1x16xi8 >, tensor <1x110x16xi8 > ->\n13\ntensor <8192x110x16xi8 >\n14\n%3 = cim.popcount %2 : tensor <8192x110x16xi8 >,\n15\n-> tensor <8192x110xi64 >\n16\n%4 = cim.cmp lt %3, %cst3 : tensor <8192x110xi64 >,", "mimetype": "text/plain", "start_char_idx": 41570, "end_char_idx": 41794, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "277d09d8-91ee-47d8-af25-8d2f3c892b86": {"__data__": {"id_": "277d09d8-91ee-47d8-af25-8d2f3c892b86", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95f9778e-5e99-404d-be88-5fc8e59e695d", "node_type": "1", "metadata": {}, "hash": "c6a7bd92a2f890b95d3b1ae040fa6e600278fa13b639a53c3e7009ddf74bb4d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77ddb39f-f747-4ca6-98c2-7d1e8c2f4853", "node_type": "1", "metadata": {}, "hash": "b1e7540345c6f411e59c00e14e905b18179dfe42574899e506b312041635e417", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "17\ni64 -> tensor <8192x110xi1 >\n18\n%5 = cim.nonzero %4 : tensor <8192x110xi1 >,\n19\n-> tensor <?x?xi64>\n20\ncim.yield %5: tensor <?x?xi64>\n21\n}) : (index, tensor <8192x1x16xi8 >,\n22\ntensor <1x110x16xi8 >, i64) -> (tensor <?x?xi64 >)\n23\n// 3.", "mimetype": "text/plain", "start_char_idx": 41795, "end_char_idx": 42034, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77ddb39f-f747-4ca6-98c2-7d1e8c2f4853": {"__data__": {"id_": "77ddb39f-f747-4ca6-98c2-7d1e8c2f4853", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "277d09d8-91ee-47d8-af25-8d2f3c892b86", "node_type": "1", "metadata": {}, "hash": "97fa50f83f73f22afabc1383f252f5f80d1383f4f5ecc7bad27d4acc1db862a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa1f3b8b-9337-4e1b-99e2-de02e4d2e126", "node_type": "1", "metadata": {}, "hash": "23dfa616d6d4ee7eebcbba0f47ca3aa701af552d5fec8462492db94acdad1d41", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Voting\n24\n%6 = func.call @voting (%1) : (tensor <?x?xi64>)\n25\n-> tensor <8192xi64>\n26\n...} ...\nFigure 7. Seed-reference lookup in FVSA\nefficiency across three array sizes and compared them to\nBWA-MEM implementation on GPUs [42], both with read\nsequences consisting of 100-bp reads.", "mimetype": "text/plain", "start_char_idx": 42035, "end_char_idx": 42316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa1f3b8b-9337-4e1b-99e2-de02e4d2e126": {"__data__": {"id_": "fa1f3b8b-9337-4e1b-99e2-de02e4d2e126", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77ddb39f-f747-4ca6-98c2-7d1e8c2f4853", "node_type": "1", "metadata": {}, "hash": "b1e7540345c6f411e59c00e14e905b18179dfe42574899e506b312041635e417", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e2b87b3-85c1-40e8-a2d8-e69968573b61", "node_type": "1", "metadata": {}, "hash": "8a3f94952b817679e69f31775e711b9ea75661cf5613d782dda93da3e191f16b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5.3\nValidation\nTo validate the C4CAM framework, we use the CAM-design\nand hand-optimized designs that for the HDC use case [23]\nand the DNA read mapping use case [29].\nWe use the manual designs from [23] as baseline for the\nHDC use case. We generate code for binary and multi-bit\nimplementations of HDC for different CAM architectures, i.e.,\nwith array sizes of 32 \u00d7\ud835\udc36where \ud835\udc36is varied to 16, 32, 64, and\n128.", "mimetype": "text/plain", "start_char_idx": 42317, "end_char_idx": 42724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e2b87b3-85c1-40e8-a2d8-e69968573b61": {"__data__": {"id_": "4e2b87b3-85c1-40e8-a2d8-e69968573b61", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa1f3b8b-9337-4e1b-99e2-de02e4d2e126", "node_type": "1", "metadata": {}, "hash": "23dfa616d6d4ee7eebcbba0f47ca3aa701af552d5fec8462492db94acdad1d41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5728b055-b0f6-4f84-a469-d959e5aca37e", "node_type": "1", "metadata": {}, "hash": "f57d33b6c1484e08d27445b0d630a3228d75613d891db8878a621484386d8a52", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The validation results for accuracy, latency and energy\nare shown in Figure 8a, Figure 8b, and Figure 8c respectively.\nFor a fair comparison, we use the same system configuration\nas in the baseline, i.e., four mats per bank, four arrays per mat,\neight sub-arrays per array, and as many banks as needed to\nstore the whole dataset.", "mimetype": "text/plain", "start_char_idx": 42725, "end_char_idx": 43054, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5728b055-b0f6-4f84-a469-d959e5aca37e": {"__data__": {"id_": "5728b055-b0f6-4f84-a469-d959e5aca37e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e2b87b3-85c1-40e8-a2d8-e69968573b61", "node_type": "1", "metadata": {}, "hash": "8a3f94952b817679e69f31775e711b9ea75661cf5613d782dda93da3e191f16b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6abbc3fa-b2f1-4845-a8e6-f04b90d662d8", "node_type": "1", "metadata": {}, "hash": "575c85596f4d853ea71d59f53d378aee570d938a67c63e941f45f2d78ba4e44d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In this experiment, the observed deviation in the latency\nand the energy consumption is, on average (geomean), 0.9%\nand 5.5%, respectively (notice that the y-axes do not start\nat 0 for better visualization). These small deviations can be\nattributed to slight differences in the versions of the simulation\nenvironment rather than to fundamental differences in the\nimplementations.", "mimetype": "text/plain", "start_char_idx": 43055, "end_char_idx": 43434, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6abbc3fa-b2f1-4845-a8e6-f04b90d662d8": {"__data__": {"id_": "6abbc3fa-b2f1-4845-a8e6-f04b90d662d8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5728b055-b0f6-4f84-a469-d959e5aca37e", "node_type": "1", "metadata": {}, "hash": "f57d33b6c1484e08d27445b0d630a3228d75613d891db8878a621484386d8a52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba128f38-243f-4a38-9819-b78ffffb84a6", "node_type": "1", "metadata": {}, "hash": "31926278b8b79f3ae9cb003540544d3e2a6da2f9bb1cecb7e7931ad8abc9f04e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Except for the 32-column design in the\n1-bit implementation and the 128-column design in the 2-bit\nimplementation, where the generated code shows a 1% higher\naccuracy, the accuracy is equal to the baseline. Hence, C4CAM\neffectively matches the quality of hand-tuned implementations\nby expert CAM designers.\nTo understand the latency results in Figure 8b, it is important\nto note that all search operations happen in parallel, and the\nmatch line (ML) discharges more slowly for larger columns.", "mimetype": "text/plain", "start_char_idx": 43435, "end_char_idx": 43927, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba128f38-243f-4a38-9819-b78ffffb84a6": {"__data__": {"id_": "ba128f38-243f-4a38-9819-b78ffffb84a6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6abbc3fa-b2f1-4845-a8e6-f04b90d662d8", "node_type": "1", "metadata": {}, "hash": "575c85596f4d853ea71d59f53d378aee570d938a67c63e941f45f2d78ba4e44d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "991221b8-a930-4f83-9aa1-9fa1ed038555", "node_type": "1", "metadata": {}, "hash": "580471ed713219a07b52f2d2a50c515074621dfef9a86a8f56a9be0b368d5985", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As for the energy numbers shown in Figure 8c, larger \ud835\udc36\nleads to lower energy consumption because fewer peripherals\nand fewer levels (arrays, mats, and banks) are required as \ud835\udc36\nincreases. Moreover, as observed in [23], we corroborate that\n1-bit implementations are more energy efficient than multi-bit\nones. This improvement is associated with the higher ML and\ndata line voltages of the multi-bit implementations.", "mimetype": "text/plain", "start_char_idx": 43928, "end_char_idx": 44341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "991221b8-a930-4f83-9aa1-9fa1ed038555": {"__data__": {"id_": "991221b8-a930-4f83-9aa1-9fa1ed038555", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba128f38-243f-4a38-9819-b78ffffb84a6", "node_type": "1", "metadata": {}, "hash": "31926278b8b79f3ae9cb003540544d3e2a6da2f9bb1cecb7e7931ad8abc9f04e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30030db8-9b2a-4ec7-9083-c3c773c89e2a", "node_type": "1", "metadata": {}, "hash": "be59c78ad2b1f49c9fd4a6a68fce3af42f1b86cea34b171a62ae6b512ef18865", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Figure 9 shows the versatility of C4CAM to support a\nfundamentally different and more complex algorithm, namely\nthe FVSA inspired by [29]. The figure shows the accuracy,\nperformance (as throughput in millions of reads per second)\nand energy efficiency (throughput per Watt) for both manual\ndesign and C4CAM-generated code, applied to 100-bp reads\non the human genome [38] across three array sizes. These units\nare commonly used to compare the efficiency of accelerators\ndue to the streaming nature of the read-mapping execution.", "mimetype": "text/plain", "start_char_idx": 44342, "end_char_idx": 44870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30030db8-9b2a-4ec7-9083-c3c773c89e2a": {"__data__": {"id_": "30030db8-9b2a-4ec7-9083-c3c773c89e2a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "991221b8-a930-4f83-9aa1-9fa1ed038555", "node_type": "1", "metadata": {}, "hash": "580471ed713219a07b52f2d2a50c515074621dfef9a86a8f56a9be0b368d5985", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9ce3809-fe35-47af-b1f9-6e6b1e45b34c", "node_type": "1", "metadata": {}, "hash": "d3c922782136a015c7e356b202696366696c8ef3821d68e7499a19c325cb1275", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Compared to the manual design, the C4CAM-generated codes\nachieve the same baseline accuracy for all setups. Regarding\nperformance and energy estimation, our results vary by 7.8%\nand 6% on average for throughput and throughput per Watt,\nrespectively. This variation might be attributed to minor\narchitectural differences not presented in [29]. This example\nillustrates C4CAM\u2019s capability to handle the parallel operation\nof a substantial array workload totaling 1.5 GB of the encoded\nhuman reference genome.", "mimetype": "text/plain", "start_char_idx": 44871, "end_char_idx": 45377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9ce3809-fe35-47af-b1f9-6e6b1e45b34c": {"__data__": {"id_": "d9ce3809-fe35-47af-b1f9-6e6b1e45b34c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30030db8-9b2a-4ec7-9083-c3c773c89e2a", "node_type": "1", "metadata": {}, "hash": "be59c78ad2b1f49c9fd4a6a68fce3af42f1b86cea34b171a62ae6b512ef18865", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7903e73b-3135-4461-8842-c7bcab8d5fd0", "node_type": "1", "metadata": {}, "hash": "7e8e6dc5ac6fe9022281d7aa824e9837aa0789a9fe2c3af19a990bbb9be86877", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Specifically, this dataset requires\nfrom 805K to 12.8M subarrays in the 128 \u00d7 128 and 32 \u00d7\n32 setups, respectively. Despite also managing a significant\nvolume of encoded queries (29 GB of reads), the compiler\ndoes not significantly impede the processing speed per read.\nThis is because the bottleneck of the FVSA lies in the seed-\nreference lookup, and the problem size is determined by the\nsize of the reference genome.", "mimetype": "text/plain", "start_char_idx": 45378, "end_char_idx": 45798, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7903e73b-3135-4461-8842-c7bcab8d5fd0": {"__data__": {"id_": "7903e73b-3135-4461-8842-c7bcab8d5fd0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9ce3809-fe35-47af-b1f9-6e6b1e45b34c", "node_type": "1", "metadata": {}, "hash": "d3c922782136a015c7e356b202696366696c8ef3821d68e7499a19c325cb1275", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58c10eff-f1ca-4aae-a78d-2b3492de8699", "node_type": "1", "metadata": {}, "hash": "302bc5bab8ea32b5daf52594d54c6587518b5b3054bd8b7733f90e18e88fca80", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5.4\nImpact of CAM cell precision\nDue to limited CAM cell precision, the data entries and queries\nof applications must be encoded, quantized or hashed to be\nsuitable for CAM search. The results for the HDC execution, as\nillustrated in Figure 8a, show that more bits per cell consistently\nimprove accuracy. On the other hand, as depicted in Figure 8b,\n2-bit and 3-bit CAMs have similar search latency, which is,\non average, 12% higher than the 1-bit setting across various\ncolumn counts.", "mimetype": "text/plain", "start_char_idx": 45799, "end_char_idx": 46284, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58c10eff-f1ca-4aae-a78d-2b3492de8699": {"__data__": {"id_": "58c10eff-f1ca-4aae-a78d-2b3492de8699", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7903e73b-3135-4461-8842-c7bcab8d5fd0", "node_type": "1", "metadata": {}, "hash": "7e8e6dc5ac6fe9022281d7aa824e9837aa0789a9fe2c3af19a990bbb9be86877", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bee446f-5cc1-4f67-95f7-d866d7cbd711", "node_type": "1", "metadata": {}, "hash": "7c964776a5434c1cb1f7d2ae83de4c155df0b8f3abb4474675cc54ec4076ead5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This latency increase is attributed to the\ngreater complexity of the sensing circuit [23], and its impact\nis more pronounced in energy consumption. Across different\ncolumn sizes, employing the 3-bit configuration, on average,\nresults in a 4.5% and 29.1% increase in energy consumption in\ncomparison to 2-bit and 1-bit configurations.\nIncreasing the number of bits per cell per feature would\nintuitively improve the accuracy of other applications that use\nthe same distance metric.", "mimetype": "text/plain", "start_char_idx": 46285, "end_char_idx": 46765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4bee446f-5cc1-4f67-95f7-d866d7cbd711": {"__data__": {"id_": "4bee446f-5cc1-4f67-95f7-d866d7cbd711", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58c10eff-f1ca-4aae-a78d-2b3492de8699", "node_type": "1", "metadata": {}, "hash": "302bc5bab8ea32b5daf52594d54c6587518b5b3054bd8b7733f90e18e88fca80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f56c20b0-5491-4d65-9d23-3fc8eec4afd6", "node_type": "1", "metadata": {}, "hash": "485020e912aa550da506083fee18d45d71f08c30626cb391449fb2f0b28cf9d8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In KNN, the real-valued features\nof the query and memory entries can be simply quantized\nto match the same bit precision as the multi-bit CAM. Fig-\nures 10a, 10b, and 10c report the accuracy, latency, and energy\n172\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nFarzaneh et al.", "mimetype": "text/plain", "start_char_idx": 46766, "end_char_idx": 47049, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f56c20b0-5491-4d65-9d23-3fc8eec4afd6": {"__data__": {"id_": "f56c20b0-5491-4d65-9d23-3fc8eec4afd6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bee446f-5cc1-4f67-95f7-d866d7cbd711", "node_type": "1", "metadata": {}, "hash": "7c964776a5434c1cb1f7d2ae83de4c155df0b8f3abb4474675cc54ec4076ead5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e046c9ff-e1a8-4e85-a50f-d0607036e8ca", "node_type": "1", "metadata": {}, "hash": "3166ef34defd4ed427ec844558c4143dfbc1e753cab2a45132d9fab2fa4bba52", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "16\n32\n64\n128\n80\n85\n90\n95\n100\n# of columns\nAccuracy (%)\nC4CAM-1b\nManual-1b\nC4CAM-2b\nManual-2b\nC4CAM-3b\nManual-3b\n(a) Accuracy\n16\n32\n64\n128\n6\n8\n10\n12\n14\n# of columns\nLatency (ns)\n(b) Latency\n16\n32\n64\n128\n0\n0.2\n0.4\n0.", "mimetype": "text/plain", "start_char_idx": 47050, "end_char_idx": 47264, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e046c9ff-e1a8-4e85-a50f-d0607036e8ca": {"__data__": {"id_": "e046c9ff-e1a8-4e85-a50f-d0607036e8ca", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f56c20b0-5491-4d65-9d23-3fc8eec4afd6", "node_type": "1", "metadata": {}, "hash": "485020e912aa550da506083fee18d45d71f08c30626cb391449fb2f0b28cf9d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f29e913-0b92-4c8a-83dc-2f98d4f314bc", "node_type": "1", "metadata": {}, "hash": "d869c7b98892a2950cbd51224abf0c336a955b994dff816acafcf400be8c79a6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "6\n# of columns\nEnergy\nconsumption (nJ)\n(c) Energy consumption\nFigure 8.", "mimetype": "text/plain", "start_char_idx": 47264, "end_char_idx": 47335, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f29e913-0b92-4c8a-83dc-2f98d4f314bc": {"__data__": {"id_": "8f29e913-0b92-4c8a-83dc-2f98d4f314bc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e046c9ff-e1a8-4e85-a50f-d0607036e8ca", "node_type": "1", "metadata": {}, "hash": "3166ef34defd4ed427ec844558c4143dfbc1e753cab2a45132d9fab2fa4bba52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "282f896e-0556-4794-9cfb-6d2e0e8a601e", "node_type": "1", "metadata": {}, "hash": "75bc30256559dd1989df4c1835e7f62710f4501637edb44ef52c5ff6a38cb646", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Validation results per query against manual designs [23] for HDC\n32\n64\n128\n80\n85\n90\n95\n100\n# of columns\nAccuracy (%)\nC4CAM\nManual\nBWA-GPU\n(a) Accuracy over BWA-MEM\n32\n64\n128\n0\n10\n20\n30\n# of columns\nMreads/s\n(b) Throughput\n32\n64\n128\n0\n20\n40\n# of columns\nkreads/s/W\n(c) Throughput/Watt\nFigure 9.", "mimetype": "text/plain", "start_char_idx": 47336, "end_char_idx": 47629, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "282f896e-0556-4794-9cfb-6d2e0e8a601e": {"__data__": {"id_": "282f896e-0556-4794-9cfb-6d2e0e8a601e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f29e913-0b92-4c8a-83dc-2f98d4f314bc", "node_type": "1", "metadata": {}, "hash": "d869c7b98892a2950cbd51224abf0c336a955b994dff816acafcf400be8c79a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91e0412f-bfd0-44f4-887d-bdd9907d1ca4", "node_type": "1", "metadata": {}, "hash": "afbe1f8c4c2d256a43161e16b648ccff5c2a37bd705a2eb9c9a9346c8b851f70", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Comparison of accuracy, throughput and throughput per Watt against manual designs [29] and GPU implementation [42]\nfor read mapping\nconsumption of CAM designs with various bit precision set-\ntings and different GPU implementations executing KNN on\ndifferent datasets. A similar pattern to HDC is observed in\nKNN, where a 3-bit configuration yields, on average, 6.4%\nand 10.3% more accurate results compared to 2-bit and 1-bit\ndesigns, respectively.", "mimetype": "text/plain", "start_char_idx": 47630, "end_char_idx": 48078, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91e0412f-bfd0-44f4-887d-bdd9907d1ca4": {"__data__": {"id_": "91e0412f-bfd0-44f4-887d-bdd9907d1ca4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "282f896e-0556-4794-9cfb-6d2e0e8a601e", "node_type": "1", "metadata": {}, "hash": "75bc30256559dd1989df4c1835e7f62710f4501637edb44ef52c5ff6a38cb646", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d506d3b-eec9-4854-b9df-c579b2713bea", "node_type": "1", "metadata": {}, "hash": "94f9282c5506b6e81eddbb0e8e6008e6d7efe61a874c465575e6a2a61a70555d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Similarly, the 3-bit configuration also\nincurs an average of 2% increase in the energy compared to the\n2-bit configuration. However, with binary CAMs, real-valued\nquery and memory entries must undergo a transformation\nusing a Locality-sensitive Hashing (LSH) algorithm [7]. In\nthis implementation, a CAM array stores the LSH signatures\nof the memory entries and computes the Hamming distance of\nthe LSH signature of the query.", "mimetype": "text/plain", "start_char_idx": 48079, "end_char_idx": 48505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d506d3b-eec9-4854-b9df-c579b2713bea": {"__data__": {"id_": "1d506d3b-eec9-4854-b9df-c579b2713bea", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91e0412f-bfd0-44f4-887d-bdd9907d1ca4", "node_type": "1", "metadata": {}, "hash": "afbe1f8c4c2d256a43161e16b648ccff5c2a37bd705a2eb9c9a9346c8b851f70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6ad5f65-0ecf-431a-b370-654f9037d95f", "node_type": "1", "metadata": {}, "hash": "4056722aa8c311b2bc52c4aa215520cc4bd2da82dfe51b55cfce7e17a3ea5f5f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The hashing step for queries\nis done using the GPU and results in a 7.3\u00d7 and \u223c12, 000\u00d7\nincrease, on average, in latency and energy consumption,\nrespectively, compared to the 3-bit configuration. Although\nthe binary CAM+LSH approach can be more energy and\ntime-efficient than the GPU-only approach, especially in han-\ndling larger and high-dimensional datasets, LSH sacrifices on\naccuracy.", "mimetype": "text/plain", "start_char_idx": 48506, "end_char_idx": 48894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6ad5f65-0ecf-431a-b370-654f9037d95f": {"__data__": {"id_": "b6ad5f65-0ecf-431a-b370-654f9037d95f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d506d3b-eec9-4854-b9df-c579b2713bea", "node_type": "1", "metadata": {}, "hash": "94f9282c5506b6e81eddbb0e8e6008e6d7efe61a874c465575e6a2a61a70555d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35c6cbef-2e0e-47d8-b81e-a4dcea1502be", "node_type": "1", "metadata": {}, "hash": "ca836e38b21adb092dbab6088607f4f06110b1a8916dd414dd6cd85672b9557c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5.5\nComparison to GPU\nDue to their parallel processing capabilities, optimized archi-\ntecture for matrix operations and high memory bandwidth\nand throughput, GPUs are often used in machine learning and\nsimilarity search domains. To compare CAM-based systems to\nGPUs, we also include results from CUDA implementations\nof cosine similarity and Euclidean norm in Figure 10.", "mimetype": "text/plain", "start_char_idx": 48895, "end_char_idx": 49265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35c6cbef-2e0e-47d8-b81e-a4dcea1502be": {"__data__": {"id_": "35c6cbef-2e0e-47d8-b81e-a4dcea1502be", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6ad5f65-0ecf-431a-b370-654f9037d95f", "node_type": "1", "metadata": {}, "hash": "4056722aa8c311b2bc52c4aa215520cc4bd2da82dfe51b55cfce7e17a3ea5f5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd6c2fe7-94f1-4704-8d0e-0bef046b3a5b", "node_type": "1", "metadata": {}, "hash": "9ec69a274018a743af978f907d5da2129af252f1f8165b8b0e6a82260f3abe8a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "While\nit is known that cosine similarity performs better with higher\ndimensions than Euclidean distance, we demonstrate that 3-bit\nCAMs outperform GPU-based cosine similarity, operating\n\u223c14\u00d7 faster with \u223c13, 700\u00d7 less energy consumption. The\ndatasets used in Figure 10 have low dimensionality which\nleads to resource under-utilization in the GPU. The larger\nPneumonia dataset provides a better perspective since these\nexperiments can exhaust the computational power of the GPU.", "mimetype": "text/plain", "start_char_idx": 49266, "end_char_idx": 49743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd6c2fe7-94f1-4704-8d0e-0bef046b3a5b": {"__data__": {"id_": "bd6c2fe7-94f1-4704-8d0e-0bef046b3a5b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35c6cbef-2e0e-47d8-b81e-a4dcea1502be", "node_type": "1", "metadata": {}, "hash": "ca836e38b21adb092dbab6088607f4f06110b1a8916dd414dd6cd85672b9557c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "496676d1-ff3c-4d88-b62e-01a8896b1869", "node_type": "1", "metadata": {}, "hash": "b379ca42075085a64972bddec1ce70dde5c1d0f138753e60fbcd877221be29c8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The results for the 3-bit CAM-based system on this large\nKNN classification are shown in Table 2 (under cam-base).\nAs can be seen, the power consumption of the CAM-based\nimplementation is considerably lower than that of the GPU\n(around 200 W).\nWe run the HDC implementation from [52] on the GPU,\nwhich achieves a 95.4% accuracy rate. On average, it takes 5\u00d7\nlonger and consumes 14, 900\u00d7 more energy compared to the\nimplementations generated by C4CAM.", "mimetype": "text/plain", "start_char_idx": 49744, "end_char_idx": 50194, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "496676d1-ff3c-4d88-b62e-01a8896b1869": {"__data__": {"id_": "496676d1-ff3c-4d88-b62e-01a8896b1869", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd6c2fe7-94f1-4704-8d0e-0bef046b3a5b", "node_type": "1", "metadata": {}, "hash": "9ec69a274018a743af978f907d5da2129af252f1f8165b8b0e6a82260f3abe8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05aa52b2-6751-4540-84e0-4a91056ececc", "node_type": "1", "metadata": {}, "hash": "5a6ef60676b59f651ca0a3f267bfc92cf608d85c0821d09174abc474cc4e32dc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Corroborating with\nprior works [19, 23, 24], CAM systems can outperform GPU\nenergy by orders of magnitude while still being significantly\nfaster. Note that the experiments on GPU and CAMs measure\n173\nC4CAM: A Compiler for CAM-based In-memory Accelerators\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA,", "mimetype": "text/plain", "start_char_idx": 50195, "end_char_idx": 50497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05aa52b2-6751-4540-84e0-4a91056ececc": {"__data__": {"id_": "05aa52b2-6751-4540-84e0-4a91056ececc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "496676d1-ff3c-4d88-b62e-01a8896b1869", "node_type": "1", "metadata": {}, "hash": "b379ca42075085a64972bddec1ce70dde5c1d0f138753e60fbcd877221be29c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56fed4cc-c350-42c6-a812-e759579f7677", "node_type": "1", "metadata": {}, "hash": "1509d0fa78698af7abe9703b0279846e952a04e81ac611702d99ec86d5c05da3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "USA\nI\nW\nC\nWQ\n40\n60\n80\n100\nDataset\nAccuracy (%)\nC4CAM-3b\nC4CAM-2b\nC4CAM-1b+LSH\nCosine-GPU\nEuclidean-GPU\n(a) Accuracy\nI\nW\nC\nWQ\n101\n103\n105\nDataset\nLatency (ns)\n(b) Execution time\nI\nW\nC\nWQ\n10\u22121\n102\n105\nDataset\nEnergy\nconsumption (nJ)\n(c) Energy Consumption\nFigure 10.", "mimetype": "text/plain", "start_char_idx": 50498, "end_char_idx": 50762, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56fed4cc-c350-42c6-a812-e759579f7677": {"__data__": {"id_": "56fed4cc-c350-42c6-a812-e759579f7677", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05aa52b2-6751-4540-84e0-4a91056ececc", "node_type": "1", "metadata": {}, "hash": "5a6ef60676b59f651ca0a3f267bfc92cf608d85c0821d09174abc474cc4e32dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae04eaaf-33bd-419f-8f58-66fce7329ae7", "node_type": "1", "metadata": {}, "hash": "ad0ec2f15805f1a17edc405f94cda0fa791b563e08ba73b0d2bead51e31c4f60", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Accuracy, latency, and energy consumption comparison of KNN implementations on 128\u00d7128 CAM arrays contrasting\nwith GPU execution. I, W, C, and WQ represent Iris, Wine, Cancer, and Wine Quality datasets, respectively\nTable 2.", "mimetype": "text/plain", "start_char_idx": 50763, "end_char_idx": 50987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae04eaaf-33bd-419f-8f58-66fce7329ae7": {"__data__": {"id_": "ae04eaaf-33bd-419f-8f58-66fce7329ae7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56fed4cc-c350-42c6-a812-e759579f7677", "node_type": "1", "metadata": {}, "hash": "1509d0fa78698af7abe9703b0279846e952a04e81ac611702d99ec86d5c05da3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2afb8b8f-d547-4cb8-b04b-7857ff1a6606", "node_type": "1", "metadata": {}, "hash": "181c54e21dd940080106bc861a0df7c58677e6f42bc0092c913900934fbe61c3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "EDP and power analysis for KNN execution on the Pneumonia dataset\nEDP (nJ\u00b7s)\nPOWER (W)\nsubarray size\n16x16\n32x32\n64x64\n128x128\n256x256\n16x16\n32x32\n64x64\n128x128\n256x256\ncam-base\n0.75\n0.30\n0.15\n0.08\n0.05\n44.14\n16.30\n5.97\n2.34\n0.86\ncam-power\n1.32\n0.", "mimetype": "text/plain", "start_char_idx": 50988, "end_char_idx": 51235, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2afb8b8f-d547-4cb8-b04b-7857ff1a6606": {"__data__": {"id_": "2afb8b8f-d547-4cb8-b04b-7857ff1a6606", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae04eaaf-33bd-419f-8f58-66fce7329ae7", "node_type": "1", "metadata": {}, "hash": "ad0ec2f15805f1a17edc405f94cda0fa791b563e08ba73b0d2bead51e31c4f60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4b359ad-eaca-4ba2-8a31-d315725895cc", "node_type": "1", "metadata": {}, "hash": "ca0cfd4ef06ce88347977ab63b50d7f47c67e76178c2ca8df88fd8f1f00ce91d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "61\n0.44\n0.29\n0.23\n25.23\n8.15\n2.10\n0.66\n0.19\nsolely the similarity search kernel. This kernel is by far the\nmost time-consuming part in KNN and HDC. By Amdahl\u2019s\nlaw, the end-to-end performance can be improved by the\nfraction of the time that similarity search is actually used. For\nexample, Euclidean distance dominates the execution time\nof standard KNN and takes 52 \u221296% of the time in k-means\nalgorithms [49].", "mimetype": "text/plain", "start_char_idx": 51235, "end_char_idx": 51646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4b359ad-eaca-4ba2-8a31-d315725895cc": {"__data__": {"id_": "b4b359ad-eaca-4ba2-8a31-d315725895cc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2afb8b8f-d547-4cb8-b04b-7857ff1a6606", "node_type": "1", "metadata": {}, "hash": "181c54e21dd940080106bc861a0df7c58677e6f42bc0092c913900934fbe61c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b1bb255-2fbf-4bfc-b2af-c1b271e160e1", "node_type": "1", "metadata": {}, "hash": "c136e474bbfdb2be9fde9cc5a6bc347a91cc4a428368bf6b080dbff4a8657b4c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In standard KNN, for instance, Euclidean\ndistance dominates the execution time, while in k-means\nalgorithms, it constitutes 52 \u221296% of the total time [49]. In\nHDC, the associative search accounts for 51\u221285% of the GPU\nexecution [23]. In Memory Augmented Neural Networks,\nsimilarity kernels contribute approximately 50 \u221280% of the\ntime, depending on the platform [48].", "mimetype": "text/plain", "start_char_idx": 51647, "end_char_idx": 52014, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b1bb255-2fbf-4bfc-b2af-c1b271e160e1": {"__data__": {"id_": "0b1bb255-2fbf-4bfc-b2af-c1b271e160e1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4b359ad-eaca-4ba2-8a31-d315725895cc", "node_type": "1", "metadata": {}, "hash": "ca0cfd4ef06ce88347977ab63b50d7f47c67e76178c2ca8df88fd8f1f00ce91d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97c1cd5d-a391-443b-bbdf-c53a45dd536f", "node_type": "1", "metadata": {}, "hash": "75d49857562ea6ba7d64c16ca704e272c79727a6b0984c44b940e5a9ffcb9a15", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For the DNA read mapping use case, we compare the\nimplementation of FVSA on TCAMs to the BWA-MEM\nimplementation on GPUs from [42]. BWA-MEM remains\nthe most efficient algorithm for GPUs and thus represents\na strong baseline. It is important to note that while BWA-\nMEM and FVSA are distinct algorithms for the same problem,\nBWA-MEM solves it with slightly improved accuracy, as\nillustrated in Figure 9a.", "mimetype": "text/plain", "start_char_idx": 52015, "end_char_idx": 52417, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97c1cd5d-a391-443b-bbdf-c53a45dd536f": {"__data__": {"id_": "97c1cd5d-a391-443b-bbdf-c53a45dd536f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b1bb255-2fbf-4bfc-b2af-c1b271e160e1", "node_type": "1", "metadata": {}, "hash": "c136e474bbfdb2be9fde9cc5a6bc347a91cc4a428368bf6b080dbff4a8657b4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7388e734-dd8d-4459-b754-c773ec082e05", "node_type": "1", "metadata": {}, "hash": "ab9d2dfaee2d1b9889719994bb8af4c9e6a354c6601c220c95556412e067959d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For aligning 100-bp reads, the GPU\nbaseline achieves 1 million reads per second (Mreads/s) with\nan energy efficiency of 4 thousand reads per second per\nWatt (kreads/S/W), as depicted in Figure 9. This implies\nthat CAM-based systems can potentially deliver up to 26\u00d7\nhigher mapping throughput than a highly-optimized GPU\nimplementation while consuming \u223c9\u00d7 less energy.\n5.6\nDesign space exploration\nSince C4CAM is retargetable, it can be used to quickly\ngenerate implementations for different system parameters and\noptimization targets.", "mimetype": "text/plain", "start_char_idx": 52418, "end_char_idx": 52952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7388e734-dd8d-4459-b754-c773ec082e05": {"__data__": {"id_": "7388e734-dd8d-4459-b754-c773ec082e05", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97c1cd5d-a391-443b-bbdf-c53a45dd536f", "node_type": "1", "metadata": {}, "hash": "75d49857562ea6ba7d64c16ca704e272c79727a6b0984c44b940e5a9ffcb9a15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da6be919-c62f-4dc3-ba9c-8e893701e92e", "node_type": "1", "metadata": {}, "hash": "6116906e855d96e48d3701573a0b6121eb05779a422ba3ec8af898094108e2c0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In this section, we explore the design-\nspace exploration capabilities for hardware/software designs.\n5.6.1\nFixed architectural parameters. As discussed above,\nC4CAM can reproduce the results of single manual designs.", "mimetype": "text/plain", "start_char_idx": 52953, "end_char_idx": 53170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da6be919-c62f-4dc3-ba9c-8e893701e92e": {"__data__": {"id_": "da6be919-c62f-4dc3-ba9c-8e893701e92e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7388e734-dd8d-4459-b754-c773ec082e05", "node_type": "1", "metadata": {}, "hash": "ab9d2dfaee2d1b9889719994bb8af4c9e6a354c6601c220c95556412e067959d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f81466c6-0f61-488d-aba6-cd11ab2305a4", "node_type": "1", "metadata": {}, "hash": "8d3779bd316920b8e28e49b78a7a60522c47f7e334c5bb09bf01a61655a98c1e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To\ndemonstrate its retargetability, we evaluate systems consisting\nof sub-arrays with sizes of \ud835\udc45\u00d7 \ud835\udc36, where \ud835\udc36= \ud835\udc45assuming\nvalues of 16, 32, 64, 128, and 256 with different configurations\nfor the same, as outlined in Section 5.1.1, namely:\n\u2022 cam-base: In this configuration, applications are allo-\ncated to the CAM accelerator without incorporating the\noptimizations discussed in Section 4.4.2. In this setup,\nparallel execution is enabled at each level.", "mimetype": "text/plain", "start_char_idx": 53171, "end_char_idx": 53622, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f81466c6-0f61-488d-aba6-cd11ab2305a4": {"__data__": {"id_": "f81466c6-0f61-488d-aba6-cd11ab2305a4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da6be919-c62f-4dc3-ba9c-8e893701e92e", "node_type": "1", "metadata": {}, "hash": "6116906e855d96e48d3701573a0b6121eb05779a422ba3ec8af898094108e2c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff68e3a0-82be-4b72-b400-a02777a30c35", "node_type": "1", "metadata": {}, "hash": "fab5b1ca7c8514c66040d98701ce4fb4c321a31cf796b228ed69571c5a7b2981", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\u2022 cam-power: This configuration implements a restric-\ntion on the maximum number of sub-arrays activated\nconcurrently. Specifically, for each application, we have\nchosen to enable only one sub-array per array at a time.\n\u2022 cam-density: This configuration demonstrates the im-\npact of employing selective search [55] to enhance\nboth the utilization of arrays and the system\u2019s overall\ncapacity, as shown in Table 3.\n\u2022 cam-power+density: This configuration imposes limi-\ntations on the number of enabled sub-arrays at a time.", "mimetype": "text/plain", "start_char_idx": 53623, "end_char_idx": 54144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff68e3a0-82be-4b72-b400-a02777a30c35": {"__data__": {"id_": "ff68e3a0-82be-4b72-b400-a02777a30c35", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f81466c6-0f61-488d-aba6-cd11ab2305a4", "node_type": "1", "metadata": {}, "hash": "8d3779bd316920b8e28e49b78a7a60522c47f7e334c5bb09bf01a61655a98c1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e8073cb-01ee-459f-a27d-9ee328d6a9ef", "node_type": "1", "metadata": {}, "hash": "919983e47eb66cc1101badc4849bba37a6cf29ae9af0c8baecfa4173c8607029", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Simultaneously, it incorporates selective search tech-\nnique to enhance the system\u2019s capacity.\nFor all sub-array sizes, the configuration remains consistent,\nwith 4 mats per bank, 4 arrays per mat, and 8 sub-arrays per\narray. We use as many banks as needed to accommodate the\ninput data. Figure 11a and Figure 11b illustrate the energy\nconsumption and latency of the configurations mentioned\nabove respectively when executing the HDC application on\nthe MNIST dataset with 8k dimensions.", "mimetype": "text/plain", "start_char_idx": 54145, "end_char_idx": 54631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e8073cb-01ee-459f-a27d-9ee328d6a9ef": {"__data__": {"id_": "5e8073cb-01ee-459f-a27d-9ee328d6a9ef", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff68e3a0-82be-4b72-b400-a02777a30c35", "node_type": "1", "metadata": {}, "hash": "fab5b1ca7c8514c66040d98701ce4fb4c321a31cf796b228ed69571c5a7b2981", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "401d21a1-7f10-44bc-84a5-f5712336967d", "node_type": "1", "metadata": {}, "hash": "aa14ba950af42c08f43eca256bfc51ffb8acc890921ca275b355a2b187bd40d2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "174\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nFarzaneh et al.\n16\n32\n64 128 256\n100.", "mimetype": "text/plain", "start_char_idx": 54632, "end_char_idx": 54725, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "401d21a1-7f10-44bc-84a5-f5712336967d": {"__data__": {"id_": "401d21a1-7f10-44bc-84a5-f5712336967d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e8073cb-01ee-459f-a27d-9ee328d6a9ef", "node_type": "1", "metadata": {}, "hash": "919983e47eb66cc1101badc4849bba37a6cf29ae9af0c8baecfa4173c8607029", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09afc8f6-2bd6-4caf-b6e3-5a01aa090e68", "node_type": "1", "metadata": {}, "hash": "55d5827239ea3513ebe1be02fdb4e1d1c8d5bc269900f8e490257b49994919ed", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5\n101\nsubarray size (\ud835\udc41\u00d7 \ud835\udc41)\nEnergy (\ud835\udf07J)\ncam-base\ncam-density\ncam-power\ncam-density+power\n(a) Energy\n16\n32\n64 128 256\n10\u22121\n100\n101\nsubarray size (\ud835\udc41\u00d7 \ud835\udc41)\nLatency (ms)\n(b) Latency\n16\n32\n64 128 256\n100\n101\nsubarray size (\ud835\udc41\u00d7 \ud835\udc41)\nPower (mW)\n(c) Power\nFigure 11.", "mimetype": "text/plain", "start_char_idx": 54725, "end_char_idx": 54977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09afc8f6-2bd6-4caf-b6e3-5a01aa090e68": {"__data__": {"id_": "09afc8f6-2bd6-4caf-b6e3-5a01aa090e68", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "401d21a1-7f10-44bc-84a5-f5712336967d", "node_type": "1", "metadata": {}, "hash": "aa14ba950af42c08f43eca256bfc51ffb8acc890921ca275b355a2b187bd40d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ae5b474-1b0c-41cf-b508-60338d005044", "node_type": "1", "metadata": {}, "hash": "93db4c4a3b542b81a08363ea410dc883af7c9f34cf3ee4f552b70a297cde125d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Impact of subarray size and C4CAM optimizations on latency, energy, and power\nIn the cam-power configuration, only one sub-array within\nthe array is active at a time. With a sub-array of size 16 \u00d7 16,\nthe power consumption is reduced to approximately 0.57\u00d7\nwith respect to the base configuration (Figure 11c). Similarly,\nthe power requirement for the largest array size is merely\n20% of the base configuration. However, this reduction in\npower consumption results in increased latency.", "mimetype": "text/plain", "start_char_idx": 54978, "end_char_idx": 55463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ae5b474-1b0c-41cf-b508-60338d005044": {"__data__": {"id_": "2ae5b474-1b0c-41cf-b508-60338d005044", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09afc8f6-2bd6-4caf-b6e3-5a01aa090e68", "node_type": "1", "metadata": {}, "hash": "55d5827239ea3513ebe1be02fdb4e1d1c8d5bc269900f8e490257b49994919ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e729d654-a5df-4c8a-a1d6-9e8e9623e471", "node_type": "1", "metadata": {}, "hash": "816263faae01f2bf51b6ae25796664d030f371e38359d070e892b0393505d17d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For instance,\nexecuting the application on a 32 \u00d7 32-sized subarray incurs\na latency increase of approximately 2\u00d7 compared to the\nbaseline. As the array size increases, the latency rises, reaching\nup to 4.86\u00d7 the baseline for the largest sub-array size. The\noverall energy consumption remains the same between the\ntwo configurations, cam-power and cam-base.\nThe analysis of the KNN benchmark is similar to the\nanalysis for HDC. We summarize the results in Table 2 for\nEDP and power.", "mimetype": "text/plain", "start_char_idx": 55464, "end_char_idx": 55946, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e729d654-a5df-4c8a-a1d6-9e8e9623e471": {"__data__": {"id_": "e729d654-a5df-4c8a-a1d6-9e8e9623e471", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ae5b474-1b0c-41cf-b508-60338d005044", "node_type": "1", "metadata": {}, "hash": "93db4c4a3b542b81a08363ea410dc883af7c9f34cf3ee4f552b70a297cde125d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64d5ecf7-8e50-42a0-b969-a8181296a2e7", "node_type": "1", "metadata": {}, "hash": "823151f0d612e76a6d154311fee367f64095ac183bc1c135ca99b8d0de5ecd46", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The absolute values of energy and latency are\nconsiderably higher than in the HDC case. This is simply due\nto the sheer size of the Pneumonia dataset, requiring many\nbanks in the CAM accelerator.\nThe cam-density configuration uses selective search to\nimprove resource utilization, as shown in Table 3. In the case\nof the smallest array size (16 \u00d7 16), the execution time is less\nthan twice compared to the base configuration.", "mimetype": "text/plain", "start_char_idx": 55947, "end_char_idx": 56372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64d5ecf7-8e50-42a0-b969-a8181296a2e7": {"__data__": {"id_": "64d5ecf7-8e50-42a0-b969-a8181296a2e7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e729d654-a5df-4c8a-a1d6-9e8e9623e471", "node_type": "1", "metadata": {}, "hash": "816263faae01f2bf51b6ae25796664d030f371e38359d070e892b0393505d17d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc16d4f4-8226-4975-bc35-765dfc85b28a", "node_type": "1", "metadata": {}, "hash": "9ddeb4fce373dab4139c304e873e4d4628ea6f52a4a1524b2bdfefc157ea67a5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This trend\nscales further, and with the largest subarray size (256 \u00d7 256),\nthe execution time is nearly 23\u00d7 longer compared to the cam-\nbase configuration. The energy consumption for subarray\nsizes ranging from 16 \u00d7 16 to 64 \u00d7 64 in the cam-density\nconfiguration is, on average, 0.6\u00d7 that of the corresponding\nsub-array size in the baseline configuration.", "mimetype": "text/plain", "start_char_idx": 56373, "end_char_idx": 56728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc16d4f4-8226-4975-bc35-765dfc85b28a": {"__data__": {"id_": "fc16d4f4-8226-4975-bc35-765dfc85b28a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64d5ecf7-8e50-42a0-b969-a8181296a2e7", "node_type": "1", "metadata": {}, "hash": "823151f0d612e76a6d154311fee367f64095ac183bc1c135ca99b8d0de5ecd46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7a4c3e0-3d48-439e-bf42-be805a6538dd", "node_type": "1", "metadata": {}, "hash": "9dc60d927e3800105df906859533885b242772dd673a909284b04eadd2d3ae62", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "However, for\nsub-arrays of 128 \u00d7 128 or 256 \u00d7 256, the energy consumption\nincreases compared to the baseline, reaching 1.4\u00d7 and 5.1\u00d7,\nrespectively. It is worth noting that by fixing the system\nconfiguration and enabling selective search, the number of\nbanks required for application execution is reduced, thus\nreducing the overall power consumption.\nThe cam-power-density configuration combines the ap-\nproaches of both cam-power and cam-density to achieve the\nmost significant reduction in power consumption.", "mimetype": "text/plain", "start_char_idx": 56729, "end_char_idx": 57238, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7a4c3e0-3d48-439e-bf42-be805a6538dd": {"__data__": {"id_": "c7a4c3e0-3d48-439e-bf42-be805a6538dd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc16d4f4-8226-4975-bc35-765dfc85b28a", "node_type": "1", "metadata": {}, "hash": "9ddeb4fce373dab4139c304e873e4d4628ea6f52a4a1524b2bdfefc157ea67a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa10dd7d-ce7a-4186-b844-8085b240b5a1", "node_type": "1", "metadata": {}, "hash": "4856ac469991913eb8c1ed78d7cbd4e1e0c606493ab3074b85172b1c78b54a05", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A 16 \u00d7 16-\nsized subarray utilizes only 23.4% of the base power, while\nthe largest sub-array requires only 4.2% of the base power.\nHowever, this reduction in power consumption comes at the\ncost of significantly increasing the execution time. In the case\nof the largest subarray configuration, the execution time is ap-\nproximately 121\u00d7 longer compared to the base configuration.\nTable 3.", "mimetype": "text/plain", "start_char_idx": 57239, "end_char_idx": 57626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa10dd7d-ce7a-4186-b844-8085b240b5a1": {"__data__": {"id_": "fa10dd7d-ce7a-4186-b844-8085b240b5a1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7a4c3e0-3d48-439e-bf42-be805a6538dd", "node_type": "1", "metadata": {}, "hash": "9dc60d927e3800105df906859533885b242772dd673a909284b04eadd2d3ae62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "631a7a0d-247b-44f5-a0b7-1454d7f709b4", "node_type": "1", "metadata": {}, "hash": "1691fe3f9546c4b463872a413cdd8e46e772f11a96f9b0168774f54dd8a9dcd8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Number of subarrays used to implement HDC\n16 \u00d7 16\n32 \u00d7 32\n64 \u00d7 64\n128 \u00d7 128\n256 \u00d7 256\ncam-base\n512\n256\n128\n64\n32\ncam-density\n512\n86\n22\n6\n2\n5.6.2\nIso-capacity analysis. With the iso-capacity experi-\nments, we investigate the relationship between energy con-\nsumption and latency by changing the size of subarrays and\nthe number of subarrays per array while keeping the capacity\nfixed to 216 TCAM cells per array.", "mimetype": "text/plain", "start_char_idx": 57627, "end_char_idx": 58038, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "631a7a0d-247b-44f5-a0b7-1454d7f709b4": {"__data__": {"id_": "631a7a0d-247b-44f5-a0b7-1454d7f709b4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa10dd7d-ce7a-4186-b844-8085b240b5a1", "node_type": "1", "metadata": {}, "hash": "4856ac469991913eb8c1ed78d7cbd4e1e0c606493ab3074b85172b1c78b54a05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e09d2c22-beaa-4ebd-919a-9b25c00c1bdc", "node_type": "1", "metadata": {}, "hash": "85ab9ecf59f532c9bd25c1f80089ea0ba3216c4a95a64855a4eb2d5064e05fed", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To achieve this, we modify\nthe subarray size, starting from 256 \u00d7 256 which corresponds\nto one subarray per array, and gradually decrease it to 16 \u00d7 16,\nresulting in 256 subarrays per array. The numbers of arrays\nper mat and mats per bank are fixed as in the previous sections.\nIt is important to note that these systems are not iso-area since\neach subarray has its own set of peripherals. This means that\nas the size of the subarrays is reduced, more peripherals are\nneeded, and chip area increases.", "mimetype": "text/plain", "start_char_idx": 58039, "end_char_idx": 58539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e09d2c22-beaa-4ebd-919a-9b25c00c1bdc": {"__data__": {"id_": "e09d2c22-beaa-4ebd-919a-9b25c00c1bdc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "631a7a0d-247b-44f5-a0b7-1454d7f709b4", "node_type": "1", "metadata": {}, "hash": "1691fe3f9546c4b463872a413cdd8e46e772f11a96f9b0168774f54dd8a9dcd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f604e1da-31a2-45d9-86fc-da0c6fe0b5b0", "node_type": "1", "metadata": {}, "hash": "5e10fd436825317d9c82d9d9f1f73c742a9dcf2a9269726db717f92ba4a59d60", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Figure 12b shows that the energy consumption in iso-base\nremains nearly constant across subarray sizes. Moreover,\ncam-density and cam-power+density, on average, achieve\n1.75\u00d7 energy improvement over iso-capacity-base, except\nfor large subarray sizes like 128 \u00d7 128 and 256 \u00d7 25.", "mimetype": "text/plain", "start_char_idx": 58540, "end_char_idx": 58818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f604e1da-31a2-45d9-86fc-da0c6fe0b5b0": {"__data__": {"id_": "f604e1da-31a2-45d9-86fc-da0c6fe0b5b0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e09d2c22-beaa-4ebd-919a-9b25c00c1bdc", "node_type": "1", "metadata": {}, "hash": "85ab9ecf59f532c9bd25c1f80089ea0ba3216c4a95a64855a4eb2d5064e05fed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e3932f7-cc32-4a84-8a68-86445cd9f294", "node_type": "1", "metadata": {}, "hash": "17fb2a7ead60e152461062486760c3a212e9b1079deea8cac4e7de98cdb27ae0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The\ntotal execution time across different subarray sizes also varies\nwithin a moderate range, i.e., from 58\ud835\udf07\ud835\udc60for 16\u00d716 to 150\ud835\udf07\ud835\udc60for\n256\u00d7256 , as shown in Figure 12b. Again, as the search latency\nincreases for larger columns, the execution time also increases\ndespite the consistent number of cells within an array. As\nfor the cam-density and cam-power+density transformations,\n175\nC4CAM: A Compiler for CAM-based In-memory Accelerators\nASPLOS \u201924,", "mimetype": "text/plain", "start_char_idx": 58819, "end_char_idx": 59265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e3932f7-cc32-4a84-8a68-86445cd9f294": {"__data__": {"id_": "1e3932f7-cc32-4a84-8a68-86445cd9f294", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f604e1da-31a2-45d9-86fc-da0c6fe0b5b0", "node_type": "1", "metadata": {}, "hash": "5e10fd436825317d9c82d9d9f1f73c742a9dcf2a9269726db717f92ba4a59d60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6dcc80f4-ead8-4f1a-8f76-c4dfb16146f4", "node_type": "1", "metadata": {}, "hash": "64d9b4bd8e395d873b6a7a8bba38e579f8213b00ef89d32aaba98f9fc003232d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "April 27-May 1, 2024, La Jolla, CA, USA\n16\n32\n64\n128\n256\n10\u22121\n100\nsubarray size (\ud835\udc41\u00d7 \ud835\udc41)\nLatency (ms)\niso-base\niso-density\niso-density+power\n(a) Latency\n16\n32\n64\n128\n256\n100\n101\nsubarray size (\ud835\udc41\u00d7 \ud835\udc41)\nPower (mW)\n(b) Power\nFigure 12.", "mimetype": "text/plain", "start_char_idx": 59266, "end_char_idx": 59494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6dcc80f4-ead8-4f1a-8f76-c4dfb16146f4": {"__data__": {"id_": "6dcc80f4-ead8-4f1a-8f76-c4dfb16146f4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e3932f7-cc32-4a84-8a68-86445cd9f294", "node_type": "1", "metadata": {}, "hash": "17fb2a7ead60e152461062486760c3a212e9b1079deea8cac4e7de98cdb27ae0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd55f36d-9909-46da-8ff2-820cbfdb84c2", "node_type": "1", "metadata": {}, "hash": "54e6c4f39c53346f3a00e9a11b558691e0cc95d1193081485ebb33d138904cf0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Impact of optimizations on iso-capacity setups\nFigure 12b shows a significant decrease in power consumption,\noffering a potential CAM configuration that can be used in\npower-constrained system setups.\n6\nConclusions\nWe present C4CAM, the first framework for programming and\nexploring trade-offs in CAM-based accelerators. We introduce\na retargetable MLIR-based code transformation flow from\nhigh-level TorchScript code, featuring a novel cam abstraction\nthat is specifically tailored for CAM-based accelerators.", "mimetype": "text/plain", "start_char_idx": 59495, "end_char_idx": 60005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd55f36d-9909-46da-8ff2-820cbfdb84c2": {"__data__": {"id_": "fd55f36d-9909-46da-8ff2-820cbfdb84c2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6dcc80f4-ead8-4f1a-8f76-c4dfb16146f4", "node_type": "1", "metadata": {}, "hash": "64d9b4bd8e395d873b6a7a8bba38e579f8213b00ef89d32aaba98f9fc003232d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64efd21b-333a-4b1c-9899-9cec164d31ea", "node_type": "1", "metadata": {}, "hash": "50dcbfaac5896232f5b111869b6e6a82f8cb7d1a9eb49ba2f7493b59bffee5a4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This\nabstraction provides control knobs that allow for the tuning of\nvarious metrics by adjusting the mapping of applications to\nthe CAM arrays. To validate the effectiveness of C4CAM, we\ncompare our results with those obtained from a hand-crafted\ndesigns and demonstrate that C4CAM produces comparable\nresults. Moreover, we demonstrate C4CAM capabilities by\nautomatically generating implementations optimized for per-\nformance, power and device utilization. Finally, we show how\nC4CAM retargetability facilitates design space exploration\nby varying architectural parameters without any application\nrecoding effort.", "mimetype": "text/plain", "start_char_idx": 60006, "end_char_idx": 60621, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64efd21b-333a-4b1c-9899-9cec164d31ea": {"__data__": {"id_": "64efd21b-333a-4b1c-9899-9cec164d31ea", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd55f36d-9909-46da-8ff2-820cbfdb84c2", "node_type": "1", "metadata": {}, "hash": "54e6c4f39c53346f3a00e9a11b558691e0cc95d1193081485ebb33d138904cf0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d55ca5ed-7929-4410-a9e4-853529e82ef3", "node_type": "1", "metadata": {}, "hash": "960512319a66c856135b2f691dd148eee2a8c5cf3142cdc32626ef634ca68b9c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The architecture specification supported by\nC4CAM, along with its compilation flow, also enables the\nspecification of heterogeneous systems. However, determining\nthe optimal mapping strategy for heterogeneous systems based\non different optimization targets remains a subject for future\nresearch.\nAcknowledgments\nThis work was partially funded by the Center for Advancing\nElectronics Dresden (cfaed) and the German Research Council\n(DFG) through the HetCIM project (502388442) under the\nPriority Program on \u2018Disruptive Memory Technologies\u2019 (SPP\n2377), the AI competence center ScaDS.", "mimetype": "text/plain", "start_char_idx": 60622, "end_char_idx": 61204, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d55ca5ed-7929-4410-a9e4-853529e82ef3": {"__data__": {"id_": "d55ca5ed-7929-4410-a9e4-853529e82ef3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "1d8e55aac100dcccbeb5b2b0fb654e0e16c7ad11119d97cdfa5980a5cc89c459", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64efd21b-333a-4b1c-9899-9cec164d31ea", "node_type": "1", "metadata": {}, "hash": "50dcbfaac5896232f5b111869b6e6a82f8cb7d1a9eb49ba2f7493b59bffee5a4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620666.3651386", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "AI Dresden/Leipzig\nin Germany (01IS18026A-D), the Semiconductor Research\nCorporation (SRC), the Logic and Memory Devices Program\n(LMD), and the AI Chip Center for Emerging Smart Systems\n(ACCESS) sponsored by InnoHK funding, Hong Kong SAR.", "mimetype": "text/plain", "start_char_idx": 61204, "end_char_idx": 61442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10.1145/3620666.3651386": {"__data__": {"id_": "10.1145/3620666.3651386", "embedding": null, "metadata": {"node_type": "doi_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"4": {"node_id": "root_node", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "ce6beca6-91b1-4a1e-bb9e-5e2aef65f7bf", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8a34a36d-0ff6-43b9-9b6f-8fdc4497a8dc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1e2ebfad-89b9-4d83-b74b-d5746082b6a2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b3e3932a-b0bb-4749-99ea-edf00ee93a5f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "33178490-e400-4113-bbf7-7f978ef19fe2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "29978223-f6ac-44b6-b30c-c94c15f525e0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "58c0c755-97ee-4e50-b41f-8395cf490ebd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c20a560a-e946-4202-bc5d-d7eafc0748e5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4db71d11-1e03-4ced-9a5a-ecdbf4b8d893", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c256a51b-751d-4338-a3cf-f82f72e51766", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3877773d-f394-4ada-bd5f-612912b7546e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "dde9786d-e7bf-4fc1-baaa-639a4161b487", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9d1f07fb-d417-425b-bacd-2e2091174c99", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a24779c4-6c4b-4e32-930d-c687149ca7f6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "aa55ac90-ed41-41d5-aceb-1afdaa0a4b8a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "02119eb9-8504-4083-ba41-22f37e599f29", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f7cfcfb5-ed7f-4c5f-9d78-ccfa5bed2ccc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9318ad85-4d09-4acc-8bb5-04e4182061b8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "78ea12e7-82c1-4cb3-8726-745b61e8741e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5859417b-2dd1-4b3c-8fa7-7cd5cd3ae3e6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2996cee2-bdba-4ffc-b43f-b558d59696e4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e8714f23-6cd1-4440-b8cd-7fad4cd6d068", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "baef1291-a654-4c00-8f4c-63c65a6d71ab", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "78a77e59-3b3c-4b1b-8ac3-6758df3750e1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7f2f15cc-42dc-4bea-9899-0221f9286888", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a864e6a5-4b5a-46fa-98ee-4e060bead9a7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1e5f7931-ee12-496a-a66c-02b52df3449c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "216abd8a-aff8-4768-b247-11f3f384b8b7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5d8da926-8381-4901-b1d2-9719080a6dc9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1c54d84a-6815-4859-8152-c971e049b6e2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7fbc1f57-49fe-4f17-90db-b249ba235f4a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "56631314-cff8-4964-9c57-d6ab1019aa40", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f4f5f59c-eab0-4eff-9506-716203d706d4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ded2ef5f-9f03-4ddf-adf9-3c0cdb3065d1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "09de1bb1-ddc6-4c63-8556-bc80420029d0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "71da9476-65fd-44b0-bc1a-253d08c47b3a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "854008a0-4e99-4b8e-b495-b88b9dc090a1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "816881c4-a614-43c0-8d14-984014afd457", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ce83d2a3-2280-4a71-93a1-e25407d1b548", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "dc9faad3-0c6a-48fb-be64-4e8b32de0891", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "54417010-3505-47e3-be62-a9740807383b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "670d4244-7cc5-4a33-95ab-5e03a994198c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7409d33a-3652-4002-8c97-87fbdc4a7bfb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ce6c6eb7-e015-4572-94fe-81884a1c0a6b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "471fb5ad-4434-49db-a405-c75619cbc511", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "537b0ab5-144d-4d5f-b014-b48212fc1c44", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4b4bdbc7-f17b-4754-88a2-2704af18b3db", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d7ab454b-d73e-4558-a4b3-3ab76b473001", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "557e8d1e-f6bf-4c69-8f84-8cf0e0762a34", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f934dfee-396b-4f19-a869-1f5c3ed52fad", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "01253bd9-b089-49f8-a571-9b1a9b8ee380", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4c58947c-bfd2-4a53-9c22-0df6255f21e5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ec38f990-1634-4209-a465-676674df7c22", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d8c18495-ad01-4e85-81e4-d93ef23795b2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "17a8dc1d-2514-4bd0-bb6e-6ea070567a47", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5c796ebd-8863-4ae2-a86d-409a8ec7f21f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5eb85767-dbf4-48fd-a118-9b08e1bc849a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3fb7aa01-72d7-4552-abb3-a30e88ba1c8a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "00794754-fc63-45d5-9d6d-73a1d041162c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "413c7c40-c43d-44ea-828d-1348a3b97ea4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2c184e13-a2e1-4b6d-80a0-f96d5e819a63", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "49e9d009-c89d-496a-beb5-dc968978421f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "500f8d9b-1a0e-4bea-8443-34eb023678bb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "761432ee-0a5c-4331-96e3-98fccc17f028", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3efb12e8-48aa-42b1-b8a0-7c17b031051b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cce391b7-00ab-4456-9032-fc4bd361911c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "12e75cd0-66b2-4c7e-b5fa-90ed1949b5da", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "18cd6ee6-e7c7-4314-8c31-f39a431205db", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "dabff7c6-6f21-43ca-b1a1-dcc8b8c4e843", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7dcfcdbe-c07c-4039-acb1-d192252a1b50", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fb7f4a98-93be-46a4-b4ee-fe82bb1f2349", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a49871ef-1677-4bde-97d9-abc11fa8d738", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "971a6b47-4916-45c6-9d97-2f2a1eb16643", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "72ce1635-7d22-40d0-aee9-7f73daecc27f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "001eb75c-bc35-4e70-9854-219ce8474aae", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "245911d7-bde2-4736-954a-5e097feae088", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "df27660a-ffba-4c72-adad-509aa46c096a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ec8798bc-3283-40e5-b2d1-9ddee2b41f2a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "85c869f0-a5d1-4785-a5b9-0cec9c18b045", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "03b403a4-e9bf-4301-8ef6-ef0ab9e5b204", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6a4f4cdb-c95c-4498-ab84-aacb1c5c9b4d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "778be731-0acb-41b8-a061-bb1e22ddca5e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "18ac7c6d-a6af-4c41-bfc0-bf41eeed3cd8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "12d254d7-14aa-4063-8d3a-a6b4155d3750", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4a172d1b-4b1b-492d-b96d-57e7810f3a6e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "85b7c7f5-dff1-4eea-a6b4-7b5ad7493abc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c9929cc7-ed3d-421d-bf6b-0cfcfaa3041a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "37427d17-0b90-47e1-96db-dac275939524", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1379cd16-9541-4653-93bb-b076d7d15e11", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9c935663-660d-4652-b0d5-49ad7c2ab4ac", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9db3837b-faf5-46fc-b985-cdb443f76f25", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0f4aa01c-f415-4fda-a8c9-bb332a4b6c1d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5f154dde-95ef-4afe-b861-10abfd744e38", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "327e205d-63cc-452f-9378-810c133eeabd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1701c0e8-06f6-4410-9cb4-ec66139cfacf", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8e333101-0b0a-418d-a9f2-ee9546ef71af", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "95f9778e-5e99-404d-be88-5fc8e59e695d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "277d09d8-91ee-47d8-af25-8d2f3c892b86", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "77ddb39f-f747-4ca6-98c2-7d1e8c2f4853", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fa1f3b8b-9337-4e1b-99e2-de02e4d2e126", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4e2b87b3-85c1-40e8-a2d8-e69968573b61", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5728b055-b0f6-4f84-a469-d959e5aca37e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6abbc3fa-b2f1-4845-a8e6-f04b90d662d8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ba128f38-243f-4a38-9819-b78ffffb84a6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "991221b8-a930-4f83-9aa1-9fa1ed038555", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "30030db8-9b2a-4ec7-9083-c3c773c89e2a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d9ce3809-fe35-47af-b1f9-6e6b1e45b34c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7903e73b-3135-4461-8842-c7bcab8d5fd0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "58c10eff-f1ca-4aae-a78d-2b3492de8699", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4bee446f-5cc1-4f67-95f7-d866d7cbd711", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f56c20b0-5491-4d65-9d23-3fc8eec4afd6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e046c9ff-e1a8-4e85-a50f-d0607036e8ca", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8f29e913-0b92-4c8a-83dc-2f98d4f314bc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "282f896e-0556-4794-9cfb-6d2e0e8a601e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "91e0412f-bfd0-44f4-887d-bdd9907d1ca4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1d506d3b-eec9-4854-b9df-c579b2713bea", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b6ad5f65-0ecf-431a-b370-654f9037d95f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "35c6cbef-2e0e-47d8-b81e-a4dcea1502be", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bd6c2fe7-94f1-4704-8d0e-0bef046b3a5b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "496676d1-ff3c-4d88-b62e-01a8896b1869", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "05aa52b2-6751-4540-84e0-4a91056ececc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "56fed4cc-c350-42c6-a812-e759579f7677", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ae04eaaf-33bd-419f-8f58-66fce7329ae7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2afb8b8f-d547-4cb8-b04b-7857ff1a6606", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b4b359ad-eaca-4ba2-8a31-d315725895cc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0b1bb255-2fbf-4bfc-b2af-c1b271e160e1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "97c1cd5d-a391-443b-bbdf-c53a45dd536f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7388e734-dd8d-4459-b754-c773ec082e05", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "da6be919-c62f-4dc3-ba9c-8e893701e92e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f81466c6-0f61-488d-aba6-cd11ab2305a4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ff68e3a0-82be-4b72-b400-a02777a30c35", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5e8073cb-01ee-459f-a27d-9ee328d6a9ef", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "401d21a1-7f10-44bc-84a5-f5712336967d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "09afc8f6-2bd6-4caf-b6e3-5a01aa090e68", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2ae5b474-1b0c-41cf-b508-60338d005044", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e729d654-a5df-4c8a-a1d6-9e8e9623e471", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "64d5ecf7-8e50-42a0-b969-a8181296a2e7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fc16d4f4-8226-4975-bc35-765dfc85b28a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c7a4c3e0-3d48-439e-bf42-be805a6538dd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fa10dd7d-ce7a-4186-b844-8085b240b5a1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "631a7a0d-247b-44f5-a0b7-1454d7f709b4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e09d2c22-beaa-4ebd-919a-9b25c00c1bdc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f604e1da-31a2-45d9-86fc-da0c6fe0b5b0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1e3932f7-cc32-4a84-8a68-86445cd9f294", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6dcc80f4-ead8-4f1a-8f76-c4dfb16146f4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fd55f36d-9909-46da-8ff2-820cbfdb84c2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "64efd21b-333a-4b1c-9899-9cec164d31ea", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d55ca5ed-7929-4410-a9e4-853529e82ef3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}]}, "text": "DOI: 10.1145/3620666.3651386", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3ae19d3-aa0b-417d-bfb3-f31df984df31": {"__data__": {"id_": "d3ae19d3-aa0b-417d-bfb3-f31df984df31", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a7e07c2-6dd4-43e2-a6c0-22becd234523", "node_type": "1", "metadata": {}, "hash": "d1e1717b38e8d565de43bcd27541b938c0107c6ab997b131e8d807b11a3ee201", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1\nIntroduction\nAdvancements in computer performance have been trapped\nby the famous \u201cmemory wall\u201d problem for decades [45]. The\nadvent of data-intensive workloads, like DNNs [32], fur-\nther exacerbates this problem. As a promising technology\nto combat the memory wall, computing-in-memory (CIM)\nhas garnered significant attention for its capability of re-\nducing frequent data movement and parallelizing multiply-\nand-accumulate (MAC) operations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a7e07c2-6dd4-43e2-a6c0-22becd234523": {"__data__": {"id_": "5a7e07c2-6dd4-43e2-a6c0-22becd234523", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3ae19d3-aa0b-417d-bfb3-f31df984df31", "node_type": "1", "metadata": {}, "hash": "5c347e0a35256a9c7d67aa5b548059d04abc2c88fa2096110eadcdb5bf4201bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5dc9c18-9720-42d8-b9f6-38d547f7254a", "node_type": "1", "metadata": {}, "hash": "d952953ae67e5a2cb8406127ddc0b2db776da06f420ccbc9be31058cfebe6e51", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A variety of CIM-based\n185\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nSongyun Qu, Shixin Zhao, Bing Li, Yintao He, Xuyi Cai, Lei Zhang, and Ying Wang\narchitectures emerge, achieving significantly improved com-\nputing efficiency for DNN applications over traditional ar-\nchitectures [4, 6, 11, 13, 18, 39, 42].", "mimetype": "text/plain", "start_char_idx": 447, "end_char_idx": 765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5dc9c18-9720-42d8-b9f6-38d547f7254a": {"__data__": {"id_": "f5dc9c18-9720-42d8-b9f6-38d547f7254a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a7e07c2-6dd4-43e2-a6c0-22becd234523", "node_type": "1", "metadata": {}, "hash": "d1e1717b38e8d565de43bcd27541b938c0107c6ab997b131e8d807b11a3ee201", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6e87dcb-e811-4c4e-98f2-2a384c8b31b7", "node_type": "1", "metadata": {}, "hash": "cdd72993292da3818054adfcb3e68fde0ccbcf9e6dc96eb4363219a53e3981d8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To fully exploit CIM, an efficient compilation tool that\nbridges the DNN scope and various CIM hardware is needed.\nHowever, the development of CIM accelerators shows the\nfollowing trends that pose thorny challenges to the design of\na general compilation tool: (1) The diversity of CIM mem-\nory devices. Unlike the traditional computing paradigm,\nCIM hinges on memory devices to perform computations, in\nwhich the attributes of memory devices, like their types and\nprecision levels, exert a substantial influence on the feasible\nscheduling space within a CIM compilation framework.", "mimetype": "text/plain", "start_char_idx": 766, "end_char_idx": 1346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6e87dcb-e811-4c4e-98f2-2a384c8b31b7": {"__data__": {"id_": "b6e87dcb-e811-4c4e-98f2-2a384c8b31b7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5dc9c18-9720-42d8-b9f6-38d547f7254a", "node_type": "1", "metadata": {}, "hash": "d952953ae67e5a2cb8406127ddc0b2db776da06f420ccbc9be31058cfebe6e51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "728fd86c-0df5-482c-b8d4-730334ea7f93", "node_type": "1", "metadata": {}, "hash": "62992d00cc7f7b25244d651798e005d7b3460ba823a1f677938921fc43f0ead7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For\nexample, when comparing SRAM[26] and ReRAM, although\nboth have similar latency for read operations, the cost of\nwriting data is considerably higher in ReRAM [3]. Thus,\nSRAM-based CIM supports flexible data read and write up-\ndates on CIM memory [6], while ReRAM-based CIM usually\nassumes that weights are frozen in the crossbar, avoiding the\npenalty of frequent writes [13, 39]. These factors seriously\nimpact the scheduling strategy and space of a CIM compiler.\n(2) The diversity of CIM architectures.", "mimetype": "text/plain", "start_char_idx": 1347, "end_char_idx": 1853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "728fd86c-0df5-482c-b8d4-730334ea7f93": {"__data__": {"id_": "728fd86c-0df5-482c-b8d4-730334ea7f93", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6e87dcb-e811-4c4e-98f2-2a384c8b31b7", "node_type": "1", "metadata": {}, "hash": "cdd72993292da3818054adfcb3e68fde0ccbcf9e6dc96eb4363219a53e3981d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bdfae30-83fc-4d76-9bfa-2adc5edb04d4", "node_type": "1", "metadata": {}, "hash": "186f8cb9ab04d93a9f84d1d9fd43f2f5b6ac22fb3c47f98bdf22668c9ddd385f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In contrast\nto the common tiled PE-arrays of typical DNN accelera-\ntors [12], the architecture of CIM exhibits a diverse and\ndistinct organization[30], like the structure of an individ-\nual computing unit, the number of crossbars within each\ncomputing unit, crossbar size, etc. Consequently, the opti-\nmization space of the CIM compiler becomes much larger.\nThe CIM compiler must possess an understanding of the\nmanifold forms within CIM architecture when scheduling\nand mapping DNN operations onto CIMs.\n(3) The diversity of CIM programming interface.", "mimetype": "text/plain", "start_char_idx": 1854, "end_char_idx": 2406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bdfae30-83fc-4d76-9bfa-2adc5edb04d4": {"__data__": {"id_": "1bdfae30-83fc-4d76-9bfa-2adc5edb04d4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "728fd86c-0df5-482c-b8d4-730334ea7f93", "node_type": "1", "metadata": {}, "hash": "62992d00cc7f7b25244d651798e005d7b3460ba823a1f677938921fc43f0ead7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c6d7c3a-bf16-4804-9ea9-3822049f0373", "node_type": "1", "metadata": {}, "hash": "ad0fb6e88f3e6659dc8254e22417fb1d92aca209708b86a98e91a7fca55b5e3a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "CIM\narchitectures have rich hierarchies, and the manner in which\nthey are exposed to programmers varies across different chip\ndesigns. For example, some CIMs support fined-grained row-\nwise operations to realize more general-purpose arithmetic\nexcept convolutions and matrix multiplication [27]. To fully\nexploit CIM architecture across different applications, re-\nsearchers have developed the CIM-oriented programming\ninterface [4, 19]. However, the existing programming inter-\nface is deeply bound to a specific CIM design.", "mimetype": "text/plain", "start_char_idx": 2407, "end_char_idx": 2932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c6d7c3a-bf16-4804-9ea9-3822049f0373": {"__data__": {"id_": "8c6d7c3a-bf16-4804-9ea9-3822049f0373", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bdfae30-83fc-4d76-9bfa-2adc5edb04d4", "node_type": "1", "metadata": {}, "hash": "186f8cb9ab04d93a9f84d1d9fd43f2f5b6ac22fb3c47f98bdf22668c9ddd385f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a22a2ca3-77b8-4a21-8f6e-a53fec7a0ad3", "node_type": "1", "metadata": {}, "hash": "b887978b67d69c0f5665eb04f13adda01c3bfba6ca4d74b4e800fe8840de98a5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "With different\nprogramming interfaces, the granularity of the elementary\ncomputing unit that the scheduler can manage during opti-\nmization varies. A fine granularity leads to a more complex\nscheduling space, which increases the difficulty of compila-\ntion optimization.\nTherefore, the key challenges to developing a general-\npurpose CIM compiler are: 1. How to perform multi-dimensional\nhardware abstraction on CIM to enhance the generality of the\ncompiler? 2. How to generate the optimal mapping and sched-\nuling for different CIMs that expose different operation levels?", "mimetype": "text/plain", "start_char_idx": 2933, "end_char_idx": 3506, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a22a2ca3-77b8-4a21-8f6e-a53fec7a0ad3": {"__data__": {"id_": "a22a2ca3-77b8-4a21-8f6e-a53fec7a0ad3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c6d7c3a-bf16-4804-9ea9-3822049f0373", "node_type": "1", "metadata": {}, "hash": "ad0fb6e88f3e6659dc8254e22417fb1d92aca209708b86a98e91a7fca55b5e3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74b1c6bb-3764-4abf-b9cb-1a2a9f88ff1f", "node_type": "1", "metadata": {}, "hash": "1e31b0ba25138bada3226453f0c465f67d39f346d2e4bc357cd890b37652692b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Unfortunately, existing works are difficult to adapt to the\ndiversity of CIM accelerators, and lack optimization consid-\nerations for the specific computing granularity in different\nCIMs, resulting in inferior performance and even failure of\ncompiling. Some works manually deployed the model on\nCIMs [39] with customized mapping and scheduling policies\nthat are hard to generalize to other CIMs. Some works pro-\nposed CIM-oriented compilation tools [2, 17, 22]. However,\nthey are insufficient to tackle the above challenges.", "mimetype": "text/plain", "start_char_idx": 3507, "end_char_idx": 4031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74b1c6bb-3764-4abf-b9cb-1a2a9f88ff1f": {"__data__": {"id_": "74b1c6bb-3764-4abf-b9cb-1a2a9f88ff1f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a22a2ca3-77b8-4a21-8f6e-a53fec7a0ad3", "node_type": "1", "metadata": {}, "hash": "b887978b67d69c0f5665eb04f13adda01c3bfba6ca4d74b4e800fe8840de98a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f4d0025-6967-4729-aa31-5b5d317c521f", "node_type": "1", "metadata": {}, "hash": "1dd50cc365ac7171baec5ec6bb115dee0375ed6482c043dd746c129e03435ae6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Most of\nthe CIM compilers neglect the generality in their design. For\nexample, Ambrosi et al. [2] propose a compilation tool that\nschedules matrix-vector computation (MVM) on a ReRAM-\nbased architecture, but its performance degrades when the\nCIM architecture and computing granularity change. Mean-\nwhile, they may also fail to fully explore the scheduling space\nof DNN operators at the corresponding computing level. For\nexample, Han et al.", "mimetype": "text/plain", "start_char_idx": 4032, "end_char_idx": 4473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f4d0025-6967-4729-aa31-5b5d317c521f": {"__data__": {"id_": "8f4d0025-6967-4729-aa31-5b5d317c521f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74b1c6bb-3764-4abf-b9cb-1a2a9f88ff1f", "node_type": "1", "metadata": {}, "hash": "1e31b0ba25138bada3226453f0c465f67d39f346d2e4bc357cd890b37652692b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc9cb447-6d78-4d59-9daa-795b6236467c", "node_type": "1", "metadata": {}, "hash": "0b9f12b2bfff6ebbaa2d0898eddc0c2bb1f0dbdab17286a1ae3cd4f478f1a220", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[22] design a compilation tool to deploy\nDNNs on the ISAAC architecture [39] via an MVM-grained\nprogramming interface, but its optimization strategy stays\nat the computing graph level neglecting the opportunity\nto fine-control the crossbar resource allocation and MVM\noperation sequencing for better results.\nIn this work, we propose a multi-level compilation stack\nfor CIM accelerators to achieve more versatile and efficient\ncompilation. In particular, we first introduce a three-tier\narchitecture abstraction of CIM hardware, from the crossbar\ntier to the chip tier.", "mimetype": "text/plain", "start_char_idx": 4474, "end_char_idx": 5043, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc9cb447-6d78-4d59-9daa-795b6236467c": {"__data__": {"id_": "cc9cb447-6d78-4d59-9daa-795b6236467c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f4d0025-6967-4729-aa31-5b5d317c521f", "node_type": "1", "metadata": {}, "hash": "1dd50cc365ac7171baec5ec6bb115dee0375ed6482c043dd746c129e03435ae6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f01f355a-9948-4a16-9511-043226e6f4c7", "node_type": "1", "metadata": {}, "hash": "9362f7b85eef0d0c2ced4f8fccbe90a19d85632dc0d8d9fb5e52c0e68c022874", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Each tier has corresponding hardware\narchitecture parameters, which can be designated to describe\na particular CIM accelerator. Then, based on the hardware\nabstraction of CIM accelerators, we propose the computing\nmode abstraction with computing granularity ranging from\nfine to coarse to suit various programming interfaces. At\ndifferent computing modes, we abstract the basic operation\ntypes of a CIM accelerator as meta-operator sets, specifically\nsupported operators in the CIM like memory row read/write,\nand MVM operator with crossbars. We can then use the meta-\noperator sets to represent the computation process of DNNs\non CIM accelerators.", "mimetype": "text/plain", "start_char_idx": 5044, "end_char_idx": 5692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f01f355a-9948-4a16-9511-043226e6f4c7": {"__data__": {"id_": "f01f355a-9948-4a16-9511-043226e6f4c7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc9cb447-6d78-4d59-9daa-795b6236467c", "node_type": "1", "metadata": {}, "hash": "0b9f12b2bfff6ebbaa2d0898eddc0c2bb1f0dbdab17286a1ae3cd4f478f1a220", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12e51f59-5338-4bde-b6d9-ac58b4a8efe2", "node_type": "1", "metadata": {}, "hash": "581b63ec7fb8f1361c7c17acb4a8c90b8efd4df24a46101d55e8c603e82e3c46", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Within the provided software and hardware space, we pro-\npose a multi-level scheduling approach to optimize mapping\nand scheduling for different computing modes. According\nto the computing mode given by the target CIM, our policy\nprogressively optimizes computations from coarse-level com-\nputing graphs to fine-level vector computing operations and\ngenerates the corresponding meta-operator flow. Compared\nto existing compilation methods that optimize schedules at\na single computational level, such as the DNN computing\ngraph, our approach offers a more holistic optimization per-\nspective, resulting in higher computing efficiency.", "mimetype": "text/plain", "start_char_idx": 5693, "end_char_idx": 6327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12e51f59-5338-4bde-b6d9-ac58b4a8efe2": {"__data__": {"id_": "12e51f59-5338-4bde-b6d9-ac58b4a8efe2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f01f355a-9948-4a16-9511-043226e6f4c7", "node_type": "1", "metadata": {}, "hash": "9362f7b85eef0d0c2ced4f8fccbe90a19d85632dc0d8d9fb5e52c0e68c022874", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2487ff97-7439-45ec-8ea0-b72b4aa37da6", "node_type": "1", "metadata": {}, "hash": "ab0bf7ad4b4cc59036cb69a5ecce84843279e8c3dd5bfc7b62e067ba152956e7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The contributions of this work are summarized as follows:\n\u2022 This paper proposes a compilation stack, CIM-MLC,\nwhich enables automatic compilation for various CIM\naccelerators. In CIM-MLC, we propose a hardware\nabstraction of hierarchical architecture and computing\nmode of CIM architecture to support a wide range of\nCIM accelerators.", "mimetype": "text/plain", "start_char_idx": 6328, "end_char_idx": 6662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2487ff97-7439-45ec-8ea0-b72b4aa37da6": {"__data__": {"id_": "2487ff97-7439-45ec-8ea0-b72b4aa37da6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12e51f59-5338-4bde-b6d9-ac58b4a8efe2", "node_type": "1", "metadata": {}, "hash": "581b63ec7fb8f1361c7c17acb4a8c90b8efd4df24a46101d55e8c603e82e3c46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "814efabd-7847-40c0-a39c-d8af3757b357", "node_type": "1", "metadata": {}, "hash": "b14e8735a56881ec59b1ad53c9a870b107b14865cb20e88e55b2860d862cd345", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "186\nCIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\n\u2022 Then, we propose a multi-level DNN scheduling ap-\nproach, which realizes the flexible DNN operators map-\nping and scheduling based on the computing granular-\nity exposed by specific CIM accelerators. We conduct\nthorough optimization at each abstraction level to gen-\nerate the operation flow.", "mimetype": "text/plain", "start_char_idx": 6663, "end_char_idx": 7091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "814efabd-7847-40c0-a39c-d8af3757b357": {"__data__": {"id_": "814efabd-7847-40c0-a39c-d8af3757b357", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2487ff97-7439-45ec-8ea0-b72b4aa37da6", "node_type": "1", "metadata": {}, "hash": "ab0bf7ad4b4cc59036cb69a5ecce84843279e8c3dd5bfc7b62e067ba152956e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cda1734a-9df0-43a1-ac37-aead62549627", "node_type": "1", "metadata": {}, "hash": "9fb68c58a2874a2b23347ce8a9e6facc62d9cc049e080225f8bf2b3ec122a187", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The proposed multi-level opti-\nmization flow covers a much broader scheduling space\nthan the previous graph or MVM-level scheduling but\nalso avoids the intractability faced by single-level fine-\ngrained scheduling.\n\u2022 Compared with existing compilation work [22], the\nproposed CIM-MLC can improve the inference speed\nby up to 3.2\u00d7. We also verify the proposed compila-\ntion stack on existing CIM accelerators, PUMA [4],\nCIM work [27] and CIM work [29], to demonstrate\nthe generality of CIM-MLC.", "mimetype": "text/plain", "start_char_idx": 7092, "end_char_idx": 7585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cda1734a-9df0-43a1-ac37-aead62549627": {"__data__": {"id_": "cda1734a-9df0-43a1-ac37-aead62549627", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "814efabd-7847-40c0-a39c-d8af3757b357", "node_type": "1", "metadata": {}, "hash": "b14e8735a56881ec59b1ad53c9a870b107b14865cb20e88e55b2860d862cd345", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa3e398f-2316-4e36-a9e4-706aa288c7d3", "node_type": "1", "metadata": {}, "hash": "bb9297fd1ccbc785dce8ac253fd605d8d6ea878a66568eb2d5c68a851eea036b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Experiments show that\nCIM-MLC can accelerate the inference speed of CIM\nwork [27] and CIM work [29] by 2.3\u00d7 and 3.7\u00d7, respec-\ntively, while reducing the peak power consumption by\n75% for PUMA [4].\n2\nBackground and Motivation\n2.1\nThe Diversity of CIM for DNNs\nArchitecture hierarchy\nProgramming \ninterface\nFine\nCoarse\nSingle-tier\nMulti-tier\n\uf070PRIME [13]\n\uf070ISAAC[39]\n\u25c6Jia  et al.", "mimetype": "text/plain", "start_char_idx": 7586, "end_char_idx": 7961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa3e398f-2316-4e36-a9e4-706aa288c7d3": {"__data__": {"id_": "fa3e398f-2316-4e36-a9e4-706aa288c7d3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cda1734a-9df0-43a1-ac37-aead62549627", "node_type": "1", "metadata": {}, "hash": "9fb68c58a2874a2b23347ce8a9e6facc62d9cc049e080225f8bf2b3ec122a187", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed7bbe97-f402-48e4-9746-a18a91c096a0", "node_type": "1", "metadata": {}, "hash": "ce5a4f9105e393a53ecf003894c0782f98fef6b7bbc0a6206b9c13662b3051c9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[29]\n\uf070PUMA[4]\n\uf070IMDP[19]\n\uf070FPSA[28]\n\u25c6Neural Cache[18]\n\u25c6ConvSRAM[6]\n\uf070Zhu et al.[51]\n\uf070SRE[47]\n\uf070nvCIM[46]\n\uf070XNOR-RRAM[43]\n\u25c6XNOR-SRAM[49]\n\u26abHan et al.", "mimetype": "text/plain", "start_char_idx": 7961, "end_char_idx": 8103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed7bbe97-f402-48e4-9746-a18a91c096a0": {"__data__": {"id_": "ed7bbe97-f402-48e4-9746-a18a91c096a0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa3e398f-2316-4e36-a9e4-706aa288c7d3", "node_type": "1", "metadata": {}, "hash": "bb9297fd1ccbc785dce8ac253fd605d8d6ea878a66568eb2d5c68a851eea036b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f088e130-bfda-4ed1-baec-9f4d208b8a56", "node_type": "1", "metadata": {}, "hash": "2ea07c2b16305ab2488161ac439866f51d955b023da632fb6f5eab5dca0cb3f7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[23]\n\uf070MAX2[34]\n\uf070ReRAM-RNN[33]\n\uf070FORMS[50]\n\u26abGUO  et al.[21]\n\uf070ReRAM-Based\n\u25c6SRAM-Based\n\u26abFLASH-Based\n\u25c6Vesti[48]\nFigure 1. Diversity of CIM architecture.\nResearchers have proposed various CIM-based DNN accel-\nerators.", "mimetype": "text/plain", "start_char_idx": 8103, "end_char_idx": 8314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f088e130-bfda-4ed1-baec-9f4d208b8a56": {"__data__": {"id_": "f088e130-bfda-4ed1-baec-9f4d208b8a56", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed7bbe97-f402-48e4-9746-a18a91c096a0", "node_type": "1", "metadata": {}, "hash": "ce5a4f9105e393a53ecf003894c0782f98fef6b7bbc0a6206b9c13662b3051c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "730c6909-f24b-45c4-80aa-f8eaac484169", "node_type": "1", "metadata": {}, "hash": "fe142359ebe524e2f0df6e80c75eae82cdcf37f7ef768f9249f93bcc8a5fa0f2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We sort out the designs of recent CIM accelerators\nfrom three dimensions: memory device, architecture hier-\narchy, and programming interface, and summarize them\nin Figure 1 [4, 6, 13, 18, 19, 21, 23, 28, 29, 33, 34, 39, 43,\n46\u201351]. SRAM, ReRAM, and FLASH are included in the di-\nmension of the memory device. Their different read/write\nlatency and storage density affect the data mapping and\nscheduling policy of these CIMs.", "mimetype": "text/plain", "start_char_idx": 8315, "end_char_idx": 8739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "730c6909-f24b-45c4-80aa-f8eaac484169": {"__data__": {"id_": "730c6909-f24b-45c4-80aa-f8eaac484169", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f088e130-bfda-4ed1-baec-9f4d208b8a56", "node_type": "1", "metadata": {}, "hash": "2ea07c2b16305ab2488161ac439866f51d955b023da632fb6f5eab5dca0cb3f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "baac4dc3-e5ae-4510-a94d-2c8e42948aca", "node_type": "1", "metadata": {}, "hash": "c765e1f0431135287a866cc719c8fe585978f929261d8fb2e277454860eab54c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For example, the write la-\ntency of ReRAM/FLASH is relatively long, so CIMs based\non ReRAM/FLASH often ford write operations during com-\nputation (e.g., Guo et al. [21] and PRIME [13]).", "mimetype": "text/plain", "start_char_idx": 8740, "end_char_idx": 8925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baac4dc3-e5ae-4510-a94d-2c8e42948aca": {"__data__": {"id_": "baac4dc3-e5ae-4510-a94d-2c8e42948aca", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "730c6909-f24b-45c4-80aa-f8eaac484169", "node_type": "1", "metadata": {}, "hash": "fe142359ebe524e2f0df6e80c75eae82cdcf37f7ef768f9249f93bcc8a5fa0f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33c17ef5-a17b-44fe-8ac8-19391e0cde9f", "node_type": "1", "metadata": {}, "hash": "fb56b085923af758bc9b399b5f0407a7c8a0e6232952ca611ea3dbb78d454c96", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As for the\narchitecture hierarchy, some CIM designs hierarchically or-\nganize tiles, cores, and crossbars from top to bottom (e.g.,\nDAC \nS&H \nIR\nS&A\nOR\nADC \nweight\nactivation\nMemory unit\nweight\nactivation\nComputing  unit\nload\nstore\n(a)\n(b)\nFigure 2. Computation dataflow comparison of (a) CIM and\n(b) traditional architecture.", "mimetype": "text/plain", "start_char_idx": 8926, "end_char_idx": 9252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33c17ef5-a17b-44fe-8ac8-19391e0cde9f": {"__data__": {"id_": "33c17ef5-a17b-44fe-8ac8-19391e0cde9f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "baac4dc3-e5ae-4510-a94d-2c8e42948aca", "node_type": "1", "metadata": {}, "hash": "c765e1f0431135287a866cc719c8fe585978f929261d8fb2e277454860eab54c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3816d36f-9bf2-4ec3-85ae-e8a886a8e490", "node_type": "1", "metadata": {}, "hash": "2408c89baf9b8401816cfd28e3a477b37a0f0fbaa771346387f2fc8fb7093caf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "ISSAC [39] and MAX2 [34] ) while others do not (e.g., a\nsingle-tier hierarchy only having tiled crossbars in ConvS-\nRAM [6] and nvCIM [46]). Additionally, even though ISAAC\nand PUMA [4, 39] have a similar hierarchical structure, they\nstill use different crossbar sizes and the number of crossbars.\nAccordingly, their model mapping ways are different. The\nscheduling space during the compilation is different as well.", "mimetype": "text/plain", "start_char_idx": 9253, "end_char_idx": 9669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3816d36f-9bf2-4ec3-85ae-e8a886a8e490": {"__data__": {"id_": "3816d36f-9bf2-4ec3-85ae-e8a886a8e490", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33c17ef5-a17b-44fe-8ac8-19391e0cde9f", "node_type": "1", "metadata": {}, "hash": "fb56b085923af758bc9b399b5f0407a7c8a0e6232952ca611ea3dbb78d454c96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "901c803f-e256-42e5-9aa7-c8304deb0cf9", "node_type": "1", "metadata": {}, "hash": "e384f7d7369e9a34890de8c0f439c469859d20bea405a9e161a016e1024cd13f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The programming interface refers to the computing granu-\nlarity that CIM can support from the users\u2019 view. As for the\ncoarse-grained programming interface, the DNN computa-\ntion is decomposed into convolution operations and then is\nperformed by CIM cores (e.g., Jia et al.\u2019s work [29]), while\nfor the fine-grained programming interface, the bit-wise\nvector operations are supported by CIM crossbars (e.g., Con-\nvSRAM [6]).", "mimetype": "text/plain", "start_char_idx": 9670, "end_char_idx": 10092, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "901c803f-e256-42e5-9aa7-c8304deb0cf9": {"__data__": {"id_": "901c803f-e256-42e5-9aa7-c8304deb0cf9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3816d36f-9bf2-4ec3-85ae-e8a886a8e490", "node_type": "1", "metadata": {}, "hash": "2408c89baf9b8401816cfd28e3a477b37a0f0fbaa771346387f2fc8fb7093caf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d26c473d-a378-4bc7-a8de-914beb018f85", "node_type": "1", "metadata": {}, "hash": "f0c2a440fac068c5d970cb5b98862cbf3becc7e1f4b2b9f98e212ce85439fac4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Other works propose to decompose DNN op-\nerators and schedule MVM-grained operation in CIM cross-\nbars [4, 19, 36, 43, 49]. To sum up, the diversity of existing\nCIMs for DNN raises a growing demand for an efficient gen-\neral compilation tool.\n2.2\nCompilation Tools for CIM\nNeural network compilers for different computing archi-\ntectures have been widely discussed in prior works, such\nas TVM [9].", "mimetype": "text/plain", "start_char_idx": 10093, "end_char_idx": 10490, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d26c473d-a378-4bc7-a8de-914beb018f85": {"__data__": {"id_": "d26c473d-a378-4bc7-a8de-914beb018f85", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "901c803f-e256-42e5-9aa7-c8304deb0cf9", "node_type": "1", "metadata": {}, "hash": "e384f7d7369e9a34890de8c0f439c469859d20bea405a9e161a016e1024cd13f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ee57639-b5e4-4b9c-b67b-b8e0a2c8ba81", "node_type": "1", "metadata": {}, "hash": "b6126bbcc4089dfef48fac54f13501f393e093323978af9cc07bc6b1c6db6b9f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "However, existing machine learning compil-\ners primarily focus on compilation optimization related to\ntraditional architecture hardware where the memory and\ncomputing units are separated [7, 10, 14, 20, 37, 38, 44]. As\nFigure 2 shows, traditional architecture and CIM are totally\ndifferent computation paradigms. As for traditional archi-\ntectures being bottlenecked by memory accesses, their com-\npilation tools are composed of optimization methods for\nmemory efficiency improvement.", "mimetype": "text/plain", "start_char_idx": 10491, "end_char_idx": 10975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ee57639-b5e4-4b9c-b67b-b8e0a2c8ba81": {"__data__": {"id_": "0ee57639-b5e4-4b9c-b67b-b8e0a2c8ba81", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d26c473d-a378-4bc7-a8de-914beb018f85", "node_type": "1", "metadata": {}, "hash": "f0c2a440fac068c5d970cb5b98862cbf3becc7e1f4b2b9f98e212ce85439fac4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2151de96-b998-4774-b23a-fae618ed43ed", "node_type": "1", "metadata": {}, "hash": "98e9f79ae8783ce1ac66f80a37784d3c634725ad250ac61f0b6df94c630c8e18", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For instance, the popular\nloop unrolling and unfolding approaches in the traditional\ncompilation tools are for improving data locality [31]. In\ncontrast, CIM architectures performing computations inside\nmemory [25, 39] shall focus on improving the efficiency of\nin-situ computation instead of improving the memory ef-\nficiency in the compilation techniques and hence desire a\nspecific compilation tool.\nSome researchers have noticed the problem and developed\nsome CIM compilation tools [17, 22].", "mimetype": "text/plain", "start_char_idx": 10976, "end_char_idx": 11471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2151de96-b998-4774-b23a-fae618ed43ed": {"__data__": {"id_": "2151de96-b998-4774-b23a-fae618ed43ed", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ee57639-b5e4-4b9c-b67b-b8e0a2c8ba81", "node_type": "1", "metadata": {}, "hash": "b6126bbcc4089dfef48fac54f13501f393e093323978af9cc07bc6b1c6db6b9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edd47315-333b-46bb-953c-211b0ec105c8", "node_type": "1", "metadata": {}, "hash": "7ad519ef4d4954778312128ec931f56491eeb5a2466530b7f0dcee0a2b4b4063", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "However, we observe\nthese works are inadequate as a general compilation tool\nfor CIM when summarizing them from the dimensions of\n187\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nSongyun Qu, Shixin Zhao, Bing Li, Yintao He, Xuyi Cai, Lei Zhang, and Ying Wang\nTable 1. Comparison of the generality of this work and the existing works.\nSupported Device Type\nSupported Programming Interface\nOptimization Granularity\nSRAM\nReRAM\nMISC(e.g. PCM,", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edd47315-333b-46bb-953c-211b0ec105c8": {"__data__": {"id_": "edd47315-333b-46bb-953c-211b0ec105c8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2151de96-b998-4774-b23a-fae618ed43ed", "node_type": "1", "metadata": {}, "hash": "98e9f79ae8783ce1ac66f80a37784d3c634725ad250ac61f0b6df94c630c8e18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29f86f38-2e31-45f2-822c-4ee5ebb15574", "node_type": "1", "metadata": {}, "hash": "c832d6e1da6f9839ef4644898963cf290fb2b160ff054c9ef9399347f7068d5a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "FLASH)\nVVM\nMVM\nDNN Operators\n/\nPUMA [2, 4]\n\u00d7\n\u2713\n/\n\u00d7\n\u2713\n\u00d7\nMVM\nIMDP [19]\n\u00d7\n\u2713\n/\n\u2713\n\u2713\n\u00d7\nMVM\nTC-CIM [17]\n\u00d7\n\u2713\n/\n\u00d7\n\u2713\n\u00d7\nMVM\nPolyhedral-based [22]\n\u00d7\n\u2713\n/\n\u00d7\n\u2713\n\u2713\nMVM, MM,", "mimetype": "text/plain", "start_char_idx": 11919, "end_char_idx": 12074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29f86f38-2e31-45f2-822c-4ee5ebb15574": {"__data__": {"id_": "29f86f38-2e31-45f2-822c-4ee5ebb15574", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edd47315-333b-46bb-953c-211b0ec105c8", "node_type": "1", "metadata": {}, "hash": "7ad519ef4d4954778312128ec931f56491eeb5a2466530b7f0dcee0a2b4b4063", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48e6855e-0ba8-453e-89f3-ece7ae5a3d53", "node_type": "1", "metadata": {}, "hash": "347ccdc26587343223f6efd95e95887d9232a51c12a5b7660a6ebcbe8b9c65e9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Conv\nOCC [40]\n\u2713\n\u2713\n/\n\u2713\n\u2713\n\u00d7\n/\nOurs\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nVVM, MVM, DNN Operators\nCG-grained optimization\nSection 3.3.2\nMVM-grained \noptimization\nSection 3.3.3\nNN model\nVVM-grained \noptimization\nSection 3.3.", "mimetype": "text/plain", "start_char_idx": 12075, "end_char_idx": 12269, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48e6855e-0ba8-453e-89f3-ece7ae5a3d53": {"__data__": {"id_": "48e6855e-0ba8-453e-89f3-ece7ae5a3d53", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29f86f38-2e31-45f2-822c-4ee5ebb15574", "node_type": "1", "metadata": {}, "hash": "c832d6e1da6f9839ef4644898963cf290fb2b160ff054c9ef9399347f7068d5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bb6ebde-86a8-4f61-a8e8-f8a08d861e66", "node_type": "1", "metadata": {}, "hash": "075a414ff97a7b8d3468259b89fce2e7ec84a59b06b8fa8dd35a9ea65a8b0fae", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4\nChip-tier abstraction\nCore-tier abstraction\nCrossbar-tier abstraction\nCIM is XBM or \nWLM\nCIM is WLM\nY\nN\no\nY\nMeta-operator flow\nUse meta-operators specific \nto CM\nMeta-operator flow\nUse meta-operators specific \nto XBM\nMeta-operator flow\nUse meta-operators specific \nto WLM\nEnd\nCIM \narchitecture\nHardware  abstraction\nArchitecture parameters\n(Abs-arch)\n&\nComputing mode\n(Abs-com)\nSection 3.2.1-3.2.", "mimetype": "text/plain", "start_char_idx": 12269, "end_char_idx": 12667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bb6ebde-86a8-4f61-a8e8-f8a08d861e66": {"__data__": {"id_": "3bb6ebde-86a8-4f61-a8e8-f8a08d861e66", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48e6855e-0ba8-453e-89f3-ece7ae5a3d53", "node_type": "1", "metadata": {}, "hash": "347ccdc26587343223f6efd95e95887d9232a51c12a5b7660a6ebcbe8b9c65e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "786a24a6-813a-4c69-befa-69d4b5e89a35", "node_type": "1", "metadata": {}, "hash": "06b491ac0fbd6d2678c8eb6a75c11935a47246d71a27cbd75a66f6da084db00b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "3\nInput \nN\no\nEnd\nOutput \nMode \nInfo\nArch.\nInfo\nFigure 3. Overall workflow of CIM-MLC.\nthe device type, programming interface, and optimization\ngranularity (as shown in Table 1).\nConsidering the device types involved, PUMA [2, 4] and\nIMDP [19] present compilation methodologies tailored to\nparticular CIM architectures that rely on ReRAM. However,\nthese approaches lack interoperability with alternative de-\nvices and programming interfaces. A few works have ex-\nplored general compilation frameworks.", "mimetype": "text/plain", "start_char_idx": 12667, "end_char_idx": 13167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "786a24a6-813a-4c69-befa-69d4b5e89a35": {"__data__": {"id_": "786a24a6-813a-4c69-befa-69d4b5e89a35", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bb6ebde-86a8-4f61-a8e8-f8a08d861e66", "node_type": "1", "metadata": {}, "hash": "075a414ff97a7b8d3468259b89fce2e7ec84a59b06b8fa8dd35a9ea65a8b0fae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7bb60d6-70df-4d54-acad-faeebb7d76d0", "node_type": "1", "metadata": {}, "hash": "b5bfe49f6cec095dfbc32adfd0e9cecffafd11ab856df072179ea3f53420851c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "TC-CIM [17] and\nPolyhedral-based [22] harness polyhedral models to facil-\nitate the compilation process on CIM accelerators, which\nautomatically identify and map MVM operations inherent\nin DNNs onto CIM. However, these two works also mainly\nfocus on ReRAM-based CIM and assume that there are ample\nmemory resources available for parameter loading or dupli-\ncation, which overlooks crucial facts of resource-constrained\nsituations that are typically encountered in SRAM-based\nCIMs.", "mimetype": "text/plain", "start_char_idx": 13168, "end_char_idx": 13648, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7bb60d6-70df-4d54-acad-faeebb7d76d0": {"__data__": {"id_": "c7bb60d6-70df-4d54-acad-faeebb7d76d0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "786a24a6-813a-4c69-befa-69d4b5e89a35", "node_type": "1", "metadata": {}, "hash": "06b491ac0fbd6d2678c8eb6a75c11935a47246d71a27cbd75a66f6da084db00b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e839550d-9e47-4b97-94c1-2e73bebe411d", "node_type": "1", "metadata": {}, "hash": "b459b45d340d7b578e44d0892105b0761b08f532af4ae670e3e5d7a56b405c9c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Relatively, OCC [40] is a comprehensive compilation\nthat encompasses abundant device types as well as numer-\nous programming interfaces. Built upon a specialized MILR\nwith an ISA, this work enables the optimization at various\nlevels of granularity. Nonetheless, this work does not incor-\nporate the coarser-grained programming interface like the\nDNN operator and does not explore the mapping optimiza-\ntion of DNN computation on CIMs. To summarize, current\nresearch on CIM compilers lacks sufficient abstraction of\nhardware and is limited in support for various architectures\nand programming interfaces in CIM accelerators.", "mimetype": "text/plain", "start_char_idx": 13649, "end_char_idx": 14272, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e839550d-9e47-4b97-94c1-2e73bebe411d": {"__data__": {"id_": "e839550d-9e47-4b97-94c1-2e73bebe411d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7bb60d6-70df-4d54-acad-faeebb7d76d0", "node_type": "1", "metadata": {}, "hash": "b5bfe49f6cec095dfbc32adfd0e9cecffafd11ab856df072179ea3f53420851c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5816647a-73e2-4ac7-a11a-057ddc2eb376", "node_type": "1", "metadata": {}, "hash": "99b17b324fe6bf86b874e338caed68a7202539565490ccf72b816142a93c3299", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Regarding optimization granularity, most of the works [2,\n4, 17, 22] support the mapping and optimization of DNN at\nthe level of MVM operations. However, due to the increased\ncomplexity of the scheduling space, existing work has not\nfully explored the optimization possibilities at the MVM\ngranularity. For example, some of them [2, 4, 22] support\nthe pipeline inter-network layers but do not consider the\ncomputing pipeline opportunity at the MVM-grained.", "mimetype": "text/plain", "start_char_idx": 14273, "end_char_idx": 14729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5816647a-73e2-4ac7-a11a-057ddc2eb376": {"__data__": {"id_": "5816647a-73e2-4ac7-a11a-057ddc2eb376", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e839550d-9e47-4b97-94c1-2e73bebe411d", "node_type": "1", "metadata": {}, "hash": "b459b45d340d7b578e44d0892105b0761b08f532af4ae670e3e5d7a56b405c9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84f716f5-d65f-41cd-9889-b277271a8a23", "node_type": "1", "metadata": {}, "hash": "315946bc2a73b76f7feda9e2c8b1e22aacecd0f4a982dbfbea6a284de096a07e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In\nfact, optimizing the scheduling of MVM operations helps to\nfurther optimize computing efficiency.\n3\nMethodology\n3.1\nOverview\nFigure 3 demonstrates the overview of the proposed CIM-\nMLC. Existing CIM compilation works have poor generality\nbecause of their single-level scheduling policy, while CIM-\nMLC is a general compiler that features unified abstraction\nfrom diverse hardware and multi-level scheduling with abun-\ndant meta-operators. The hardware abstraction provides the\nsame description interface of architecture parameters (Abs-\narch) and computing mode (Abs-com) to various CIM designs.", "mimetype": "text/plain", "start_char_idx": 14730, "end_char_idx": 15328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84f716f5-d65f-41cd-9889-b277271a8a23": {"__data__": {"id_": "84f716f5-d65f-41cd-9889-b277271a8a23", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5816647a-73e2-4ac7-a11a-057ddc2eb376", "node_type": "1", "metadata": {}, "hash": "99b17b324fe6bf86b874e338caed68a7202539565490ccf72b816142a93c3299", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c25a12c-5d26-411f-a335-cde275a1cdb3", "node_type": "1", "metadata": {}, "hash": "134c9f966df38e8c45a9449e3c684d9291a29812d2837d4b374ff942aed4f49b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To decouple the data mapping and computing scheduling\nwith one architectural design, we propose the multi-level\nscheduling technology to handle the computing mode for dif-\nferent architectural tiers in CIMs. The multi-level scheduler\ntailors the optimization method for each computing mode,\napplies the optimization method independently or jointly ac-\ncording to the abstraction of the CIM accelerator, and finally\ngenerates the meta-operator flow for the CIM accelerator.\n3.2\nCIM Hardware Abstraction\nIn order to support a wide range of CIM accelerators, we\nfirst construct the hardware abstraction,", "mimetype": "text/plain", "start_char_idx": 15329, "end_char_idx": 15929, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c25a12c-5d26-411f-a335-cde275a1cdb3": {"__data__": {"id_": "9c25a12c-5d26-411f-a335-cde275a1cdb3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84f716f5-d65f-41cd-9889-b277271a8a23", "node_type": "1", "metadata": {}, "hash": "315946bc2a73b76f7feda9e2c8b1e22aacecd0f4a982dbfbea6a284de096a07e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4078c9a-7d16-4a64-b8d7-b39677f41d80", "node_type": "1", "metadata": {}, "hash": "921a7564c59a61b25dd0e7d27629d2bca2dd75486dc36295b479b7c43f02e1ec", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "covering two key\n188\nCIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nCore\nL0 Buffer\nC\nT\nR\nL\nALU\nXB\nXB\nXB\nXB\nXB\nXB\nXB\nXB\nXB\nL1 Buffer\nC\nT\nR\nL\nALU\n(b) Core\n(a) Chip\nSRAM\nReg", "mimetype": "text/plain", "start_char_idx": 15930, "end_char_idx": 16223, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4078c9a-7d16-4a64-b8d7-b39677f41d80": {"__data__": {"id_": "c4078c9a-7d16-4a64-b8d7-b39677f41d80", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c25a12c-5d26-411f-a335-cde275a1cdb3", "node_type": "1", "metadata": {}, "hash": "134c9f966df38e8c45a9449e3c684d9291a29812d2837d4b374ff942aed4f49b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9db0b3f3-6a36-4d90-8b1a-fdb0dcf551af", "node_type": "1", "metadata": {}, "hash": "a201b29a2e15645638823c238be76eac246c88eaf8cbca82ff254a0d6e54c335", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "File\nD\nA\nC\nADC\nC\nT\nR\nL\n(c) Crossbar (XB)\nA\nC\nB\nD\nA\nC\nB\nD\nC\nC\nA\nB\nActivate one or more cores\nCore\nCore\nCore\nCore\nBuffer\nC\nT\nR\nL\nA\nL\nU\nCM\n(d)\nXBM\n(e)\nWLM\n(f)\nActivate partial rows \nReRAM\nFlash,", "mimetype": "text/plain", "start_char_idx": 16224, "end_char_idx": 16415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9db0b3f3-6a36-4d90-8b1a-fdb0dcf551af": {"__data__": {"id_": "9db0b3f3-6a36-4d90-8b1a-fdb0dcf551af", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4078c9a-7d16-4a64-b8d7-b39677f41d80", "node_type": "1", "metadata": {}, "hash": "921a7564c59a61b25dd0e7d27629d2bca2dd75486dc36295b479b7c43f02e1ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2391d070-3b54-4f15-b97b-3dfa4adacfbe", "node_type": "1", "metadata": {}, "hash": "2f35267a3dcc1f9365fd0863d1eb74f4688f5a527a98b78a7429d618407bea65", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "STT-MRAM\n\u2026\u2026\nD\nActivate one or more physical XBs\nXB\nXB\nXB\nXB\nBuffer\nC\nT\nR\nL\nA\nL\nU\nD\nA\nC\nADC\nC\nT\nR\nL\nReg File\nFigure 4. CIM Hardware Abstraction.\ndescription\nformat\n\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52_\ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f\n[number of cores per row * \nnumber of cores per", "mimetype": "text/plain", "start_char_idx": 16416, "end_char_idx": 16638, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2391d070-3b54-4f15-b97b-3dfa4adacfbe": {"__data__": {"id_": "2391d070-3b54-4f15-b97b-3dfa4adacfbe", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9db0b3f3-6a36-4d90-8b1a-fdb0dcf551af", "node_type": "1", "metadata": {}, "hash": "a201b29a2e15645638823c238be76eac246c88eaf8cbca82ff254a0d6e54c335", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e49f2d8-c264-4fe5-8041-b96847310cd0", "node_type": "1", "metadata": {}, "hash": "96d6b1b6c6ad09d43a6f5fada3eed6c0b1f172744698b6228489e0811c69958d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "column]\nparameter\n\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52_\ud835\udc5b\ud835\udc5c\ud835\udc50\n\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52_\ud835\udc5b\ud835\udc5c\ud835\udc50_\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc61\n\ud835\udc3f0 \ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52\n\ud835\udc3f0 \ud835\udc35\ud835\udc4a\n\ud835\udc34\ud835\udc3f\ud835\udc48\nThe number of cores in the chip\nNetwork on chip(NoC) type \nNetwork on chip(NoC) cost\nDigit computing capacity\nGlobal buffer bandwidth\n'Mesh',", "mimetype": "text/plain", "start_char_idx": 16639, "end_char_idx": 16839, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e49f2d8-c264-4fe5-8041-b96847310cd0": {"__data__": {"id_": "1e49f2d8-c264-4fe5-8041-b96847310cd0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2391d070-3b54-4f15-b97b-3dfa4adacfbe", "node_type": "1", "metadata": {}, "hash": "2f35267a3dcc1f9365fd0863d1eb74f4688f5a527a98b78a7429d618407bea65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bcd02b2-99a1-4a13-9043-20a1c8f432c8", "node_type": "1", "metadata": {}, "hash": "a04f634d5d142c8589fc5c35ad88705d30e36d0df56cc1b3de6161ff7554efbf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "'H-tree', etc\nX kb\nGlobal buffer size\nX operations per second\nmatrix record the data transfer \ncost between each core\nX b/cycle\nFigure 5. Chip tier architecture abstraction parameters.\ndescription\nformat\n\ud835\udc65\ud835\udc4f_\ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5f\n[number of xbs per row * \nnumber of xbs per", "mimetype": "text/plain", "start_char_idx": 16840, "end_char_idx": 17097, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bcd02b2-99a1-4a13-9043-20a1c8f432c8": {"__data__": {"id_": "1bcd02b2-99a1-4a13-9043-20a1c8f432c8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e49f2d8-c264-4fe5-8041-b96847310cd0", "node_type": "1", "metadata": {}, "hash": "96d6b1b6c6ad09d43a6f5fada3eed6c0b1f172744698b6228489e0811c69958d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18be9a7b-ae16-412f-917d-8a7127b47d69", "node_type": "1", "metadata": {}, "hash": "09559133b8a99dbdd2134648a3b1c6f59572909d4fa099edfde44a1e522b68ac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "column]\nparameter\n\ud835\udc65\ud835\udc4f_\ud835\udc5b\ud835\udc5c\ud835\udc50\n\ud835\udc65\ud835\udc4f_\ud835\udc5b\ud835\udc5c\ud835\udc50_\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc61\n\ud835\udc3f1 \ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52\n\ud835\udc3f1 \ud835\udc35\ud835\udc4a\n\ud835\udc34\ud835\udc3f\ud835\udc48\nThe number of xbs in the core\nNoC type \nNoC cost\nDigit computing capacity\nLocal buffer bandwidth\n'Mesh', 'H-tree',", "mimetype": "text/plain", "start_char_idx": 17098, "end_char_idx": 17267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18be9a7b-ae16-412f-917d-8a7127b47d69": {"__data__": {"id_": "18be9a7b-ae16-412f-917d-8a7127b47d69", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bcd02b2-99a1-4a13-9043-20a1c8f432c8", "node_type": "1", "metadata": {}, "hash": "a04f634d5d142c8589fc5c35ad88705d30e36d0df56cc1b3de6161ff7554efbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e920e28-34e8-4f9e-acef-2998fea02905", "node_type": "1", "metadata": {}, "hash": "b3e514417055b520b14099a574dbda3f0e874e662da077eca3c58b2c87565017", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "etc\nX kb\nLocal buffer size\nX operations per second\nmatrix record the data transfer \ncost between each xb\nX b/cycle\nFigure 6. Core tier architecture abstraction parameters.\naspects: the architecture parameters (Abs-arch) and the com-\nputing modes (Abs-com), respectively. We model the CIM-\nbased DNN accelerator as a hierarchical architecture, which\ncontains three tiers from top to bottom: (a) chip, (b) core,\nand (c) crossbar. Meanwhile, similar architectures may sup-\nport different levels of CIM operation granularity.", "mimetype": "text/plain", "start_char_idx": 17268, "end_char_idx": 17789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e920e28-34e8-4f9e-acef-2998fea02905": {"__data__": {"id_": "0e920e28-34e8-4f9e-acef-2998fea02905", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18be9a7b-ae16-412f-917d-8a7127b47d69", "node_type": "1", "metadata": {}, "hash": "09559133b8a99dbdd2134648a3b1c6f59572909d4fa099edfde44a1e522b68ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff328eb8-1f09-4dcd-8706-dd160b6f7cdd", "node_type": "1", "metadata": {}, "hash": "9b62b0d2c7715e0b81ac049791601cd466644ab10d932bab92ba756d90ff2d0c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Some CIM\ndesigns only support specific operators or fixed-granularity\nMVM computation, while others offer interfaces for con-\ntrolling rows, enabling a wide range of computation. So,\nwe design a three-level computing abstraction for CIM. As\nillustrated in Figure 4(d)-(f), the top-to-bottom levels are\nCore Mode (CM), Crossbar Mode (XBM), and Wordline Mode\n(WLM), denoting the coarse-grained to fine-grained DNN\noperators, respectively.", "mimetype": "text/plain", "start_char_idx": 17790, "end_char_idx": 18226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff328eb8-1f09-4dcd-8706-dd160b6f7cdd": {"__data__": {"id_": "ff328eb8-1f09-4dcd-8706-dd160b6f7cdd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e920e28-34e8-4f9e-acef-2998fea02905", "node_type": "1", "metadata": {}, "hash": "b3e514417055b520b14099a574dbda3f0e874e662da077eca3c58b2c87565017", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "166b8eab-c60e-4a52-83fa-948ba2ec2109", "node_type": "1", "metadata": {}, "hash": "e3423429ebc7ec6a26ba763f8687fe93d4adaa97c01ce945c682147d42fe61b0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Architecture abstraction tiers and\ncomputing mode abstraction levels maintain a one-to-one\ncorrespondence. The hardware scheduling granularity pro-\nvided by the CIM architecture determines the supported com-\nputing mode and the architecture abstraction parameters ex-\nposed to the compiler. We also develop a comprehensive set\nof meta-operators, which facilitate compilation scheduling\noptimization and instruction generation and enable users to\ndefine and customize hardware-supported operations within\nour framework.\n3.2.1\nChip tier architecture abstraction and core mode.", "mimetype": "text/plain", "start_char_idx": 18227, "end_char_idx": 18801, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "166b8eab-c60e-4a52-83fa-948ba2ec2109": {"__data__": {"id_": "166b8eab-c60e-4a52-83fa-948ba2ec2109", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff328eb8-1f09-4dcd-8706-dd160b6f7cdd", "node_type": "1", "metadata": {}, "hash": "9b62b0d2c7715e0b81ac049791601cd466644ab10d932bab92ba756d90ff2d0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f486df71-0c06-45b1-a45a-5c6f662ba243", "node_type": "1", "metadata": {}, "hash": "10b3f726ce7818c2c6740eff6af58bec97674c68fceb35f8b57ec2dea17031b3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As shown in the Figure 4(a), in the chip tier, multiple cores\n( A ) connected via a network-on-chip (NoC C ) are grouped\ninto a single chip, each used for operators like convolution.\nA shared on-chip memory ( D ) is used for data storage, and\ndigital arithmetic logic units (ALU B ) are used for computa-\ntions beyond the scope of CIM-supported operators.\nAt this tier, the minimum scheduling granularity provided\nby the CIM architecture is core. We abstract its computing\nmode as core mode (CM).", "mimetype": "text/plain", "start_char_idx": 18802, "end_char_idx": 19298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f486df71-0c06-45b1-a45a-5c6f662ba243": {"__data__": {"id_": "f486df71-0c06-45b1-a45a-5c6f662ba243", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "166b8eab-c60e-4a52-83fa-948ba2ec2109", "node_type": "1", "metadata": {}, "hash": "e3423429ebc7ec6a26ba763f8687fe93d4adaa97c01ce945c682147d42fe61b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e0694ca-48b9-4de4-9c75-9cc96c87f3dd", "node_type": "1", "metadata": {}, "hash": "42aada2f149fcfc4fe0ac3e11539437f814f92c6693ab778ffe2d2b44f7bb695", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In this mode, the compiler assigns\none or more cores to complete one DNN operator (e.g., con-\nvolution) according to the operator\u2019s demand and the core\u2019s\ncapability. The scale of computation supported by a single\ncore and the total number of cores determine the maximum\nnumber of operators that can be concurrently mapped on\na chip. Hence, we employ the parameter core_number to\nrecord the total number of cores within the chip, thus ab-\nstracting the chip\u2019s CIM computational capacity.", "mimetype": "text/plain", "start_char_idx": 19299, "end_char_idx": 19785, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e0694ca-48b9-4de4-9c75-9cc96c87f3dd": {"__data__": {"id_": "7e0694ca-48b9-4de4-9c75-9cc96c87f3dd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f486df71-0c06-45b1-a45a-5c6f662ba243", "node_type": "1", "metadata": {}, "hash": "10b3f726ce7818c2c6740eff6af58bec97674c68fceb35f8b57ec2dea17031b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f01407aa-78c3-4d06-beb4-ace5d05edda1", "node_type": "1", "metadata": {}, "hash": "a6ab5fd333365d88727e4d7a3ee405a7382db4ac5bd3426338636e6a5332aec1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The ALU performs commonly used neural network opera-\ntors such as activation functions, pooling, and CIM-specific\noperations like shift-accumulate. ALU\u2019s functionality and\ncomputational speed impact the compilation of digital com-\nputation operators. We use ALU to denote the ALU com-\nputation speed and the functionality will be recorded by\nmeta-operator, which we will show in Section 3.3.2.\nThe type of on-chip network and data transfer rate de-\ntermine the data transfer efficiency between cores.", "mimetype": "text/plain", "start_char_idx": 19786, "end_char_idx": 20286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f01407aa-78c3-4d06-beb4-ace5d05edda1": {"__data__": {"id_": "f01407aa-78c3-4d06-beb4-ace5d05edda1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e0694ca-48b9-4de4-9c75-9cc96c87f3dd", "node_type": "1", "metadata": {}, "hash": "42aada2f149fcfc4fe0ac3e11539437f814f92c6693ab778ffe2d2b44f7bb695", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec137a4a-1c66-483b-85ab-cb3e5f97c354", "node_type": "1", "metadata": {}, "hash": "49de9217b681586c68ba8870c31d747ad3c2048d2d7b174c9c7908ebb51e99fd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We use\ncore_noc and core_noc_cost to abstract the on-chip net-\nwork. Meanwhile, as the storage capacity and bandwidth\nof the global buffer jointly determine the execution time of\ndata movement, L0 size and L0 BW are employed to record\non-chip buffer capacity and bandwidth.\nThe architecture parameters in chip tier are summarized\nin the Figure 5. We use the above mentioned parameters to\nabstract the CIM architecture characteristics in the chip tier\nand expose the parameters to the compilers in CM.", "mimetype": "text/plain", "start_char_idx": 20287, "end_char_idx": 20787, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec137a4a-1c66-483b-85ab-cb3e5f97c354": {"__data__": {"id_": "ec137a4a-1c66-483b-85ab-cb3e5f97c354", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f01407aa-78c3-4d06-beb4-ace5d05edda1", "node_type": "1", "metadata": {}, "hash": "a6ab5fd333365d88727e4d7a3ee405a7382db4ac5bd3426338636e6a5332aec1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7488f89b-8a3f-40cc-94cb-73d30ca8d5ee", "node_type": "1", "metadata": {}, "hash": "2727b39e0a6022fda3dee49ae3818c3765ca3c21080a5e7d8cfffcad5f90ce29", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "CIM-\nMLC will combine the characteristics of operators in DNN\nand CIM architecture parameters to optimize latency and\nenergy consumption during operator mapping.\n3.2.2\nCore tier architecture abstraction and crossbar\nmode. As shown in the Figure 4 (b), the core tier abstraction\nincludes the features within a core, which consists of multiple\n189\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nSongyun Qu, Shixin Zhao, Bing Li, Yintao He, Xuyi Cai, Lei Zhang,", "mimetype": "text/plain", "start_char_idx": 20788, "end_char_idx": 21251, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7488f89b-8a3f-40cc-94cb-73d30ca8d5ee": {"__data__": {"id_": "7488f89b-8a3f-40cc-94cb-73d30ca8d5ee", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec137a4a-1c66-483b-85ab-cb3e5f97c354", "node_type": "1", "metadata": {}, "hash": "49de9217b681586c68ba8870c31d747ad3c2048d2d7b174c9c7908ebb51e99fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30b68c42-066e-4641-affa-25da36e97709", "node_type": "1", "metadata": {}, "hash": "f6b74f60893d8c75c528e585838b1e8fc029515aa96127c637b62ec27a9b5e8d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "and Ying Wang\nCIM crossbar \nDimension\nMatrix\nDimension\nR\nC\nB\nXB\nXBR\nXBC\nR : matrix row           \nC : matrix column             \nB : data bit-width  \nXB   : crossbar          \nXBR : crossbar row            \nXBC : crossbar column\nFigure 7. Dimension binding for DNNs mapping on CIM\ncrossbar.", "mimetype": "text/plain", "start_char_idx": 21252, "end_char_idx": 21542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30b68c42-066e-4641-affa-25da36e97709": {"__data__": {"id_": "30b68c42-066e-4641-affa-25da36e97709", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7488f89b-8a3f-40cc-94cb-73d30ca8d5ee", "node_type": "1", "metadata": {}, "hash": "2727b39e0a6022fda3dee49ae3818c3765ca3c21080a5e7d8cfffcad5f90ce29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf635e73-8cfa-4246-8ea3-d1eefa59cae4", "node_type": "1", "metadata": {}, "hash": "4f103240dad41b3b120f4cba79d32ec53948fd676eab9557fc499d42fdfca0c8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "description\nformat\n\ud835\udc65\ud835\udc4f_\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52\nparameter\n\ud835\udc37\ud835\udc34\ud835\udc36\n\ud835\udc34\ud835\udc37\ud835\udc36\n\ud835\udc47\ud835\udc66\ud835\udc5d\ud835\udc52\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b\n\ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc4e\ud835\udc59\ud835\udc59\ud835\udc52\ud835\udc59\ud835\udc5f\ud835\udc5c\ud835\udc64\nThe shape of crossbar\nDAC precision\nADC", "mimetype": "text/plain", "start_char_idx": 21543, "end_char_idx": 21654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf635e73-8cfa-4246-8ea3-d1eefa59cae4": {"__data__": {"id_": "bf635e73-8cfa-4246-8ea3-d1eefa59cae4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30b68c42-066e-4641-affa-25da36e97709", "node_type": "1", "metadata": {}, "hash": "f6b74f60893d8c75c528e585838b1e8fc029515aa96127c637b62ec27a9b5e8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6032776-bd28-48b7-a52d-1337ae85edb1", "node_type": "1", "metadata": {}, "hash": "dd0d9ad8e76f5fc1af2b867adfcbb2743851728ffb426e87bfe718f0361882b6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "precision\nMaximum number of rows that can be activated simultaneously\nprecision of storage cell\nX bit\n\u2018SRAM\u2019, \u2018ReRAM\u2019, etc\ntype of storage cell\nX bit\nX bit\n[number of cells per row \n* number of cells per column]\nFigure 8. Crossbar tier architecture abstraction parameters.\ncrossbars ( A ), local buffers ( D ), and digital computation\nunits ( B ). The crossbars are connected via NoC ( C ).\nThe minimum scheduling granularity at this tier is cross-\nbar, which we abstract as crossbar mode(XBM).", "mimetype": "text/plain", "start_char_idx": 21655, "end_char_idx": 22149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6032776-bd28-48b7-a52d-1337ae85edb1": {"__data__": {"id_": "e6032776-bd28-48b7-a52d-1337ae85edb1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf635e73-8cfa-4246-8ea3-d1eefa59cae4", "node_type": "1", "metadata": {}, "hash": "4f103240dad41b3b120f4cba79d32ec53948fd676eab9557fc499d42fdfca0c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a95430b-f941-4dc7-b97c-db99e5a02163", "node_type": "1", "metadata": {}, "hash": "2b2a8203fbdde8a77abf3776ea811817ad00d6ef0fffdb0bc1baaf317c7e13f9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "One or mul-\ntiple crossbars work together to complete one matrix-vector\nmultiplication in this mode. Similar to chip tier, we use the\nxb_number to record the number of crossbars that work\nat the same time, which can decide the maximum scale\nof matrix-vector multiplications that can be concurrently\nmapped on the core. The convolution operator in DNN is de-\ncomposed into a sequence of matrix-vector multiplications\nin the XBM.", "mimetype": "text/plain", "start_char_idx": 22150, "end_char_idx": 22577, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a95430b-f941-4dc7-b97c-db99e5a02163": {"__data__": {"id_": "1a95430b-f941-4dc7-b97c-db99e5a02163", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6032776-bd28-48b7-a52d-1337ae85edb1", "node_type": "1", "metadata": {}, "hash": "dd0d9ad8e76f5fc1af2b867adfcbb2743851728ffb426e87bfe718f0361882b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a50743c2-1383-4511-9cbb-c7b9bde4eba9", "node_type": "1", "metadata": {}, "hash": "0918cfffd777f3913bffdbb0592ec137cb0f491d3bc4c6329382cfdda321b700", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To accommodate different CIM designs for executing MVM\non crossbars [4, 39, 42, 51], we introduce the concept of\nVXB (Virtual Crossbar) as the computational unit rather than\nphysical crossbars to facilitate the computing scheduling in\nthe compiler. A dimension-binding scheme is designed to\nconstruct a VXB, specifying which crossbars collaborate to\nperform a single MVM. Figure 7 illustrates the dimension-\nbinding scheme, where the matrix dimension has matrix row\n(R), column (C), and data bit-width (B).", "mimetype": "text/plain", "start_char_idx": 22578, "end_char_idx": 23084, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a50743c2-1383-4511-9cbb-c7b9bde4eba9": {"__data__": {"id_": "a50743c2-1383-4511-9cbb-c7b9bde4eba9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a95430b-f941-4dc7-b97c-db99e5a02163", "node_type": "1", "metadata": {}, "hash": "2b2a8203fbdde8a77abf3776ea811817ad00d6ef0fffdb0bc1baaf317c7e13f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5916c988-4dd2-41c8-85ea-dbcf1bdaf775", "node_type": "1", "metadata": {}, "hash": "126d298354255bb53490e88a5cbe91ec42fda46f234f695fe933f8864b0a1a21", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The computing cross-\nbar dimension refers to the crossbar itself (XB), crossbar row\n(XBR), and column (XBC). Figure 7 shows that the matrix di-\nmension R/C/B is bound to the XBR/XBC/XBC, respectively.\nThis binding represents that the data bits are spread to the\nadjacent column in the crossbar. If the matrix dimension B is\nbound to the XB, the data bits will be spread to the different\ncrossbars.", "mimetype": "text/plain", "start_char_idx": 23085, "end_char_idx": 23482, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5916c988-4dd2-41c8-85ea-dbcf1bdaf775": {"__data__": {"id_": "5916c988-4dd2-41c8-85ea-dbcf1bdaf775", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a50743c2-1383-4511-9cbb-c7b9bde4eba9", "node_type": "1", "metadata": {}, "hash": "0918cfffd777f3913bffdbb0592ec137cb0f491d3bc4c6329382cfdda321b700", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "131b9936-99f7-459d-91b0-73a619f8492c", "node_type": "1", "metadata": {}, "hash": "5118007abe906d79a6e25799506ae4af737a0bc899c6f7f5937b994de45b3ff3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Similar to chip tier, we utilize ALU to denote the speed of\nthe digital computation unit, xb_noc and xb_noc_cost for\nNoC abstraction, and L1 size and L1 BW to represent the\nlocal buffer characteristics at the core tier.\nThe architecture parameters in core tier are summarized in\nthe Figure 6. These chip and core tier parameters are exposed\nto the compilers in XBM as core tier abstraction records the\narchitecture details within the core, and chip tier parameters\nrecord the whole top tier architecture.", "mimetype": "text/plain", "start_char_idx": 23483, "end_char_idx": 23987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "131b9936-99f7-459d-91b0-73a619f8492c": {"__data__": {"id_": "131b9936-99f7-459d-91b0-73a619f8492c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5916c988-4dd2-41c8-85ea-dbcf1bdaf775", "node_type": "1", "metadata": {}, "hash": "126d298354255bb53490e88a5cbe91ec42fda46f234f695fe933f8864b0a1a21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce2ab9a2-0f94-401b-9ed1-968d0a7e4b8a", "node_type": "1", "metadata": {}, "hash": "2ef21c7c8588b24ec625792c7e6e8d9baf674160a5283afc8735261e1d8d3c11", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "CIM-MLC leverages\nthese parameters to map operators to crossbars, optimizing\nthe crossbar utilization.\n3.2.3\nCrossbar tier architecture abstraction and word-\nline mode. As shown in the Figure 4 (c), The crossbar tier\nabstraction serves as the fundamental computational unit,\ndescribing the details component within the crossbar. The\ncrossbar tier has a memory crossbar ( A ) with its peripheral\ncircuits (e.g., wordline drivers, bitline drivers, and signal con-\nverters (ADC C , DAC C , etc.)).", "mimetype": "text/plain", "start_char_idx": 23988, "end_char_idx": 24482, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce2ab9a2-0f94-401b-9ed1-968d0a7e4b8a": {"__data__": {"id_": "ce2ab9a2-0f94-401b-9ed1-968d0a7e4b8a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "131b9936-99f7-459d-91b0-73a619f8492c", "node_type": "1", "metadata": {}, "hash": "5118007abe906d79a6e25799506ae4af737a0bc899c6f7f5937b994de45b3ff3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65454d62-cf1a-44e6-b301-041249136f55", "node_type": "1", "metadata": {}, "hash": "5f215573f98cee9479e85736fe071797d5f7602e41c4490fb34638df79f7f0f5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Each crossbar array\u2019s rows\n( B ) can work independently.\nAt this tier, the minimum scheduling granularity provided\nby the CIM architecture is rows. We abstract this as wordline\nmode (WLM) and use parallel row to record the number\nof rows that can be activated concurrently. The mode allows\nthe CIM to optimize the power cost or alleviate the variation\nby closing partial rows of one crossbar [27, 47]. Users can de-\nfine the number of rows that can be activated simultaneously\nin a crossbar.", "mimetype": "text/plain", "start_char_idx": 24483, "end_char_idx": 24974, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65454d62-cf1a-44e6-b301-041249136f55": {"__data__": {"id_": "65454d62-cf1a-44e6-b301-041249136f55", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce2ab9a2-0f94-401b-9ed1-968d0a7e4b8a", "node_type": "1", "metadata": {}, "hash": "2ef21c7c8588b24ec625792c7e6e8d9baf674160a5283afc8735261e1d8d3c11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1011f77c-a35f-4d6b-a620-da78e8ac9cb3", "node_type": "1", "metadata": {}, "hash": "2dc57edd1cdf01da8ebc2d67e4e0bf5183c5b63d830f8018322337887047a841", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Meanwhile, the shape of the crossbar deter-\nmines the maximum scale of matrix-vector multiplication\ncalculations. So, we use the parameter xb_size to record the\nsize of a crossbar.\nThe attributes of memory cell ( D ) within the crossbar\nprofoundly impact the computation process. Additionally,\nmemory cell precision influences data representation and\nthe requisite number of crossbars for operators, affecting\nscheduling decisions. Hence, we use the parameters Type\nand Precision to respectively record the memory cell type\nand precision.", "mimetype": "text/plain", "start_char_idx": 24975, "end_char_idx": 25513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1011f77c-a35f-4d6b-a620-da78e8ac9cb3": {"__data__": {"id_": "1011f77c-a35f-4d6b-a620-da78e8ac9cb3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65454d62-cf1a-44e6-b301-041249136f55", "node_type": "1", "metadata": {}, "hash": "5f215573f98cee9479e85736fe071797d5f7602e41c4490fb34638df79f7f0f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13d90e69-51ed-4b76-aaa9-8ee59fa4dae8", "node_type": "1", "metadata": {}, "hash": "635e0c8f8fa1d456e323212ce1bda48be9382cb7c3917bbc2f4d2f0d521d5c9b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For the peripheral circuits, the Analog-to-Digital Con-\nverter (ADC) or a special sense amplifier usually performs\npre-defined calculations on the readout analog signals from\nthe crossbar. The Digital-to-Analog Converter (DAC) is also\nneeded to complete data signal conversion. The precision of\nDAC and ADC influences computation accuracy and latency.\nWe use parameters DAC and ADC to record the precision of\nDAC and ADC.\nThe architecture parameters in crossbar tier are summa-\nrized in the Figure 8. We expose the whole three tiers of\narchitecture abstraction parameters to the compiler in WLM.", "mimetype": "text/plain", "start_char_idx": 25514, "end_char_idx": 26109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13d90e69-51ed-4b76-aaa9-8ee59fa4dae8": {"__data__": {"id_": "13d90e69-51ed-4b76-aaa9-8ee59fa4dae8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1011f77c-a35f-4d6b-a620-da78e8ac9cb3", "node_type": "1", "metadata": {}, "hash": "2dc57edd1cdf01da8ebc2d67e4e0bf5183c5b63d830f8018322337887047a841", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a60a4f7-d928-4020-99a1-13e5edfe04b7", "node_type": "1", "metadata": {}, "hash": "c47c2f1285b934cf4738a735c2ffec6cb12225f07699bd94ba045a1b5eb09431", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "CIM-MLC will transform the computing process in DNNs\ninto the row-wise read and write of the CIM crossbar.\n3.3\nMulti-Level Scheduling\nWith the architecture abstraction and computing mode, we\napply a multi-level scheduling strategy to optimize the effi-\nciency of DNN inference on CIM, achieving low-latency, high\nenergy-efficient deployment of models. For each computing\nmode abstraction in the associated abstracted architecture\ntier, we design the optimization method.\nNext, we introduce the whole workflow and the detailed\noptimization methods in the multi-level scheduling strategy.", "mimetype": "text/plain", "start_char_idx": 26110, "end_char_idx": 26696, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a60a4f7-d928-4020-99a1-13e5edfe04b7": {"__data__": {"id_": "6a60a4f7-d928-4020-99a1-13e5edfe04b7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13d90e69-51ed-4b76-aaa9-8ee59fa4dae8", "node_type": "1", "metadata": {}, "hash": "635e0c8f8fa1d456e323212ce1bda48be9382cb7c3917bbc2f4d2f0d521d5c9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c75efa3-3354-4488-9dff-16bfe0a7c630", "node_type": "1", "metadata": {}, "hash": "53c1db13001552147300e028c31b89ffdcef386e1ca630f1e662dfa3266c96ea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "3.3.1\nMulti-Level Scheduling workflow. The compila-\ntion process is illustrated in Figure 3. Firstly, the compiler\n190\nCIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nCore\nCore \nConv\nFeature map\nRelu\nDuplicate \nConv\nSub feature map\nRelu\nConv\nConv weight\nConv weight\nConv weight\nDNN \ncomputing \ngraph(CG)\nChip tier \narchitecture", "mimetype": "text/plain", "start_char_idx": 26697, "end_char_idx": 27106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c75efa3-3354-4488-9dff-16bfe0a7c630": {"__data__": {"id_": "2c75efa3-3354-4488-9dff-16bfe0a7c630", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a60a4f7-d928-4020-99a1-13e5edfe04b7", "node_type": "1", "metadata": {}, "hash": "c47c2f1285b934cf4738a735c2ffec6cb12225f07699bd94ba045a1b5eb09431", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "946eb577-b93f-4c08-901a-cca01cc10d09", "node_type": "1", "metadata": {}, "hash": "28e6ba3e4509fa8f46be1e4026d64debeb8d883a9f47c8e6ca58d2e779653f36", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "parameters\nCIM can hold \nwhole DNN\nCG\nExtract the largest  \nsub-graph that can \nbe hold on CIM   \nInput \nDynamic \nprogramming\nY \nPipeline \nbalance\nOperator \nduplication\nN\nAll node belong to \na sub-graph\nN\nSub-graph \nset\nY\nSub-graph\nOutput  \n(a)\n(b)\nresource-adaptive compute graph segmentation\npipelined duplication\nFigure 9. CG-grained optimization (a) Operator duplication\nillustration (b) Optimization algorithm.\ngets the DNN models in ONNX format [5].", "mimetype": "text/plain", "start_char_idx": 27108, "end_char_idx": 27563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "946eb577-b93f-4c08-901a-cca01cc10d09": {"__data__": {"id_": "946eb577-b93f-4c08-901a-cca01cc10d09", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c75efa3-3354-4488-9dff-16bfe0a7c630", "node_type": "1", "metadata": {}, "hash": "53c1db13001552147300e028c31b89ffdcef386e1ca630f1e662dfa3266c96ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7887d5d-fe1c-403c-9583-a3453f71fc9c", "node_type": "1", "metadata": {}, "hash": "fa8288d03817750c5cc4698ab224341df76aae919c02af6f6856ae5f154f183c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "ONNX represents\nthe model by computation graph, in which nodes correspond\nto operators, and edges denote the data dependency between\neach operator. The compiler progresses from coarse to fine\ngranularity, optimizing the computations in Computational\nGraph Grained (CG-Grained), Matrix-Vector Multiplication\nGrained (MVM-Grained), and Vector-Vector Multiplication\nGrained (VVM-Grained). These optimizations are tailored\nto the CM, XBM, and WLM architectures.", "mimetype": "text/plain", "start_char_idx": 27564, "end_char_idx": 28021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7887d5d-fe1c-403c-9583-a3453f71fc9c": {"__data__": {"id_": "e7887d5d-fe1c-403c-9583-a3453f71fc9c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "946eb577-b93f-4c08-901a-cca01cc10d09", "node_type": "1", "metadata": {}, "hash": "28e6ba3e4509fa8f46be1e4026d64debeb8d883a9f47c8e6ca58d2e779653f36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96477ab0-f88c-4522-bb00-354110c85c8a", "node_type": "1", "metadata": {}, "hash": "f7e02638d55858d4fabbfc8c3ffe75539a44452e55e51ba28b0620a92b59df97", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We adopt a multi-\nlevel joint scheduling optimization strategy to explore the\nscheduling space of the CIM architecture effectively.\nWhen targeting the CM architecture, the compiler per-\nforms scheduling optimization at the CG-grained only. CG-\ngrained optimization aims to explore operator duplication\nand pipeline strategies while considering resource constraints.\nThe compiler takes the ONNX model and chip-tier hardware\nparameters as inputs. The optimization information for each\noperator is recorded by adding attributes to the nodes in\nthe ONNX graph, such as the operator\u2019s duplication.", "mimetype": "text/plain", "start_char_idx": 28022, "end_char_idx": 28614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96477ab0-f88c-4522-bb00-354110c85c8a": {"__data__": {"id_": "96477ab0-f88c-4522-bb00-354110c85c8a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7887d5d-fe1c-403c-9583-a3453f71fc9c", "node_type": "1", "metadata": {}, "hash": "fa8288d03817750c5cc4698ab224341df76aae919c02af6f6856ae5f154f183c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d11f2831-dab5-46b8-b0e2-adae4a9b6f9a", "node_type": "1", "metadata": {}, "hash": "eadd2db67e0c8a15c4f8b31073a3605daa8b09fcc0d385a0fa946aea5c445512", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For the\nXBM architecture, the compiler inherits the optimization\nresults from the CG-grained, and chip and core-tier hard-\nware parameters to perform the MVM-grained optimization.\nMVM-grained optimization further explores the finer oper-\nator duplication and pipeline before mapping operators to\nthe crossbars. When dealing with the WLM mode architec-\nture, the compiler builds upon the optimizations from the\nCG-grained and MVM-grained and, at a finer granularity of\nrow scheduling, performs optimization at the VVM-grained.\n3.3.2\nCG-Grained Optimization.", "mimetype": "text/plain", "start_char_idx": 28615, "end_char_idx": 29171, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d11f2831-dab5-46b8-b0e2-adae4a9b6f9a": {"__data__": {"id_": "d11f2831-dab5-46b8-b0e2-adae4a9b6f9a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96477ab0-f88c-4522-bb00-354110c85c8a", "node_type": "1", "metadata": {}, "hash": "f7e02638d55858d4fabbfc8c3ffe75539a44452e55e51ba28b0620a92b59df97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5930a112-951d-4417-a7e0-1c0ee380c9e4", "node_type": "1", "metadata": {}, "hash": "e0ec835f0fd1a9e8820459e64b26b0f45f390ed66d6177dca343f3bff2606302", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In the core mode, the\ncompiler maps DNN operators onto cores following the chip\ntier architecture parameters. We propose a computing graph-\ngrained optimization approach to improve operator mapping,\nall without altering the operator execution process. The pri-\nmary objective of this optimization level is to judiciously\nutilize hardware resources, thereby reducing latency and\npower consumption.\nCurrent graph-level optimization technology often over-\nlook hardware characteristics [9].", "mimetype": "text/plain", "start_char_idx": 29172, "end_char_idx": 29659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5930a112-951d-4417-a7e0-1c0ee380c9e4": {"__data__": {"id_": "5930a112-951d-4417-a7e0-1c0ee380c9e4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d11f2831-dab5-46b8-b0e2-adae4a9b6f9a", "node_type": "1", "metadata": {}, "hash": "eadd2db67e0c8a15c4f8b31073a3605daa8b09fcc0d385a0fa946aea5c445512", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e9303bd-a766-4ffb-a89a-401ac929af1e", "node_type": "1", "metadata": {}, "hash": "caf0816c6be5072c85bd29324d7f72e647c9d4e827e657ed66dd52925954d5ed", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We introduce a novel CG-\ngrained optimization, exploring the duplication scheme and\nMeta-operator flow Syntax\n<code> ::=<operators>* | parallel \u201c { \u201d <operators>* \u201c } \u201d\n<operators> ::= <operators>* <CIM>* <DCOM>* <DMOV>*\n<DCOM> ::=Relu(src,dst,len) | add(src1,src2,dst,len)\u2026\n<DMOV> ::=mov(src,dst,len)\nFigure 10. The syntax of code generation in Backus Naur\nForm (BNF).", "mimetype": "text/plain", "start_char_idx": 29660, "end_char_idx": 30029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e9303bd-a766-4ffb-a89a-401ac929af1e": {"__data__": {"id_": "6e9303bd-a766-4ffb-a89a-401ac929af1e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5930a112-951d-4417-a7e0-1c0ee380c9e4", "node_type": "1", "metadata": {}, "hash": "e0ec835f0fd1a9e8820459e64b26b0f45f390ed66d6177dca343f3bff2606302", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64b61fdd-91d1-4083-9bb1-b2f0fa6b1346", "node_type": "1", "metadata": {}, "hash": "ea6945fc670c20d6f6351048cf2f8abd092b354e74d80c95a52dc4d33ee0e2c8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "CG-grained Codegen Syntax\n<CIM> ::= <MOP_CM>\n<MOP_CM> ::= cim.readcore(type, params, coreaddr, src, dst)\nData from src with parameters params are performed type operation,\nlike convolution, on core coreaddr. The result is placed in Buffer dst.\nMOP_CM Semantics\nFigure 11. The syntax of CG-grained codegen in BNF format\nand MOP_CM semantics.\npipeline of DNN operators on CIM cores under the total\ncore_number constraint.", "mimetype": "text/plain", "start_char_idx": 30030, "end_char_idx": 30449, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64b61fdd-91d1-4083-9bb1-b2f0fa6b1346": {"__data__": {"id_": "64b61fdd-91d1-4083-9bb1-b2f0fa6b1346", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e9303bd-a766-4ffb-a89a-401ac929af1e", "node_type": "1", "metadata": {}, "hash": "caf0816c6be5072c85bd29324d7f72e647c9d4e827e657ed66dd52925954d5ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cb8820f-9f7c-46a0-b721-fa481020d96f", "node_type": "1", "metadata": {}, "hash": "d5eb7a7ca3af012b14ceaa7c5b7d522bfbf7950a63f38e3b4f07de7503a79857", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As shown in the Figure 9 (a), the op-\nerator duplication strategy enhances computational through-\nput by appropriately duplicating CIM-supported operators\non cores. Meanwhile, the inter-operator pipeline strategy\nseeks to enhance execution efficiency.\nConsidering various hardware resource constraints of the\nCIM architecture, we propose a resource-adaptive compute\ngraph segmentation and intra-segment dynamic balancing\npipelined duplication algorithm as Figure 9 (b) illustrated.", "mimetype": "text/plain", "start_char_idx": 30450, "end_char_idx": 30931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cb8820f-9f7c-46a0-b721-fa481020d96f": {"__data__": {"id_": "3cb8820f-9f7c-46a0-b721-fa481020d96f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64b61fdd-91d1-4083-9bb1-b2f0fa6b1346", "node_type": "1", "metadata": {}, "hash": "ea6945fc670c20d6f6351048cf2f8abd092b354e74d80c95a52dc4d33ee0e2c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db44e558-f3e8-4971-ac33-f8be2f7b1ec8", "node_type": "1", "metadata": {}, "hash": "3f75fbc7a0ed509be104dc7c5dd86d70b9e678aba2872a115881fe2eb3bf43d9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This algorithm takes the ONNX format computation graph\nand chip-tier hardware parameters as input and outputs the\nsubgraph set of the computing graph and the duplication\nresults for CIM-supported operators.\nFirst, we initialize the resources and computation latency\nof each node. For convolution nodes, the computation la-\ntency scales with the output feature map size. We use dy-\nnamic programming to search for all operators\u2019 duplication\nnumbers under the core_number constraint.", "mimetype": "text/plain", "start_char_idx": 30932, "end_char_idx": 31413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db44e558-f3e8-4971-ac33-f8be2f7b1ec8": {"__data__": {"id_": "db44e558-f3e8-4971-ac33-f8be2f7b1ec8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3cb8820f-9f7c-46a0-b721-fa481020d96f", "node_type": "1", "metadata": {}, "hash": "d5eb7a7ca3af012b14ceaa7c5b7d522bfbf7950a63f38e3b4f07de7503a79857", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4140fe1-7410-4703-a922-a2ace8c833d9", "node_type": "1", "metadata": {}, "hash": "e14591e40cbf3eb610dce748853e2e6995c6829980f408191d2cf9f51180e06e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Then, to avoid\nthe pipeline stall because of the imbalance between com-\nputing and data access of adjacent layers, we adjust the\nduplication number for each node. Meanwhile, considering\nthe constraint of core_noc_cost and L0 BW, CIM-MLC will\nupdate the duplication number to keep the data transfer\namount within the NOC and buffer capability. Once the CIM-\nunsupported node, like Relu, follows the operator, we will\nalso update the duplication number under the constraint of\nALU.", "mimetype": "text/plain", "start_char_idx": 31414, "end_char_idx": 31893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4140fe1-7410-4703-a922-a2ace8c833d9": {"__data__": {"id_": "c4140fe1-7410-4703-a922-a2ace8c833d9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db44e558-f3e8-4971-ac33-f8be2f7b1ec8", "node_type": "1", "metadata": {}, "hash": "3f75fbc7a0ed509be104dc7c5dd86d70b9e678aba2872a115881fe2eb3bf43d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccb1104d-2553-408f-accd-febb66469018", "node_type": "1", "metadata": {}, "hash": "0d753099a2e9029bef743a43e76eeef99c9b8a0aa5575bfaae8a7903d602c575", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As for the case that CIM resources are not able to hold a\nwhole DNN, we iteratively construct the maximal sub-graphs\nthat can fit within CIM capacity. For each constructed com-\nputation sub-graph, we update it by successively popping\nthe last nodes from the subgraph. Then, we use the dynamic\nprogramming algorithm to get the latency of the remaining\ncomputation sub-graph after a node is popped out. Once the\n191\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nSongyun Qu, Shixin Zhao,", "mimetype": "text/plain", "start_char_idx": 31894, "end_char_idx": 32384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccb1104d-2553-408f-accd-febb66469018": {"__data__": {"id_": "ccb1104d-2553-408f-accd-febb66469018", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4140fe1-7410-4703-a922-a2ace8c833d9", "node_type": "1", "metadata": {}, "hash": "e14591e40cbf3eb610dce748853e2e6995c6829980f408191d2cf9f51180e06e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21e1af3a-fd6b-4838-8daf-a1787afc18c6", "node_type": "1", "metadata": {}, "hash": "13465441cfdc6fca16779cf102aab6ff5d65693957ddd9b3e45407558f1bc590", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Bing Li, Yintao He, Xuyi Cai, Lei Zhang, and Ying Wang\n\ud835\udc12\ud835\udfcf\ud835\udfce, xb{0,1} \ud835\udc12\ud835\udfcf\ud835\udfcf, xb{0,1} \n\ud835\udc12\ud835\udfcf\ud835\udfd0, xb{0,1} \n\ud835\udc12\ud835\udfcf\ud835\udfd1, xb{0,1} \n\ud835\udc12\ud835\udfd0\ud835\udfce\ud835\udfce, xb{2,3} \n\ud835\udc12\ud835\udfd0\ud835\udfce_\ud835\udfcf, xb{4,", "mimetype": "text/plain", "start_char_idx": 32385, "end_char_idx": 32522, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21e1af3a-fd6b-4838-8daf-a1787afc18c6": {"__data__": {"id_": "21e1af3a-fd6b-4838-8daf-a1787afc18c6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccb1104d-2553-408f-accd-febb66469018", "node_type": "1", "metadata": {}, "hash": "0d753099a2e9029bef743a43e76eeef99c9b8a0aa5575bfaae8a7903d602c575", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d368213-7f2a-4476-a7b0-ff42a57aabb0", "node_type": "1", "metadata": {}, "hash": "c694967a828e95eef39a3e117336c0ed5d9caf9a015a257ce3acd915cd671b7a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5} \n\ud835\udc12\ud835\udfcf\ud835\udfce, xb{0,1} \ud835\udc12\ud835\udfcf\ud835\udfcf, xb{0,1} \n\ud835\udc12\ud835\udfcf\ud835\udfd0, xb{0,1} \n\ud835\udc12\ud835\udfcf\ud835\udfd1, xb{0,1} \n\ud835\udc12\ud835\udfd0\ud835\udfce_\ud835\udfce, xb{2,3} \n\ud835\udc12\ud835\udfd0\ud835\udfce_\ud835\udfcf, xb{4,5} \n\ud835\udc12\ud835\udfcf\ud835\udfd2, xb{0,", "mimetype": "text/plain", "start_char_idx": 32522, "end_char_idx": 32623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d368213-7f2a-4476-a7b0-ff42a57aabb0": {"__data__": {"id_": "0d368213-7f2a-4476-a7b0-ff42a57aabb0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21e1af3a-fd6b-4838-8daf-a1787afc18c6", "node_type": "1", "metadata": {}, "hash": "13465441cfdc6fca16779cf102aab6ff5d65693957ddd9b3e45407558f1bc590", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f3f6f5d-e3eb-4bee-95d0-595c9e3ac97a", "node_type": "1", "metadata": {}, "hash": "64e262d5fb25074c5851a68f1a53e0513a9e7ca1d83cb70a4b60b460450ff3f3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1} \n\ud835\udc12\ud835\udfcf\ud835\udfd2, xb{0,1} \nActivated crossbars: 6xbs\nActivated \ncrossbars:\n4xbs\n(a)\n(c)\ncycle\n1\n2\n3\n4\n5\n(d)\n0\n0 \nout0 \nout1 \nout2 \nout3 \n1\n2\n3\n1 \n2 \n3 \nVXB2\nVXB0 VXB1\n\ud835\udc4610\nOP 1\nOP", "mimetype": "text/plain", "start_char_idx": 32623, "end_char_idx": 32792, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f3f6f5d-e3eb-4bee-95d0-595c9e3ac97a": {"__data__": {"id_": "8f3f6f5d-e3eb-4bee-95d0-595c9e3ac97a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d368213-7f2a-4476-a7b0-ff42a57aabb0", "node_type": "1", "metadata": {}, "hash": "c694967a828e95eef39a3e117336c0ed5d9caf9a015a257ce3acd915cd671b7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "811f2858-7385-46d7-b22f-3a3f8919f484", "node_type": "1", "metadata": {}, "hash": "553521229c1323d05b976ce5703de7c0a78414168af093d1b57358c161f3ec10", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2\n\ud835\udc4620_0\n\ud835\udc4620_1\n4\n\ud835\udc4611\n\ud835\udc4612\n\ud835\udc4613\n\ud835\udc4614\nVXB3\nVXB4\nVXB5\nout\nMulti cycle input\nOutput \nfeature map\n(b)\nDuplication: 2\nDuplication: 3\nCore\nCore\nCore\nCore\nMVM-grained duplication\nCG-grained duplication\nXB\nVXB\nFigure 12.", "mimetype": "text/plain", "start_char_idx": 32793, "end_char_idx": 33000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "811f2858-7385-46d7-b22f-3a3f8919f484": {"__data__": {"id_": "811f2858-7385-46d7-b22f-3a3f8919f484", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f3f6f5d-e3eb-4bee-95d0-595c9e3ac97a", "node_type": "1", "metadata": {}, "hash": "64e262d5fb25074c5851a68f1a53e0513a9e7ca1d83cb70a4b60b460450ff3f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a02a361b-2fc6-46d1-b6ee-ac7da3f0d1ac", "node_type": "1", "metadata": {}, "hash": "b71f075a213f9fb31a7c509833be160f1f3c402f124b3368180448ea088eefe2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "MVM-Grained Optimization (a) Operator duplica-\ntion at VXB granularity (b) MVM-grained operator mapping\nand pipeline (c) Traditional pipeline activated crossbars (d)\nMVM-grained pipeline activated crossbars.\nlatency no longer decreases, the construction of that sub-\ngraph is complete. The nodes that pop out will be used to\nconstruct the next maximal sub-graphs until all nodes are\npart of a single sub-graph and have duplication numbers\nassigned.", "mimetype": "text/plain", "start_char_idx": 33001, "end_char_idx": 33449, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a02a361b-2fc6-46d1-b6ee-ac7da3f0d1ac": {"__data__": {"id_": "a02a361b-2fc6-46d1-b6ee-ac7da3f0d1ac", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "811f2858-7385-46d7-b22f-3a3f8919f484", "node_type": "1", "metadata": {}, "hash": "553521229c1323d05b976ce5703de7c0a78414168af093d1b57358c161f3ec10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb6a1f4e-e3fd-45b2-8239-ae4853a8ed78", "node_type": "1", "metadata": {}, "hash": "fc8f9dc25e96f74581cfe5ee2c53d5989b9ac870816f181a792c2aeea1864794", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Ultimately, we obtain the results of computing graph seg-\nmentation and operator duplication numbers at this grained\noptimization, which are subsequently passed on to the next\noptimization level.\nMeta-operator Flow Generation After the optimiza-\ntion, the compiler backend binds operator execution to the\ncorresponding cores. As primitives used in the current ML\ncompiler [31] can not support the description of CIM, we in-\ntroduce a CIM meta-operator set tailored for CM (MOP_CM)\nto describe the hardware activation at the chip tier.", "mimetype": "text/plain", "start_char_idx": 33450, "end_char_idx": 33984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb6a1f4e-e3fd-45b2-8239-ae4853a8ed78": {"__data__": {"id_": "fb6a1f4e-e3fd-45b2-8239-ae4853a8ed78", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a02a361b-2fc6-46d1-b6ee-ac7da3f0d1ac", "node_type": "1", "metadata": {}, "hash": "b71f075a213f9fb31a7c509833be160f1f3c402f124b3368180448ea088eefe2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa9049b3-ffe6-4916-b095-885e60aba117", "node_type": "1", "metadata": {}, "hash": "279676594179f171060031454cee324ec9e61104e0cec661fc18f21f0eb0d167", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As the\nFigure 11 shows, MOP_CM includes the \ud835\udc36\ud835\udc3c\ud835\udc40.\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52in-\nstruction. In addition to the CIM meta-operators, we have\nalso designed corresponding meta-operators for other op-\nerations (i.e., DCOM for digit computing operations and\nDMOV for data movement, respectively). Users have the\nflexibility to extend meta operators, aligning them with the\nhardware-supported functions.", "mimetype": "text/plain", "start_char_idx": 33985, "end_char_idx": 34360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa9049b3-ffe6-4916-b095-885e60aba117": {"__data__": {"id_": "aa9049b3-ffe6-4916-b095-885e60aba117", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb6a1f4e-e3fd-45b2-8239-ae4853a8ed78", "node_type": "1", "metadata": {}, "hash": "fc8f9dc25e96f74581cfe5ee2c53d5989b9ac870816f181a792c2aeea1864794", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "add5e0c5-8277-453f-8e87-0b5a980a95d8", "node_type": "1", "metadata": {}, "hash": "15dd66ae6ba506ce73db9f4b2d77216d4a725c7378c8c126453f16f4fa372f7b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The compiler will compile\nrelevant operators based on the provided meta-operators.\nThe simplified syntax of code generation is shown in the Fig-\nure 10, in which the label parallel indicates the operators\nexecuting in parallel.\n3.3.3\nMVM-Grained Optimization. As for the XBM, the\ncompiler unrolls the CIM-supported operator to matrix-vector\nmultiply and then maps and schedules the MVM to crossbars.", "mimetype": "text/plain", "start_char_idx": 34361, "end_char_idx": 34760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "add5e0c5-8277-453f-8e87-0b5a980a95d8": {"__data__": {"id_": "add5e0c5-8277-453f-8e87-0b5a980a95d8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa9049b3-ffe6-4916-b095-885e60aba117", "node_type": "1", "metadata": {}, "hash": "279676594179f171060031454cee324ec9e61104e0cec661fc18f21f0eb0d167", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9df6ece-fd89-40cb-8d2a-76578d4f0cd7", "node_type": "1", "metadata": {}, "hash": "bc5e4f853772c460c2205e83387cfec43197abd25b89370ed01630b863ee1616", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We design MVM-grained optimization after completing CG-\ngrained scheduling, aiming to improve resource utilization\nand computing throughput during the slide of weight ker-\nnels on feature maps in the convolution by effectively using\nthe crossbars in each core.\nSpecifically, MVM-grained optimization explores two key\ntechniques: the duplication of the operator in the crossbars\nand MVM-grained computing pipeline to boost the comput-\ning throughput under the power limitation.", "mimetype": "text/plain", "start_char_idx": 34761, "end_char_idx": 35237, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9df6ece-fd89-40cb-8d2a-76578d4f0cd7": {"__data__": {"id_": "a9df6ece-fd89-40cb-8d2a-76578d4f0cd7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "add5e0c5-8277-453f-8e87-0b5a980a95d8", "node_type": "1", "metadata": {}, "hash": "15dd66ae6ba506ce73db9f4b2d77216d4a725c7378c8c126453f16f4fa372f7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7303f6d5-950b-4ef1-b527-74a463def7c8", "node_type": "1", "metadata": {}, "hash": "53ff3c5bf14f041c288169661555c722f08964699f9e88c4ef9e4b8047413275", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The compiler\nreceives the segmentation of the computing graph enriched\nwith CG-level optimization information and chip and core-\ntier hardware abstraction, completing the MVM-grained op-\ntimization under the constraints of available xb_number.\nWe updated the duplication number of an operator within\ncrossbars (as shown in Figure 12 (a)),", "mimetype": "text/plain", "start_char_idx": 35238, "end_char_idx": 35576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7303f6d5-950b-4ef1-b527-74a463def7c8": {"__data__": {"id_": "7303f6d5-950b-4ef1-b527-74a463def7c8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9df6ece-fd89-40cb-8d2a-76578d4f0cd7", "node_type": "1", "metadata": {}, "hash": "bc5e4f853772c460c2205e83387cfec43197abd25b89370ed01630b863ee1616", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0acb7d48-8b0d-4068-8b78-5b75098eab09", "node_type": "1", "metadata": {}, "hash": "b01315639d289a2ec38128a8f029941741a81cbf051616bb8c65d76f85429a87", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "which get the duplicated\nnumber using the following Equation:\n\ud835\udc37\u2032\ud835\udc42\ud835\udc56= \u230a\ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc42\ud835\udc56\n\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52\u2217\ud835\udc37\ud835\udc42\ud835\udc56\u2217\ud835\udc36\ud835\udc5c\ud835\udc5f\ud835\udc52\ud835\udc49\ud835\udc4b\ud835\udc35\n\ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc42\ud835\udc56\n\ud835\udc49\ud835\udc4b\ud835\udc35\n\u230b,", "mimetype": "text/plain", "start_char_idx": 35577, "end_char_idx": 35681, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0acb7d48-8b0d-4068-8b78-5b75098eab09": {"__data__": {"id_": "0acb7d48-8b0d-4068-8b78-5b75098eab09", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7303f6d5-950b-4ef1-b527-74a463def7c8", "node_type": "1", "metadata": {}, "hash": "53ff3c5bf14f041c288169661555c722f08964699f9e88c4ef9e4b8047413275", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "533e8751-04d2-47d0-8e64-bcd7ee462dfd", "node_type": "1", "metadata": {}, "hash": "b467492ce8dba8dfb18d756ae6a5834d1c92a832f3eb27d373b15d21ee5cf93b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(1)\nwhere \ud835\udc36\ud835\udc5c\ud835\udc5f\ud835\udc52\ud835\udc49\ud835\udc4b\ud835\udc35is the number of VXBs in each core, \ud835\udc37\ud835\udc42\ud835\udc56\nis the duplication number of the operator determined by\nthe CG-grained optimization, \ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc42\ud835\udc56\n\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52is the number of cores\noccupied by this operator\ud835\udc42\ud835\udc56,", "mimetype": "text/plain", "start_char_idx": 35682, "end_char_idx": 35885, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "533e8751-04d2-47d0-8e64-bcd7ee462dfd": {"__data__": {"id_": "533e8751-04d2-47d0-8e64-bcd7ee462dfd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0acb7d48-8b0d-4068-8b78-5b75098eab09", "node_type": "1", "metadata": {}, "hash": "b01315639d289a2ec38128a8f029941741a81cbf051616bb8c65d76f85429a87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20a292c9-a5be-4a54-80e1-6491d1ad655f", "node_type": "1", "metadata": {}, "hash": "b51d7d0610ab6ec1f7bb72b5bfab8299c056984842e0c4eb63d2fab3df8d6f9b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc42\ud835\udc56\n\ud835\udc49\ud835\udc4b\ud835\udc35is the number of VXBs\noccupied by this operator \ud835\udc42\ud835\udc56, which can be calculated based\non the dimension and the size of VXB and the operator.\nUsually, one operator demands multiple VXBs to store its\nweights and complete the calculation.\nMVM-grained computing pipeline strategically staggers\nthe activation time of different crossbars to reduce peak\npower consumption as illustrated in Figure 12 (b).", "mimetype": "text/plain", "start_char_idx": 35886, "end_char_idx": 36289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20a292c9-a5be-4a54-80e1-6491d1ad655f": {"__data__": {"id_": "20a292c9-a5be-4a54-80e1-6491d1ad655f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "533e8751-04d2-47d0-8e64-bcd7ee462dfd", "node_type": "1", "metadata": {}, "hash": "b467492ce8dba8dfb18d756ae6a5834d1c92a832f3eb27d373b15d21ee5cf93b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e7a12a1-62ec-4f28-99e8-93fd581cc0d8", "node_type": "1", "metadata": {}, "hash": "23172dc19a3f189c1ca546402524a0ad0f282eaf99b08dc89ca9430d18dfc197", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As we need\nto map a matrix-vector multiplication to multiple crossbars,\nwe usually wait until all crossbars receive their inputs before\ncomputing in the traditional scheduling [39]. The pipeline\nstrategy we propose, however, activates a crossbar as soon as\nit receives its input, completing the computing mapping to\nthe crossbar. This reduces the number of crossbars that need\nto be activated simultaneously, thus lowering peak power\nconsumption.", "mimetype": "text/plain", "start_char_idx": 36290, "end_char_idx": 36736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e7a12a1-62ec-4f28-99e8-93fd581cc0d8": {"__data__": {"id_": "0e7a12a1-62ec-4f28-99e8-93fd581cc0d8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20a292c9-a5be-4a54-80e1-6491d1ad655f", "node_type": "1", "metadata": {}, "hash": "b51d7d0610ab6ec1f7bb72b5bfab8299c056984842e0c4eb63d2fab3df8d6f9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dd7843e-e16f-4965-9b4f-c1d0f4146621", "node_type": "1", "metadata": {}, "hash": "534c24e934c4b2408e53169e6fb3acee55092d2727f23c90595bce9d11b1c1f9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In the example, Operator 1 (OP 1) is mapped to VXB 0 and\nVXB 1, and Operator 2 (OP 2) is mapped to VXB 2-5. Input\n\ud835\udc461\ud835\udc56, corresponding to one sliding window in convolution,\nenters OP 1\u2019s VXBs in sequence to perform the MVM of\nthe convolution operator, passing its output to OP 2. The\ntraditional approach waits four cycles for OP 2 to begin its\ncomputation with \ud835\udc4620.(Figure 12 (c).)", "mimetype": "text/plain", "start_char_idx": 36737, "end_char_idx": 37117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3dd7843e-e16f-4965-9b4f-c1d0f4146621": {"__data__": {"id_": "3dd7843e-e16f-4965-9b4f-c1d0f4146621", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e7a12a1-62ec-4f28-99e8-93fd581cc0d8", "node_type": "1", "metadata": {}, "hash": "23172dc19a3f189c1ca546402524a0ad0f282eaf99b08dc89ca9430d18dfc197", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9385e64e-d5a4-4d4c-84cb-f859d745f8f5", "node_type": "1", "metadata": {}, "hash": "9d13bf6eff11dfd6bf53baed22c396f5637f5fe29bc522b0264662a251dc2d0a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In our MVM-grained\npipeline, after OP 1 takes two cycles, OP 2 can start running\non VXB 2,3 and then run on VXB4,5 after the next cycle, as\nshown in Figure 12 (d). At most, four VXBs are activated\nsimultaneously in contrast to that 6 VXBs in Figure 12 (c),\nreducing the peak power by \u223c30%.", "mimetype": "text/plain", "start_char_idx": 37118, "end_char_idx": 37407, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9385e64e-d5a4-4d4c-84cb-f859d745f8f5": {"__data__": {"id_": "9385e64e-d5a4-4d4c-84cb-f859d745f8f5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dd7843e-e16f-4965-9b4f-c1d0f4146621", "node_type": "1", "metadata": {}, "hash": "534c24e934c4b2408e53169e6fb3acee55092d2727f23c90595bce9d11b1c1f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ee10036-70f0-4e5c-a310-80400ee46def", "node_type": "1", "metadata": {}, "hash": "9e63e3bc8bfba1d31e513a044c77f79d841bd0ce99e87a4e327b1b2e3aac6fdb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Meanwhile, OP 2\u2019s inputs,\n\ud835\udc4620\u22120 and \ud835\udc4620\u22121, are half the size of \ud835\udc4620 in the traditional\npipeline. Thus, the communication overhead in each com-\nputing stage is reduced, alleviating the pressure on on-chip\nbandwidth and the risk of pipeline stall.\nWe obtain the updated duplication number and more com-\npact pipeline in this optimization grained and we will pass\nthe duplication result to the next optimization grained.", "mimetype": "text/plain", "start_char_idx": 37408, "end_char_idx": 37825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ee10036-70f0-4e5c-a310-80400ee46def": {"__data__": {"id_": "7ee10036-70f0-4e5c-a310-80400ee46def", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9385e64e-d5a4-4d4c-84cb-f859d745f8f5", "node_type": "1", "metadata": {}, "hash": "9d13bf6eff11dfd6bf53baed22c396f5637f5fe29bc522b0264662a251dc2d0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a182dc1a-2aea-41ef-9cd2-8cea23807662", "node_type": "1", "metadata": {}, "hash": "f0b8428b59b2d1a7c22a58b9c0e1b69004b0de88b0417ad9b5cebc900f5ff4e8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "192\nCIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nMVM-grained Codegen syntax\n<CIM> ::= <MOP_XBM>\n<MOP_XBM> ::= cim.readxb xbaddr,len |cim.writexb(xbaddr,mat)\nThe len xbs from the xbaddr are readed,", "mimetype": "text/plain", "start_char_idx": 37826, "end_char_idx": 38107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a182dc1a-2aea-41ef-9cd2-8cea23807662": {"__data__": {"id_": "a182dc1a-2aea-41ef-9cd2-8cea23807662", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ee10036-70f0-4e5c-a310-80400ee46def", "node_type": "1", "metadata": {}, "hash": "9e63e3bc8bfba1d31e513a044c77f79d841bd0ce99e87a4e327b1b2e3aac6fdb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f8dcee6-529f-44e0-8f4b-1f2db164e133", "node_type": "1", "metadata": {}, "hash": "95cb4d2c07dd3134fd01c737ecbc0f4cb585b3bdbdd194fa631f80c30174f68b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "which complete\nthe multiply of input and the data stored in the xbs\nMOP_XBM Semantics\nThe mat is written to the xbaddr\ncim.readxb\ncim.writexb\nFigure 13. The syntax of MVM-grained codegen in BNF\nformat and MOP_XBM semantics.\n\ud835\udc4b\ud835\udc3522\n\ud835\udc4b\ud835\udc3512\n0 4 8 c 2 6 a e\n1 5 9 d 3 7 b f\nVXB0\nVXB2\nOP 1\n0", "mimetype": "text/plain", "start_char_idx": 38108, "end_char_idx": 38390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f8dcee6-529f-44e0-8f4b-1f2db164e133": {"__data__": {"id_": "6f8dcee6-529f-44e0-8f4b-1f2db164e133", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a182dc1a-2aea-41ef-9cd2-8cea23807662", "node_type": "1", "metadata": {}, "hash": "f0b8428b59b2d1a7c22a58b9c0e1b69004b0de88b0417ad9b5cebc900f5ff4e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b241cc96-de18-4a7b-be9a-77271c5bc125", "node_type": "1", "metadata": {}, "hash": "7fd24632223a49c41f0217bc171973f3b7b9895696fc8334fc8eec4cb9bcd67e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4 8 c 2 6 a e\n1 5 9 d 3 7 b f\n(b)\n(c)\n1 5\n9 d\nA B\nC D\nB D\nOP 2\n3 7\nb\nf\nVXB4\nVXB6\nVXB5\nVXB7\n1 5\n9 d\n3 7\nb\nf\nB\nD\nVXB0\nVXB2\nVXB1\nVXB3\nOP 1\n4 c\n6 e\nOP 2\n5", "mimetype": "text/plain", "start_char_idx": 38391, "end_char_idx": 38541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b241cc96-de18-4a7b-be9a-77271c5bc125": {"__data__": {"id_": "b241cc96-de18-4a7b-be9a-77271c5bc125", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f8dcee6-529f-44e0-8f4b-1f2db164e133", "node_type": "1", "metadata": {}, "hash": "95cb4d2c07dd3134fd01c737ecbc0f4cb585b3bdbdd194fa631f80c30174f68b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25469213-9191-4a78-9f31-bfd562d3ce71", "node_type": "1", "metadata": {}, "hash": "ddb81919f089bbb1a7d63c9b93687ba21b9a50e428512b849b3be1492479357f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "d\n7\nf\nVXB4\nVXB6\nVXB5\nVXB7\n1 5\n9 d\n3 7\nb\nf\nB\nD\nA C\nXB1\nXB2\n\ud835\udc4b\ud835\udc3511\n\ud835\udc4b\ud835\udc3521\n\ud835\udc3c\n\ud835\udc3c0\n\ud835\udc3c1\nGet out1 in one cycle\n(a)\nHow?\nOUT\nOUT1\nGet out1 in two cycles\nOUT\nOUT1\n\ud835\udc4b\ud835\udc3511 \ud835\udc4b\ud835\udc3512", "mimetype": "text/plain", "start_char_idx": 38542, "end_char_idx": 38699, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25469213-9191-4a78-9f31-bfd562d3ce71": {"__data__": {"id_": "25469213-9191-4a78-9f31-bfd562d3ce71", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b241cc96-de18-4a7b-be9a-77271c5bc125", "node_type": "1", "metadata": {}, "hash": "7fd24632223a49c41f0217bc171973f3b7b9895696fc8334fc8eec4cb9bcd67e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b959207-cbb3-48e9-a73a-82ba017c6673", "node_type": "1", "metadata": {}, "hash": "1888e5fe9a0081bab4d6db4a79c012ee69ccbcb5c32196eefb3469a43c0e63dd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\ud835\udc4b\ud835\udc3521 \ud835\udc4b\ud835\udc3522\nRemapping \ncycle\n1\n2\n3\n4\n0 4\n8 c\n0 4\n8 c\nA\nVXB1\nVXB3\n2 6\na e\n2 6\na e\nC\n0 8\n2 a\n0 4\n8 c\nA\n1 9\n3 b\n2 6\na e\nC\n0 8 2 a 1 9 3 b\n4 c 6 e 5 d 7 f\n0", "mimetype": "text/plain", "start_char_idx": 38700, "end_char_idx": 38850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b959207-cbb3-48e9-a73a-82ba017c6673": {"__data__": {"id_": "4b959207-cbb3-48e9-a73a-82ba017c6673", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25469213-9191-4a78-9f31-bfd562d3ce71", "node_type": "1", "metadata": {}, "hash": "ddb81919f089bbb1a7d63c9b93687ba21b9a50e428512b849b3be1492479357f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9084e53d-a3b5-4132-b865-cd4d6101302b", "node_type": "1", "metadata": {}, "hash": "ade815c42487a52ffbf8a3f7d4076f399575f9e8034660795f0cc35f08440230", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4 8 c 2 6 a e\n1 5 9 d 3 7 b f\n(d)\ncycle\n1\n2\n3\n4\n\ud835\udc3c0\n\ud835\udc3c1\n\ud835\udc3c2\n\ud835\udc3c3\n\ud835\udc3c0\n\ud835\udc3c0\n\ud835\udc3c1\n\ud835\udc3c1\n\ud835\udc3c2\n\ud835\udc3c2\n\ud835\udc3c3\n\ud835\udc3c3\n\ud835\udc4b\ud835\udc3511 \ud835\udc4b\ud835\udc3512\n\ud835\udc4b\ud835\udc3521 \ud835\udc4b\ud835\udc3522\nFigure", "mimetype": "text/plain", "start_char_idx": 38851, "end_char_idx": 38961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9084e53d-a3b5-4132-b865-cd4d6101302b": {"__data__": {"id_": "9084e53d-a3b5-4132-b865-cd4d6101302b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b959207-cbb3-48e9-a73a-82ba017c6673", "node_type": "1", "metadata": {}, "hash": "1888e5fe9a0081bab4d6db4a79c012ee69ccbcb5c32196eefb3469a43c0e63dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d64c0a3-2bc2-4c5b-a284-577cc72df072", "node_type": "1", "metadata": {}, "hash": "a26c40d450b56022944d070e843dd06386522ba712062eb4e3461615c75aada9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "14. VVM-Grained Optimization (a) Parallelism oppor-\ntunities in WLM (b) Na\u00efve data mapping (c) Data remapping\nstrategy (d) VVM-grained pipeline with na\u00efve data mapping\n(up) and with our proposed data remapping (down).\nMeta-operator Flow Generation Upon completing MVM-\ngrained optimization, the compiler utilizes a meta-operator\ndesigned for XBM (MOP_XBM) to describe the hardware acti-\nvation at the core tier.", "mimetype": "text/plain", "start_char_idx": 38962, "end_char_idx": 39373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d64c0a3-2bc2-4c5b-a284-577cc72df072": {"__data__": {"id_": "0d64c0a3-2bc2-4c5b-a284-577cc72df072", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9084e53d-a3b5-4132-b865-cd4d6101302b", "node_type": "1", "metadata": {}, "hash": "ade815c42487a52ffbf8a3f7d4076f399575f9e8034660795f0cc35f08440230", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98473a04-c2e4-4376-b563-fb7e9d23871c", "node_type": "1", "metadata": {}, "hash": "9a7878614c5f2c34e9660da773013780ec6fd80200e33fb374e343e910797aa7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The syntax at this grained is shown in\nthe Figure 10 and Figure 13.", "mimetype": "text/plain", "start_char_idx": 39374, "end_char_idx": 39441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98473a04-c2e4-4376-b563-fb7e9d23871c": {"__data__": {"id_": "98473a04-c2e4-4376-b563-fb7e9d23871c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d64c0a3-2bc2-4c5b-a284-577cc72df072", "node_type": "1", "metadata": {}, "hash": "a26c40d450b56022944d070e843dd06386522ba712062eb4e3461615c75aada9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df2c8ad6-a2ad-453a-87fe-017e60bf787a", "node_type": "1", "metadata": {}, "hash": "02fcc7e50aa9b20d62d622d41eafc567a5d70e1635a378fb6e5f33a172aa729d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Specifically, MOP_XBM includes\nthe \ud835\udc36\ud835\udc3c\ud835\udc40.\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc4f\ud835\udc4e\ud835\udc5finstruction for reading a specific cross-\nbar to perform an MVM and \ud835\udc36\ud835\udc3c\ud835\udc40.\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc4f\ud835\udc4e\ud835\udc5finstruction\nfor writing values like convolution weights to the crossbar.", "mimetype": "text/plain", "start_char_idx": 39442, "end_char_idx": 39650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df2c8ad6-a2ad-453a-87fe-017e60bf787a": {"__data__": {"id_": "df2c8ad6-a2ad-453a-87fe-017e60bf787a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98473a04-c2e4-4376-b563-fb7e9d23871c", "node_type": "1", "metadata": {}, "hash": "9a7878614c5f2c34e9660da773013780ec6fd80200e33fb374e343e910797aa7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65d6f179-674a-4762-893a-af6316bf0c23", "node_type": "1", "metadata": {}, "hash": "f63207730613ad0baf2f4c15ee21fab8051d098a3eddf0d7b95bcea6570d4b37", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "3.3.4\nVVM-Grained Optimization. In the WLM, partial\nrows within the crossbar can be activated at once, providing\na more compact interface for vector-matrix multiplication\ncompared to XBM which activates a whole crossbar for one\ncomputation. As shown in the Figure 14(a), when coarse-\ngrained operators can be decomposed into fine-grained ones,\nwe can explore additional parallelism opportunities.", "mimetype": "text/plain", "start_char_idx": 39651, "end_char_idx": 40047, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65d6f179-674a-4762-893a-af6316bf0c23": {"__data__": {"id_": "65d6f179-674a-4762-893a-af6316bf0c23", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df2c8ad6-a2ad-453a-87fe-017e60bf787a", "node_type": "1", "metadata": {}, "hash": "02fcc7e50aa9b20d62d622d41eafc567a5d70e1635a378fb6e5f33a172aa729d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89c51624-4ace-45b5-92bd-beb86a011448", "node_type": "1", "metadata": {}, "hash": "da48223b31adb606f67953683e8d8a90ea7fedf5650b94d7bc51459b95832c75", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To fur-\nther improve computing throughput, we propose an inno-\nvative data remapping strategy to enable a finer pipeline for\nthe WLM CIM, which accounts for the updated computing\ngraph with operator duplication results and whole three-tier\nabstraction.\nThe main idea of the remapping strategy is to distribute\nthe data that contributes to the same computation to different\ncrossbars.", "mimetype": "text/plain", "start_char_idx": 40048, "end_char_idx": 40431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89c51624-4ace-45b5-92bd-beb86a011448": {"__data__": {"id_": "89c51624-4ace-45b5-92bd-beb86a011448", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65d6f179-674a-4762-893a-af6316bf0c23", "node_type": "1", "metadata": {}, "hash": "f63207730613ad0baf2f4c15ee21fab8051d098a3eddf0d7b95bcea6570d4b37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7048bba5-b259-44fa-a3ec-dbec58a52420", "node_type": "1", "metadata": {}, "hash": "2a37a2b63db87f651a236831e2c3cf4e9672fa6bcb11d55fd9ed18b612cfd789", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The proposed remapping strategy is illustrated in\nFigure 14, where OP 1 and OP 2 are two adjacent operators\nVVM-grained Codegen Syntax\n<CIM> ::= <MOP_WLM>\n<MOP_WLM> ::= cim.readrow(rowaddr,len)|cim.writerow(rowaddr,value)\nMOP_WLM Semantics\nThe value is written to the rowaddr\ncim.readrow\ncim.writerow\nThe len rows from the rowaddr are readed, which complete\nthe multiply of input and the data stored in the xbs\nFigure 15.", "mimetype": "text/plain", "start_char_idx": 40432, "end_char_idx": 40853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7048bba5-b259-44fa-a3ec-dbec58a52420": {"__data__": {"id_": "7048bba5-b259-44fa-a3ec-dbec58a52420", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89c51624-4ace-45b5-92bd-beb86a011448", "node_type": "1", "metadata": {}, "hash": "da48223b31adb606f67953683e8d8a90ea7fedf5650b94d7bc51459b95832c75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8eecfff-ae1e-49cc-af79-6fee76ebe9ea", "node_type": "1", "metadata": {}, "hash": "7f2f4d6bda85a3d3c3bb76c0b89af517e65db8b8dd31cf34a41c3e93f7ef0b7a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The syntax of VVM-grained codegen in BNF\nformat and MOP_WLM semantics.\nTable 2. Architecture Parameters of CIM example.\nChip_tier\nCore_tier\nCrossbar_tier\ncore_number\n[2*1]\nxb_number\n[2*1]\nxb_size\n[32*128]\nparallel row\n16\nPrecision\n2-bit\nin DNN and the output of OP 1 will be the input of OP\n2.", "mimetype": "text/plain", "start_char_idx": 40854, "end_char_idx": 41147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8eecfff-ae1e-49cc-af79-6fee76ebe9ea": {"__data__": {"id_": "b8eecfff-ae1e-49cc-af79-6fee76ebe9ea", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7048bba5-b259-44fa-a3ec-dbec58a52420", "node_type": "1", "metadata": {}, "hash": "2a37a2b63db87f651a236831e2c3cf4e9672fa6bcb11d55fd9ed18b612cfd789", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e4204ea-1459-4626-ae77-0af9bd17be77", "node_type": "1", "metadata": {}, "hash": "228d58af067f6841b3e39a5a26600b7b5749a0f4e87fa50a8ec4e2471ae402b4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For the example in Figure 14 (b), parallel row is half\nof the xb_size rows, which means only row 0 and row 4\nin VXB 0 can be activated in one cycle. In the na\u00efve data\nmapping strategy (Figure 14 (b)), since OP 1\u2019s output A is\nthe accumulation of output of rows 0, 1, 2, and 3, it needs\ntwo cycles to get the output A (Figure 14 (d) up). OP 2 can\nnot start its computing until outputting A is complete.", "mimetype": "text/plain", "start_char_idx": 41148, "end_char_idx": 41549, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e4204ea-1459-4626-ae77-0af9bd17be77": {"__data__": {"id_": "4e4204ea-1459-4626-ae77-0af9bd17be77", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8eecfff-ae1e-49cc-af79-6fee76ebe9ea", "node_type": "1", "metadata": {}, "hash": "7f2f4d6bda85a3d3c3bb76c0b89af517e65db8b8dd31cf34a41c3e93f7ef0b7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4d57ef5-fd97-4269-a7c6-87c36f9c28c8", "node_type": "1", "metadata": {}, "hash": "9c78f70c82eb40fe556c226730b53fff89781b1b57c4e4a75bf7cb9ba03bb518", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As\na result, OP 2 has to delay its computation for one cycle.\nIn contrast, our data remapping scheme maps rows 1 and\n3 to different VXBs, as shown in Figure 14 (c). Then, rows\n0-3 can complete their computations and accumulate to A\nin one cycle(Figure 14 (d) down). The OP 2 can start its\ncomputation at Cycle 2.", "mimetype": "text/plain", "start_char_idx": 41550, "end_char_idx": 41862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4d57ef5-fd97-4269-a7c6-87c36f9c28c8": {"__data__": {"id_": "b4d57ef5-fd97-4269-a7c6-87c36f9c28c8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e4204ea-1459-4626-ae77-0af9bd17be77", "node_type": "1", "metadata": {}, "hash": "228d58af067f6841b3e39a5a26600b7b5749a0f4e87fa50a8ec4e2471ae402b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72afdfb0-c290-4834-98e3-e0d68ba6b7fd", "node_type": "1", "metadata": {}, "hash": "961a76191a14cd4faa5ca58e25c8c3ecd9b0aed7da54cbc44b5588790736226d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Meanwhile, rows 4-7 can perform\nthe computations and accumulate their results to get output\nB. Therefore, as shown in Figure 14 (d), a pipeline with a\nhigher throughput can be achieved using our remapping\nscheme.\nMeta-operator Flow Generation Upon completing VVM-\ngrained optimization, the compiler uses the meta operators\nspecific for WLM (MOP_WLM) to describe the correspond-\ning hardware activation at the crossbar tier.", "mimetype": "text/plain", "start_char_idx": 41863, "end_char_idx": 42286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72afdfb0-c290-4834-98e3-e0d68ba6b7fd": {"__data__": {"id_": "72afdfb0-c290-4834-98e3-e0d68ba6b7fd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4d57ef5-fd97-4269-a7c6-87c36f9c28c8", "node_type": "1", "metadata": {}, "hash": "9c78f70c82eb40fe556c226730b53fff89781b1b57c4e4a75bf7cb9ba03bb518", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d253f0c-27fb-4cfb-a8ea-082717b4bdcd", "node_type": "1", "metadata": {}, "hash": "ec9043c31020d064d6785fbae5ed574cb739812ecb39e5474f983023dadd2297", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As the Figure 15\nshows, MOP_WLM includes the \ud835\udc36\ud835\udc3c\ud835\udc40.\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc5f\ud835\udc5c\ud835\udc64instruction\nto read rows and \ud835\udc36\ud835\udc3c\ud835\udc40.\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc64instruction to write certain\nvalues to the rows.", "mimetype": "text/plain", "start_char_idx": 42287, "end_char_idx": 42432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d253f0c-27fb-4cfb-a8ea-082717b4bdcd": {"__data__": {"id_": "5d253f0c-27fb-4cfb-a8ea-082717b4bdcd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72afdfb0-c290-4834-98e3-e0d68ba6b7fd", "node_type": "1", "metadata": {}, "hash": "961a76191a14cd4faa5ca58e25c8c3ecd9b0aed7da54cbc44b5588790736226d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66f77ccf-46c7-45af-a56d-c1696c546f05", "node_type": "1", "metadata": {}, "hash": "da64bf2e86e21b06440c20a0fe15394db92ab8a66effceef8efacac5eb190711", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "CIM-MLC generates the meta-operator\nflow by invoking the MOP_WLM, DCOM, and DMOV, for\nwhich the syntax is shown in the Figure 10 and Figure 15.\n3.4\nPutting it all together\nIn this section, we put the CIM-MLC compilation process\nall together. To help enhance users\u2019 understanding of our\nhardware abstraction and optimization methods, we have\nemployed simplified networks and the CIM architecture to\nillustrate our overall compilation process.", "mimetype": "text/plain", "start_char_idx": 42433, "end_char_idx": 42874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66f77ccf-46c7-45af-a56d-c1696c546f05": {"__data__": {"id_": "66f77ccf-46c7-45af-a56d-c1696c546f05", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d253f0c-27fb-4cfb-a8ea-082717b4bdcd", "node_type": "1", "metadata": {}, "hash": "ec9043c31020d064d6785fbae5ed574cb739812ecb39e5474f983023dadd2297", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a67944fe-9abe-4082-aa63-bb0562efda06", "node_type": "1", "metadata": {}, "hash": "74d67c107eeedcf4a77c623070006da8dce81608c6726c72f8a2e884418bd60c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We extracted com-\nmonly used operation in DNN, Convolution-Relu [1], as\nan example. The parameters of the convolution are: input\n193\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nSongyun Qu, Shixin Zhao, Bing Li, Yintao He, Xuyi Cai, Lei Zhang, and Ying Wang\n(e) WLM \u2013 Rows Interface (Crossbar tier)\nParallel{\n      cim.readcore (conv, params, coreaddr = 0,src=0,", "mimetype": "text/plain", "start_char_idx": 42875, "end_char_idx": 43244, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a67944fe-9abe-4082-aa63-bb0562efda06": {"__data__": {"id_": "a67944fe-9abe-4082-aa63-bb0562efda06", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66f77ccf-46c7-45af-a56d-c1696c546f05", "node_type": "1", "metadata": {}, "hash": "da64bf2e86e21b06440c20a0fe15394db92ab8a66effceef8efacac5eb190711", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e4f061b-042f-4d35-9c93-3040c3a89140", "node_type": "1", "metadata": {}, "hash": "532bac1078460a96afe6e7a1a0b213047d88f95723356f2f70a505e1dd44d4f1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "dst=3072)\n      cim.readcore (conv, params, coreaddr = 1,src=1440,dst=19456)\n} \nRelu(src=3072,dst=3072+32*32*32,len=32*32*32)\nInit:  \ncim.writexb(xbaddr , weight_matrix) for xbaddr in 0 to 3\nCompute:\nmov(src = L0 buffer0, dst=L1 buffer0) \nParallel{\n      cim.readxb (xbaddr =0,", "mimetype": "text/plain", "start_char_idx": 43244, "end_char_idx": 43521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e4f061b-042f-4d35-9c93-3040c3a89140": {"__data__": {"id_": "1e4f061b-042f-4d35-9c93-3040c3a89140", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a67944fe-9abe-4082-aa63-bb0562efda06", "node_type": "1", "metadata": {}, "hash": "74d67c107eeedcf4a77c623070006da8dce81608c6726c72f8a2e884418bd60c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6acd20d-06e0-4bc2-9e1d-7813e50ce73d", "node_type": "1", "metadata": {}, "hash": "21096ee62f9722a9cfc834daaaaec1ddaa8228f7415ebf360c17aa2b31d8e06e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "len = 1)  //activate one crossbar from xb0\n      \u2026\n      cim.readxb (xbaddr =3, len = 1)  \n} \nmov(src = L1 buffer, dst=L0 buffer3072) \nRelu(src=3072,dst=3072+32*32*32,len=32*4)\nInit: \nA = weight_matrix[ 0:16 , :] \nB = weight_matrix[16: , :]             //remap weight\ncim.writerow(xb0_row0~15 , A) \ncim.", "mimetype": "text/plain", "start_char_idx": 43522, "end_char_idx": 43825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6acd20d-06e0-4bc2-9e1d-7813e50ce73d": {"__data__": {"id_": "c6acd20d-06e0-4bc2-9e1d-7813e50ce73d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e4f061b-042f-4d35-9c93-3040c3a89140", "node_type": "1", "metadata": {}, "hash": "532bac1078460a96afe6e7a1a0b213047d88f95723356f2f70a505e1dd44d4f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "753408dd-545b-4377-ad0a-546e13225bea", "node_type": "1", "metadata": {}, "hash": "b371c1443f4e0e863efdadc68a51a446ab157f1afd4a5f162208abfea3e2b442", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "writerow(xb1_row0~15 , B) \n\u2026\nCompute:\nmov(src = L0 buffer0, dst=L1 buffer0) \nParallel{\n      cim.readrow (rowaddr = xb0_row0, len = 16) \n      cim.readrow (rowaddr = xb1_row0, len = 16) \n       \u2026 // activate xb2_row0~15, xb3_row0~15\n} \nmov(src = L1 buffer, dst=L0 buffer3072) \nRelu(src=3072,dst=3072+32*32*32,", "mimetype": "text/plain", "start_char_idx": 43825, "end_char_idx": 44134, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "753408dd-545b-4377-ad0a-546e13225bea": {"__data__": {"id_": "753408dd-545b-4377-ad0a-546e13225bea", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6acd20d-06e0-4bc2-9e1d-7813e50ce73d", "node_type": "1", "metadata": {}, "hash": "21096ee62f9722a9cfc834daaaaec1ddaa8228f7415ebf360c17aa2b31d8e06e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bcf6805-1d3d-45df-bd75-4157dfee23bd", "node_type": "1", "metadata": {}, "hash": "342e2a93a44785007008fe60c9ff6ea95a9266a8a7ed33d99529d03d971aadfd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "len=32*2)\n\u2026 // activate xb0_row16~31 in the next cycle\n(a) Current ML compiler - Conv Interface \ndata = te.placeholder((1, 3, 32, 32))\nkernel = te.placeholder((32, 3, 3, 3))\nwith tvm.target.Target(\" CIM \") :\n    conv = topi.cuda.conv2d_nchw(data, kernel, 1, 2)\n    out = topi.nn.relu(conv)\n    sconv = topi.cuda.", "mimetype": "text/plain", "start_char_idx": 44134, "end_char_idx": 44446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bcf6805-1d3d-45df-bd75-4157dfee23bd": {"__data__": {"id_": "6bcf6805-1d3d-45df-bd75-4157dfee23bd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "753408dd-545b-4377-ad0a-546e13225bea", "node_type": "1", "metadata": {}, "hash": "b371c1443f4e0e863efdadc68a51a446ab157f1afd4a5f162208abfea3e2b442", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ff93fa7-fad9-4216-9035-10c64b29c928", "node_type": "1", "metadata": {}, "hash": "4aa03265447670dc5fadc6ea2625c6b0b3c03aee964cfe758a08c822e6c127e0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "schedule_conv2d_nchw([out])\n(b) Current ML compiler - MVM Interface\ndef conv2d2mvm(A: T.Buffer[(1024,27),], B: \nT.Buffer[(27,32),],C: T.Buffer[(1024,32),]) :\n        for i in T.serial(1024):\n              with T.block(\u201dC\u201d):\n                     vi = T.axis.spatial(1024,i)\n                     T.reads(A[vi])\n                     T.reads(B[vi])\n                     T.", "mimetype": "text/plain", "start_char_idx": 44446, "end_char_idx": 44814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ff93fa7-fad9-4216-9035-10c64b29c928": {"__data__": {"id_": "3ff93fa7-fad9-4216-9035-10c64b29c928", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bcf6805-1d3d-45df-bd75-4157dfee23bd", "node_type": "1", "metadata": {}, "hash": "342e2a93a44785007008fe60c9ff6ea95a9266a8a7ed33d99529d03d971aadfd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7eb9ef0-7eb7-4e76-a581-32918df9288a", "node_type": "1", "metadata": {}, "hash": "96b8ca6041c1d4b79a30fd7b77c17e8c18329ed1fe7825912c9ac3a94a9bd768", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "writes(C[vi])\n                     C[vi] = mvm(A[vi],B[vi])\n(d) XBM \u2013 Crossbar Interface (Core tier)\n(c)  CM \u2013 Core Interface (Chip tier)\nFigure 16. Generated code example for the Convolution-Relu. Left: Traditional DNN compilers; Right: CIM-MLC.\nsize:(3,32,32), kernel size:(32,3,3,3), stride:1, padding:1 with\n8-bit precision for weight.", "mimetype": "text/plain", "start_char_idx": 44814, "end_char_idx": 45153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7eb9ef0-7eb7-4e76-a581-32918df9288a": {"__data__": {"id_": "f7eb9ef0-7eb7-4e76-a581-32918df9288a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ff93fa7-fad9-4216-9035-10c64b29c928", "node_type": "1", "metadata": {}, "hash": "4aa03265447670dc5fadc6ea2625c6b0b3c03aee964cfe758a08c822e6c127e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f8683e7-f2e4-4e10-bcaa-306a580e7a98", "node_type": "1", "metadata": {}, "hash": "fccee746e2ee17a24218353cb65ccbbeb1bb5991a0c4ca3ca328c80f83eee06e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We assume that the target CIM\narchitecture has 2 cores, each housing 2 crossbars with 32\nrow \u00d7 128 column memory cells. Each cell is capable of map-\nping 2 bits. As we take a portion of the compilation process\nfrom the complete network as an example, we simplify this\narchitecture to support all common digital arithmetic oper-\nations, and the buffer bandwidths are ample, imposing no\nmemory access limitations on the computation process. We\nuse shared memory communication as NoC example. The\nmain architecture abstraction parameters are shown in the\nTable 2.", "mimetype": "text/plain", "start_char_idx": 45154, "end_char_idx": 45714, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f8683e7-f2e4-4e10-bcaa-306a580e7a98": {"__data__": {"id_": "7f8683e7-f2e4-4e10-bcaa-306a580e7a98", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7eb9ef0-7eb7-4e76-a581-32918df9288a", "node_type": "1", "metadata": {}, "hash": "96b8ca6041c1d4b79a30fd7b77c17e8c18329ed1fe7825912c9ac3a94a9bd768", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97e67368-c103-433a-b808-5172fbeaf390", "node_type": "1", "metadata": {}, "hash": "1c50ff9d6c6fd83a2bddf8308be0597b1469e9801a852c254ab559d0d91605ea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We will illustrate the generated code when assum-\ning the architecture provides activation interfaces for cores,\ncrossbars, and rows, separately.\nCG-Grained When this architecture only provides ac-\ntivation interfaces for cores, which means CM, we apply\nCG-grained optimizations.\nIn this stage, we duplicate operators based on the hardware\nresource constraint. In this example, core_number is 2 and\neach core can support the convolution with a kernel size of\n(32x3x3x3). Consequently, CIM-MLC decides the operator\ncan be duplicated twice.", "mimetype": "text/plain", "start_char_idx": 45715, "end_char_idx": 46253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97e67368-c103-433a-b808-5172fbeaf390": {"__data__": {"id_": "97e67368-c103-433a-b808-5172fbeaf390", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f8683e7-f2e4-4e10-bcaa-306a580e7a98", "node_type": "1", "metadata": {}, "hash": "fccee746e2ee17a24218353cb65ccbbeb1bb5991a0c4ca3ca328c80f83eee06e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfb8514d-9493-4708-a9da-2a997ee71fdd", "node_type": "1", "metadata": {}, "hash": "d839aa0b61d06e78628acdef8c786631172e2bb02a12fa7fffb0fc7fc4c15684", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The final compiled meta-operator flow is illustrated in Fig-\nure 16(c) CM, which sequentially completes the convolution\nand ReLU operations. To enable the parallel calculation of\nthe duplicated operators, we partitioned the input feature\nmaps into several sub-ones with the same number as the du-\nplicated operators. Then, we could get the buffer address of\neach sub-feature map. The duplication number for the opera-\ntor is 2 and the feature map is partitioned into two sub-ones.", "mimetype": "text/plain", "start_char_idx": 46254, "end_char_idx": 46734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfb8514d-9493-4708-a9da-2a997ee71fdd": {"__data__": {"id_": "cfb8514d-9493-4708-a9da-2a997ee71fdd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97e67368-c103-433a-b808-5172fbeaf390", "node_type": "1", "metadata": {}, "hash": "1c50ff9d6c6fd83a2bddf8308be0597b1469e9801a852c254ab559d0d91605ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d156f5bb-f4b4-47f6-8d4d-b33a95a27b3e", "node_type": "1", "metadata": {}, "hash": "08f7a84dff45932a3ff782dd149ef7ef06fd40e44d1bd453056f787f763cfb89", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "So, we use two \ud835\udc50\ud835\udc56\ud835\udc5a.\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52to complete the convolution\noperation. As the two \ud835\udc50\ud835\udc56\ud835\udc5a.\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52have the same weights,\ntheir parameters are the same except for the \ud835\udc60\ud835\udc5f\ud835\udc50and \ud835\udc51\ud835\udc52\ud835\udc60\nvalues.", "mimetype": "text/plain", "start_char_idx": 46735, "end_char_idx": 46910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d156f5bb-f4b4-47f6-8d4d-b33a95a27b3e": {"__data__": {"id_": "d156f5bb-f4b4-47f6-8d4d-b33a95a27b3e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfb8514d-9493-4708-a9da-2a997ee71fdd", "node_type": "1", "metadata": {}, "hash": "d839aa0b61d06e78628acdef8c786631172e2bb02a12fa7fffb0fc7fc4c15684", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b228fd9-bdfa-4266-8559-88dec33f6a0e", "node_type": "1", "metadata": {}, "hash": "9adbf3eb74d5226648a6ac6c283150b96f18924b8562390b61d8cddfc0d83a65", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We use parallel{\ud835\udc50\ud835\udc56\ud835\udc5a.\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52(conv,params,0,0,3072),\n\ud835\udc50\ud835\udc56\ud835\udc5a.\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52(conv,params,1,1440,19456)} to denote executing\ncomputation on core 0 and core 1 in parallel.", "mimetype": "text/plain", "start_char_idx": 46911, "end_char_idx": 47068, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b228fd9-bdfa-4266-8559-88dec33f6a0e": {"__data__": {"id_": "3b228fd9-bdfa-4266-8559-88dec33f6a0e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d156f5bb-f4b4-47f6-8d4d-b33a95a27b3e", "node_type": "1", "metadata": {}, "hash": "08f7a84dff45932a3ff782dd149ef7ef06fd40e44d1bd453056f787f763cfb89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b23a1a4-fb99-4691-befd-6f5d513aa4bd", "node_type": "1", "metadata": {}, "hash": "fbb7147e68830ef724beba6a61000986b66001618b9eaea41bab381694b54358", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Furthermore,\nas we assume the architecture supports the ReLU operation,\nwe can directly invoke the Relu meta-operator. So far, we\nhave gotten the meta-operator flow for performing conv-relu\noperation on CM CIM by MOP-CM, DCOM and DMOV.\nTraditional DNN compilers view this architecture merely\nas hardware for handling convolution and ReLU computa-\ntions [31]. Therefore, during the CG level optimization, they\nmay directly invoke these interfaces to perform computation\nor merge the operators to reduce memory access.", "mimetype": "text/plain", "start_char_idx": 47069, "end_char_idx": 47585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b23a1a4-fb99-4691-befd-6f5d513aa4bd": {"__data__": {"id_": "0b23a1a4-fb99-4691-befd-6f5d513aa4bd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b228fd9-bdfa-4266-8559-88dec33f6a0e", "node_type": "1", "metadata": {}, "hash": "9adbf3eb74d5226648a6ac6c283150b96f18924b8562390b61d8cddfc0d83a65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44186064-86d9-44bb-bdb7-c27a23d7c284", "node_type": "1", "metadata": {}, "hash": "7b447500060a3332f157eb0a58eb92a6847942c2bc101015142c0f58bc2c43a2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The gen-\nerated code may be like Figure 16 (a) [9]. Compared to our\nmethod, they do not account for CIM-specific characteristics\nsuch as duplication opportunities during the mapping phase,\nwhich leads to a restricted exploration of the optimization\npossibilities.\nMVM-Grained When this architecture provides program-\nming interfaces at the core tier, which means XBM, we can\nperform the corresponding optimization, MVM-Grained op-\ntimization. In this stage, we explore the operator duplica-\ntion within a core after converting the convolution to MVM.", "mimetype": "text/plain", "start_char_idx": 47586, "end_char_idx": 48136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44186064-86d9-44bb-bdb7-c27a23d7c284": {"__data__": {"id_": "44186064-86d9-44bb-bdb7-c27a23d7c284", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b23a1a4-fb99-4691-befd-6f5d513aa4bd", "node_type": "1", "metadata": {}, "hash": "fbb7147e68830ef724beba6a61000986b66001618b9eaea41bab381694b54358", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b84fb40-428c-4962-84fb-f265473d9bd5", "node_type": "1", "metadata": {}, "hash": "7e16ea8c81eb4cd125e25ffc3789b3a65bea2049e81cd92f5e718a96416a4a45", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Given that each core has two crossbars, our approach allows\nus to update the operator duplication from 2 to 4 as each\ncrossbar can support an MVM.\nAs shown in the Figure 16(d) XBM, after determining\nthe updated duplication number, we first write the weight\nmatrix to corresponding crossbars by \ud835\udc50\ud835\udc56\ud835\udc5a.\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc4fbefore\ncomputing.", "mimetype": "text/plain", "start_char_idx": 48137, "end_char_idx": 48459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b84fb40-428c-4962-84fb-f265473d9bd5": {"__data__": {"id_": "7b84fb40-428c-4962-84fb-f265473d9bd5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44186064-86d9-44bb-bdb7-c27a23d7c284", "node_type": "1", "metadata": {}, "hash": "7b447500060a3332f157eb0a58eb92a6847942c2bc101015142c0f58bc2c43a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4805184-cc85-4df9-990b-2a6ad5a21c43", "node_type": "1", "metadata": {}, "hash": "6da383e754425db909a2a6f8cdb3210fff122c6ef38b1431d549c655bd00e083", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Then, we load the data into local buffers using\n\ud835\udc5a\ud835\udc5c\ud835\udc63operations and activate the four crossbars through the\n\ud835\udc50\ud835\udc56\ud835\udc5a.\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc65\ud835\udc4fto complete four MVM operations in parallel.\nSince 1024 MVM operations are needed for one convolution,\nwe generate 256 similar code segments to execute the entire\nconvolution computation. Upon completing a batch of MVM\noperations, we perform ReLU calculations on the output of\nMVM to facilitate subsequent pipelines.", "mimetype": "text/plain", "start_char_idx": 48460, "end_char_idx": 48892, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4805184-cc85-4df9-990b-2a6ad5a21c43": {"__data__": {"id_": "b4805184-cc85-4df9-990b-2a6ad5a21c43", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b84fb40-428c-4962-84fb-f265473d9bd5", "node_type": "1", "metadata": {}, "hash": "7e16ea8c81eb4cd125e25ffc3789b3a65bea2049e81cd92f5e718a96416a4a45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55422b37-2367-4f08-b2ba-352448c96424", "node_type": "1", "metadata": {}, "hash": "e788bda22429abf4955c1ab4c64140bfdb8f019ca57af0012c356d507043f375", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As we mentioned earlier in Section 1, traditional DNN\ncompilation may be difficult to explore compilation opti-\nmizations suitable for CIM at MVM-grained. They usually\nfocus on operators split and rearranged by loop unrolling\netc. from the perspective of tensor size, but they cannot take\nadvantage of the computing opportunity provided by the\nmemory [31]. The generated code is similar to the one de-\npicted in Figure 16 (b) [9].", "mimetype": "text/plain", "start_char_idx": 48893, "end_char_idx": 49323, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55422b37-2367-4f08-b2ba-352448c96424": {"__data__": {"id_": "55422b37-2367-4f08-b2ba-352448c96424", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4805184-cc85-4df9-990b-2a6ad5a21c43", "node_type": "1", "metadata": {}, "hash": "6da383e754425db909a2a6f8cdb3210fff122c6ef38b1431d549c655bd00e083", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71902272-2de6-4cd9-b6bb-c504b6c3e657", "node_type": "1", "metadata": {}, "hash": "526ca62f29bbc4eb5f4e87f33310f5e5afee3d68ea9ed1e588b835b321d29266", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For example, if we want TVM [9]\nto support the optimization at MVM-grained for CIM, we\nmust first register plenty of templates for various CIM archi-\ntectures.", "mimetype": "text/plain", "start_char_idx": 49324, "end_char_idx": 49483, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71902272-2de6-4cd9-b6bb-c504b6c3e657": {"__data__": {"id_": "71902272-2de6-4cd9-b6bb-c504b6c3e657", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55422b37-2367-4f08-b2ba-352448c96424", "node_type": "1", "metadata": {}, "hash": "e788bda22429abf4955c1ab4c64140bfdb8f019ca57af0012c356d507043f375", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3628b81-dad8-4954-9cf4-d9170594e7b4", "node_type": "1", "metadata": {}, "hash": "3e276db62aae846d05f225824bf9ba229893d07c3338b6de8b5504fc960a596e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Then, we need to modify the whole optimization\n194\nCIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\npass of TVM, register the function interfaces supported by\nCIM, add new classes to expose the resources of the storage\nunit, and train a new automatic compilation optimization\nmodule. It is time-consuming and may completely destroy\nthe existing compilation framework of TVM, which is not\nworth the loss.", "mimetype": "text/plain", "start_char_idx": 49484, "end_char_idx": 49968, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3628b81-dad8-4954-9cf4-d9170594e7b4": {"__data__": {"id_": "d3628b81-dad8-4954-9cf4-d9170594e7b4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71902272-2de6-4cd9-b6bb-c504b6c3e657", "node_type": "1", "metadata": {}, "hash": "526ca62f29bbc4eb5f4e87f33310f5e5afee3d68ea9ed1e588b835b321d29266", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73d0e642-7981-45ef-9ca6-287ea4299d46", "node_type": "1", "metadata": {}, "hash": "3b6b390d8d21891738eb68ee0a42861a4043a9bf4b0cb12da1384280f6f9ed74", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "VVM-Grained When this architecture offers rows acti-\nvation interface within the crossbar, which means WLM, we\nwill extend the data remapping at VVM-grained optimiza-\ntion, building upon the updated duplication.\nWithin the VVM optimization, we perform fine-grained\ncontrol over the remapping of the weight matrix onto the\ncrossbars.", "mimetype": "text/plain", "start_char_idx": 49969, "end_char_idx": 50301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73d0e642-7981-45ef-9ca6-287ea4299d46": {"__data__": {"id_": "73d0e642-7981-45ef-9ca6-287ea4299d46", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3628b81-dad8-4954-9cf4-d9170594e7b4", "node_type": "1", "metadata": {}, "hash": "3e276db62aae846d05f225824bf9ba229893d07c3338b6de8b5504fc960a596e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c3ca567-6e2f-433a-85cc-48656256688f", "node_type": "1", "metadata": {}, "hash": "8115f4238cf1d1b73a35d3801b99a52ad6a57c6242930b08f62ed2637e22d06a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As the xb_size row is 32 and parallel row is\n16, the original data that mapped to rows 0 to 15 (designated\nas \u2018A\u2019) and rows 16 to 31 (designated as \u2018B\u2019) in crossbar 0, is\nnow divided into two distinct crossbars.", "mimetype": "text/plain", "start_char_idx": 50302, "end_char_idx": 50513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c3ca567-6e2f-433a-85cc-48656256688f": {"__data__": {"id_": "7c3ca567-6e2f-433a-85cc-48656256688f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73d0e642-7981-45ef-9ca6-287ea4299d46", "node_type": "1", "metadata": {}, "hash": "3b6b390d8d21891738eb68ee0a42861a4043a9bf4b0cb12da1384280f6f9ed74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afffc6de-4b84-4e6e-9605-177137f149c8", "node_type": "1", "metadata": {}, "hash": "d10e50ebf457f345af6c573a9dc41834ae0c863cdb0954d337a874c2098479fc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\u2018A\u2019 is remapped to\ncrossbar 0\u2019s rows 0 to 15, while \u2018B\u2019 is remapped to crossbar\n1\u2019s rows 0 to 15 by the two \ud835\udc50\ud835\udc56\ud835\udc5a.\ud835\udc64\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc64instructions.", "mimetype": "text/plain", "start_char_idx": 50514, "end_char_idx": 50647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afffc6de-4b84-4e6e-9605-177137f149c8": {"__data__": {"id_": "afffc6de-4b84-4e6e-9605-177137f149c8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c3ca567-6e2f-433a-85cc-48656256688f", "node_type": "1", "metadata": {}, "hash": "8115f4238cf1d1b73a35d3801b99a52ad6a57c6242930b08f62ed2637e22d06a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c94405ec-8091-475f-82fb-af342fd478f1", "node_type": "1", "metadata": {}, "hash": "77b9270b9271e6fd526f8d023e003a9a1feeea4a2b936e5d1d14af57f0013455", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This\ndata remapping enables the concurrent computation of \u2018A\u2019\nand \u2018B\u2019 within a single cycle by \ud835\udc50\ud835\udc56\ud835\udc5a.\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc5f\ud835\udc5c\ud835\udc64\ud835\udc60in parallel, a\nmarked improvement over the previous mapping where \u2018A\u2019\nand \u2018B\u2019 required separate activation in two cycles to per-\nform the cumulative calculation essential for a single MVM\ncomputation. This simultaneous calculation of \u2018A\u2019 and \u2018B\u2019\nwithin a single cycle facilitates subsequent pipelines.", "mimetype": "text/plain", "start_char_idx": 50648, "end_char_idx": 51056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c94405ec-8091-475f-82fb-af342fd478f1": {"__data__": {"id_": "c94405ec-8091-475f-82fb-af342fd478f1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afffc6de-4b84-4e6e-9605-177137f149c8", "node_type": "1", "metadata": {}, "hash": "d10e50ebf457f345af6c573a9dc41834ae0c863cdb0954d337a874c2098479fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc23c79d-5e73-457b-b022-67edd5a95d25", "node_type": "1", "metadata": {}, "hash": "ff96b15f6cddc477047ed543025ffe1f970942bdab661de4f8aede7f6a927c62", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Similar\nto code generation in the XBM, the compiler generated the\nmeta-operators flow as shown in the Figure 16(e) WLM. A\ntotal of 512 similar compute blocks are needed to complete\nthe convolution.\n4\nExperiment\nThis section presents the evaluation of CIM-MLC. We first\npresent the implementation method of CIM-MLC, and then\nconduct a comprehensive comparison and evaluation of CIM-\nMLC with different CIM designs.", "mimetype": "text/plain", "start_char_idx": 51057, "end_char_idx": 51470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc23c79d-5e73-457b-b022-67edd5a95d25": {"__data__": {"id_": "cc23c79d-5e73-457b-b022-67edd5a95d25", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c94405ec-8091-475f-82fb-af342fd478f1", "node_type": "1", "metadata": {}, "hash": "77b9270b9271e6fd526f8d023e003a9a1feeea4a2b936e5d1d14af57f0013455", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00abe0e0-c902-4f44-8116-236ac44733a2", "node_type": "1", "metadata": {}, "hash": "05147d613fdf6fc955fd9506c33220ed8b658c614470f0d0ed17cbd531bcb0f0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4.1\nExperiment Setup\nSimulator We developed a Python-based CIM functional\nsimulator to verify the scheduling results, i.e., the meta-\noperator execution trace. Additionally, we expanded upon\nan open-source simulator from previous studies [4, 8, 15] to\nevaluate the execution latency and power efficiency of the\ngenerated scheduling results.\nWe verify the effectiveness of the functional simulator\nby comparing it with the Pytorch framework [35]. In our\nbuilt functional simulator, the hardware abstraction of CIM\nis described by a data structure, and meta-operators are im-\nplemented by specific functions.", "mimetype": "text/plain", "start_char_idx": 51471, "end_char_idx": 52077, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00abe0e0-c902-4f44-8116-236ac44733a2": {"__data__": {"id_": "00abe0e0-c902-4f44-8116-236ac44733a2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc23c79d-5e73-457b-b022-67edd5a95d25", "node_type": "1", "metadata": {}, "hash": "ff96b15f6cddc477047ed543025ffe1f970942bdab661de4f8aede7f6a927c62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1a79e65-190b-4abb-8f3f-328332de4838", "node_type": "1", "metadata": {}, "hash": "f4028d9d315ddde048c3de2ab221837c8a1d248d06d6a5896725b0b2529fe6f3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In this way, our functional\nsimulator can perform the meta-operator flows as the DNN\nexecution trace in the CIM.\nWe extended the open-source simulators proposed in pre-\nvious works [4, 8, 15] as the performance simulator to sup-\nport the execution cycle and power consumption evaluation\nTable 3. Architecture Parameters of CIM Hardware Baseline.\nChip_tier\nCore_tier\nCrossbar_tier\ncore_number\n768\nxb_number\n16\nxb_size\n[128, 128]\nALU (ops/cycle)\n1024\nALU", "mimetype": "text/plain", "start_char_idx": 52078, "end_char_idx": 52530, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1a79e65-190b-4abb-8f3f-328332de4838": {"__data__": {"id_": "c1a79e65-190b-4abb-8f3f-328332de4838", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00abe0e0-c902-4f44-8116-236ac44733a2", "node_type": "1", "metadata": {}, "hash": "05147d613fdf6fc955fd9506c33220ed8b658c614470f0d0ed17cbd531bcb0f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04013ec3-e99c-4941-a3ce-639cacea06f7", "node_type": "1", "metadata": {}, "hash": "ef929765c6195470fc97eb6071ca1b602790b34a97fc4646dac8987638f7818e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(ops/cycle)\n1024\nparallel row\n8\nL0_BW\n384 b\nL1_BW\n8192 b\nDAC/ADC\n1/8-bit\nType/Precision\nRRAM/2-bit\nChip_tier = {\n\"core_number\": 16 \n\"ALU\": \\\n\"core_noc\": \n\"Disjoint Buffer Switch\"\n\"core_noc_cost\": \\\n\"L0 size\": \\\n\"L0 BW\": \\}\nCore_tier = {\n\"xb_number\": 1\n\"ALU\": \\\n\"xb_noc\": \\\n\"xb_noc_cost\": \\\n\"L1", "mimetype": "text/plain", "start_char_idx": 52531, "end_char_idx": 52824, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04013ec3-e99c-4941-a3ce-639cacea06f7": {"__data__": {"id_": "04013ec3-e99c-4941-a3ce-639cacea06f7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1a79e65-190b-4abb-8f3f-328332de4838", "node_type": "1", "metadata": {}, "hash": "f4028d9d315ddde048c3de2ab221837c8a1d248d06d6a5896725b0b2529fe6f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8af6cba4-72c5-4c55-9671-84d3924034b3", "node_type": "1", "metadata": {}, "hash": "477a6c4ef3030d292a5b39d8c9848301de5756b666b2568dc7ed474ddf15b5f1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "size\": \\\n\"L1 BW\": \\}\nXB_tier = {\n\"xb_size\" :[1152,256]\n\u201cparallel row\":1152\n\"DAC\": 1-bit\n\"ADC\": 8-bit \n\"Type\": \"SRAM\"\n\"Precision\" :1-bit}\nComputing_Mode= \u2018CM\u2019\nFigure 17. Architecture Abstraction of Jia et al.\u2019s work [29].", "mimetype": "text/plain", "start_char_idx": 52825, "end_char_idx": 53045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8af6cba4-72c5-4c55-9671-84d3924034b3": {"__data__": {"id_": "8af6cba4-72c5-4c55-9671-84d3924034b3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04013ec3-e99c-4941-a3ce-639cacea06f7", "node_type": "1", "metadata": {}, "hash": "ef929765c6195470fc97eb6071ca1b602790b34a97fc4646dac8987638f7818e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9cee74f-4ae8-439c-887c-b4120b0f1a8c", "node_type": "1", "metadata": {}, "hash": "c513516f3492a022fd3418188356b004d7ea55e3602f92a5b35689d3fad67af3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Chip_tier = {\n\"core_number\": 138 \n\"ALU\": \\\n\"core_noc\": \"mesh\u201c\n\"core_noc_cost\": \\\n\"L0 size\": 96 KB \n\"L0 BW\": 384 b/cycle }\nCore_ tier = {\n\"xb_number\": 2\n\"ALU\": \\\n\"xb_noc\": \\\n\"xb_noc_cost\": \\\n\"L1 size\": 1 KB\n\"L1 BW\": \\}\nXB_ tier = {\n\"xb_size\" :[128,", "mimetype": "text/plain", "start_char_idx": 53046, "end_char_idx": 53293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9cee74f-4ae8-439c-887c-b4120b0f1a8c": {"__data__": {"id_": "b9cee74f-4ae8-439c-887c-b4120b0f1a8c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8af6cba4-72c5-4c55-9671-84d3924034b3", "node_type": "1", "metadata": {}, "hash": "477a6c4ef3030d292a5b39d8c9848301de5756b666b2568dc7ed474ddf15b5f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bbaca6d-21de-40aa-b72a-5131eee75c61", "node_type": "1", "metadata": {}, "hash": "05deb54522758bcb00990b577ba1dbdd21d2a44d7c9b6fac97d1edd86dfaf3f0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "128]\n\u201cparallel row\":128\n\"ADC\": 1-bit\n\"DAC\": 8-bit \n\"Type\": \"ReRAM\"\n\"Precision\" :2-bit}\nComputing_Mode= \u2018XBM\u2019\nFigure 18. Architecture Abstraction of PUMA [4].", "mimetype": "text/plain", "start_char_idx": 53293, "end_char_idx": 53450, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4bbaca6d-21de-40aa-b72a-5131eee75c61": {"__data__": {"id_": "4bbaca6d-21de-40aa-b72a-5131eee75c61", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9cee74f-4ae8-439c-887c-b4120b0f1a8c", "node_type": "1", "metadata": {}, "hash": "c513516f3492a022fd3418188356b004d7ea55e3602f92a5b35689d3fad67af3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c5cbc7f-e6ba-4b31-a886-8b211cbc52ed", "node_type": "1", "metadata": {}, "hash": "9e308567703b8b04c9e4f402b89a87380cb6f4efe4e8e7c3a761bf134cb8bdc7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Chip_tier = {\n\"core_number\": 4 \n\"ALU\": \\\n\"core_noc \" : \\\n\"core_noc_cost\": \\\n\"L0 size\": \\\n\"L0 BW\": \\}\nCore_tier = {\n\"xb_number\": 2\n\"ALU\": \\\n\"xb_noc\": \\\n\"xb_noc_cost\": \\\n\"L1 size\": \\\n\"L1 BW\": \\}\nXB_tier = {\n\"xb_size\" :[256,", "mimetype": "text/plain", "start_char_idx": 53451, "end_char_idx": 53672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c5cbc7f-e6ba-4b31-a886-8b211cbc52ed": {"__data__": {"id_": "2c5cbc7f-e6ba-4b31-a886-8b211cbc52ed", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bbaca6d-21de-40aa-b72a-5131eee75c61", "node_type": "1", "metadata": {}, "hash": "05deb54522758bcb00990b577ba1dbdd21d2a44d7c9b6fac97d1edd86dfaf3f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "546c4fc7-cfdf-4307-90ce-9add0588b0ca", "node_type": "1", "metadata": {}, "hash": "21b3013954c1571b2ad1bb9d256e684fe67e4ae1ab3884d8de6103a8d135de38", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "64]\n\u201cparallel row\":32\n\"DAC\": 1-bit\n\"ADC\": 6-bit \n\"Type\": \"SRAM\"\n\"Precision\" :1-bit}\nComputing_Mode= \u2018WLM\u2019\nFigure 19. Architecture Abstraction of Jain et al.\u2019s work [27].\nof meta-operators flow on the CIM-based DNN accelerators.\nThe primary extensions include: 1. We developed compu-\ntational functions to facilitate simulating meta-operation\nexecution, allowing the simulator to represent CIM archi-\ntectures with varying computation granularity levels. 2.", "mimetype": "text/plain", "start_char_idx": 53672, "end_char_idx": 54128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "546c4fc7-cfdf-4307-90ce-9add0588b0ca": {"__data__": {"id_": "546c4fc7-cfdf-4307-90ce-9add0588b0ca", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c5cbc7f-e6ba-4b31-a886-8b211cbc52ed", "node_type": "1", "metadata": {}, "hash": "9e308567703b8b04c9e4f402b89a87380cb6f4efe4e8e7c3a761bf134cb8bdc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cb70317-e840-44c4-8f61-b0ef5a49fab5", "node_type": "1", "metadata": {}, "hash": "9c490ee77ce553d598b6e0b4f736f7cf92a8e80d89771f6610e62b90cc3d7a5a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We\nestablish the latency model, including computation, data\nmovement, etc. to evaluate the overall DNN latency.\nCIM Architecture Baseline We refer to ISAAC [39] to\nestablish a CIM architecture and use it as the baseline. The\nparameters of the baseline architecture are listed in Table 3.\nThe parameters that are not elaborated are considered ideal,\nindicating that their influence on the evaluation is disre-\ngarded. For example, if the on-chip buffer is assumed to have\nsufficient bandwidth, load/store time can be hidden within\nthe computation time.", "mimetype": "text/plain", "start_char_idx": 54129, "end_char_idx": 54680, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4cb70317-e840-44c4-8f61-b0ef5a49fab5": {"__data__": {"id_": "4cb70317-e840-44c4-8f61-b0ef5a49fab5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "546c4fc7-cfdf-4307-90ce-9add0588b0ca", "node_type": "1", "metadata": {}, "hash": "21b3013954c1571b2ad1bb9d256e684fe67e4ae1ab3884d8de6103a8d135de38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d195d361-fc63-4315-8fc7-3f140bc13744", "node_type": "1", "metadata": {}, "hash": "cf9e8a84462811380f786cff7618e9d70f9138a9da43aa114eebc8aec196d2d6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We evaluate our CIM-MLC on the\nbaseline to verify its effectiveness and compare it with Poly-\nSchedule [22], which is also a compilation work for CIMs.\nNetwork Benchmark First, to verify the generality of\nthe scheduling method for different neural network tasks,\nwe tested our operator scheduling method on multiple clas-\nsic network models, including the VGG series [41], ResNet\nseries [24], visual transformer (ViT) [16], etc.", "mimetype": "text/plain", "start_char_idx": 54681, "end_char_idx": 55109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d195d361-fc63-4315-8fc7-3f140bc13744": {"__data__": {"id_": "d195d361-fc63-4315-8fc7-3f140bc13744", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cb70317-e840-44c4-8f61-b0ef5a49fab5", "node_type": "1", "metadata": {}, "hash": "9c490ee77ce553d598b6e0b4f736f7cf92a8e80d89771f6610e62b90cc3d7a5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35b0a8f7-0aa9-4837-8c58-3c180fff32a5", "node_type": "1", "metadata": {}, "hash": "3ed04b05095004974d69e9e7697a4fa458bf5ce4ff58b5eea39a793b549e0c89", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "All models\u2019\n195\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\nSongyun Qu, Shixin Zhao, Bing Li, Yintao He, Xuyi Cai, Lei Zhang, and Ying Wang\nweights and activation values are quantized with 8-bit preci-\nsion and are tested on the ImageNet dataset.", "mimetype": "text/plain", "start_char_idx": 55110, "end_char_idx": 55364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35b0a8f7-0aa9-4837-8c58-3c180fff32a5": {"__data__": {"id_": "35b0a8f7-0aa9-4837-8c58-3c180fff32a5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d195d361-fc63-4315-8fc7-3f140bc13744", "node_type": "1", "metadata": {}, "hash": "cf9e8a84462811380f786cff7618e9d70f9138a9da43aa114eebc8aec196d2d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ac0724a-5cba-4014-bbda-3626cf04f5ef", "node_type": "1", "metadata": {}, "hash": "50c6b684a32dfc54f93d0ab6358cbfaaec42b8747188876ca8753b22a48529ab", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Hardware Benchmark To verify the generality of CIM-\nMLC for different CIM architectures, we conduct experi-\nments on three CIM-based accelerators [4, 27, 29]with differ-\nent device types, precision, architecture hierarchy, and pro-\ngramming interfaces. Among them, Jia et al. [29] proposed an\nSRAM-based CIM accelerator that includes 16 CIMUs with a\nsize of 1152x256, which incorporates embedded digital logic\nand high-precision ADC to achieve parallel activation cal-\nculation of 1152 rows.", "mimetype": "text/plain", "start_char_idx": 55365, "end_char_idx": 55856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ac0724a-5cba-4014-bbda-3626cf04f5ef": {"__data__": {"id_": "1ac0724a-5cba-4014-bbda-3626cf04f5ef", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35b0a8f7-0aa9-4837-8c58-3c180fff32a5", "node_type": "1", "metadata": {}, "hash": "3ed04b05095004974d69e9e7697a4fa458bf5ce4ff58b5eea39a793b549e0c89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d97b1890-ebbd-495a-856f-a17415b6a208", "node_type": "1", "metadata": {}, "hash": "3103ea04cacfb4fbaba91e9d1e7a923283494e53f81cc6211450ba79b65a4f5f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "PUMA [4] is a programmable CIM\narchitecture based on ReRAM to support a wide range of\nneural network applications. Jain et al. [27] introduces a\nCIM SRAM macro, in which only limited rows (\u226432) can\nbe activated simultaneously in a crossbar to alleviate the\ncomputing variation.", "mimetype": "text/plain", "start_char_idx": 55857, "end_char_idx": 56134, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d97b1890-ebbd-495a-856f-a17415b6a208": {"__data__": {"id_": "d97b1890-ebbd-495a-856f-a17415b6a208", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ac0724a-5cba-4014-bbda-3626cf04f5ef", "node_type": "1", "metadata": {}, "hash": "50c6b684a32dfc54f93d0ab6358cbfaaec42b8747188876ca8753b22a48529ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5959cee1-6887-4d70-81ab-349054816bc5", "node_type": "1", "metadata": {}, "hash": "0d6b530f938d242c85c532b97f68a24d423fb10716352cd2c47f90076ff95fb7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4.2\nEffectiveness of CIM-MLC\nIn this section, we apply the CIM-MLC in existing CIM-based\naccelerators [4, 27, 29] to verify the generality of CIM-MLC\nand also compare CIM-MLC with previous work to show the\neffectiveness of our proposed optimization.\nFirstly, we demonstrate the comparison results of CIM-\nMLC with these three different CIM-based accelerators. These\nworks have not only the specific CIM architecture but also\nthe performance optimization methods.", "mimetype": "text/plain", "start_char_idx": 56135, "end_char_idx": 56597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5959cee1-6887-4d70-81ab-349054816bc5": {"__data__": {"id_": "5959cee1-6887-4d70-81ab-349054816bc5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d97b1890-ebbd-495a-856f-a17415b6a208", "node_type": "1", "metadata": {}, "hash": "3103ea04cacfb4fbaba91e9d1e7a923283494e53f81cc6211450ba79b65a4f5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdbdd574-6557-47f7-947b-3b1b5a07d17a", "node_type": "1", "metadata": {}, "hash": "adbf753d8418abe6eb76d97b2f03f63aea3f5458a2ff7202b934ec5d7f0c16ad", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "So, we first verify\nour abstraction techniques on their CIM distinct architec-\ntures and then compare the optimized performance by our\nmethod with their performance.\nWork 1: The hardware abstraction results of Jia et al.\u2019s\nwork are shown in Figure 17. The parameters that are not\nelaborated are considered ideal and denoted with \"\\\". The\ncomputing mode abstraction of this design is the core mode\n(CM). CIM-MLC will apply CG-grained optimization to gen-\nerate the meta-operator flow when mapping and schedul-\ning the DNN.", "mimetype": "text/plain", "start_char_idx": 56598, "end_char_idx": 57119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdbdd574-6557-47f7-947b-3b1b5a07d17a": {"__data__": {"id_": "bdbdd574-6557-47f7-947b-3b1b5a07d17a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5959cee1-6887-4d70-81ab-349054816bc5", "node_type": "1", "metadata": {}, "hash": "0d6b530f938d242c85c532b97f68a24d423fb10716352cd2c47f90076ff95fb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdb97048-86ad-4596-a180-c5ee577bfad3", "node_type": "1", "metadata": {}, "hash": "e989c08d50e4608bc42b566cdf9c52818ec5ff4c4b526824e2e5d6c9b53bf2b5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The performance comparison between CIM-\nMLC and Jia et al.\u2019s work is shown in Figure 20 (a). The CG-\ngrained P&D is the combination of the dynamic programming-\nbased duplication and the pipeline, bringing about 3.7\u00d7 speedup\nover Jia et al.\u2019s work as it can make full use of the limited re-\nsources and speed up the computing of the bottleneck layer\nin DNNs.", "mimetype": "text/plain", "start_char_idx": 57120, "end_char_idx": 57477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdb97048-86ad-4596-a180-c5ee577bfad3": {"__data__": {"id_": "cdb97048-86ad-4596-a180-c5ee577bfad3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdbdd574-6557-47f7-947b-3b1b5a07d17a", "node_type": "1", "metadata": {}, "hash": "adbf753d8418abe6eb76d97b2f03f63aea3f5458a2ff7202b934ec5d7f0c16ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a184e5b0-3a97-470b-98cd-1f8b9e1acb09", "node_type": "1", "metadata": {}, "hash": "a83cf8339c633b22b88534a2fbe347ecd19af538b3bfd79d9939cb56e079f8d4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The pipeline strategy can only achieve the speedup\nby 1.2\u00d7 over Jia et al.\u2019s work because the model size exceeds\non-chip resources and the performance would not be fully\noptimized without the data mapping design.\nWork 2: Our hardware abstraction results for PUMA [4]\nare shown in Figure 18, where the computing mode is XBM.\nThus, CIM-MLC can perform CG-grained and MVM-grained\noptimization. We compare PUMA with our work on the\nVGG16, and the results are shown in Figure 20 (b).", "mimetype": "text/plain", "start_char_idx": 57478, "end_char_idx": 57956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a184e5b0-3a97-470b-98cd-1f8b9e1acb09": {"__data__": {"id_": "a184e5b0-3a97-470b-98cd-1f8b9e1acb09", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdb97048-86ad-4596-a180-c5ee577bfad3", "node_type": "1", "metadata": {}, "hash": "e989c08d50e4608bc42b566cdf9c52818ec5ff4c4b526824e2e5d6c9b53bf2b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c069c6d9-2b0c-45a4-81c5-10c438813aa2", "node_type": "1", "metadata": {}, "hash": "8cb84772e078d52953bb4314f370866a16a149de5c5beb5e76922e02ab239660", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "When\nmapping the VGG16 model, the proposed MVM-grained op-\ntimization fully utilized the scheduling space of the XBM\nmode in PUMA architecture and introduced a fine-grained\nMVM-grained pipeline. Our evaluation of peak power in-\ncludes the power consumption of ADC/DAC, XB activation\ncomputation, and data movement. Our assessment shows\nthat these three parts account for 10%, 83%, and 7%, respec-\ntively.", "mimetype": "text/plain", "start_char_idx": 57957, "end_char_idx": 58361, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c069c6d9-2b0c-45a4-81c5-10c438813aa2": {"__data__": {"id_": "c069c6d9-2b0c-45a4-81c5-10c438813aa2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a184e5b0-3a97-470b-98cd-1f8b9e1acb09", "node_type": "1", "metadata": {}, "hash": "a83cf8339c633b22b88534a2fbe347ecd19af538b3bfd79d9939cb56e079f8d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6263bf63-7175-42c0-93df-82ddcfdfc4c9", "node_type": "1", "metadata": {}, "hash": "8c5d36fa7fa29dfcbe6f9dc3b3dde7614b45a3ad9b5bdea62cb048e924f0d21c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Thus, CIM-MLC performs fine-grained time-division\nactivation of XBs and associated ADC/DACs, thereby signif-\nicantly reducing the peak power by 75%.\nWork 3: Figure 19 presents the hardware abstraction\nof Jain et al.\u2019s CIM macro [27] that has \ud835\udc4a\ud835\udc3f\ud835\udc40computing\nmode. For a fair comparison, we use the VGG7 model as\nthe benchmark and evaluate the scheduling results of CIM-\nMLC and the original result under the same resource con-\nstraints.", "mimetype": "text/plain", "start_char_idx": 58362, "end_char_idx": 58795, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6263bf63-7175-42c0-93df-82ddcfdfc4c9": {"__data__": {"id_": "6263bf63-7175-42c0-93df-82ddcfdfc4c9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c069c6d9-2b0c-45a4-81c5-10c438813aa2", "node_type": "1", "metadata": {}, "hash": "8cb84772e078d52953bb4314f370866a16a149de5c5beb5e76922e02ab239660", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d58f374-c620-4eeb-abd1-5341196fabf5", "node_type": "1", "metadata": {}, "hash": "a563b8c332808e599476eed3fd9fc936b70904eee794b7cb6b1d775a058402ec", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As shown in Figure 20 (c), with the three-level\nscheduling optimization (i.e., CG-grained, MVM-grained,\nand VVM-grained), we achieve a speedup of about 2.3\u00d7 over\nJain et al.\u2019s work. Meanwhile, we show the speedup of CG-\ngrained optimization and MVM-grained optimization, re-\nspectively. CG-grained optimization can achieve a speedup\nof 1.2\u00d7, while MVM-grained optimization cannot further\nbring effective acceleration improvement.", "mimetype": "text/plain", "start_char_idx": 58796, "end_char_idx": 59225, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d58f374-c620-4eeb-abd1-5341196fabf5": {"__data__": {"id_": "9d58f374-c620-4eeb-abd1-5341196fabf5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6263bf63-7175-42c0-93df-82ddcfdfc4c9", "node_type": "1", "metadata": {}, "hash": "8c5d36fa7fa29dfcbe6f9dc3b3dde7614b45a3ad9b5bdea62cb048e924f0d21c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a39b38e-6419-4ce9-b4ae-6db06d20cdac", "node_type": "1", "metadata": {}, "hash": "72b6ef9c3559160b5e52443f95dabdf72d4d49087152c1e98b047d48b277d6e5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This is because\nthis CIM macro has limited on-chip resources, especially\nthe small number of XBs in a Core, leading to the ineffec-\ntiveness of MVM-grained optimization for improving the\nspeedup. The VVM-grained optimization fully improves the\ncomputing pipeline efficiency between adjacent operators by\nconverting serial computations into parallel computations,\nthereby achieving a speedup.\nComparison to CIM-oriented compilers: We compared\nour method with the existing general-purpose CIM compiler\ntool Poly-Schedule [22].", "mimetype": "text/plain", "start_char_idx": 59226, "end_char_idx": 59750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a39b38e-6419-4ce9-b4ae-6db06d20cdac": {"__data__": {"id_": "0a39b38e-6419-4ce9-b4ae-6db06d20cdac", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d58f374-c620-4eeb-abd1-5341196fabf5", "node_type": "1", "metadata": {}, "hash": "a563b8c332808e599476eed3fd9fc936b70904eee794b7cb6b1d775a058402ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca649d76-1076-40b0-917c-fc515ee25dd5", "node_type": "1", "metadata": {}, "hash": "95037c24e0f36558b1a7449c04f58d4e67d8d696c40cfc638fd5c810c4306f49", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Poly-Schedule [22] supports the\ncompilation of CIM-based accelerators for CM and XBM, us-\ning operator duplication techniques based on greedy strate-\ngies and batch pipeline strategies for acceleration. Compared\nwith this work, CIM-MLC can optimize the internal com-\nputation pipeline of a single input image and explore the\nfine-grained scheduling space of the XBM mode to achieve\nbetter scheduling. We compared the operator scheduling re-\nsults between Poly-Schedule and CIM-MLC in the same CIM\narchitecture abstracted in Table 3.", "mimetype": "text/plain", "start_char_idx": 59751, "end_char_idx": 60283, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca649d76-1076-40b0-917c-fc515ee25dd5": {"__data__": {"id_": "ca649d76-1076-40b0-917c-fc515ee25dd5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a39b38e-6419-4ce9-b4ae-6db06d20cdac", "node_type": "1", "metadata": {}, "hash": "72b6ef9c3559160b5e52443f95dabdf72d4d49087152c1e98b047d48b277d6e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0713f40c-5bba-4f62-863f-f5df74e60599", "node_type": "1", "metadata": {}, "hash": "9613fda0fb7831e72cc7238ab9059be29d3a84dee567f5c80c4acf4b17c78017", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As shown in Figure 20 (d),\ncompared to the latency result without any optimization, the\nPoly-Schedule [22] utilizes on-chip resources with a greedy\nstrategy to reduce 84% computation cycles. Our work ex-\nplores the fine-grained optimization space and reduces the\ncomputation cycles by up to 95%, which achieves about 3.2\u00d7\nspeedup compared to the work [22].", "mimetype": "text/plain", "start_char_idx": 60284, "end_char_idx": 60640, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0713f40c-5bba-4f62-863f-f5df74e60599": {"__data__": {"id_": "0713f40c-5bba-4f62-863f-f5df74e60599", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca649d76-1076-40b0-917c-fc515ee25dd5", "node_type": "1", "metadata": {}, "hash": "95037c24e0f36558b1a7449c04f58d4e67d8d696c40cfc638fd5c810c4306f49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1a344f5-db37-4369-a773-e33274346cb1", "node_type": "1", "metadata": {}, "hash": "756620a6e8e657d39c6ed91a620c4803224becbbdb5489c643668d21bcb22f7f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4.3\nPerformance Analysis\nIn this section, we analyze the speedup results of differ-\nent granularity scheduling optimization in the multi-level\nscheduling for the baseline architecture in Table 3. The net-\nwork benchmark is the ResNet series [24]. The results are\nshown in Figure 21. In Figure 21 (a), we separate the op-\ntimization methods (i.e. CG-Pipeline and CG-Duplication,", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1a344f5-db37-4369-a773-e33274346cb1": {"__data__": {"id_": "d1a344f5-db37-4369-a773-e33274346cb1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0713f40c-5bba-4f62-863f-f5df74e60599", "node_type": "1", "metadata": {}, "hash": "9613fda0fb7831e72cc7238ab9059be29d3a84dee567f5c80c4acf4b17c78017", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f55ae1bc-2d41-4058-9444-a1131cebd5ef", "node_type": "1", "metadata": {}, "hash": "481108426751d944501657fbcf6db704acb8134a9df7762596b778ae2fede9db", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "CG-P&D) in the CG-grained optimization and investigate\n196\nCIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA, USA\n0\n50\n100\n150\n200\n250\n300\nPUMA[2, 4]\nCG+MVM-grained\n75%\nNormalized peak power\n1\n1.2\n3.7\n0\n1\n2\n3\n4\nJia et al.", "mimetype": "text/plain", "start_char_idx": 61020, "end_char_idx": 61316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f55ae1bc-2d41-4058-9444-a1131cebd5ef": {"__data__": {"id_": "f55ae1bc-2d41-4058-9444-a1131cebd5ef", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1a344f5-db37-4369-a773-e33274346cb1", "node_type": "1", "metadata": {}, "hash": "756620a6e8e657d39c6ed91a620c4803224becbbdb5489c643668d21bcb22f7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "679d9b84-6142-429c-9ef1-4b5ef77d27fe", "node_type": "1", "metadata": {}, "hash": "a8aec122f742b750bcb5608b47d91055801eb32d60e475252e82b37c98af7087", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[29]\nCG-grained w/ Pipeline\nCG-grained w/ P&D\nSpeedup\n1\n1.2\n1.2\n2.3\n0\n1\n2\nJain et al.[27]\nCG-grained\nCG+MVM-grained\nCG+MVM+VVM-grained\nSpeedup\n0\n250000\n500000\nw/o optimization\nPoly-Schedule[22]\nCIM-MLC\n(a)\n(b)\n(c)\n(d)\nLatency\n84%\n95%\nFigure 20.", "mimetype": "text/plain", "start_char_idx": 61316, "end_char_idx": 61560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "679d9b84-6142-429c-9ef1-4b5ef77d27fe": {"__data__": {"id_": "679d9b84-6142-429c-9ef1-4b5ef77d27fe", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f55ae1bc-2d41-4058-9444-a1131cebd5ef", "node_type": "1", "metadata": {}, "hash": "481108426751d944501657fbcf6db704acb8134a9df7762596b778ae2fede9db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98f0f3c1-d81b-4603-b0ce-c529d844818d", "node_type": "1", "metadata": {}, "hash": "221f5225c98d16ab9ff6a1c8b04ca44685b3b42add99b10742fb328085ef732f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(a) Comparison of the speedup between this work and the schedule method in work [29]; (b) Comparison of the\npeak power consumption between this work and the schedule method in PUMA [2, 4]; (c) Comparison of the speedup between\nthis work and the schedule method in work[27]; (d) Comparison of the latency between this work and the schedule method in\nwork [22].", "mimetype": "text/plain", "start_char_idx": 61561, "end_char_idx": 61920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98f0f3c1-d81b-4603-b0ce-c529d844818d": {"__data__": {"id_": "98f0f3c1-d81b-4603-b0ce-c529d844818d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "679d9b84-6142-429c-9ef1-4b5ef77d27fe", "node_type": "1", "metadata": {}, "hash": "a8aec122f742b750bcb5608b47d91055801eb32d60e475252e82b37c98af7087", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eddfd3bb-1f51-4635-87be-37b010a11127", "node_type": "1", "metadata": {}, "hash": "e91d9d01936b92d048dee4f8babe334cc97e19e05815c031d1e393436e3664cf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "0\n5\n10\n15\n20\nResNet18\nResNet34\nResNet50\nResNet101\n1\n10\n100\nResNet18 ResNet34 ResNet50 ResNet101\nCG-Pipeline\nCG-Duplication\nCG-P&D\n0.8\n1\n1.2\n1.4\n1.6\n1.8\nResNet18 ResNet34 ResNet50 ResNet101\n1\n1.02\n1.04\n1.06\n1.08\n1.1\n1.", "mimetype": "text/plain", "start_char_idx": 61921, "end_char_idx": 62138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eddfd3bb-1f51-4635-87be-37b010a11127": {"__data__": {"id_": "eddfd3bb-1f51-4635-87be-37b010a11127", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98f0f3c1-d81b-4603-b0ce-c529d844818d", "node_type": "1", "metadata": {}, "hash": "221f5225c98d16ab9ff6a1c8b04ca44685b3b42add99b10742fb328085ef732f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9058e019-cc2e-45fd-bf9a-958b67c89a1f", "node_type": "1", "metadata": {}, "hash": "27863e351db195b8c8dc8f5b06ee0049a68dd9fb4bda1bf4a8ad54f365a086a5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "12\nResNet18 ResNet34 ResNet50 ResNet101\nSpeedup\nCG+MVM-Duplication\nCG+MVM+VVM-Remap\n(a)\n(b)\n(c)\n(d)\nNormalized peak power\nW/O \noptimization\nCG\nCG+MVM-\nDuplication\nCG+MVM-\nP&D\nFigure 21.", "mimetype": "text/plain", "start_char_idx": 62138, "end_char_idx": 62323, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9058e019-cc2e-45fd-bf9a-958b67c89a1f": {"__data__": {"id_": "9058e019-cc2e-45fd-bf9a-958b67c89a1f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eddfd3bb-1f51-4635-87be-37b010a11127", "node_type": "1", "metadata": {}, "hash": "e91d9d01936b92d048dee4f8babe334cc97e19e05815c031d1e393436e3664cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5dc4aef-af8e-430e-bab8-bf0647274cf0", "node_type": "1", "metadata": {}, "hash": "081708e476111b666ec2562683d8980b34184c0ccede623515b17ebe4745e3e4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(a) Speedup of CG-grained optimization; (b) Speedup of CG+MVM-grained optimization; (c) Speedup of\nCG+MVM+VVM-grained optimization; (d) Comparison result of the peak power.\ntheir results, and the results are normalized to the results\nof the baseline architecture in CM mode without any opti-\nmization. In Figure 21 (b)-(c), the fine-grained optimization\nis with the coarse-grained ones to show the advantage of\nthe multi-grained optimization.", "mimetype": "text/plain", "start_char_idx": 62324, "end_char_idx": 62766, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5dc4aef-af8e-430e-bab8-bf0647274cf0": {"__data__": {"id_": "b5dc4aef-af8e-430e-bab8-bf0647274cf0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9058e019-cc2e-45fd-bf9a-958b67c89a1f", "node_type": "1", "metadata": {}, "hash": "27863e351db195b8c8dc8f5b06ee0049a68dd9fb4bda1bf4a8ad54f365a086a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ebd33d3-3c2b-4d46-87e1-8500b2da901c", "node_type": "1", "metadata": {}, "hash": "5d7f770aaaad001b470460ea2197a9ba024814909a3ead76cb501dbe681ce3f8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The results of Figure 21 (b)\nare normalized to the CG-P&D results in Figure 21 (a), and\nFigure 21 (c)\u2019s results use the results in Figure 21 (b) as the\nbaseline.\nIt can be observed that the model structure has a signifi-\ncant impact on the scheduling results. As shown in Figure 21\n(a), in CG-grained optimization, as the depth of ResNet in-\ncreases, the speedup achieved by pipeline (CG-Pipeline) in-\ncreases from 2.3\u00d7 to 4.7\u00d7.", "mimetype": "text/plain", "start_char_idx": 62767, "end_char_idx": 63195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ebd33d3-3c2b-4d46-87e1-8500b2da901c": {"__data__": {"id_": "2ebd33d3-3c2b-4d46-87e1-8500b2da901c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5dc4aef-af8e-430e-bab8-bf0647274cf0", "node_type": "1", "metadata": {}, "hash": "081708e476111b666ec2562683d8980b34184c0ccede623515b17ebe4745e3e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "032e29c9-31bf-4d4c-a141-75b44d8eb772", "node_type": "1", "metadata": {}, "hash": "536493588bc5cf819b347faffa732286508923138f0fe913ad11cb2540a1c3a3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The pipeline strategy can fully\nperform parallel computation between adjacent operators.\nHowever, with the increase of the model size (from ResNet18\nto ResNet101), the speedup of duplication (CG-Duplication)\ndecreases from 25.4\u00d7 to 3.1\u00d7. When combining the pipeline\nand duplication strategies (CG-P&D), ResNet series models\nachieve significant performance improvements up to 123\u00d7.", "mimetype": "text/plain", "start_char_idx": 63196, "end_char_idx": 63576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "032e29c9-31bf-4d4c-a141-75b44d8eb772": {"__data__": {"id_": "032e29c9-31bf-4d4c-a141-75b44d8eb772", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ebd33d3-3c2b-4d46-87e1-8500b2da901c", "node_type": "1", "metadata": {}, "hash": "5d7f770aaaad001b470460ea2197a9ba024814909a3ead76cb501dbe681ce3f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b61ca803-8a0c-426d-a376-31f0b920ab4e", "node_type": "1", "metadata": {}, "hash": "1b1736dbc88c07e539d137f2eb0ebdd7388613bb71835edd03865f24896e6bc0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As shown in Figure 21 (b), CG+MVM-Duplication increase\nthe speedup of ResNet50/ResNet101 by approximately 1.8\u00d7\n/ 1.4\u00d7 over the CG-P&D. Meanwhile, as shown in Figure 21\n(d), the normalized peak power consumption increases by\napproximately 5\u00d7 to 16\u00d7 for the ResNet series in CG-grained\noptimization since the number of crossbars that work at the\nsame time increases, leading to the high power consump-\ntion of CIM accelerator.", "mimetype": "text/plain", "start_char_idx": 63577, "end_char_idx": 64001, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b61ca803-8a0c-426d-a376-31f0b920ab4e": {"__data__": {"id_": "b61ca803-8a0c-426d-a376-31f0b920ab4e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "032e29c9-31bf-4d4c-a141-75b44d8eb772", "node_type": "1", "metadata": {}, "hash": "536493588bc5cf819b347faffa732286508923138f0fe913ad11cb2540a1c3a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f05985af-ddd1-4781-a431-89662ab67aba", "node_type": "1", "metadata": {}, "hash": "595f23cf0aadb4f171109fc47be891eeb3b852e9d49401255788d27fe225dd84", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Then, MVM-grained optimization\nreduces peak power consumption by up to 85% (ResNet101)\nthanks to its fine-grained pipeline strategy that lowers the\npeak activated crossbar number.\nThe results in Figure 21 (c) show that the speedup of\nResNet50 in the VVM-grained optimization can be 10% higher\nthan that of the MVM-grained optimization because the data\nremapping strategy improves the pipeline throughput.", "mimetype": "text/plain", "start_char_idx": 64002, "end_char_idx": 64406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f05985af-ddd1-4781-a431-89662ab67aba": {"__data__": {"id_": "f05985af-ddd1-4781-a431-89662ab67aba", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b61ca803-8a0c-426d-a376-31f0b920ab4e", "node_type": "1", "metadata": {}, "hash": "1b1736dbc88c07e539d137f2eb0ebdd7388613bb71835edd03865f24896e6bc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4acb8290-def3-4c17-8463-491aa486ddbd", "node_type": "1", "metadata": {}, "hash": "98350c22f98f563776ee8358a5cc1311423d2eb13b110067be8a19169992309a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4.4\nSensitive Study of CIM Architecture\nIn this section, we evaluate the effect of the change of CIM ar-\nchitecture parameters on the scheduling results of CIM-MLC.\nWe use a transformer network architecture, ViT [16], as the\nbenchmark. The baselined architecture uses the parameters\nfrom Table 3 except for the crossbar size is 128 \u00d7 256.\n4.4.1\nCore Number & Crossbar Number. First, we in-\nvestigate the effect of different core numbers on the effec-\ntiveness of CIM-MLC.", "mimetype": "text/plain", "start_char_idx": 64407, "end_char_idx": 64878, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4acb8290-def3-4c17-8463-491aa486ddbd": {"__data__": {"id_": "4acb8290-def3-4c17-8463-491aa486ddbd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f05985af-ddd1-4781-a431-89662ab67aba", "node_type": "1", "metadata": {}, "hash": "595f23cf0aadb4f171109fc47be891eeb3b852e9d49401255788d27fe225dd84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "076007d3-dbac-40e8-9072-f2e34c5d2d56", "node_type": "1", "metadata": {}, "hash": "79d992be4fedaec85682678bc17c5b3f324239d2557d8361ff2b896cd1c1271c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The results are shown in Figure 22 (a).\nAs the total number of cores increases from 256 to 1024, the\nspeedup achieved by CIM-MLC grows as well. Since on-chip\nresources gradually increase, CIM-MLC can fully show the\npotential of the duplication strategy as well as the pipeline.\nTherefore, the speedup of the CG-grained optimization in-\ncreased from 15\u00d7 to 30\u00d7. In the MVM-grained optimization,\n197\nASPLOS \u201924, April 27-May 1, 2024, La Jolla, CA,", "mimetype": "text/plain", "start_char_idx": 64879, "end_char_idx": 65324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "076007d3-dbac-40e8-9072-f2e34c5d2d56": {"__data__": {"id_": "076007d3-dbac-40e8-9072-f2e34c5d2d56", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4acb8290-def3-4c17-8463-491aa486ddbd", "node_type": "1", "metadata": {}, "hash": "98350c22f98f563776ee8358a5cc1311423d2eb13b110067be8a19169992309a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9405157d-0ef7-43f3-bd50-fdcd2ee50d08", "node_type": "1", "metadata": {}, "hash": "b48234fce4d6f701f9524e74f48d51ed5ba221f02599a02ad42b6c45c105775e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "USA\nSongyun Qu, Shixin Zhao, Bing Li, Yintao He, Xuyi Cai, Lei Zhang, and Ying Wang\n10\n15\n20\n25\n30\n35\n40\n64\n32\n16\n8\nParallel row\nCG -Grained\nCG+MVM-Grained\nCG+MVM+VVM-Grained\n10\n15\n20\n25\n30\n35\n40\n8\n12\n16\n20\nCrossbar number\nCG", "mimetype": "text/plain", "start_char_idx": 65325, "end_char_idx": 65550, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9405157d-0ef7-43f3-bd50-fdcd2ee50d08": {"__data__": {"id_": "9405157d-0ef7-43f3-bd50-fdcd2ee50d08", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "076007d3-dbac-40e8-9072-f2e34c5d2d56", "node_type": "1", "metadata": {}, "hash": "79d992be4fedaec85682678bc17c5b3f324239d2557d8361ff2b896cd1c1271c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7da2ddf7-be0f-4a43-873c-d8bc0d9a81bd", "node_type": "1", "metadata": {}, "hash": "e03619548826ab0acf1b16a24955fbdf14c01d7b1280a40c6b4483d946d35cad", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "-Grained\nCG+MVM-Grained\nCG+MVM+VVM-Grained\n10\n15\n20\n25\n30\n35\n40\n256\n512\n768\n1024\nSpeedup\nCore number\nCG -Grained\nCG+MVM-Grained\nCG+MVM+VVM-Grained\n(a)\n(b)\n(c)\n(d)\n10\n15\n20\n25\n30\n35\n40\n64\u00d7512\n128\u00d7256\n256\u00d7128\n512\u00d764\nCrossbar size\nCG", "mimetype": "text/plain", "start_char_idx": 65551, "end_char_idx": 65781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7da2ddf7-be0f-4a43-873c-d8bc0d9a81bd": {"__data__": {"id_": "7da2ddf7-be0f-4a43-873c-d8bc0d9a81bd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9405157d-0ef7-43f3-bd50-fdcd2ee50d08", "node_type": "1", "metadata": {}, "hash": "b48234fce4d6f701f9524e74f48d51ed5ba221f02599a02ad42b6c45c105775e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6905bfa-7ee1-4f4f-b610-d408f7084f6e", "node_type": "1", "metadata": {}, "hash": "c467f79743f0e5c16e5c90f0f124f50e334c433fcbecba7c3262bc87881b643f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "-Grained\nCG+MVM-Grained\nCG+MVM+VVM-Grained\nFigure 22. (a) Scheduling results with the different number of cores in a chip; (b) Scheduling results with different numbers of\ncrossbars in a core; (c) Scheduling results with different crossbar sizes; (d) Scheduling results with different parallel rows in a\ncrossbar.", "mimetype": "text/plain", "start_char_idx": 65782, "end_char_idx": 66095, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6905bfa-7ee1-4f4f-b610-d408f7084f6e": {"__data__": {"id_": "b6905bfa-7ee1-4f4f-b610-d408f7084f6e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7da2ddf7-be0f-4a43-873c-d8bc0d9a81bd", "node_type": "1", "metadata": {}, "hash": "e03619548826ab0acf1b16a24955fbdf14c01d7b1280a40c6b4483d946d35cad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfeeb803-6b00-476b-b25b-2d8381b45b7b", "node_type": "1", "metadata": {}, "hash": "269e64b5e76e0ca24809060e79038a1bdc1fd898323b53809fcf01783b83c4f8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "as the number of cores increased, the proposed method could\nduplicate a single operator to different crossbars to fully uti-\nlize resources, achieving a maximum speedup improvement\nof about 1.1\u00d7 compared to the CG-grained optimization. In\nthe VVM-grained optimization, a finer pipeline reduced the\nequivalent activation times of the crossbar, further resulting\nin a speedup improvement of approximately 1.2\u00d7 compared\nto the CG-grained result.", "mimetype": "text/plain", "start_char_idx": 66096, "end_char_idx": 66538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfeeb803-6b00-476b-b25b-2d8381b45b7b": {"__data__": {"id_": "bfeeb803-6b00-476b-b25b-2d8381b45b7b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6905bfa-7ee1-4f4f-b610-d408f7084f6e", "node_type": "1", "metadata": {}, "hash": "c467f79743f0e5c16e5c90f0f124f50e334c433fcbecba7c3262bc87881b643f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b4af47d-27ef-4ff6-a006-f9cb815c4598", "node_type": "1", "metadata": {}, "hash": "fceb7e30f42083a146fb7f104fed52f500ea906b3b7832b204c55e496cf4d9a6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Figure 22 (b) shows the perfor-\nmance speedup achieved by CIM-MLC when the number of\ncrossbars in each core varies. Similar to the results of the\ncore number, the speedup grows as the crossbar number\nincreases.\n4.4.2\nCrossbar Size. Figure 22 (c) compares the speedup\nachieved by CIM-MLC as the crossbar size changes. As the\ncrossbar row size increases from 64 to 256, the CG-grained\nspeedup gradually increases.", "mimetype": "text/plain", "start_char_idx": 66539, "end_char_idx": 66950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b4af47d-27ef-4ff6-a006-f9cb815c4598": {"__data__": {"id_": "8b4af47d-27ef-4ff6-a006-f9cb815c4598", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfeeb803-6b00-476b-b25b-2d8381b45b7b", "node_type": "1", "metadata": {}, "hash": "269e64b5e76e0ca24809060e79038a1bdc1fd898323b53809fcf01783b83c4f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36352219-00af-495f-9f79-b242ca2de9dc", "node_type": "1", "metadata": {}, "hash": "c2b25f736a6c0c7ad95b2fc3c4b04702ed812d8cf85dc63e06841b216ccd85f0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This is because larger row sizes\nincrease the amount of input data required by the crossbar,\nwhich puts pressure on the bandwidth and leads to longer\ncomputing latency. The CG-grained pipeline optimization\ncan reduce data feeding pressure, resulting in better accel-\neration. Since the weight matrix size of ViT matches the\ncrossbar size when it changes from (64 \u00d7 512) to (256 \u00d7 128),\nthe MVM-grained can make full use of redundant crossbar\nresources to achieve the same acceleration.", "mimetype": "text/plain", "start_char_idx": 66951, "end_char_idx": 67436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36352219-00af-495f-9f79-b242ca2de9dc": {"__data__": {"id_": "36352219-00af-495f-9f79-b242ca2de9dc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b4af47d-27ef-4ff6-a006-f9cb815c4598", "node_type": "1", "metadata": {}, "hash": "fceb7e30f42083a146fb7f104fed52f500ea906b3b7832b204c55e496cf4d9a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02255e17-8105-4869-a336-2c8bc106cc36", "node_type": "1", "metadata": {}, "hash": "90ca10df299ce36a5bbf33bbc145cd482b575e2a064f8a13206d1f186af2eacd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The speedup of\nVVM-grained gradually increases because as the crossbar col-\numn size decreases, the same weight needs to be mapped on\nmore crossbars in the horizontal direction. The VVM-grained\nremapping strategy can make full use of the advantages of\nthese horizontal crossbars, which reduce the calculation de-\nlay in the crossbar, and achieve better acceleration. Upon\nincreasing the number of crossbar rows to 512, the speedup\nhas considerably decreased.", "mimetype": "text/plain", "start_char_idx": 67437, "end_char_idx": 67895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02255e17-8105-4869-a336-2c8bc106cc36": {"__data__": {"id_": "02255e17-8105-4869-a336-2c8bc106cc36", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36352219-00af-495f-9f79-b242ca2de9dc", "node_type": "1", "metadata": {}, "hash": "c2b25f736a6c0c7ad95b2fc3c4b04702ed812d8cf85dc63e06841b216ccd85f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07bf271f-a60f-46a8-a0f3-3c72c4542c0a", "node_type": "1", "metadata": {}, "hash": "1cdbc27f46b75f73ce39bf0e106e83ddb6ed4ce5b52298bf30483ed80095c91b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This can be attributed to the fact\nthat ViT comprises numerous matrices with a row size of 768,\nnecessitating two vertical crossbars (512 \u00d7 64) for mapping.\nConsequently, ViT needs more resources, requiring it to be\nsegmented and mapped on the given CIM, ultimately leading\nto a decrease in speedup.\n4.4.3\nParallel Row. When the number of parallel rows\nin the crossbar is changed, the scheduling result can be ob-\nserved in Figure 22 (d).", "mimetype": "text/plain", "start_char_idx": 67896, "end_char_idx": 68334, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07bf271f-a60f-46a8-a0f3-3c72c4542c0a": {"__data__": {"id_": "07bf271f-a60f-46a8-a0f3-3c72c4542c0a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02255e17-8105-4869-a336-2c8bc106cc36", "node_type": "1", "metadata": {}, "hash": "90ca10df299ce36a5bbf33bbc145cd482b575e2a064f8a13206d1f186af2eacd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb077ce0-4c1f-4c65-94de-e35b4ab1c17b", "node_type": "1", "metadata": {}, "hash": "4740c902848b1d0c0ba5b8fca88c9b219b43dc0ed39d95af1961a1e84ecb5f66", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "If the number of parallel rows is\ndecreased, it may be challenging to achieve effective accel-\neration through MVM-grained scheduling, while the VVM-\ngrained remapping strategy can mitigate the impact of this\nreduction on latency. Specifically, when the number of par-\nallel rows is 8, VVM-grained scheduling can achieve an ac-\nceleration of approximately 20% beyond the optimization\nresults of MVM-grained scheduling.", "mimetype": "text/plain", "start_char_idx": 68335, "end_char_idx": 68753, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb077ce0-4c1f-4c65-94de-e35b4ab1c17b": {"__data__": {"id_": "fb077ce0-4c1f-4c65-94de-e35b4ab1c17b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07bf271f-a60f-46a8-a0f3-3c72c4542c0a", "node_type": "1", "metadata": {}, "hash": "1cdbc27f46b75f73ce39bf0e106e83ddb6ed4ce5b52298bf30483ed80095c91b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1d435a9-ca53-459f-9bc7-c799f5d37e4f", "node_type": "1", "metadata": {}, "hash": "480ad19872ffbc334cac69e98e3998f29f0b6eb9391e0ee9f933e57281124880", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5\nConclusion\nWe propose CIM-MLC, a general compilation tool for the\nCIM architecture that consists of hardware and computation\nabstraction and a multi-level operator scheduling method.\nThe hardware abstraction method, Abs-arch, provides a uni-\nfied description of CIM architectures from chip to cross-\nbar tier. Three computing mode abstractions are established\nto represent different granularity of the programming in-\nterface. A set of meta-operators is established to describe\nthe computing process under different modes.", "mimetype": "text/plain", "start_char_idx": 68754, "end_char_idx": 69278, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1d435a9-ca53-459f-9bc7-c799f5d37e4f": {"__data__": {"id_": "f1d435a9-ca53-459f-9bc7-c799f5d37e4f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb077ce0-4c1f-4c65-94de-e35b4ab1c17b", "node_type": "1", "metadata": {}, "hash": "4740c902848b1d0c0ba5b8fca88c9b219b43dc0ed39d95af1961a1e84ecb5f66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8e476a7-03fe-4c9f-9294-188e9f9d0dc0", "node_type": "1", "metadata": {}, "hash": "eb82aa00d3149bb74131d5b7f6e2cef5e4899e11d0a53a8b2810e24a397a59ad", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Besides, the\nmulti-level scheduling optimization method is proposed to\nexplore the acceleration potential of the CIM architecture,\nfrom CG-grained to MVM-grained and VVM-grained, and\noutput the corresponding meta-operator flow. Comprehen-\nsive experimental results show that CIM-MLC has wide soft-\nware and hardware adaptability. What\u2019s more, the proposed\nCIM-MLC can serve as the middleware between neural net-\nwork models and CIM hardware, reducing the design burden\non experts in the fields of CIM architecture and neural net-\nwork algorithms.", "mimetype": "text/plain", "start_char_idx": 69279, "end_char_idx": 69825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8e476a7-03fe-4c9f-9294-188e9f9d0dc0": {"__data__": {"id_": "a8e476a7-03fe-4c9f-9294-188e9f9d0dc0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "0809e041a862cb1c940deecac93b3cc084ced52106b647e7859b233b006d0368", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1d435a9-ca53-459f-9bc7-c799f5d37e4f", "node_type": "1", "metadata": {}, "hash": "480ad19872ffbc334cac69e98e3998f29f0b6eb9391e0ee9f933e57281124880", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1145/3620665.3640359", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "6\nAcknowledgments\nThis work was supported by the National Natural Science\nFoundation of China under NSFC.62222411.", "mimetype": "text/plain", "start_char_idx": 69826, "end_char_idx": 69940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10.1145/3620665.3640359": {"__data__": {"id_": "10.1145/3620665.3640359", "embedding": null, "metadata": {"node_type": "doi_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"4": {"node_id": "root_node", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "d3ae19d3-aa0b-417d-bfb3-f31df984df31", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5a7e07c2-6dd4-43e2-a6c0-22becd234523", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f5dc9c18-9720-42d8-b9f6-38d547f7254a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b6e87dcb-e811-4c4e-98f2-2a384c8b31b7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "728fd86c-0df5-482c-b8d4-730334ea7f93", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1bdfae30-83fc-4d76-9bfa-2adc5edb04d4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8c6d7c3a-bf16-4804-9ea9-3822049f0373", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a22a2ca3-77b8-4a21-8f6e-a53fec7a0ad3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "74b1c6bb-3764-4abf-b9cb-1a2a9f88ff1f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8f4d0025-6967-4729-aa31-5b5d317c521f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cc9cb447-6d78-4d59-9daa-795b6236467c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f01f355a-9948-4a16-9511-043226e6f4c7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "12e51f59-5338-4bde-b6d9-ac58b4a8efe2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2487ff97-7439-45ec-8ea0-b72b4aa37da6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "814efabd-7847-40c0-a39c-d8af3757b357", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cda1734a-9df0-43a1-ac37-aead62549627", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fa3e398f-2316-4e36-a9e4-706aa288c7d3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ed7bbe97-f402-48e4-9746-a18a91c096a0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f088e130-bfda-4ed1-baec-9f4d208b8a56", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "730c6909-f24b-45c4-80aa-f8eaac484169", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "baac4dc3-e5ae-4510-a94d-2c8e42948aca", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "33c17ef5-a17b-44fe-8ac8-19391e0cde9f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3816d36f-9bf2-4ec3-85ae-e8a886a8e490", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "901c803f-e256-42e5-9aa7-c8304deb0cf9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d26c473d-a378-4bc7-a8de-914beb018f85", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0ee57639-b5e4-4b9c-b67b-b8e0a2c8ba81", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2151de96-b998-4774-b23a-fae618ed43ed", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "edd47315-333b-46bb-953c-211b0ec105c8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "29f86f38-2e31-45f2-822c-4ee5ebb15574", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "48e6855e-0ba8-453e-89f3-ece7ae5a3d53", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3bb6ebde-86a8-4f61-a8e8-f8a08d861e66", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "786a24a6-813a-4c69-befa-69d4b5e89a35", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c7bb60d6-70df-4d54-acad-faeebb7d76d0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e839550d-9e47-4b97-94c1-2e73bebe411d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5816647a-73e2-4ac7-a11a-057ddc2eb376", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "84f716f5-d65f-41cd-9889-b277271a8a23", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9c25a12c-5d26-411f-a335-cde275a1cdb3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c4078c9a-7d16-4a64-b8d7-b39677f41d80", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9db0b3f3-6a36-4d90-8b1a-fdb0dcf551af", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2391d070-3b54-4f15-b97b-3dfa4adacfbe", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1e49f2d8-c264-4fe5-8041-b96847310cd0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1bcd02b2-99a1-4a13-9043-20a1c8f432c8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "18be9a7b-ae16-412f-917d-8a7127b47d69", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0e920e28-34e8-4f9e-acef-2998fea02905", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ff328eb8-1f09-4dcd-8706-dd160b6f7cdd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "166b8eab-c60e-4a52-83fa-948ba2ec2109", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f486df71-0c06-45b1-a45a-5c6f662ba243", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7e0694ca-48b9-4de4-9c75-9cc96c87f3dd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f01407aa-78c3-4d06-beb4-ace5d05edda1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ec137a4a-1c66-483b-85ab-cb3e5f97c354", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7488f89b-8a3f-40cc-94cb-73d30ca8d5ee", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "30b68c42-066e-4641-affa-25da36e97709", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bf635e73-8cfa-4246-8ea3-d1eefa59cae4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e6032776-bd28-48b7-a52d-1337ae85edb1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1a95430b-f941-4dc7-b97c-db99e5a02163", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a50743c2-1383-4511-9cbb-c7b9bde4eba9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5916c988-4dd2-41c8-85ea-dbcf1bdaf775", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "131b9936-99f7-459d-91b0-73a619f8492c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ce2ab9a2-0f94-401b-9ed1-968d0a7e4b8a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "65454d62-cf1a-44e6-b301-041249136f55", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1011f77c-a35f-4d6b-a620-da78e8ac9cb3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "13d90e69-51ed-4b76-aaa9-8ee59fa4dae8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6a60a4f7-d928-4020-99a1-13e5edfe04b7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2c75efa3-3354-4488-9dff-16bfe0a7c630", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "946eb577-b93f-4c08-901a-cca01cc10d09", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e7887d5d-fe1c-403c-9583-a3453f71fc9c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "96477ab0-f88c-4522-bb00-354110c85c8a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d11f2831-dab5-46b8-b0e2-adae4a9b6f9a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5930a112-951d-4417-a7e0-1c0ee380c9e4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6e9303bd-a766-4ffb-a89a-401ac929af1e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "64b61fdd-91d1-4083-9bb1-b2f0fa6b1346", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3cb8820f-9f7c-46a0-b721-fa481020d96f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "db44e558-f3e8-4971-ac33-f8be2f7b1ec8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c4140fe1-7410-4703-a922-a2ace8c833d9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ccb1104d-2553-408f-accd-febb66469018", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "21e1af3a-fd6b-4838-8daf-a1787afc18c6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0d368213-7f2a-4476-a7b0-ff42a57aabb0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8f3f6f5d-e3eb-4bee-95d0-595c9e3ac97a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "811f2858-7385-46d7-b22f-3a3f8919f484", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a02a361b-2fc6-46d1-b6ee-ac7da3f0d1ac", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fb6a1f4e-e3fd-45b2-8239-ae4853a8ed78", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "aa9049b3-ffe6-4916-b095-885e60aba117", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "add5e0c5-8277-453f-8e87-0b5a980a95d8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a9df6ece-fd89-40cb-8d2a-76578d4f0cd7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7303f6d5-950b-4ef1-b527-74a463def7c8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0acb7d48-8b0d-4068-8b78-5b75098eab09", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "533e8751-04d2-47d0-8e64-bcd7ee462dfd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "20a292c9-a5be-4a54-80e1-6491d1ad655f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0e7a12a1-62ec-4f28-99e8-93fd581cc0d8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3dd7843e-e16f-4965-9b4f-c1d0f4146621", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9385e64e-d5a4-4d4c-84cb-f859d745f8f5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7ee10036-70f0-4e5c-a310-80400ee46def", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a182dc1a-2aea-41ef-9cd2-8cea23807662", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6f8dcee6-529f-44e0-8f4b-1f2db164e133", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b241cc96-de18-4a7b-be9a-77271c5bc125", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "25469213-9191-4a78-9f31-bfd562d3ce71", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4b959207-cbb3-48e9-a73a-82ba017c6673", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9084e53d-a3b5-4132-b865-cd4d6101302b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0d64c0a3-2bc2-4c5b-a284-577cc72df072", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "98473a04-c2e4-4376-b563-fb7e9d23871c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "df2c8ad6-a2ad-453a-87fe-017e60bf787a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "65d6f179-674a-4762-893a-af6316bf0c23", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "89c51624-4ace-45b5-92bd-beb86a011448", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7048bba5-b259-44fa-a3ec-dbec58a52420", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b8eecfff-ae1e-49cc-af79-6fee76ebe9ea", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4e4204ea-1459-4626-ae77-0af9bd17be77", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b4d57ef5-fd97-4269-a7c6-87c36f9c28c8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "72afdfb0-c290-4834-98e3-e0d68ba6b7fd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5d253f0c-27fb-4cfb-a8ea-082717b4bdcd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "66f77ccf-46c7-45af-a56d-c1696c546f05", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a67944fe-9abe-4082-aa63-bb0562efda06", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1e4f061b-042f-4d35-9c93-3040c3a89140", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c6acd20d-06e0-4bc2-9e1d-7813e50ce73d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "753408dd-545b-4377-ad0a-546e13225bea", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6bcf6805-1d3d-45df-bd75-4157dfee23bd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3ff93fa7-fad9-4216-9035-10c64b29c928", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f7eb9ef0-7eb7-4e76-a581-32918df9288a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7f8683e7-f2e4-4e10-bcaa-306a580e7a98", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "97e67368-c103-433a-b808-5172fbeaf390", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cfb8514d-9493-4708-a9da-2a997ee71fdd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d156f5bb-f4b4-47f6-8d4d-b33a95a27b3e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3b228fd9-bdfa-4266-8559-88dec33f6a0e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0b23a1a4-fb99-4691-befd-6f5d513aa4bd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "44186064-86d9-44bb-bdb7-c27a23d7c284", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7b84fb40-428c-4962-84fb-f265473d9bd5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b4805184-cc85-4df9-990b-2a6ad5a21c43", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "55422b37-2367-4f08-b2ba-352448c96424", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "71902272-2de6-4cd9-b6bb-c504b6c3e657", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d3628b81-dad8-4954-9cf4-d9170594e7b4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "73d0e642-7981-45ef-9ca6-287ea4299d46", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7c3ca567-6e2f-433a-85cc-48656256688f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "afffc6de-4b84-4e6e-9605-177137f149c8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c94405ec-8091-475f-82fb-af342fd478f1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cc23c79d-5e73-457b-b022-67edd5a95d25", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "00abe0e0-c902-4f44-8116-236ac44733a2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c1a79e65-190b-4abb-8f3f-328332de4838", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "04013ec3-e99c-4941-a3ce-639cacea06f7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8af6cba4-72c5-4c55-9671-84d3924034b3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b9cee74f-4ae8-439c-887c-b4120b0f1a8c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4bbaca6d-21de-40aa-b72a-5131eee75c61", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2c5cbc7f-e6ba-4b31-a886-8b211cbc52ed", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "546c4fc7-cfdf-4307-90ce-9add0588b0ca", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4cb70317-e840-44c4-8f61-b0ef5a49fab5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d195d361-fc63-4315-8fc7-3f140bc13744", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "35b0a8f7-0aa9-4837-8c58-3c180fff32a5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1ac0724a-5cba-4014-bbda-3626cf04f5ef", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d97b1890-ebbd-495a-856f-a17415b6a208", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5959cee1-6887-4d70-81ab-349054816bc5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bdbdd574-6557-47f7-947b-3b1b5a07d17a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cdb97048-86ad-4596-a180-c5ee577bfad3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a184e5b0-3a97-470b-98cd-1f8b9e1acb09", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c069c6d9-2b0c-45a4-81c5-10c438813aa2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6263bf63-7175-42c0-93df-82ddcfdfc4c9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9d58f374-c620-4eeb-abd1-5341196fabf5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0a39b38e-6419-4ce9-b4ae-6db06d20cdac", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ca649d76-1076-40b0-917c-fc515ee25dd5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0713f40c-5bba-4f62-863f-f5df74e60599", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d1a344f5-db37-4369-a773-e33274346cb1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f55ae1bc-2d41-4058-9444-a1131cebd5ef", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "679d9b84-6142-429c-9ef1-4b5ef77d27fe", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "98f0f3c1-d81b-4603-b0ce-c529d844818d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "eddfd3bb-1f51-4635-87be-37b010a11127", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9058e019-cc2e-45fd-bf9a-958b67c89a1f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b5dc4aef-af8e-430e-bab8-bf0647274cf0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2ebd33d3-3c2b-4d46-87e1-8500b2da901c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "032e29c9-31bf-4d4c-a141-75b44d8eb772", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b61ca803-8a0c-426d-a376-31f0b920ab4e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f05985af-ddd1-4781-a431-89662ab67aba", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4acb8290-def3-4c17-8463-491aa486ddbd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "076007d3-dbac-40e8-9072-f2e34c5d2d56", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9405157d-0ef7-43f3-bd50-fdcd2ee50d08", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7da2ddf7-be0f-4a43-873c-d8bc0d9a81bd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b6905bfa-7ee1-4f4f-b610-d408f7084f6e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bfeeb803-6b00-476b-b25b-2d8381b45b7b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8b4af47d-27ef-4ff6-a006-f9cb815c4598", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "36352219-00af-495f-9f79-b242ca2de9dc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "02255e17-8105-4869-a336-2c8bc106cc36", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "07bf271f-a60f-46a8-a0f3-3c72c4542c0a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fb077ce0-4c1f-4c65-94de-e35b4ab1c17b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f1d435a9-ca53-459f-9bc7-c799f5d37e4f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a8e476a7-03fe-4c9f-9294-188e9f9d0dc0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}]}, "text": "DOI: 10.1145/3620665.3640359", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17f93544-8b86-4758-b5a0-db9d8d93c5e1": {"__data__": {"id_": "17f93544-8b86-4758-b5a0-db9d8d93c5e1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4dd1b630-56ee-419b-95aa-e119bd43cac4", "node_type": "1", "metadata": {}, "hash": "d469e4fb34e19e28ace0766d2c331899afdc2fc7950d5b81592f6dc3b2e56dd6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "REFERENCES \n[1] N.T. Theis, and  H. S. P. Wong . \"The End of Moore\u2019s Law: A New \nBeginning for Information Technology.\" Computing in Science & \nEngineering 19.2(2017):41-50. \n[2] S. Gao, F. Yang, L. Zhao and Y. Zhao, \"Current Research Status and \nFuture Prospect of the In-Memory Computing,\" 2021 IEEE 14th \nInternational Conference on ASIC (ASICON), 2021, pp. 1-4.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 365, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4dd1b630-56ee-419b-95aa-e119bd43cac4": {"__data__": {"id_": "4dd1b630-56ee-419b-95aa-e119bd43cac4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17f93544-8b86-4758-b5a0-db9d8d93c5e1", "node_type": "1", "metadata": {}, "hash": "de181898c6e757777bfaf24abf54b9e752109922a5ea554c9344207392854156", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a61f347a-3ad6-478c-bd3a-34aa9be50061", "node_type": "1", "metadata": {}, "hash": "b98d0532c62949c4dee974d77becf4dc5d9637b80cbbf66b51f2e89131362a6b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[3] N. Verma et al., \"In-Memory Computing: Advances and Prospects,\" in \nIEEE Solid-State Circuits Magazine, vol. 11, no. 3, pp. 43-55, Summer \n2019. \n[4] B. Crafton et al., \"CIM-SECDED: A 40nm 64Kb Compute In-Memory \nRRAM Macro with ECC Enabling Reliable Operation,\" 2021 IEEE Asian \nSolid-State Circuits Conference (A-SSCC), 2021, pp. 1-3.", "mimetype": "text/plain", "start_char_idx": 367, "end_char_idx": 707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a61f347a-3ad6-478c-bd3a-34aa9be50061": {"__data__": {"id_": "a61f347a-3ad6-478c-bd3a-34aa9be50061", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dd1b630-56ee-419b-95aa-e119bd43cac4", "node_type": "1", "metadata": {}, "hash": "d469e4fb34e19e28ace0766d2c331899afdc2fc7950d5b81592f6dc3b2e56dd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cedebc15-8eff-4a16-b4d3-688515a66738", "node_type": "1", "metadata": {}, "hash": "303a1a074d15a4b25edb128a78c277e31f94e1dfd8a964784adce325df7d6ed4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[5] F. Jiao, B. Chen, K. Ding, K. Li, L. Wang, X. Zeng, F. Rao, \u201cMonatomic \n2D phase-change memory for precise neuromorphic computing\u201d, Appl. \nMater. Today 20 (2020), 100641.", "mimetype": "text/plain", "start_char_idx": 709, "end_char_idx": 883, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cedebc15-8eff-4a16-b4d3-688515a66738": {"__data__": {"id_": "cedebc15-8eff-4a16-b4d3-688515a66738", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a61f347a-3ad6-478c-bd3a-34aa9be50061", "node_type": "1", "metadata": {}, "hash": "b98d0532c62949c4dee974d77becf4dc5d9637b80cbbf66b51f2e89131362a6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98abcde2-874a-45bd-bd59-57c5974b5d64", "node_type": "1", "metadata": {}, "hash": "230a277fbff4cfbedcfed8da209e91c2887674a1f82713a957539bc489811fcc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[6] S. Prajapati, V. Nehra and B. K. Kaushik, \"High-Performance \nComputing-in-Memory Architecture Based on Single-Level and \nMultilevel Cell Differential Spin Hall MRAM,\" in IEEE Transactions on \nMagnetics, vol. 57, no. 9, pp. 1-15, Sept. 2021. \n[7] H. -T. Lue, H. -W. Hu, T. -H. Hsu, P. -K. Hsu, K. -C. Wang and C.", "mimetype": "text/plain", "start_char_idx": 885, "end_char_idx": 1200, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98abcde2-874a-45bd-bd59-57c5974b5d64": {"__data__": {"id_": "98abcde2-874a-45bd-bd59-57c5974b5d64", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cedebc15-8eff-4a16-b4d3-688515a66738", "node_type": "1", "metadata": {}, "hash": "303a1a074d15a4b25edb128a78c277e31f94e1dfd8a964784adce325df7d6ed4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "750946c2-149f-4ead-b810-1c4132a618d1", "node_type": "1", "metadata": {}, "hash": "85d276f4e5645653e86151f371396278ad52a3fc9e52acf9bc101d7c714c212f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "-Y. \nLu, \"Design of Computing-in-Memory (CIM) with Vertical Split-Gate \nFlash Memory for Deep Neural Network (DNN) Inference Accelerator,\" \n2021 IEEE International Symposium on Circuits and Systems (ISCAS), \n2021, pp. 1-4.", "mimetype": "text/plain", "start_char_idx": 1201, "end_char_idx": 1423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "750946c2-149f-4ead-b810-1c4132a618d1": {"__data__": {"id_": "750946c2-149f-4ead-b810-1c4132a618d1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98abcde2-874a-45bd-bd59-57c5974b5d64", "node_type": "1", "metadata": {}, "hash": "230a277fbff4cfbedcfed8da209e91c2887674a1f82713a957539bc489811fcc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11fd5a8f-0034-41e4-b839-7256e6a5ed99", "node_type": "1", "metadata": {}, "hash": "780ddc00af007d8ef4ff2705e466a827bfd01876c61d611cfaa6e7bea84091f9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[8] W. Xu, J. Luo, Y. Du, Q. Huang and R. Huang, \"Novel Negative-\nFeedback Method for Writing Variation Suppression in FeFET-Based \nComputing-in-Memory Macro,\" 2022 China Semiconductor Technology \nInternational Conference (CSTIC), 2022, pp. 1-3.", "mimetype": "text/plain", "start_char_idx": 1425, "end_char_idx": 1670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11fd5a8f-0034-41e4-b839-7256e6a5ed99": {"__data__": {"id_": "11fd5a8f-0034-41e4-b839-7256e6a5ed99", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "750946c2-149f-4ead-b810-1c4132a618d1", "node_type": "1", "metadata": {}, "hash": "85d276f4e5645653e86151f371396278ad52a3fc9e52acf9bc101d7c714c212f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce73f64b-0034-4847-a7fa-297f1f64516d", "node_type": "1", "metadata": {}, "hash": "e0048912324a67c5af5a810bd6c89e329a398d461493d71f83d6824d2c565861", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[9] S. Zeng et al., \"MLFlash-CIM: Embedded Multi-Level NOR-Flash Cell \nbased Computing in Memory Architecture for Edge AI Devices,\" 2021 \nIEEE 3rd International Conference on Artificial Intelligence Circuits and \nSystems (AICAS), 2021, pp. 1-4. \n[10] A. Paszke, et al., \"Pytorch: An imperative style, high-performance deep \nlearning library. \" Advances in Neural Information Processing Systems, \n32, 2019.", "mimetype": "text/plain", "start_char_idx": 1672, "end_char_idx": 2077, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce73f64b-0034-4847-a7fa-297f1f64516d": {"__data__": {"id_": "ce73f64b-0034-4847-a7fa-297f1f64516d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11fd5a8f-0034-41e4-b839-7256e6a5ed99", "node_type": "1", "metadata": {}, "hash": "780ddc00af007d8ef4ff2705e466a827bfd01876c61d611cfaa6e7bea84091f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a180214-0c81-41f3-b217-865bdc373458", "node_type": "1", "metadata": {}, "hash": "e8bde06e2183bdbdf1f08d348b47cf7c05cbd7f52f87ee660d03f0d9c46bfb92", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[11] M. Abadi et al., \"Tensorflow: A system for large-scale machine \nlearning.\"In 12th USENIX Symposium on Operating Systems Design and \nImplementation (OSDI 16).2016: 265\u2013283. \n[12] T. Chen , et al., \"Mxnet: A flexible and efficient machine learning library \nfor heterogeneous distributed systems. \"arXiv preprint arXiv:1512.01274 \n(2015). \n[13] B. Gaskill, \"ONNX: the Open Neural Network Exchange Format. \"", "mimetype": "text/plain", "start_char_idx": 2079, "end_char_idx": 2487, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a180214-0c81-41f3-b217-865bdc373458": {"__data__": {"id_": "3a180214-0c81-41f3-b217-865bdc373458", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce73f64b-0034-4847-a7fa-297f1f64516d", "node_type": "1", "metadata": {}, "hash": "e0048912324a67c5af5a810bd6c89e329a398d461493d71f83d6824d2c565861", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6f9b395-fe73-4520-b734-8deeeb67c05d", "node_type": "1", "metadata": {}, "hash": "6e6e09969071b34d77057e174d6ee3bc9270a8030ec07947273f901cc9165160", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Linux \nJournal, 2018. \n[14] T. Chen , et al. \"TVM: An automated End-to-End optimizing compiler for \ndeep learning.\" 13th USENIX Symposium on Operating Systems Design \nand Implementation (OSDI 18). 2018. \n[15] C. Leary and T. Wang, \"XLA: TensorFlow, compiled. \" TensorFlow Dev \nSummit (2017). \n[16] P. Norman, et al,\"In-Datacenter Performance Analysis of a Tensor \nProcessing Unit.\"", "mimetype": "text/plain", "start_char_idx": 2488, "end_char_idx": 2869, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6f9b395-fe73-4520-b734-8deeeb67c05d": {"__data__": {"id_": "b6f9b395-fe73-4520-b734-8deeeb67c05d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a180214-0c81-41f3-b217-865bdc373458", "node_type": "1", "metadata": {}, "hash": "e8bde06e2183bdbdf1f08d348b47cf7c05cbd7f52f87ee660d03f0d9c46bfb92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0edc4b3c-fe16-415b-8b02-a903de626e43", "node_type": "1", "metadata": {}, "hash": "5c59a54b51eca4f9cbb87166fe6d1650e6256179cc2d3683b0154e66a75b93a2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Computer architecture news 45.2(2017):1-12.  \n[17] T. -K. Chien et al., \"Low-Power MCU With Embedded ReRAM Buffers \nas Sensor Hub for IoT Applications,\" in IEEE Journal on Emerging and \nSelected Topics in Circuits and Systems, June 2016,pp. 247-257.", "mimetype": "text/plain", "start_char_idx": 2870, "end_char_idx": 3119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0edc4b3c-fe16-415b-8b02-a903de626e43": {"__data__": {"id_": "0edc4b3c-fe16-415b-8b02-a903de626e43", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6f9b395-fe73-4520-b734-8deeeb67c05d", "node_type": "1", "metadata": {}, "hash": "6e6e09969071b34d77057e174d6ee3bc9270a8030ec07947273f901cc9165160", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "660493fc-edda-4c92-bbcf-dfa162356527", "node_type": "1", "metadata": {}, "hash": "bcf868ffad7bd7ee72aacf038afdf46ced3671a2ae5bcb05fe499d26145eb548", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[18] L. Zhao et al., \"A Compute-in-Memory Architecture Compatible with 3D \nNAND Flash that Parallelly Activates Multi-Layers,\" 2021 58th \nACM/IEEE Design Automation Conference (DAC), 2021, pp. 193-198.", "mimetype": "text/plain", "start_char_idx": 3121, "end_char_idx": 3322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "660493fc-edda-4c92-bbcf-dfa162356527": {"__data__": {"id_": "660493fc-edda-4c92-bbcf-dfa162356527", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0edc4b3c-fe16-415b-8b02-a903de626e43", "node_type": "1", "metadata": {}, "hash": "5c59a54b51eca4f9cbb87166fe6d1650e6256179cc2d3683b0154e66a75b93a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca6c40b0-025d-4e8e-a3c7-ae13e2e120c6", "node_type": "1", "metadata": {}, "hash": "1fa49d3356fe1bbf5be6ab398db6347b6f23cc80e947ab7d4562c3e7df8b2e07", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[19] S. Gao, Y. Cong, Z. Zhang, X. Qiu, C. Lee and Y. Zhao, \"Superior Data \nRetention of Programmable Linear RAM (PLRAM) for Compute-in-\nMemory Application,\" 2020 IEEE International Reliability Physics \nSymposium (IRPS), 2020, pp. 1-5.", "mimetype": "text/plain", "start_char_idx": 3324, "end_char_idx": 3559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca6c40b0-025d-4e8e-a3c7-ae13e2e120c6": {"__data__": {"id_": "ca6c40b0-025d-4e8e-a3c7-ae13e2e120c6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "660493fc-edda-4c92-bbcf-dfa162356527", "node_type": "1", "metadata": {}, "hash": "bcf868ffad7bd7ee72aacf038afdf46ced3671a2ae5bcb05fe499d26145eb548", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f37b1acf-e65f-4940-ab36-c89fe045d463", "node_type": "1", "metadata": {}, "hash": "508b2798f62021f598c7013237d300f7baa4bf061b25e31773c5ce152b4f8dfd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[20] L. Zhao et al., \"Neural Network Acceleration and Voice Recognition with \na Flash-based In-Memory Computing SoC,\" 2021 IEEE 3rd International \nConference on Artificial Intelligence Circuits and Systems (AICAS), \n2021, pp. 1-5. \n[21] G. W. Burr et al., \"Experimental demonstration and tolerancing of a large-\nscale neural network (165,000 synapses), using phase-change memory as \nthe synaptic weight element,\" 2014 IEEE International Electron Devices \nMeeting, 2014, pp.", "mimetype": "text/plain", "start_char_idx": 3561, "end_char_idx": 4034, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f37b1acf-e65f-4940-ab36-c89fe045d463": {"__data__": {"id_": "f37b1acf-e65f-4940-ab36-c89fe045d463", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca6c40b0-025d-4e8e-a3c7-ae13e2e120c6", "node_type": "1", "metadata": {}, "hash": "1fa49d3356fe1bbf5be6ab398db6347b6f23cc80e947ab7d4562c3e7df8b2e07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c999e23-13bf-4234-89cb-57d294f23ee9", "node_type": "1", "metadata": {}, "hash": "ea91673d77cc42f9b2a4d1f86f3141b725df7c0901db7ec202d313a319f9b0f9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "29.5.1-29.5.4.  \n[22] F. Pedregosa, G. Varoquaux, A. Gramfort et al., \"Scikit-learn: Machine \nLearning in Python\", Journal of Machine Learning Research, vol. 12, no. \n2011, pp. 2825-2830. \n156\nAuthorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:47:57 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 4035, "end_char_idx": 4365, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c999e23-13bf-4234-89cb-57d294f23ee9": {"__data__": {"id_": "8c999e23-13bf-4234-89cb-57d294f23ee9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f37b1acf-e65f-4940-ab36-c89fe045d463", "node_type": "1", "metadata": {}, "hash": "508b2798f62021f598c7013237d300f7baa4bf061b25e31773c5ce152b4f8dfd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce9cb202-e79f-4a6d-abf9-4409ad296965", "node_type": "1", "metadata": {}, "hash": "b82622ec0ba9e1f11d5d02e93866f8999a0e2ab67e54bdd255f1c80f44ca803c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[23] X. Peng, R. Liu and S. Yu, \"Optimizing Weight Mapping and Data Flow \nfor Convolutional Neural Networks on RRAM Based Processing-In-\nMemory Architecture,\" 2019 IEEE International Symposium on Circuits \nand Systems (ISCAS), 2019, pp. 1-5.  \n[24] J. Xia,  et al, \"ResNet15: Weather Recognition on Traffic Road with Deep \nConvolutional \nNeural \nNetwork.\" \nAdvances \nin \nMeteorology \n2020(2020):1-11.", "mimetype": "text/plain", "start_char_idx": 4367, "end_char_idx": 4767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce9cb202-e79f-4a6d-abf9-4409ad296965": {"__data__": {"id_": "ce9cb202-e79f-4a6d-abf9-4409ad296965", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "41f2ec57303f4fff71847f20823776bfd4f4c87fff3bd4e34c635c9616695dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c999e23-13bf-4234-89cb-57d294f23ee9", "node_type": "1", "metadata": {}, "hash": "ea91673d77cc42f9b2a4d1f86f3141b725df7c0901db7ec202d313a319f9b0f9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/wccct56755.2023.10052488", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[25] G. Andrew, et al, \"Mobilenets: Efficient convolutional neural networks \nfor mobile vision applications.\" arXiv preprint arXiv:1704.04861 (2017).\n \n157\nAuthorized licensed use limited to: Nanjing University. Downloaded on June 02,2024 at 08:47:57 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 4770, "end_char_idx": 5063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10.1109/wccct56755.2023.10052488": {"__data__": {"id_": "10.1109/wccct56755.2023.10052488", "embedding": null, "metadata": {"node_type": "doi_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"4": {"node_id": "root_node", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "17f93544-8b86-4758-b5a0-db9d8d93c5e1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4dd1b630-56ee-419b-95aa-e119bd43cac4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a61f347a-3ad6-478c-bd3a-34aa9be50061", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cedebc15-8eff-4a16-b4d3-688515a66738", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "98abcde2-874a-45bd-bd59-57c5974b5d64", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "750946c2-149f-4ead-b810-1c4132a618d1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "11fd5a8f-0034-41e4-b839-7256e6a5ed99", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ce73f64b-0034-4847-a7fa-297f1f64516d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3a180214-0c81-41f3-b217-865bdc373458", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b6f9b395-fe73-4520-b734-8deeeb67c05d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0edc4b3c-fe16-415b-8b02-a903de626e43", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "660493fc-edda-4c92-bbcf-dfa162356527", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ca6c40b0-025d-4e8e-a3c7-ae13e2e120c6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f37b1acf-e65f-4940-ab36-c89fe045d463", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8c999e23-13bf-4234-89cb-57d294f23ee9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ce9cb202-e79f-4a6d-abf9-4409ad296965", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}]}, "text": "DOI: 10.1109/wccct56755.2023.10052488", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69f46ecb-64f0-4828-be69-6fdea351df8f": {"__data__": {"id_": "69f46ecb-64f0-4828-be69-6fdea351df8f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0eb49248-6451-4dd4-af95-84debb4e62f2", "node_type": "1", "metadata": {}, "hash": "b6098620574d2824c513415290e23fa440169888c3d83a5e80a6478c16893eee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Efficient and self-adaptive in-situ learning\nin multilayer memristor neural networks\nCan Li\n1, Daniel Belkin1,2, Yunning Li1, Peng Yan1,3, Miao Hu\n4,7, Ning Ge5, Hao Jiang1, Eric Montgomery4,\nPeng Lin1, Zhongrui Wang1, Wenhao Song1, John Paul Strachan4, Mark Barnell6, Qing Wu6,\nR. Stanley Williams\n4, J.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0eb49248-6451-4dd4-af95-84debb4e62f2": {"__data__": {"id_": "0eb49248-6451-4dd4-af95-84debb4e62f2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69f46ecb-64f0-4828-be69-6fdea351df8f", "node_type": "1", "metadata": {}, "hash": "dee4df467afa3d1f0afba8ce5e1f40f3f91637371fb3548fc1263ca526345a23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ef651ff-766a-4a1b-acc7-1b0fe4358d2a", "node_type": "1", "metadata": {}, "hash": "12f0fa979e40624709ffdc22bd93cd25dfd65f7b419caa29bca86115e86b4655", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Joshua Yang\n1 & Qiangfei Xia1\nMemristors with tunable resistance states are emerging building blocks of artificial neural\nnetworks. However, in situ learning on a large-scale multiple-layer memristor network has yet\nto be demonstrated because of challenges in device property engineering and circuit inte-\ngration. Here we monolithically integrate hafnium oxide-based memristors with a foundry-\nmade transistor array into a multiple-layer neural network.", "mimetype": "text/plain", "start_char_idx": 305, "end_char_idx": 759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ef651ff-766a-4a1b-acc7-1b0fe4358d2a": {"__data__": {"id_": "3ef651ff-766a-4a1b-acc7-1b0fe4358d2a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0eb49248-6451-4dd4-af95-84debb4e62f2", "node_type": "1", "metadata": {}, "hash": "b6098620574d2824c513415290e23fa440169888c3d83a5e80a6478c16893eee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7e1eb8d-cc5e-4529-80cd-86880ab1c579", "node_type": "1", "metadata": {}, "hash": "c33b04444316759e8ca9a18b94bd4a1dd2235a7880db489168c79ecfa70b1b37", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We experimentally demonstrate\nin situ learning capability and achieve competitive classification accuracy on a standard\nmachine learning dataset, which further confirms that the training algorithm allows the\nnetwork to adapt to hardware imperfections. Our simulation using the experimental para-\nmeters suggests that a larger network would further increase the classification accuracy. The\nmemristor neural network is a promising hardware platform for artificial intelligence with high\nspeed-energy efficiency.", "mimetype": "text/plain", "start_char_idx": 760, "end_char_idx": 1270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7e1eb8d-cc5e-4529-80cd-86880ab1c579": {"__data__": {"id_": "a7e1eb8d-cc5e-4529-80cd-86880ab1c579", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ef651ff-766a-4a1b-acc7-1b0fe4358d2a", "node_type": "1", "metadata": {}, "hash": "12f0fa979e40624709ffdc22bd93cd25dfd65f7b419caa29bca86115e86b4655", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61b10b43-1127-46eb-9dac-29bf234c551d", "node_type": "1", "metadata": {}, "hash": "90585fcbce8f2305aa2453bb8ac643d625f1e4d87122ef03f0fdb0b053770b98", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "DOI: 10.1038/s41467-018-04484-2\nOPEN\n1 Department of Electrical and Computer Engineering, University of Massachusetts, Amherst, MA 01003, USA. 2 Swarthmore College, Swarthmore, PA\n19081, USA. 3 Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan 430074, China. 4 Hewlett Packard\nLabs, Hewlett Packard Enterprise, Palo Alto, CA 94304, USA.", "mimetype": "text/plain", "start_char_idx": 1271, "end_char_idx": 1657, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61b10b43-1127-46eb-9dac-29bf234c551d": {"__data__": {"id_": "61b10b43-1127-46eb-9dac-29bf234c551d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7e1eb8d-cc5e-4529-80cd-86880ab1c579", "node_type": "1", "metadata": {}, "hash": "c33b04444316759e8ca9a18b94bd4a1dd2235a7880db489168c79ecfa70b1b37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6554170d-6516-4dad-9c9e-c0b3671ec8ad", "node_type": "1", "metadata": {}, "hash": "c7d9d44d666737fde2f3a63fc5d1ef0bff6281c5a5d28bd1cf55e28ff6cc4274", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5 HP Labs, HP Inc., Palo Alto, CA 94304, USA. 6 Air Force Research Laboratory, Information\nDirectorate, Rome, NY 13441, USA. 7Present address: Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY 13902,\nUSA. Correspondence and requests for materials should be addressed to J.J.Y. (email: jjyang@umass.edu) or to Q.X.", "mimetype": "text/plain", "start_char_idx": 1658, "end_char_idx": 2010, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6554170d-6516-4dad-9c9e-c0b3671ec8ad": {"__data__": {"id_": "6554170d-6516-4dad-9c9e-c0b3671ec8ad", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61b10b43-1127-46eb-9dac-29bf234c551d", "node_type": "1", "metadata": {}, "hash": "90585fcbce8f2305aa2453bb8ac643d625f1e4d87122ef03f0fdb0b053770b98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "887da0a6-a918-45f8-97f3-6eb363aec90b", "node_type": "1", "metadata": {}, "hash": "4d9ba4588e4b54adad24a7174ac0031bf94da3fdc37b2ea263663e091e4babd0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(email: qxia@umass.edu)\nNATURE COMMUNICATIONS |  (2018) 9:2385 | DOI: 10.1038/s41467-018-04484-2 | www.nature.com/naturecommunications\n1\n1234567890():,;\nW\nith the introduction of hardware accelerators1\u20134 for\ninference in deep neural networks (DNNs)5\u20139, the\nfocus on improving overall energy and time perfor-\nmance for artificial intelligence applications is now on training.", "mimetype": "text/plain", "start_char_idx": 2011, "end_char_idx": 2385, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "887da0a6-a918-45f8-97f3-6eb363aec90b": {"__data__": {"id_": "887da0a6-a918-45f8-97f3-6eb363aec90b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6554170d-6516-4dad-9c9e-c0b3671ec8ad", "node_type": "1", "metadata": {}, "hash": "c7d9d44d666737fde2f3a63fc5d1ef0bff6281c5a5d28bd1cf55e28ff6cc4274", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b9365ff-ecad-478d-9922-4dccf7c469f4", "node_type": "1", "metadata": {}, "hash": "8ea1557b27a6a819510c3f14bf4650cb8166f22e535f97d0983d863bf2081e28", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "One promising approach is in-memory analog computation\nbased on memristor crossbars10\u201318, for which simulations have\nindicated potentially significant speed and power advantages over\ndigital complementary metal-oxide-semiconductor (CMOS)19\u201323.\nHowever, experimental demonstrations to date have been limited\nto\ndiscrete\ndevices24,25\nor\nsmall\narrays\nand\nsimplified\nproblems26\u201331. Here we report an experimental demonstration of\nhighly efficient in situ learning in a multilayer neural network\nimplemented in a 128 \u00d7 64 memristor array.", "mimetype": "text/plain", "start_char_idx": 2386, "end_char_idx": 2919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b9365ff-ecad-478d-9922-4dccf7c469f4": {"__data__": {"id_": "4b9365ff-ecad-478d-9922-4dccf7c469f4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "887da0a6-a918-45f8-97f3-6eb363aec90b", "node_type": "1", "metadata": {}, "hash": "4d9ba4588e4b54adad24a7174ac0031bf94da3fdc37b2ea263663e091e4babd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e9dedfd-7b47-4cfd-9ba3-f37e71163773", "node_type": "1", "metadata": {}, "hash": "8d9f1811867528ab7655fbc6ee172bdee901467b195f3806d6a5b57712bfaccc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The network is\ntrained on 80 000 samples from the Modified National Institute of\nStandards and Technology (MNIST)32 handwritten digit database\nwith an online algorithm, after which it correctly classifies 91.71%\nof 10 000 separate test images. This level of performance is\nobtained with 11% devices in the crossbar unresponsive to pro-\ngramming pulses and the training algorithm blind to the defec-\ntivity, demonstrating the self-adapting capability of the in situ\nlearning to hardware imperfections.", "mimetype": "text/plain", "start_char_idx": 2920, "end_char_idx": 3420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e9dedfd-7b47-4cfd-9ba3-f37e71163773": {"__data__": {"id_": "4e9dedfd-7b47-4cfd-9ba3-f37e71163773", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b9365ff-ecad-478d-9922-4dccf7c469f4", "node_type": "1", "metadata": {}, "hash": "8ea1557b27a6a819510c3f14bf4650cb8166f22e535f97d0983d863bf2081e28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d112cca-1060-43ff-a555-823b666028ce", "node_type": "1", "metadata": {}, "hash": "0c0ff67b752c64723312ea71834bd16b353ba1a27a4f084e57c5568e8ce1bcfc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Our simulation based on the\nmemristor parameters suggested that the accuracy could be\nhigher than 97% with a larger (e.g., 1024 \u00d7 512) memristor array.\nOur results indicate that analog memristor neural networks can\nachieve accuracy approaching that of state-of-the-art digital\nCMOS systems with potentially significant improvements in\nspeed-energy efficiency.\nMemristors offer excellent size scalability (down to 2 nm)33,\nfast switching (faster than 100 ps)34, and low energy per con-\nductance update (lower than 3 fJ)34.", "mimetype": "text/plain", "start_char_idx": 3421, "end_char_idx": 3942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d112cca-1060-43ff-a555-823b666028ce": {"__data__": {"id_": "9d112cca-1060-43ff-a555-823b666028ce", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e9dedfd-7b47-4cfd-9ba3-f37e71163773", "node_type": "1", "metadata": {}, "hash": "8d9f1811867528ab7655fbc6ee172bdee901467b195f3806d6a5b57712bfaccc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "932ded43-76d8-4843-b571-05e6af61a054", "node_type": "1", "metadata": {}, "hash": "b83ea6ac61e2b4659b72e763991e7400b7834462ce192ab2dde491765c87269a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Their tunable resistance\nstates can be used both to store information and to perform\ncomputation, allowing computing and memory to be integrated\nin a highly parallel architecture. However, given the level of\ntechnology maturity, attempts to implement memristive neural\nnetworks have struggled with device non-uniformity, resistance\nlevel instability, sneak path currents, and wire resistance, which\nhave limited array sizes and system performance.", "mimetype": "text/plain", "start_char_idx": 3943, "end_char_idx": 4390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "932ded43-76d8-4843-b571-05e6af61a054": {"__data__": {"id_": "932ded43-76d8-4843-b571-05e6af61a054", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d112cca-1060-43ff-a555-823b666028ce", "node_type": "1", "metadata": {}, "hash": "0c0ff67b752c64723312ea71834bd16b353ba1a27a4f084e57c5568e8ce1bcfc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49c1a25c-8aa6-4512-82e5-94fe17760319", "node_type": "1", "metadata": {}, "hash": "9db2c15d8fd6df9e7962184dfd662b9446d1a1f070a409fa29019fadd4cabfea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In particular,\nlearning in memristor neural networks has been hampered by\nsignificant statistical variations and fluctuations in programmed\nconductance states and the lack of linear and symmetric\nresponses to electric pulses35.\nHere we develop a reliable two-pulse conductance program-\nming scheme utilizing on-chip series transistors to address the\nchallenges in memristor conductance programming. This in situ\ntraining scheme enables the network to continuously adapt and\nupdate its knowledge as more training data become available,\nwhich significantly improves accuracy and defect tolerance.\nResults\nLinear and symmetric conductance tuning.", "mimetype": "text/plain", "start_char_idx": 4391, "end_char_idx": 5034, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49c1a25c-8aa6-4512-82e5-94fe17760319": {"__data__": {"id_": "49c1a25c-8aa6-4512-82e5-94fe17760319", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "932ded43-76d8-4843-b571-05e6af61a054", "node_type": "1", "metadata": {}, "hash": "b83ea6ac61e2b4659b72e763991e7400b7834462ce192ab2dde491765c87269a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74adacaa-4ffd-4d51-8bb1-af16038aa398", "node_type": "1", "metadata": {}, "hash": "48891e9316b6f089d0912996312efe2b998930e95630e37dbf0f5a03e59f1ae9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We used recently\ndeveloped Ta/HfO2/Pt memristors to achieve stable tunable\nmultilevel behavior with a linear current\u2013voltage (IV) relation-\nship36,37. The memristors were monolithically integrated with\nfoundry-made transistor arrays on a 6-inch wafer (see Methods).\nEach memristor was connected to a series transistor in a \u201c1T1R\u201d\nconfiguration (Fig. 1a\u2013e shows the integrated memristor array\nfrom wafer scale to nanometer scale).", "mimetype": "text/plain", "start_char_idx": 5035, "end_char_idx": 5464, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74adacaa-4ffd-4d51-8bb1-af16038aa398": {"__data__": {"id_": "74adacaa-4ffd-4d51-8bb1-af16038aa398", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49c1a25c-8aa6-4512-82e5-94fe17760319", "node_type": "1", "metadata": {}, "hash": "9db2c15d8fd6df9e7962184dfd662b9446d1a1f070a409fa29019fadd4cabfea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fffd3454-7a13-4317-9cf4-93e6e1aaf4c9", "node_type": "1", "metadata": {}, "hash": "b01736dba939d7c957db29eaaf452d1c37146ecceac7622a683b0c217d1c6bb8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To increase the con-\nductance of a given cross point, we applied synchronized positive\nvoltage pulses from a driving circuit board to the memristor top\nelectrode and the gate of the series transistor. The gate voltage,\nwhich specifies a compliance current, determines the resulting\nmemristor conductance. We decreased the conductance by first\napplying a sufficient positive pulse to the memristor bottom\nelectrode to initialize the state, and then used the conductance\nincrease scheme to set the memristor to the desired level\n(illustrated in Supplementary Fig. 1).", "mimetype": "text/plain", "start_char_idx": 5465, "end_char_idx": 6030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fffd3454-7a13-4317-9cf4-93e6e1aaf4c9": {"__data__": {"id_": "fffd3454-7a13-4317-9cf4-93e6e1aaf4c9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74adacaa-4ffd-4d51-8bb1-af16038aa398", "node_type": "1", "metadata": {}, "hash": "48891e9316b6f089d0912996312efe2b998930e95630e37dbf0f5a03e59f1ae9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "deddc4ff-b450-4c19-a581-605ba421c0bf", "node_type": "1", "metadata": {}, "hash": "90d9352ef1e3887c416737946e0d22de250ec0ceb0596d6aa053ef85e740ae1d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "With this scheme, we\nachieved linear and symmetric conductance increase and decrease\nwith minimal cycle-to-cycle (Fig. 1f, g) and device-to-device\n(Fig. 1f, h) variations. We were able to set the conductance values\nacross the entire 128 \u00d7 64 array, except for the stuck devices, with\nreasonably high accuracy using only two electrical pulses to each\nmemristor (Fig. 1i and Supplementary Fig. 2).", "mimetype": "text/plain", "start_char_idx": 6031, "end_char_idx": 6426, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "deddc4ff-b450-4c19-a581-605ba421c0bf": {"__data__": {"id_": "deddc4ff-b450-4c19-a581-605ba421c0bf", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fffd3454-7a13-4317-9cf4-93e6e1aaf4c9", "node_type": "1", "metadata": {}, "hash": "b01736dba939d7c957db29eaaf452d1c37146ecceac7622a683b0c217d1c6bb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8264a7a0-17c0-4a36-b3ba-5bdb87ca5fa3", "node_type": "1", "metadata": {}, "hash": "16de0ce61dbcb342afd80b4c6009792b7e04221e29d77ba33c7b15f2fc660249", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The speed and\nreliability of the conductance update scheme make it possible to\ntrain the network in situ with almost any standard algorithm.\nHere the network was trained using stochastic gradient descent\n(SGD)32 to classify handwritten digits in MNIST dataset. For each\nnew sample of training data, the network first performs inference\nto get the log-probability of the label for each output by the\nsoftmax function, and then the weights in each layer are updated\naccordingly (see Methods).\nIn situ training in memristor crossbar.", "mimetype": "text/plain", "start_char_idx": 6427, "end_char_idx": 6957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8264a7a0-17c0-4a36-b3ba-5bdb87ca5fa3": {"__data__": {"id_": "8264a7a0-17c0-4a36-b3ba-5bdb87ca5fa3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "deddc4ff-b450-4c19-a581-605ba421c0bf", "node_type": "1", "metadata": {}, "hash": "90d9352ef1e3887c416737946e0d22de250ec0ceb0596d6aa053ef85e740ae1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02ffb4cf-a7cf-4921-9b13-0f04fbd1c7e9", "node_type": "1", "metadata": {}, "hash": "fbe62f610444d815e1faa531e7c892305841ca13c6d488ceb89e2f9baa11b76e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To implement the SGD\nalgorithm in the memristor crossbar, each synaptic weight was\nencoded as the difference of the conductance between two\nmemristors. Inference was performed by biasing the top elec-\ntrodes of memristors in the first layer with a set of voltages whose\namplitudes encode an image, then reading the currents from the\nbottom electrodes of devices in the final layer (Fig. 2a, b) using\ncustom-built circuit boards that can address up to 64 channels in\nparallel (Supplementary Fig. 3 and Supplementary Fig. 4).", "mimetype": "text/plain", "start_char_idx": 6958, "end_char_idx": 7481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02ffb4cf-a7cf-4921-9b13-0f04fbd1c7e9": {"__data__": {"id_": "02ffb4cf-a7cf-4921-9b13-0f04fbd1c7e9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8264a7a0-17c0-4a36-b3ba-5bdb87ca5fa3", "node_type": "1", "metadata": {}, "hash": "16de0ce61dbcb342afd80b4c6009792b7e04221e29d77ba33c7b15f2fc660249", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f757292-10ed-4d25-8c32-eb8eaa7f60e1", "node_type": "1", "metadata": {}, "hash": "54e760c2399237f87c5fbe36974e8fe765636fd51cb35d3b4bd8ed2c00f4ecf8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "During\ninference, all transistors operate in the deep triode region, and the\nmemristor array becomes a pseudo-crossbar capable of per-\nforming\nmatrix\nmultiplication\nfollowing\nOhm\u2019s\nlaw\nand\nKirchhoff\u2019s current law37,38 (see Supplementary Fig. 5a). The\nhidden neurons after each layer apply a nonlinear activation (in\nthis work, a rectified linear function in software) to the weighted\nsums computed in the crossbar. The desired weight update (\u0394W)\nfor each layer was calculated in software using Eq.", "mimetype": "text/plain", "start_char_idx": 7482, "end_char_idx": 7979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f757292-10ed-4d25-8c32-eb8eaa7f60e1": {"__data__": {"id_": "8f757292-10ed-4d25-8c32-eb8eaa7f60e1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02ffb4cf-a7cf-4921-9b13-0f04fbd1c7e9", "node_type": "1", "metadata": {}, "hash": "fbe62f610444d815e1faa531e7c892305841ca13c6d488ceb89e2f9baa11b76e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11d58726-a1f4-4efd-be95-4060eb6e8311", "node_type": "1", "metadata": {}, "hash": "be50a284c62dbb969ab1fac4a0eb5108da43a895f367555e8c2fd6ea608a3092", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1, then applied\nto the crossbar by the measurement system (see Fig. 2c for the\nalgorithm flow chart, Supplementary Fig. 6 for a unified modeling\nlanguage class diagram, and Supplementary Fig. 5b, c for a par-\nallel weight update scheme).", "mimetype": "text/plain", "start_char_idx": 7980, "end_char_idx": 8217, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11d58726-a1f4-4efd-be95-4060eb6e8311": {"__data__": {"id_": "11d58726-a1f4-4efd-be95-4060eb6e8311", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f757292-10ed-4d25-8c32-eb8eaa7f60e1", "node_type": "1", "metadata": {}, "hash": "54e760c2399237f87c5fbe36974e8fe765636fd51cb35d3b4bd8ed2c00f4ecf8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93432d76-aa5d-44c5-9966-232c066a1e11", "node_type": "1", "metadata": {}, "hash": "50914ba634613ae6fb21475a4daee200ad13b9637c4489236d9e8f0e87fd5519", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\u0394Wl \u00bc \u03b7 \ufffd\nX\nB\nn\u00bc1\n\u03b4l\u00f0n\u00devl\u00f0n\u00de>\n\u00f01\u00de\nwhere \u03b7 is the learning rate, v is the input voltage column vector\nfor the lth layer, \u03b4l the output error column vector for the layer,\nn indexes over the sample, and B is the batch size.", "mimetype": "text/plain", "start_char_idx": 8218, "end_char_idx": 8438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93432d76-aa5d-44c5-9966-232c066a1e11": {"__data__": {"id_": "93432d76-aa5d-44c5-9966-232c066a1e11", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11d58726-a1f4-4efd-be95-4060eb6e8311", "node_type": "1", "metadata": {}, "hash": "be50a284c62dbb969ab1fac4a0eb5108da43a895f367555e8c2fd6ea608a3092", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64fb4016-0551-46ef-9dd6-b376c71311bd", "node_type": "1", "metadata": {}, "hash": "1c7c3e743627218d2f8ee6aff95cbcf2a78a5c1b17ace74d466f1d91811a5538", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For a network\nwith L layers, the error row vectors are computed using\n\u03b4l\nj \u00bc\nyj \ufffdtj;\nl \u00bc L;\nP\ni\nwl\nij\u03b4l\u00fe1\ni\n;\nl < L and Ij > 0;\n0;\nl < L and Ij \ufffd0:\n8\n>\n>\n<\n>\n>\n:\n\u00f02\u00de\nwhere yj is the Bayesian probability computed by the network and\ntj is 1 if this sample belongs to class j and 0 otherwise.", "mimetype": "text/plain", "start_char_idx": 8439, "end_char_idx": 8728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64fb4016-0551-46ef-9dd6-b376c71311bd": {"__data__": {"id_": "64fb4016-0551-46ef-9dd6-b376c71311bd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93432d76-aa5d-44c5-9966-232c066a1e11", "node_type": "1", "metadata": {}, "hash": "50914ba634613ae6fb21475a4daee200ad13b9637c4489236d9e8f0e87fd5519", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddb2c5a2-1269-4c29-805e-009d49124ed5", "node_type": "1", "metadata": {}, "hash": "97c1f9820de3418bab2ac89400501eddfc56a1619388b2f55c2ac24977b8d299", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This\ncalculation ensures that the network maximizes the log-likelihood\nof the correct classification for each example.\nThe error backpropagation39,40 in this work is calculated in\nsoftware from the values of the readout weights (see Methods). In\nthe future, backpropagation can be implemented within the\nmemristor crossbar by applying a voltage vector representing the\ncurrent-layer error to the bottom electrodes of the crossbar and\nreading out the current vector from the top electrodes for the\nprevious-layer error.", "mimetype": "text/plain", "start_char_idx": 8729, "end_char_idx": 9247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddb2c5a2-1269-4c29-805e-009d49124ed5": {"__data__": {"id_": "ddb2c5a2-1269-4c29-805e-009d49124ed5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64fb4016-0551-46ef-9dd6-b376c71311bd", "node_type": "1", "metadata": {}, "hash": "1c7c3e743627218d2f8ee6aff95cbcf2a78a5c1b17ace74d466f1d91811a5538", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f20b7187-4e07-4ee0-9f99-fa7eb00302a4", "node_type": "1", "metadata": {}, "hash": "9dd7a03d901e799e36bd53ce38c08e0e148b9e69a8ff2b08b820127ca8961791", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "An on-chip integrated peripheral for full\nhardware implemented functionality is under development,\nwhich has been discussed and simulated in the literature as\nwell41\u201345.\nARTICLE\nNATURE COMMUNICATIONS | DOI: 10.1038/s41467-018-04484-2\n2\nNATURE COMMUNICATIONS |  (2018) 9:2385 | DOI: 10.1038/s41467-018-04484-2 | www.nature.com/naturecommunications\nClassification of MNIST handwritten digits.", "mimetype": "text/plain", "start_char_idx": 9248, "end_char_idx": 9638, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f20b7187-4e07-4ee0-9f99-fa7eb00302a4": {"__data__": {"id_": "f20b7187-4e07-4ee0-9f99-fa7eb00302a4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddb2c5a2-1269-4c29-805e-009d49124ed5", "node_type": "1", "metadata": {}, "hash": "97c1f9820de3418bab2ac89400501eddfc56a1619388b2f55c2ac24977b8d299", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da295ceb-8a43-4690-9fbe-b678aedf5db4", "node_type": "1", "metadata": {}, "hash": "a859ac12da0e9a18c3171dd12a99676a95b94272b8da93823cf46b40a7bd105b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We partitioned a\nsingle 128 \u00d7 64 array and constructed a two-layer perceptron with\n64 input neurons, 54 hidden neurons, and 10 output neurons to\nbe trained on the MNIST dataset of handwritten digits \u201c0\u201d\nthrough \u201c9\u201d, which has become a standard benchmark by which\nto gauge new machine learning algorithms. Each input image was\nrescaled to 8 pixels by 8 pixels (see Supplementary Fig. 7, and\nsample images in Fig. 3a) to match our network size.", "mimetype": "text/plain", "start_char_idx": 9639, "end_char_idx": 10081, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da295ceb-8a43-4690-9fbe-b678aedf5db4": {"__data__": {"id_": "da295ceb-8a43-4690-9fbe-b678aedf5db4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f20b7187-4e07-4ee0-9f99-fa7eb00302a4", "node_type": "1", "metadata": {}, "hash": "9dd7a03d901e799e36bd53ce38c08e0e148b9e69a8ff2b08b820127ca8961791", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eebd11f7-da9d-48df-8e8e-35717b84d6d6", "node_type": "1", "metadata": {}, "hash": "d9aeae34eca551f47f52d44c09ba53eb180a51782ebc139c450c5c0ae2e0138e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The\nintensities of each pixel of the grayscale images were unrolled into\n64-dimensional input feature vectors, which were duplicated to\nproduce 128 analogue voltages to enable negative effective\nweights (Fig. 2b). The two-layer network used 7992 memristors\n(see Fig. 3b for the partition on a 128 \u00d7 64 array), each of which\nwas initialized with a single pulse with a 1.0 V gate voltage from a\nlow-conductance state.", "mimetype": "text/plain", "start_char_idx": 10082, "end_char_idx": 10497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eebd11f7-da9d-48df-8e8e-35717b84d6d6": {"__data__": {"id_": "eebd11f7-da9d-48df-8e8e-35717b84d6d6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da295ceb-8a43-4690-9fbe-b678aedf5db4", "node_type": "1", "metadata": {}, "hash": "a859ac12da0e9a18c3171dd12a99676a95b94272b8da93823cf46b40a7bd105b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05b7bc04-844d-401c-aed2-1ebf29dcfe55", "node_type": "1", "metadata": {}, "hash": "b4a84d837b66c6464f480da68ae91ea0ddd1cf6a2279188add380cbbd977ccc5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The network was then trained on 80 000\nimages drawn from the training database (some images were\ndrawn more than once), with a minibatch size B = 50 for a total\nof 1600 training cycles. The smoothed minibatch experimental\naccuracy (compared with a defect-free simulation) during online\ntraining is shown in Fig. 3c. Figure 3d shows the linear rela-\ntionship between the conductance and the applied gate voltage\nduring each update cycle, which was critical for this demonstra-\ntion. More analyses are shown in Supplementary Figs.", "mimetype": "text/plain", "start_char_idx": 10498, "end_char_idx": 11026, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05b7bc04-844d-401c-aed2-1ebf29dcfe55": {"__data__": {"id_": "05b7bc04-844d-401c-aed2-1ebf29dcfe55", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eebd11f7-da9d-48df-8e8e-35717b84d6d6", "node_type": "1", "metadata": {}, "hash": "d9aeae34eca551f47f52d44c09ba53eb180a51782ebc139c450c5c0ae2e0138e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99c461c9-39dc-441d-9e20-5658778103a8", "node_type": "1", "metadata": {}, "hash": "22a5778a55a7ba55adddd425e19086064599d47555fa5a743e0775be9b71340d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "8, 9. After\nutilizing the entire training database, the network correctly\nclassified 91.71% of the 10 000 images in the separate test set\n(Figs. 3e\u2013j, Supplementary Table 1). Many of the misclassified\nimages are in fact difficult for humans to identify at the available\nresolution (Fig. 3h, and more in Supplementary Fig. 10).\nTo understand the potential of the memristor array, we\ndeveloped a simulation model (see Supplementary Fig.", "mimetype": "text/plain", "start_char_idx": 11027, "end_char_idx": 11461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99c461c9-39dc-441d-9e20-5658778103a8": {"__data__": {"id_": "99c461c9-39dc-441d-9e20-5658778103a8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05b7bc04-844d-401c-aed2-1ebf29dcfe55", "node_type": "1", "metadata": {}, "hash": "b4a84d837b66c6464f480da68ae91ea0ddd1cf6a2279188add380cbbd977ccc5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae4d523e-e7d3-441f-a3f9-eb7c5ce6ced2", "node_type": "1", "metadata": {}, "hash": "e37da1077bafd37fc714b0ae1f8f0e98cba84c30f9c3955acd84bf6af133a6e8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "6 for\ndetailed architecture) based on measured parameters such as the\nunresponsive rate, conductance update error, and limited memris-\ntor conductance dynamic range. We found that the simulated\naccuracy agrees well with the experimentally achieved one (Fig. 4a),\nvalidating the simulation model. A further simulation on a defect-\nfree network shows that the MNIST classification accuracy is\nsimilar to that of the same network architecture trained in\nTensorFlow46 that uses 32-bit floating point numbers.", "mimetype": "text/plain", "start_char_idx": 11462, "end_char_idx": 11966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae4d523e-e7d3-441f-a3f9-eb7c5ce6ced2": {"__data__": {"id_": "ae4d523e-e7d3-441f-a3f9-eb7c5ce6ced2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99c461c9-39dc-441d-9e20-5658778103a8", "node_type": "1", "metadata": {}, "hash": "22a5778a55a7ba55adddd425e19086064599d47555fa5a743e0775be9b71340d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d58ac39-68bc-45a9-9c8b-2a79aa7e726f", "node_type": "1", "metadata": {}, "hash": "23539fee0ccdf90ab7daffa88651405dbd6dbf630ebe12fc80960b53718e706c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This result\nsuggests that even though the analogue memristor network has\nlimited precision due to conductance variation, they do not have a\nsignificant impact on MNIST classification accuracy (more analysis\non conductance update variation is shown in Supplementary\nFig. 11). Our finding is consistent with previous theoretical and\nsimulation studies on more sophisticated problems20,24,47\u201351.", "mimetype": "text/plain", "start_char_idx": 11967, "end_char_idx": 12359, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d58ac39-68bc-45a9-9c8b-2a79aa7e726f": {"__data__": {"id_": "1d58ac39-68bc-45a9-9c8b-2a79aa7e726f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae4d523e-e7d3-441f-a3f9-eb7c5ce6ced2", "node_type": "1", "metadata": {}, "hash": "e37da1077bafd37fc714b0ae1f8f0e98cba84c30f9c3955acd84bf6af133a6e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44c32e15-c836-42c4-86f6-7389163cbe01", "node_type": "1", "metadata": {}, "hash": "566b811eefd500b550c5761348154abf7e11e2cc9ee7f7757f96b16bbf3bf331", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The\nanalog precision could potentially be improved, if needed for other\napplications, by device engineering for better IV linearity37,38, or\nusing pulse width instead of amplitude to represent analog input\n(with increased time overhead)27,28,37, or employing multiple\nmemristors to represent one synaptic weight (at the expense of\nchip area)52, etc.\nTransistor\nMemristor\nMemristor\n10 \u03bcm\nTransistor\na\nb\nc\nd\ne\n0.6\n1.1\n1.6\n1.1\n0.", "mimetype": "text/plain", "start_char_idx": 12360, "end_char_idx": 12786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44c32e15-c836-42c4-86f6-7389163cbe01": {"__data__": {"id_": "44c32e15-c836-42c4-86f6-7389163cbe01", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d58ac39-68bc-45a9-9c8b-2a79aa7e726f", "node_type": "1", "metadata": {}, "hash": "23539fee0ccdf90ab7daffa88651405dbd6dbf630ebe12fc80960b53718e706c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56c304f4-a53e-4ce6-a7cc-eb1ec11e699c", "node_type": "1", "metadata": {}, "hash": "2dad58b238a5abbe806a5b92d7e6eecd18c1910b5ef2db4d86598619202a33d0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "6\nGate voltage (V)\n0\n200\n400\n600\n800\n1000\n1200\nConductance (\u03bcS)\nConductance (\u03bcS)\nPulse number\n0\n50\n100\n150\n200\nh\ni\n16\n32\n48\n64\n80\n96\n112\n128\n16\n32\n48\n64\n200\n400\n600\n800\n1000\n1200\nConductance (\u03bcS)\nReadout conductance after single-pulse programming\nAll responsive devices in 8k array\n0.6\n1.", "mimetype": "text/plain", "start_char_idx": 12786, "end_char_idx": 13074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56c304f4-a53e-4ce6-a7cc-eb1ec11e699c": {"__data__": {"id_": "56c304f4-a53e-4ce6-a7cc-eb1ec11e699c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44c32e15-c836-42c4-86f6-7389163cbe01", "node_type": "1", "metadata": {}, "hash": "566b811eefd500b550c5761348154abf7e11e2cc9ee7f7757f96b16bbf3bf331", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53159cb3-d55d-4004-88e8-f2cffba985ed", "node_type": "1", "metadata": {}, "hash": "5c8d21f8b67916e0acdd87622548825a6241734a50b1243e6c998bf00c8548b6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1\n1.6\n1.1\n0.6\nGate voltage (V)\n0\n200\n400\n600\n800\n1000\n1200\nPulse number\n0\n50\n100\n150\n200\n20 cycles for a single device\ng\nRe(DFT matrix)\nIm(DFT matrix)\nConductance (\u03bcS)\nf\nTa\nHfO2\nPt\n1000\n500\n0\n0\n500\n1000\n1500\nPulse number\n2000\n2500\n3000\n3500\n4000\nFig.", "mimetype": "text/plain", "start_char_idx": 13074, "end_char_idx": 13324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53159cb3-d55d-4004-88e8-f2cffba985ed": {"__data__": {"id_": "53159cb3-d55d-4004-88e8-f2cffba985ed", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56c304f4-a53e-4ce6-a7cc-eb1ec11e699c", "node_type": "1", "metadata": {}, "hash": "2dad58b238a5abbe806a5b92d7e6eecd18c1910b5ef2db4d86598619202a33d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d832b5f-a58c-4190-850c-1393f8c82908", "node_type": "1", "metadata": {}, "hash": "1535cbe2f199035ab4a2b21d70c2191c5deaeaf615dd08458b9a30a7b3e9613f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1 Memristive platform for in situ learning. a An optical image of a wafer with transistor arrays. b Close-up of chip image showing arrays of various\nsizes. c Microscope image showing the 1T1R (one transistor one memristor) structure of the cell. Scale bar, 10 \u00b5m. d Cross-sectional scanning electron\nmicroscopic image of an individual 1T1R cell, which is cut in a focused ion beam microscope from the dashed line in c. Scale bar, 2 \u00b5m.", "mimetype": "text/plain", "start_char_idx": 13325, "end_char_idx": 13760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d832b5f-a58c-4190-850c-1393f8c82908": {"__data__": {"id_": "0d832b5f-a58c-4190-850c-1393f8c82908", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53159cb3-d55d-4004-88e8-f2cffba985ed", "node_type": "1", "metadata": {}, "hash": "5c8d21f8b67916e0acdd87622548825a6241734a50b1243e6c998bf00c8548b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6ce9951-3366-4b23-80c1-a54994cf97be", "node_type": "1", "metadata": {}, "hash": "d1e1c2f6d4212e7ea51d857c5c5c38bd1154e8fece3412282ec4c6a575a14a02", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "e Cross-sectional\ntransmission electron microscopic image of the integrated Ta/HfO2/Pt memristor. Scale bar, 2 nm. f All responsive devices over 20 potentiation/\ndepression epochs of 200 pulses each. g Evolution of conductance during 20 cycles of full potentiation and depression for a single cell with 200 pulses per\ncycle, showing low cycle-to-cycle variability. More results are shown in Supplementary Fig.", "mimetype": "text/plain", "start_char_idx": 13761, "end_char_idx": 14170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6ce9951-3366-4b23-80c1-a54994cf97be": {"__data__": {"id_": "c6ce9951-3366-4b23-80c1-a54994cf97be", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d832b5f-a58c-4190-850c-1393f8c82908", "node_type": "1", "metadata": {}, "hash": "1535cbe2f199035ab4a2b21d70c2191c5deaeaf615dd08458b9a30a7b3e9613f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb99e91d-09d3-4f2c-956e-86b1fc15ea01", "node_type": "1", "metadata": {}, "hash": "3587a1789ac7e44743934eb003165e4fc57e994a3723f361518c1f6b2a411d7b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1. h Evolution of conductance over one 200-pulse cycle of full\npotentiation and depression for all responsive devices in the array, with median conductance indicated by the yellow line. i Conductance of a 128 \u00d7 64 array\nafter single-pulse conductance writing of the discrete Fourier transform matrix.", "mimetype": "text/plain", "start_char_idx": 14171, "end_char_idx": 14471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb99e91d-09d3-4f2c-956e-86b1fc15ea01": {"__data__": {"id_": "bb99e91d-09d3-4f2c-956e-86b1fc15ea01", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6ce9951-3366-4b23-80c1-a54994cf97be", "node_type": "1", "metadata": {}, "hash": "d1e1c2f6d4212e7ea51d857c5c5c38bd1154e8fece3412282ec4c6a575a14a02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cffb2891-1970-4436-ac95-293ace706841", "node_type": "1", "metadata": {}, "hash": "250b8cccbcfe778041f009a064d2197fe509a510dd77d8bf576bd05988cf50c5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Several stuck devices are visible (in yellow)\nNATURE COMMUNICATIONS | DOI: 10.1038/s41467-018-04484-2\nARTICLE\nNATURE COMMUNICATIONS |  (2018) 9:2385 | DOI: 10.1038/s41467-018-04484-2 | www.nature.com/naturecommunications\n3\nA multilayer neural network trained with the online algorithm\nis more tolerant to hardware imperfections.", "mimetype": "text/plain", "start_char_idx": 14472, "end_char_idx": 14800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cffb2891-1970-4436-ac95-293ace706841": {"__data__": {"id_": "cffb2891-1970-4436-ac95-293ace706841", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb99e91d-09d3-4f2c-956e-86b1fc15ea01", "node_type": "1", "metadata": {}, "hash": "3587a1789ac7e44743934eb003165e4fc57e994a3723f361518c1f6b2a411d7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "916a3217-0ee1-4469-a9d1-d5470d1aae9c", "node_type": "1", "metadata": {}, "hash": "dae93e2eb9bad55e879cdb573ba1e42102dd3a9d9d1df060696d2d845023812a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The experimental\naccuracy of 91.71% in this work was achieved with 11% devices\nunresponsive to conductance updates. Our simulation showed\nthat even with 50% of the memristors stuck in a low-conductance\nstate, a >60% classification accuracy is still possible through\nonline training (Fig. 4b), although the accuracy is much more\nsensitive to shorted devices (Supplementary Fig. 12).", "mimetype": "text/plain", "start_char_idx": 14801, "end_char_idx": 15182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "916a3217-0ee1-4469-a9d1-d5470d1aae9c": {"__data__": {"id_": "916a3217-0ee1-4469-a9d1-d5470d1aae9c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cffb2891-1970-4436-ac95-293ace706841", "node_type": "1", "metadata": {}, "hash": "250b8cccbcfe778041f009a064d2197fe509a510dd77d8bf576bd05988cf50c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22f43c92-187b-41e0-aaf5-9fc7f4e3462b", "node_type": "1", "metadata": {}, "hash": "ede2c5c17df7a5f549ff5b88f28d3f4aa56976994e9ff398e4581f5b328a50f7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "On the other\nhand, if pre-trained weights are loaded to the memristor crossbar\n(i.e., ex situ training), the classification accuracy decreases quickly\nwith the defect rate (Fig. 4b)). There are approaches to improve\nthe robustness of ex situ training19,38, but most of them\nrequire that the parameters be tuned based on specific knowledge\nof the hardware (e.g., peripheral circuitry) and memristor array\n(e.g., device defects, wire resistance, etc.), while the in situ training\nadapts the weights and compensates them automatically.", "mimetype": "text/plain", "start_char_idx": 15183, "end_char_idx": 15715, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22f43c92-187b-41e0-aaf5-9fc7f4e3462b": {"__data__": {"id_": "22f43c92-187b-41e0-aaf5-9fc7f4e3462b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "916a3217-0ee1-4469-a9d1-d5470d1aae9c", "node_type": "1", "metadata": {}, "hash": "dae93e2eb9bad55e879cdb573ba1e42102dd3a9d9d1df060696d2d845023812a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b24b883-1e13-4ae6-afe8-e17247b7242c", "node_type": "1", "metadata": {}, "hash": "c57754c036a73fe6702d0411385203e3a2608b8d814a5a0a482f5edf1f859310", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The self-\nadaption is more powerful in a deeper network in which the\nhidden neurons are able to minimize the impact of defects, as\nsuggested by the higher classification accuracy from a two-layer\nnetwork than that from a single-layer one on the same images\n(Fig. 4c). The online training is also able to update the weights to\ncompensate for possible hardware and memristor conductance\ndrift over time (see Supplementary Fig. 13).", "mimetype": "text/plain", "start_char_idx": 15716, "end_char_idx": 16145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b24b883-1e13-4ae6-afe8-e17247b7242c": {"__data__": {"id_": "0b24b883-1e13-4ae6-afe8-e17247b7242c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22f43c92-187b-41e0-aaf5-9fc7f4e3462b", "node_type": "1", "metadata": {}, "hash": "ede2c5c17df7a5f549ff5b88f28d3f4aa56976994e9ff398e4581f5b328a50f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dab7f045-53f9-4fc6-aeb0-57e68ab6b8bc", "node_type": "1", "metadata": {}, "hash": "6e05cdecb17cac23d80f849648ed3f59043a8ad9bfafc22929809be47805b643", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We expect that the classification accuracy can be improved\nsubstantially with a larger network that has more hidden units\nand/or more inputs to support images with higher resolution. We\nperformed a simulation with our model on a 1024 \u00d7 512\nmemristor array, which is likely to be available in the near\nfuture, to recognize images of 22 \u00d7 22 pixels cropped from the\nMNIST dataset.", "mimetype": "text/plain", "start_char_idx": 16146, "end_char_idx": 16524, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dab7f045-53f9-4fc6-aeb0-57e68ab6b8bc": {"__data__": {"id_": "dab7f045-53f9-4fc6-aeb0-57e68ab6b8bc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b24b883-1e13-4ae6-afe8-e17247b7242c", "node_type": "1", "metadata": {}, "hash": "c57754c036a73fe6702d0411385203e3a2608b8d814a5a0a482f5edf1f859310", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4dc6ef75-3d29-4e86-b3fc-a3a3da0bd193", "node_type": "1", "metadata": {}, "hash": "6bc70023a3d22d3df83f8ec20f97864cf8180429593f6b76cd2014f2b8a4f29d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The network consists of 484 input neurons, 502\nhidden neurons, 10 output neurons, and a total of 495 976\nmemristors in the two layers to represent the synaptic weights\n(see Supplementary Fig. 14). After training on 1,200,000 images\n(20 epochs), a 97.3 \u00b1 0.4% classification accuracy is achieved on\nthe test set even with 11% stuck devices, approaching that\ndemonstrated with traditional hardware.", "mimetype": "text/plain", "start_char_idx": 16525, "end_char_idx": 16921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4dc6ef75-3d29-4e86-b3fc-a3a3da0bd193": {"__data__": {"id_": "4dc6ef75-3d29-4e86-b3fc-a3a3da0bd193", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dab7f045-53f9-4fc6-aeb0-57e68ab6b8bc", "node_type": "1", "metadata": {}, "hash": "6e05cdecb17cac23d80f849648ed3f59043a8ad9bfafc22929809be47805b643", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e1ba783-6d7a-4167-bc12-3bf9d0e7c5a6", "node_type": "1", "metadata": {}, "hash": "476b802c9748b97786e61d30c4631b7f1605beaf9dcc344a1823b0328030552c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "It will be straightforward\nto build deeper fully connected neural networks on an integrated\nchip with multiple large arrays in the near future, for even better\naccuracy and application to more complicated tasks. It is also\nnoteworthy that most state-of-art DNNs involve sophisticated\nmicrostructures, e.g., convolutional neural networks (CNNs) or\nlong short-term memory units (LSTMs). It may be worth\nb\na\nc\nInput neuron\nHidden neuron\nOutput neuron\nInference (feed forward)\nv1 (n)\ni1 (n)\ni2 (n)\nv2", "mimetype": "text/plain", "start_char_idx": 16922, "end_char_idx": 17418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e1ba783-6d7a-4167-bc12-3bf9d0e7c5a6": {"__data__": {"id_": "6e1ba783-6d7a-4167-bc12-3bf9d0e7c5a6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dc6ef75-3d29-4e86-b3fc-a3a3da0bd193", "node_type": "1", "metadata": {}, "hash": "6bc70023a3d22d3df83f8ec20f97864cf8180429593f6b76cd2014f2b8a4f29d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b597f4b-6bc5-49d9-b319-9e51efece81a", "node_type": "1", "metadata": {}, "hash": "70d06e9ceacead141531e401d18b00a25967d6d0a1378fad434a6d0e9a2b6f46", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(n)\nv1 (n)\ni1 (n)\nv2 (n)\n\u2013 v2 (n)\n\u2013 v1 (n)\nv3 (n)\nW1\nW2\n\u03a3\n\u03a3\n\u03a3\n\u03a3\n\u03a3\n\u03a3\nTraining (feedback)\nInference (feed forward)\nWeight update (feedback)\nInitializate\nweights\nBatch of\ntraining set\n{ v1 (n), t (n) };\nl = 1\nn = 1\nCalculate\nil (n)\nEq.", "mimetype": "text/plain", "start_char_idx": 17419, "end_char_idx": 17651, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b597f4b-6bc5-49d9-b319-9e51efece81a": {"__data__": {"id_": "4b597f4b-6bc5-49d9-b319-9e51efece81a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e1ba783-6d7a-4167-bc12-3bf9d0e7c5a6", "node_type": "1", "metadata": {}, "hash": "476b802c9748b97786e61d30c4631b7f1605beaf9dcc344a1823b0328030552c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c915ba3-d40b-452e-8c5c-cca6b4e42c66", "node_type": "1", "metadata": {}, "hash": "8f95d49e679e19c76aa5a1a5e69ec4a0b2b2b6f17617757774b832f93324a1e8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "3\nn = n + 1\nl = l + 1\nl = l \u2013 1\nLast\nlayer?\nNo\nYes\nFirst\nlayer?\nLast\nsample in\nbatch?\nUpdate\nweights\nEq.8\nYes\nLast\nbatch?\nEnd of\nEpoch\nYes\nNo\nNext batch\nNo\nYes\nNo\nRf\nRf\nRf\nRf\nRf\nRf\nPartly software in this work\nDifferential pair\nCalculate\nvl+1 (n)\nEq. 4\nCalculate\n\u0394Wl (n)\nEq.", "mimetype": "text/plain", "start_char_idx": 17652, "end_char_idx": 17926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c915ba3-d40b-452e-8c5c-cca6b4e42c66": {"__data__": {"id_": "1c915ba3-d40b-452e-8c5c-cca6b4e42c66", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b597f4b-6bc5-49d9-b319-9e51efece81a", "node_type": "1", "metadata": {}, "hash": "70d06e9ceacead141531e401d18b00a25967d6d0a1378fad434a6d0e9a2b6f46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44658f6c-6fae-4c9c-a56f-102ff8d88f8a", "node_type": "1", "metadata": {}, "hash": "815125fa48ef2b223544b2e3ca2d100b41c3b0645c549bcfbd1a95ba314c4108", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1\nCalculate\n\u03b4l (n)\nEq. 7\nCalculate\ny (n)\nEq. 5\nFig. 2 In situ training algorithm. a Schematic diagram of a two-layer neural network. Each neuron computes a weighted sum of its inputs and applies a\nnonlinear activation function. b The implementation of the network with a set of memristor crossbars. Each synaptic weight (arrows in a) corresponds to\nthe conductance difference between two memristors (as illustrated by the orange columns). Each crossbar computes weighted sums of its input voltages.", "mimetype": "text/plain", "start_char_idx": 17927, "end_char_idx": 18425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44658f6c-6fae-4c9c-a56f-102ff8d88f8a": {"__data__": {"id_": "44658f6c-6fae-4c9c-a56f-102ff8d88f8a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c915ba3-d40b-452e-8c5c-cca6b4e42c66", "node_type": "1", "metadata": {}, "hash": "8f95d49e679e19c76aa5a1a5e69ec4a0b2b2b6f17617757774b832f93324a1e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e9c3967-ccc0-4554-9aac-c88f870e3147", "node_type": "1", "metadata": {}, "hash": "7e4707f53c13c9d121c75f1b21934be7cfebc45168a3ed6c321177fb56419ed7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Between the crossbars is a layer of circuits that read the current from each wire, convert it to a voltage, and apply the activation function. The activation\nfunction was implemented in software in this work. c Flow chart of the in situ training. Steps in green boxes were implemented in hardware in this work,\nwhile those in yellow boxes were computationally expensive steps that can be accomplished with circuits integrated onto the chip in the future.", "mimetype": "text/plain", "start_char_idx": 18426, "end_char_idx": 18880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e9c3967-ccc0-4554-9aac-c88f870e3147": {"__data__": {"id_": "0e9c3967-ccc0-4554-9aac-c88f870e3147", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44658f6c-6fae-4c9c-a56f-102ff8d88f8a", "node_type": "1", "metadata": {}, "hash": "815125fa48ef2b223544b2e3ca2d100b41c3b0645c549bcfbd1a95ba314c4108", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "846f23c6-7d45-4eec-bdc0-7d18e1d86468", "node_type": "1", "metadata": {}, "hash": "a32936fcabf1136913c39da5e23ff47336e6067beddd0f374002533b3e764846", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The\nalgorithm is described in detail in Methods\nARTICLE\nNATURE COMMUNICATIONS | DOI: 10.1038/s41467-018-04484-2\n4\nNATURE COMMUNICATIONS |  (2018) 9:2385 | DOI: 10.1038/s41467-018-04484-2 | www.nature.com/naturecommunications\ninvestigating how to implement CNNs37 or LSTMs efficiently on\nmemristor crossbars in the future.", "mimetype": "text/plain", "start_char_idx": 18881, "end_char_idx": 19202, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "846f23c6-7d45-4eec-bdc0-7d18e1d86468": {"__data__": {"id_": "846f23c6-7d45-4eec-bdc0-7d18e1d86468", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e9c3967-ccc0-4554-9aac-c88f870e3147", "node_type": "1", "metadata": {}, "hash": "7e4707f53c13c9d121c75f1b21934be7cfebc45168a3ed6c321177fb56419ed7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83f1b13d-ca80-4ecb-88f4-feabe9c81073", "node_type": "1", "metadata": {}, "hash": "1dbc35bf443b9540e09aab4dfe5bbf441a39b341feed61a8c7a8a7aaa8cd2615", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "But on the other hand, such\nmicrostructure-based algorithms have been developed for use on\nconventional hardware, on which it is more efficient to process\nsparse matrices. Since the advantages of using sparse matrices in a\nmemristor crossbar are minimal, the optimal architectures for\nsophisticated tasks may look different.\nDiscussion\nA further potential benefit of utilizing analog computation in a\nmemristor-based neural network is a substantial improvement in\nspeed-energy efficiency.", "mimetype": "text/plain", "start_char_idx": 19203, "end_char_idx": 19691, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83f1b13d-ca80-4ecb-88f4-feabe9c81073": {"__data__": {"id_": "83f1b13d-ca80-4ecb-88f4-feabe9c81073", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "846f23c6-7d45-4eec-bdc0-7d18e1d86468", "node_type": "1", "metadata": {}, "hash": "a32936fcabf1136913c39da5e23ff47336e6067beddd0f374002533b3e764846", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "568a6c63-8443-4a31-8170-3b13bee1aab7", "node_type": "1", "metadata": {}, "hash": "7f61019e5ec6fb495ca6e98dd4dc62cf739baa54db091c22c8fa5dd36018dafd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The advantages mainly come from the\nfact that the computation is performed in the same location used\nto store the network data, which minimizes the time and energy\ncost of accessing the network parameters required by the con-\nventional von-Neumann architecture. The analog memristor\nnetwork is also capable of handling analog data acquired directly\nfrom sensors, which further reduces the energy overhead\nfrom analog-to-digital conversion. The memristors we used\nmaintain a highly linear IV relationship, allowing for the use of\nvoltage-amplitude as the analog input for each layer.", "mimetype": "text/plain", "start_char_idx": 19692, "end_char_idx": 20274, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "568a6c63-8443-4a31-8170-3b13bee1aab7": {"__data__": {"id_": "568a6c63-8443-4a31-8170-3b13bee1aab7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83f1b13d-ca80-4ecb-88f4-feabe9c81073", "node_type": "1", "metadata": {}, "hash": "1dbc35bf443b9540e09aab4dfe5bbf441a39b341feed61a8c7a8a7aaa8cd2615", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a967a35-c929-4297-934d-81c7294b6836", "node_type": "1", "metadata": {}, "hash": "feaa696239d13c1231e867fefe103b10074bd99769229b3e05ccadfc8755941a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This also\nminimizes circuit complexity and hence energy consumption for\nfuture hardware hidden neurons and output current readout.\nWhile the external control electronics we use in this work is not\noptimized for fast speed and low power consumption yet, pre-\nvious literature on circuit design45,51 and architecture21,53 suggest\nan on-chip integrated system would yield significant advantages\nin speed-energy efficiency.\nIn summary, we have demonstrated the in situ and self-\nadaptive learning capability of a multilayer neural network built\nby monolithically integrating memristor arrays onto a foundry-\nmade CMOS substrate.", "mimetype": "text/plain", "start_char_idx": 20275, "end_char_idx": 20899, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a967a35-c929-4297-934d-81c7294b6836": {"__data__": {"id_": "2a967a35-c929-4297-934d-81c7294b6836", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "568a6c63-8443-4a31-8170-3b13bee1aab7", "node_type": "1", "metadata": {}, "hash": "7f61019e5ec6fb495ca6e98dd4dc62cf739baa54db091c22c8fa5dd36018dafd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31586887-2b48-43d1-a7f9-fe6b0c57d3c3", "node_type": "1", "metadata": {}, "hash": "3cea9a3e481ec8cc1e83ebd9330e2dffbbe310a8bf22fb438e5f7481cd2a754e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The transistors enable reliable, linear, and\nsymmetric synaptic weight updates, allowing the network to be\ntrained with standard machine learning algorithms. After training\nwith a SGD algorithm on 80 000 images drawn from the MNIST\ntraining set, we achieved 91.71% accuracy on the complete\n10,000-image test set. This accuracy is 2.4% lower than an idea-\nlized simulation despite an 11% defect rate for the memristors\nused.", "mimetype": "text/plain", "start_char_idx": 20900, "end_char_idx": 21323, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31586887-2b48-43d1-a7f9-fe6b0c57d3c3": {"__data__": {"id_": "31586887-2b48-43d1-a7f9-fe6b0c57d3c3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a967a35-c929-4297-934d-81c7294b6836", "node_type": "1", "metadata": {}, "hash": "feaa696239d13c1231e867fefe103b10074bd99769229b3e05ccadfc8755941a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19054bc3-9198-4776-b980-2bc762d0c322", "node_type": "1", "metadata": {}, "hash": "00d156d018243219a8f3acf14f527390b6ef752231680db85d1358b812801b6d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The demonstrated performance with in situ online training\nand inference suggests that memristor crossbars are a promising\nhigh speed and energy efficiency technology for artificial\na MNIST grayscale image cropped & downsampled to 8\u00d78\n2. v2 (n) = \ufffd [i1 (n)]\n1. v1 (n)\n1. \u2013 v1 (n)\n2.", "mimetype": "text/plain", "start_char_idx": 21324, "end_char_idx": 21605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19054bc3-9198-4776-b980-2bc762d0c322": {"__data__": {"id_": "19054bc3-9198-4776-b980-2bc762d0c322", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31586887-2b48-43d1-a7f9-fe6b0c57d3c3", "node_type": "1", "metadata": {}, "hash": "3cea9a3e481ec8cc1e83ebd9330e2dffbbe310a8bf22fb438e5f7481cd2a754e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "356f2f45-993a-4742-b1e2-2fbebeadc6fa", "node_type": "1", "metadata": {}, "hash": "2656d29afa8758515153f916c935297470a328b21e2a7b38115222fa85d8ec11", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\u2013 v2 (n)\n2.  i2 (n)\n1.  i1 (n)\nVoltage (V)\n0.2 \n0\ne\nh\n0\n2\n4\n6\n8\nNo. of samples\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nAccuracy\nExperiment\nDefect-free simulation\nb\n1st layer\n(128\u00d754)\n2nd layer (108\u00d710)\nUnrolled image vector\nc\n0.5\n0.7\n0.9\n1.1\n1.", "mimetype": "text/plain", "start_char_idx": 21606, "end_char_idx": 21854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "356f2f45-993a-4742-b1e2-2fbebeadc6fa": {"__data__": {"id_": "356f2f45-993a-4742-b1e2-2fbebeadc6fa", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19054bc3-9198-4776-b980-2bc762d0c322", "node_type": "1", "metadata": {}, "hash": "00d156d018243219a8f3acf14f527390b6ef752231680db85d1358b812801b6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75244e7d-095e-4d01-8953-306760bca971", "node_type": "1", "metadata": {}, "hash": "14d0573ccfe708f11922eb44612a87a0124c3ecce9e5504546ae4e5a656225a5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "3\n1.5\n1.7\nGate voltage (V)\n100\n150\n200\n250\n300\n350\n400\n450\nConductance (\u03bcS)\n0 1 2 3 4 5 6 7 8 9\nOutput neuron\n0\n10\n20\n30\nCurrent (\u03bcA)\n0 1 2 3 4 5 6 7 8 9\nOutput neuron\n0%\n20%\n40%\n60%\n80%\n100%\nProbability\n0 1 2 3 4 5 6 7 8", "mimetype": "text/plain", "start_char_idx": 21854, "end_char_idx": 22075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75244e7d-095e-4d01-8953-306760bca971": {"__data__": {"id_": "75244e7d-095e-4d01-8953-306760bca971", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "356f2f45-993a-4742-b1e2-2fbebeadc6fa", "node_type": "1", "metadata": {}, "hash": "2656d29afa8758515153f916c935297470a328b21e2a7b38115222fa85d8ec11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf094086-9c4a-48d8-bd6b-11d862fd889c", "node_type": "1", "metadata": {}, "hash": "acdaeb5b375a26c8486cca9ab6467802d3e751ba3c747cfd0af1ce0263227463", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "9\nOutput neuron\n0\n10\n20\n30\n40\nCurrent (\u03bcA)\n0 1 2 3 4 5 6 7 8 9\nOutput neuron\n0%\n20%\n40%\n60%\n80%\n100%\nProbability\ni\nj\nd\nf\ng\n\u00d7104\nFig. 3 In situ online training and inference experiments on Modified National Institute of Standards and Technology (MNIST) handwritten digit recognition.\na Typical handwritten digits from the MNIST database. b Photo of the integrated 128 \u00d7 64 array during measurement.", "mimetype": "text/plain", "start_char_idx": 22076, "end_char_idx": 22473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf094086-9c4a-48d8-bd6b-11d862fd889c": {"__data__": {"id_": "bf094086-9c4a-48d8-bd6b-11d862fd889c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75244e7d-095e-4d01-8953-306760bca971", "node_type": "1", "metadata": {}, "hash": "14d0573ccfe708f11922eb44612a87a0124c3ecce9e5504546ae4e5a656225a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e11fc2c-92b6-436f-ae81-ba6c4d60627d", "node_type": "1", "metadata": {}, "hash": "46832051bc6a892667c80358d97f6cc9deccf5fd4441c4ff799a12190173a481", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The array was partitioned into two\nparts for the first and second layers, respectively. In all, 54 hidden neurons were used, so the first layer weight matrix is 64 \u00d7 54 (implemented using\n6912 memristors) and the second layer matrix is 54 \u00d7 10 (implemented using 1080 memristors). The blue and green false-colored areas are the positive\nand negative parts of the differential pairs. c Minibatch accuracy increases over the course of training. Experimental data followed the defect-free\nsimulation closely, with a consistent 2\u20134% gap.", "mimetype": "text/plain", "start_char_idx": 22474, "end_char_idx": 23007, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e11fc2c-92b6-436f-ae81-ba6c4d60627d": {"__data__": {"id_": "4e11fc2c-92b6-436f-ae81-ba6c4d60627d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf094086-9c4a-48d8-bd6b-11d862fd889c", "node_type": "1", "metadata": {}, "hash": "acdaeb5b375a26c8486cca9ab6467802d3e751ba3c747cfd0af1ce0263227463", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02587a1c-08eb-41f2-92d3-e5e732fccaef", "node_type": "1", "metadata": {}, "hash": "b4fcd1345bf8284195d42adf9b8abda4ee5af804ab20ff0e82d0c5674452e029", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "d The conductance-gate voltage relation extracted from data collected during training. The conductance\nwas read using the scheme described in the Methods. The conductance includes the effects of sneak-paths and wire resistance, which makes the measured\nvalues smaller and the variance larger than those in Fig. 1b, c. The dashed line indicates the mean conductance, while the error bars show a 95% confidence\ninterval for the measured conductance.", "mimetype": "text/plain", "start_char_idx": 23008, "end_char_idx": 23455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02587a1c-08eb-41f2-92d3-e5e732fccaef": {"__data__": {"id_": "02587a1c-08eb-41f2-92d3-e5e732fccaef", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e11fc2c-92b6-436f-ae81-ba6c4d60627d", "node_type": "1", "metadata": {}, "hash": "46832051bc6a892667c80358d97f6cc9deccf5fd4441c4ff799a12190173a481", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ce1be7a-a33b-41db-a132-2648c1b53f38", "node_type": "1", "metadata": {}, "hash": "778339acd760488b4f85867505bb0ca3108bdf0db909863a6b4d52a4905ac385", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The real-time online training accuracy with the readout weight values is shown in an animation in Supplementary\nMovie 1. e\u2013g Typical correctly classified digit \u201c9\u201d and h\u2013j misclassified digit \u201c8\u201d after the in situ training. e, h Images of the actual digits from the MNIST test\nset used as the input to the network. f, i The raw current measured from the output layer neurons. The neuron representing the digit \u201c9\u201d has the highest\noutput current, indicating a correct classification.", "mimetype": "text/plain", "start_char_idx": 23456, "end_char_idx": 23938, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ce1be7a-a33b-41db-a132-2648c1b53f38": {"__data__": {"id_": "9ce1be7a-a33b-41db-a132-2648c1b53f38", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02587a1c-08eb-41f2-92d3-e5e732fccaef", "node_type": "1", "metadata": {}, "hash": "b4fcd1345bf8284195d42adf9b8abda4ee5af804ab20ff0e82d0c5674452e029", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9737dc88-cfee-4ce7-b6cb-534b1d0ae78f", "node_type": "1", "metadata": {}, "hash": "9fa5a925e92ff04d7f07c79d9d03918634559402ae9b7d2050df5f029b270650", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "g, j The corresponding Bayesian probability of each digit, as calculated by a softmax function. More\ninference samples are shown in Supplementary Fig. 7 and Supplementary Movies 2 and 3\nNATURE COMMUNICATIONS | DOI: 10.1038/s41467-018-04484-2\nARTICLE\nNATURE COMMUNICATIONS |  (2018) 9:2385 | DOI: 10.1038/s41467-018-04484-2 | www.nature.com/naturecommunications\n5\nintelligence applications.", "mimetype": "text/plain", "start_char_idx": 23939, "end_char_idx": 24328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9737dc88-cfee-4ce7-b6cb-534b1d0ae78f": {"__data__": {"id_": "9737dc88-cfee-4ce7-b6cb-534b1d0ae78f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "de19f6fb9e087efa3cb5c7f47ad1b5948c8dc34a8ddfd11d5f0ddf808a3d5269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ce1be7a-a33b-41db-a132-2648c1b53f38", "node_type": "1", "metadata": {}, "hash": "778339acd760488b4f85867505bb0ca3108bdf0db909863a6b4d52a4905ac385", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The software neurons used in this\ndemonstration indicate that a hybrid digital processor and neu-\nromorphic analogue approach for DNNs can be effective, but all\nthe software functions used in the present demonstration can be\nintegrated as hardware onto a full-function chip in the near\nfuture.", "mimetype": "text/plain", "start_char_idx": 24329, "end_char_idx": 24622, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8b160eb-2ecf-433e-a204-98abb6371899": {"__data__": {"id_": "a8b160eb-2ecf-433e-a204-98abb6371899", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04bff131-cee6-4985-9ec6-33c2658f405a", "node_type": "1", "metadata": {}, "hash": "7bea61f6dc81dadb8a56d40b05fbb1fa747ef05596e7e9ea13e18d35c55fc9bb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Methods\nDevice fabrication and array integration. The transistor array and inter-\nconnection between cells are taped out from a commercial foundry with 2 \u03bcm\ntechnology node to achieve low wire resistance. We monolithically integrate\nmemristors on top of as-received chip in house. Pd/Ag contact metals are\nfirst deposited on both vias after argon plasma treatment to remove the native\noxide. The chip is then annealed at 300 \u00b0C for 30 min in 20 s.c.c.m nitrogen\nflow to achieve good electrical contact.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04bff131-cee6-4985-9ec6-33c2658f405a": {"__data__": {"id_": "04bff131-cee6-4985-9ec6-33c2658f405a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8b160eb-2ecf-433e-a204-98abb6371899", "node_type": "1", "metadata": {}, "hash": "0fab8e1dfcdba39ec11d885a9402eca39ff5811781dff10e8b344d6e712970d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7c3291a-1802-4c9d-9c14-9cc5455b9191", "node_type": "1", "metadata": {}, "hash": "851fe119eef0c33befc46231731eb3c813a669fbe2c451754faf89d0f8c689ec", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The memristor bottom electrode is\ndeposited by evaporating 20 nm Pt on top of a 2 nm-thick Ta adhesive layer\nand patterned by photolithography and lift-off in acetone. A switching layer of\n5 nm-thick HfO2 is deposited by atomic layer deposition using water and tetrakis\n(dimethylamido)hafnium as precursors at 250 \u00b0C, and then patterned by photo-\nlithography and reactive ion etch.", "mimetype": "text/plain", "start_char_idx": 503, "end_char_idx": 884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7c3291a-1802-4c9d-9c14-9cc5455b9191": {"__data__": {"id_": "b7c3291a-1802-4c9d-9c14-9cc5455b9191", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04bff131-cee6-4985-9ec6-33c2658f405a", "node_type": "1", "metadata": {}, "hash": "7bea61f6dc81dadb8a56d40b05fbb1fa747ef05596e7e9ea13e18d35c55fc9bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "935fddc8-4640-486f-bd2c-e9ddc05cd514", "node_type": "1", "metadata": {}, "hash": "d0028c65ec8f10d3ae4960380717031b8c89b87666436b064ee17bece8bf27c3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Finally, the top electrode of 50 nm-thick Ta is\ndeposited by sputtering and lift-off, followed by sputtering of a 10 nm-thick Pd\nprotection layer.\nDataset. The dataset is composed of the input feature vector (x(n) for sample n)\nand the target output vector (t(n)). For the classification problem, tc (n) = 1 if\nsample n belongs to class c, and is 0 otherwise.", "mimetype": "text/plain", "start_char_idx": 885, "end_char_idx": 1244, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "935fddc8-4640-486f-bd2c-e9ddc05cd514": {"__data__": {"id_": "935fddc8-4640-486f-bd2c-e9ddc05cd514", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7c3291a-1802-4c9d-9c14-9cc5455b9191", "node_type": "1", "metadata": {}, "hash": "851fe119eef0c33befc46231731eb3c813a669fbe2c451754faf89d0f8c689ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f67f804e-a91a-43b1-864b-0573cfbc4566", "node_type": "1", "metadata": {}, "hash": "3b704843e2735ce4afadf9401217d72ed80b0191064d65f068e60286ec076ebe", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For the MNIST dataset, feature\nvectors are the unrolled grayscale pixel values of the handwritten digital two-\ndimensional images. The original images are 28 pixels by 28 pixels. They were\ncropped to 20 \u00d7 20 and then further down sampled to 8 \u00d7 8 (using bicubic inter-\npolation) to match the input size of the memristor neural network (Supplementary\nFig. 7). The 8 \u00d7 8 grayscale images were then unrolled to 64-dimensional input\nfeature vectors.", "mimetype": "text/plain", "start_char_idx": 1245, "end_char_idx": 1690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f67f804e-a91a-43b1-864b-0573cfbc4566": {"__data__": {"id_": "f67f804e-a91a-43b1-864b-0573cfbc4566", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "935fddc8-4640-486f-bd2c-e9ddc05cd514", "node_type": "1", "metadata": {}, "hash": "d0028c65ec8f10d3ae4960380717031b8c89b87666436b064ee17bece8bf27c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f08e17b-1f33-4ae0-823c-b9455395cba7", "node_type": "1", "metadata": {}, "hash": "5182e815b27a8ea0616ed3c414f086858348e4a1d6891795f7c1c196c2fa44aa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The input feature vectors were converted to voltage values v1(n)by\na scaling factor, which was the same for all images. The output vectors have\n10 dimensions, each corresponding to one digit.\nInference. The in situ online training was composed of two stages: feedforward\ninference and feedback weight update. The multilayer inference was performed\nlayer by layer sequentially. The input voltage vector to the first layer was a feature\nvector from the dataset, while the input vector for the subsequent layer was the\noutput vector of the previous layer.", "mimetype": "text/plain", "start_char_idx": 1691, "end_char_idx": 2243, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f08e17b-1f33-4ae0-823c-b9455395cba7": {"__data__": {"id_": "9f08e17b-1f33-4ae0-823c-b9455395cba7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f67f804e-a91a-43b1-864b-0573cfbc4566", "node_type": "1", "metadata": {}, "hash": "3b704843e2735ce4afadf9401217d72ed80b0191064d65f068e60286ec076ebe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ca4ceb0-c5a7-45fe-b6d9-83e215954b9b", "node_type": "1", "metadata": {}, "hash": "2a5a1c12aa8c259cbcfa7ca9c7386215008000834518341bc879be2e85692c24", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The analog weighted sum step was performed\nin the memristor crossbar array as indicated by Eq. 3, or equivalently by Eq. 4:\nil \u00bc Wlvl\n\u00f03\u00de\nIj \u00bc\nX\nn\ni\u00bc1\nwij \ufffdVi \u00bc\nX\nn\ni\u00bc1\nG\u00fe\nij \ufffdVi \u00fe G\ufffd\nij \ufffd\ufffdVi\n\u00f0\n\u00de\nh\ni\n\u00f04\u00de\n0%\n10%\n20%\n30%\n40%\n50%\nRatio of stuck OFF", "mimetype": "text/plain", "start_char_idx": 2244, "end_char_idx": 2489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ca4ceb0-c5a7-45fe-b6d9-83e215954b9b": {"__data__": {"id_": "6ca4ceb0-c5a7-45fe-b6d9-83e215954b9b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f08e17b-1f33-4ae0-823c-b9455395cba7", "node_type": "1", "metadata": {}, "hash": "5182e815b27a8ea0616ed3c414f086858348e4a1d6891795f7c1c196c2fa44aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f32a1a3-cb24-4ce8-b5b8-0971a7b91306", "node_type": "1", "metadata": {}, "hash": "1898092fb188c414433655700ec10688df9fc2d41c2aa6d60c541bbd134e7889", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "device\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nAccuracy (simulation)\nIn situ training\nEx situ training\n0%\n10%\n20%\n30%\n40%\n50%\nRatio of stuck OFF device\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nAccuracy (simulation)\nTwo-layers\nSingle-layer\nb\nc\n102\n103\n104\n105\nNumber of training images\n10\u20132\n10\u20131\n100\nError\nTensorFlow (FP32)\nSimulation (11%", "mimetype": "text/plain", "start_char_idx": 2490, "end_char_idx": 2835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f32a1a3-cb24-4ce8-b5b8-0971a7b91306": {"__data__": {"id_": "3f32a1a3-cb24-4ce8-b5b8-0971a7b91306", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ca4ceb0-c5a7-45fe-b6d9-83e215954b9b", "node_type": "1", "metadata": {}, "hash": "2a5a1c12aa8c259cbcfa7ca9c7386215008000834518341bc879be2e85692c24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2dbc002-4aa9-4a30-9364-b71cee9431bf", "node_type": "1", "metadata": {}, "hash": "3b8cdc9dfa1941655fb2a729ec875201a056ac31adbbaac8e8d773ba3829fbce", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "stuck)\nSimulation (0% stuck)\nExperiment\na\nd\n0\n2\n4\n6\n8\n10\n12\nNumber of training images\n105\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\nAccuracy (%)\nSimulation\nFig. 4 Analysis based on experimental-calibrated simulation. a The experimental classification error (accuracy shown in Supplementary Fig. 10) matches\nthe simulated accuracy.", "mimetype": "text/plain", "start_char_idx": 2836, "end_char_idx": 3158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2dbc002-4aa9-4a30-9364-b71cee9431bf": {"__data__": {"id_": "c2dbc002-4aa9-4a30-9364-b71cee9431bf", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f32a1a3-cb24-4ce8-b5b8-0971a7b91306", "node_type": "1", "metadata": {}, "hash": "1898092fb188c414433655700ec10688df9fc2d41c2aa6d60c541bbd134e7889", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcba84b0-3be7-4f64-af6f-c137ccfc06d5", "node_type": "1", "metadata": {}, "hash": "10fd606814a696e11807d16cadd7600671e5940db24d046a75c022354ca09ed5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The simulation considers experiment parameters, including 11% devices stuck at 10 \u03bcS, 2% conductance update variation, limited\nconductance dynamic range, etc. The simulation on defect-free assumption shows an accuracy approaching that from TensorFlow. Each data point is the\nclassification error rate on the complete testing set (10 000 images) after 500 images (simulation or TensorFlow) or 5000 images (experiment). b The\nimpact of non-responsive devices on the inference accuracy with in situ and ex situ training approach.", "mimetype": "text/plain", "start_char_idx": 3159, "end_char_idx": 3685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcba84b0-3be7-4f64-af6f-c137ccfc06d5": {"__data__": {"id_": "fcba84b0-3be7-4f64-af6f-c137ccfc06d5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2dbc002-4aa9-4a30-9364-b71cee9431bf", "node_type": "1", "metadata": {}, "hash": "3b8cdc9dfa1941655fb2a729ec875201a056ac31adbbaac8e8d773ba3829fbce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91fac5af-354c-4a14-b045-b6c98a556d31", "node_type": "1", "metadata": {}, "hash": "db0be8cabf11d6311ef016ca895a67ccc64a9143b4f7626cb4536a0dac8ed898", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The non-responsive device was stuck in a very low-\nconductance state (10 \u00b5S), which is the typical defect device value observed in the experiment. The result shows that the in situ training process adapts to\nthe defects, providing a much higher defect tolerance compared with pre-loading ex situ training weights into the network. With 50% stuck OFF devices,\nthe network can still achieve over 60% accuracy. The error bar shows the s.d. over 10 simulations. c The multilayer network also helps with the defect\ntolerance.", "mimetype": "text/plain", "start_char_idx": 3686, "end_char_idx": 4206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91fac5af-354c-4a14-b045-b6c98a556d31": {"__data__": {"id_": "91fac5af-354c-4a14-b045-b6c98a556d31", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcba84b0-3be7-4f64-af6f-c137ccfc06d5", "node_type": "1", "metadata": {}, "hash": "10fd606814a696e11807d16cadd7600671e5940db24d046a75c022354ca09ed5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d666af53-ea3e-4e9e-bac4-852c89e1ca31", "node_type": "1", "metadata": {}, "hash": "9c9008d2d23577909381309129a333480d2bab8473effaaf7d1c8e8a72dfad31", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "If one device is stuck, the associated hidden neuron will adjust the connections accordingly. The error bar shows the s.d. over 10 simulations.\nd The simulation of a larger network constructed on a larger memristor crossbar (1024 \u00d7 512) with experimental parameters (e.g., 11% defect rate) could\nachieve accuracy above 97%, which suggests a large memristor network could narrow the accuracy performance gap from the conventional CMOS\nhardware. The network architecture is shown in Supplementary Fig. 14\nARTICLE\nNATURE COMMUNICATIONS | DOI: 10.", "mimetype": "text/plain", "start_char_idx": 4207, "end_char_idx": 4750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d666af53-ea3e-4e9e-bac4-852c89e1ca31": {"__data__": {"id_": "d666af53-ea3e-4e9e-bac4-852c89e1ca31", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91fac5af-354c-4a14-b045-b6c98a556d31", "node_type": "1", "metadata": {}, "hash": "db0be8cabf11d6311ef016ca895a67ccc64a9143b4f7626cb4536a0dac8ed898", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52a16434-4e96-4cf7-b124-d6119d5b727a", "node_type": "1", "metadata": {}, "hash": "82c910d8875ed82c4ea915243c5c1fb6e22823f00b201547cba312017cfc9404", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1038/s41467-018-04484-2\n6\nNATURE COMMUNICATIONS |  (2018) 9:2385 | DOI: 10.1038/s41467-018-04484-2 | www.nature.com/naturecommunications\nwhere vl is the lth layer input voltage vector that is applied to the top electrodes\nof the memristor crossbar, il is the readout current vector from the bottom\nelectrodes of the crossbar, and Wl is the weight matrix of layer l.", "mimetype": "text/plain", "start_char_idx": 4750, "end_char_idx": 5115, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52a16434-4e96-4cf7-b124-d6119d5b727a": {"__data__": {"id_": "52a16434-4e96-4cf7-b124-d6119d5b727a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d666af53-ea3e-4e9e-bac4-852c89e1ca31", "node_type": "1", "metadata": {}, "hash": "9c9008d2d23577909381309129a333480d2bab8473effaaf7d1c8e8a72dfad31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cea6327b-8740-46af-8a77-385dfda82840", "node_type": "1", "metadata": {}, "hash": "e9f7b8b5fc0de2487cb709c1c0833e7c6c142531a2f4b76f8811a4b88210946a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The total\ncurrent is the sum of the currents through each device in the same column\n(Kirchhoff\u2019s current law), while each current is the product of the conductance\nand the voltage across the memristor (Ohm\u2019s law). Each weight value is repre-\nsented by the difference in conductance between two memristors:\nWij \u00bc G \u00fe\nij \ufffdG\ufffd\nij , so that weights can be negative.", "mimetype": "text/plain", "start_char_idx": 5116, "end_char_idx": 5476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cea6327b-8740-46af-8a77-385dfda82840": {"__data__": {"id_": "cea6327b-8740-46af-8a77-385dfda82840", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52a16434-4e96-4cf7-b124-d6119d5b727a", "node_type": "1", "metadata": {}, "hash": "82c910d8875ed82c4ea915243c5c1fb6e22823f00b201547cba312017cfc9404", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8027022-cba1-4178-b36e-abe1f6bbb03c", "node_type": "1", "metadata": {}, "hash": "a1a4a0127f7132e3d50f32f2899a956e104f67fd50b4d534f0ce995c84b66cc0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This is accomplished by\nduplicating the voltage vector vl, with +vl applied to half of the array and \u2212vl\napplied to the other half, as shown in Fig. 2c.\nWe chose a rectified linear unit activation function for the hidden layer, which\nis defined in\nVl\u00fe1\ni\n\u00bc \u03c3 Il\ni\n\ufffd\ufffd\n\u00bc\ncIl\ni;\nIl\ni > 0;\n0;\nI l\ni \ufffd0:\n(\n\u00f05\u00de\nwhere c is a scaling factor to match the voltage range.", "mimetype": "text/plain", "start_char_idx": 5477, "end_char_idx": 5837, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8027022-cba1-4178-b36e-abe1f6bbb03c": {"__data__": {"id_": "d8027022-cba1-4178-b36e-abe1f6bbb03c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cea6327b-8740-46af-8a77-385dfda82840", "node_type": "1", "metadata": {}, "hash": "e9f7b8b5fc0de2487cb709c1c0833e7c6c142531a2f4b76f8811a4b88210946a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84293e49-2a25-46bf-bd91-10048448e84d", "node_type": "1", "metadata": {}, "hash": "9afca31a81dc87d87e9da1b4b9829e4b31f23650101361f2b2e42b5c6d0d1691", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For the MNIST network in\nthis work, c was set to 200 V/A, and elements of the resulting voltage vector that\nexceed 0.2 V were clipped to avoid altering the memristor states. This particular\nstep was implemented by software in this investigation, and can be easily\nimplemented in the future with a rectifying diode and amplifier on an integrated\nchip. The most active (highest amplitude) output current was interpreted as the\nclassification result.\nSoftmax and cross-entropy loss function.", "mimetype": "text/plain", "start_char_idx": 5838, "end_char_idx": 6326, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84293e49-2a25-46bf-bd91-10048448e84d": {"__data__": {"id_": "84293e49-2a25-46bf-bd91-10048448e84d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8027022-cba1-4178-b36e-abe1f6bbb03c", "node_type": "1", "metadata": {}, "hash": "a1a4a0127f7132e3d50f32f2899a956e104f67fd50b4d534f0ce995c84b66cc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a54162ee-38a0-4ce1-a5d0-4beafaa4b1bf", "node_type": "1", "metadata": {}, "hash": "e18252777bc4e85c0947cb25b422a2cb75724d9aeb214e20127329d31320d80c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The inference result can also be cal-\nculated as a Bayesian probability, using the conversion defined by Eq. 6.\nyc n\n\u00f0 \u00de \u00bc\nekIc\u00f0n\u00de\nPC\nm\u00bc1 ekIm n\n\u00f0 \u00de\n\u00f06\u00de\nwhere yc n\n\u00f0 \u00de is the probability that sample n belongs to class c, and C is the\ntotal number of classes. k was set to 5 \u00d7 105/A for the MNIST network in this\nwork.", "mimetype": "text/plain", "start_char_idx": 6327, "end_char_idx": 6644, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a54162ee-38a0-4ce1-a5d0-4beafaa4b1bf": {"__data__": {"id_": "a54162ee-38a0-4ce1-a5d0-4beafaa4b1bf", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84293e49-2a25-46bf-bd91-10048448e84d", "node_type": "1", "metadata": {}, "hash": "9afca31a81dc87d87e9da1b4b9829e4b31f23650101361f2b2e42b5c6d0d1691", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce4cb413-aa31-4b76-adf3-108fb53d2098", "node_type": "1", "metadata": {}, "hash": "f54a56cb5ec50de1ae5344edd17ff4d1dcd53c21a204f66e0dce79afbd64f8cc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The goal of the training process was to adjust the weight values to maximize the\nlog-likelihood of the true class. We used a cross-entropy loss function, which is\ndefined in Eq.", "mimetype": "text/plain", "start_char_idx": 6645, "end_char_idx": 6822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce4cb413-aa31-4b76-adf3-108fb53d2098": {"__data__": {"id_": "ce4cb413-aa31-4b76-adf3-108fb53d2098", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a54162ee-38a0-4ce1-a5d0-4beafaa4b1bf", "node_type": "1", "metadata": {}, "hash": "e18252777bc4e85c0947cb25b422a2cb75724d9aeb214e20127329d31320d80c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d1df985-c156-40cc-bdf5-7fcf777ae171", "node_type": "1", "metadata": {}, "hash": "46a69380c3eacdfcd18d274d181ba9b07d4e246ae88f57832c7c5f0e94eb6c52", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "7.\n\u03be T; Y\n\u00f0\n\u00de \u00bc\nX\nN\nn\u00bc1\n\u03be t n\n\u00f0 \u00de; y n\n\u00f0 \u00de\n\u00bd\n\ufffd\u00bc \ufffd\nX\nN\nn\u00bc1\nX\nC\nc\u00bc1\ntc n\n\u00f0 \u00de log yc n\n\u00f0 \u00de\n\u00bd\n\ufffd\n\u00f07\u00de\nwhere N is the total number of samples.\nSGD with backpropagation.", "mimetype": "text/plain", "start_char_idx": 6823, "end_char_idx": 6984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d1df985-c156-40cc-bdf5-7fcf777ae171": {"__data__": {"id_": "5d1df985-c156-40cc-bdf5-7fcf777ae171", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce4cb413-aa31-4b76-adf3-108fb53d2098", "node_type": "1", "metadata": {}, "hash": "f54a56cb5ec50de1ae5344edd17ff4d1dcd53c21a204f66e0dce79afbd64f8cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba48f775-1d63-4def-8b60-fc544042eaa7", "node_type": "1", "metadata": {}, "hash": "852535f8ebcbbe367b91b70ffc3492506a1b2ae2593f33f477f956030b3ebc59", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In order to estimate the gradient of the loss function\nfor training the weights, we withdrew a subset of B samples (termed a minibatch)\nfrom the training set without replacement at each round of training. The SGD\nalgorithm was used to update the weights along the direction of steepest descent for\nE \u03be\n\u00bd \ufffd. The desired weight update was given by Eq. 1 in the main text.", "mimetype": "text/plain", "start_char_idx": 6985, "end_char_idx": 7354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba48f775-1d63-4def-8b60-fc544042eaa7": {"__data__": {"id_": "ba48f775-1d63-4def-8b60-fc544042eaa7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d1df985-c156-40cc-bdf5-7fcf777ae171", "node_type": "1", "metadata": {}, "hash": "46a69380c3eacdfcd18d274d181ba9b07d4e246ae88f57832c7c5f0e94eb6c52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d407c38-2a83-4e3a-8a82-c9424dbf9a5a", "node_type": "1", "metadata": {}, "hash": "2e43f2e3420f93dbd7e17bff352e977b0b0c25d8a46b2a18b5811d80447b6edf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For a network\nwith L layers, the error vector is computed by\n\u03b4l\nj \u00bc\n\u2202\u03be\n\u2202Il\nj ;\nl \u00bc L;\n\u2202\u03c3\n\u2202Il\nj\nP\ni\nWl\nij\u03b4l\u00fe1\ni\n;\nl \ufffdL:\n8\n>\n<\n>\n:\n\u00f08\u00de\nwhere \u03c3 is the nonlinear activation function for the hidden layer and \u03be the loss\nfunction of the output layer. For the loss function utilized in this study and rectified\nlinear activation, Eq. 8 reduces to Eq.", "mimetype": "text/plain", "start_char_idx": 7355, "end_char_idx": 7697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d407c38-2a83-4e3a-8a82-c9424dbf9a5a": {"__data__": {"id_": "2d407c38-2a83-4e3a-8a82-c9424dbf9a5a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba48f775-1d63-4def-8b60-fc544042eaa7", "node_type": "1", "metadata": {}, "hash": "852535f8ebcbbe367b91b70ffc3492506a1b2ae2593f33f477f956030b3ebc59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a27a4f6f-6d75-40e7-b12e-6f88b75acead", "node_type": "1", "metadata": {}, "hash": "6278ab48e803672f6b8edb9fe393370991ce26f3855dfecd847b93191c1601da", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2 in the main text and was evaluated in\nsoftware. With some improvements to the measurement system, we will be able to\nimplement this step in the crossbar, as described in the main text.\nThe weight update is then applied to the crossbar. We first adjust the gate\nvoltages of the transistors following Eq. 9. The changes in the gate voltage for the\nmemristors in differential pairs are of the same magnitude but in opposite\ndirections.", "mimetype": "text/plain", "start_char_idx": 7698, "end_char_idx": 8132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a27a4f6f-6d75-40e7-b12e-6f88b75acead": {"__data__": {"id_": "a27a4f6f-6d75-40e7-b12e-6f88b75acead", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d407c38-2a83-4e3a-8a82-c9424dbf9a5a", "node_type": "1", "metadata": {}, "hash": "2e43f2e3420f93dbd7e17bff352e977b0b0c25d8a46b2a18b5811d80447b6edf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df5526b1-3875-4a25-8eb8-10ae7adfc9f5", "node_type": "1", "metadata": {}, "hash": "70a5fe58d0adabf3ed8402518ff46d08b9edde3138cde8029ca563a9a5ace0fa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We enforced a maximum and a minimum gate voltage of 1.7 and 0.6 V,\nrespectively, for the current chip to make sure the transistor-gate-voltage and\nmemristor conductance relation remains in the linear region.", "mimetype": "text/plain", "start_char_idx": 8133, "end_char_idx": 8340, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df5526b1-3875-4a25-8eb8-10ae7adfc9f5": {"__data__": {"id_": "df5526b1-3875-4a25-8eb8-10ae7adfc9f5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a27a4f6f-6d75-40e7-b12e-6f88b75acead", "node_type": "1", "metadata": {}, "hash": "6278ab48e803672f6b8edb9fe393370991ce26f3855dfecd847b93191c1601da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fde06822-4570-445a-b422-4d9e65cb81fc", "node_type": "1", "metadata": {}, "hash": "2b8f1cc8aa877e930dd6a0e4613c88fe6d2e77464ab42ff5a730f21cfdc068bb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\u0394Vgate;l \u00bc \u00fe\u0394Wl \ufffd\u0394Wl\n\u00bd\n\ufffd\n\u00f09\u00de\nFor memristors for which \u0394Vgate;l;ij < 0, we first apply a voltage pulse (1.6 V) on\nthe bottom electrodes of the array to initialize the memristor to a very low-\nconductance state.", "mimetype": "text/plain", "start_char_idx": 8341, "end_char_idx": 8550, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fde06822-4570-445a-b422-4d9e65cb81fc": {"__data__": {"id_": "fde06822-4570-445a-b422-4d9e65cb81fc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df5526b1-3875-4a25-8eb8-10ae7adfc9f5", "node_type": "1", "metadata": {}, "hash": "70a5fe58d0adabf3ed8402518ff46d08b9edde3138cde8029ca563a9a5ace0fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fb023f4-473e-4a4d-a360-94103ee37a57", "node_type": "1", "metadata": {}, "hash": "86568bc3cdfcb319099c021eea57eddb52d64e496c8938a2e3f7b42bf4fe7cd6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We then apply a voltage pulse (2.5 V) to the top electrodes with\nan updated voltage matrix applied to the gates of the series transistors, which raises\nthe memristor conductance state up to match the gate voltage. As shown in the\nmain text, the resulting memristor conductance depends linearly on the transistor\u2019s\ngate voltage during this update process (Fig. 1f).\nReading the conductance matrix.", "mimetype": "text/plain", "start_char_idx": 8551, "end_char_idx": 8947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fb023f4-473e-4a4d-a360-94103ee37a57": {"__data__": {"id_": "4fb023f4-473e-4a4d-a360-94103ee37a57", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fde06822-4570-445a-b422-4d9e65cb81fc", "node_type": "1", "metadata": {}, "hash": "2b8f1cc8aa877e930dd6a0e4613c88fe6d2e77464ab42ff5a730f21cfdc068bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c040cd72-3d48-4732-a5db-4bc57ad52201", "node_type": "1", "metadata": {}, "hash": "44a4e8a9a85aa8714a91349c3cb12bd71ce5523e1defc49f6e9c403d66ec254a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To read the conductance of cross point (i, j)\ndirectly, we turn off all transistors except for column j, which is left fully on, then\napply a voltage to row i and ground all other rows. The current out of column j is\nread, and we use the relation\nVi \u00bc Ij Rij \u00fe Rw i; j\n\u00f0\n\u00de\nh\ni\n\u00f010\u00de\nwhere Rw\u00f0i; j\u00de is the total wire resistance along the unique conductive path.", "mimetype": "text/plain", "start_char_idx": 8948, "end_char_idx": 9307, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c040cd72-3d48-4732-a5db-4bc57ad52201": {"__data__": {"id_": "c040cd72-3d48-4732-a5db-4bc57ad52201", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fb023f4-473e-4a4d-a360-94103ee37a57", "node_type": "1", "metadata": {}, "hash": "86568bc3cdfcb319099c021eea57eddb52d64e496c8938a2e3f7b42bf4fe7cd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2eba354e-cfac-4c3d-bb1a-86abc71da489", "node_type": "1", "metadata": {}, "hash": "298d61ea2fdda59447ae556442c6a8252e3f55ad7a0b8a826c299badc8c9f73d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For a\nknown wire resistance per segment Rs in an N \u00d7 M array with voltages applied from\nthe left (j = 0) edge and the lower (i = N) edge grounded, we calculate\nRw i; j\n\u00f0\n\u00de \u00bc Rs N \u00fe 2 \u00fe j \ufffdi\n\u00f0\n\u00de\n\u00f011\u00de\nTogether, these equations give\nGij \u00bc\n1\nVi\nIj \ufffdRs N \u00fe 2 \u00fe j \ufffdi\n\u00f0\n\u00de\n\u00f012\u00de\nas the exact conductance of the memristor itself.", "mimetype": "text/plain", "start_char_idx": 9308, "end_char_idx": 9627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2eba354e-cfac-4c3d-bb1a-86abc71da489": {"__data__": {"id_": "2eba354e-cfac-4c3d-bb1a-86abc71da489", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c040cd72-3d48-4732-a5db-4bc57ad52201", "node_type": "1", "metadata": {}, "hash": "44a4e8a9a85aa8714a91349c3cb12bd71ce5523e1defc49f6e9c403d66ec254a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04040bb4-5ce0-40c0-9964-92d424691a82", "node_type": "1", "metadata": {}, "hash": "80d83d5e678a26e659befc17359596ca15d238c7c09999dc18861a524fcb1bb4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "However, because of wire\nresistance and the sneak path problem, this conductance cannot be used directly to\npredict the behavior of the array during vector-matrix multiplication operations.\nAlso, each memristor must be read sequentially, so the time complexity of this\napproach is proportional to NM.\nFor any linear physical system with N inputs and M outputs, there is an\nequivalent linear transformation implemented by the system that can be\nrepresented as an N \u00d7 M matrix. We can use this fact to read the array more\nefficiently.", "mimetype": "text/plain", "start_char_idx": 9628, "end_char_idx": 10160, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04040bb4-5ce0-40c0-9964-92d424691a82": {"__data__": {"id_": "04040bb4-5ce0-40c0-9964-92d424691a82", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2eba354e-cfac-4c3d-bb1a-86abc71da489", "node_type": "1", "metadata": {}, "hash": "298d61ea2fdda59447ae556442c6a8252e3f55ad7a0b8a826c299badc8c9f73d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18c36cc6-f085-4274-8802-77b42302ae4d", "node_type": "1", "metadata": {}, "hash": "0371f839cbd237a96b838644bc16437eacde316d2a49f51cb0dbc77a08ac7b2e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In particular, for any possible network of Ohmic resistors with N voltage\ninputs and M current outputs, there is a matrix Geq such that for any matrix V\ninRN \u00b4 P, I = GeqV. Because the IV relationship in the Ta/HfO2/Pt memristor is\nhighly linear, such a matrix exists for our array, which is the equivalent\nconductance matrix. This matrix is electrically indistinguishable from our physical\narray (to the extent that the components used are linear).", "mimetype": "text/plain", "start_char_idx": 10161, "end_char_idx": 10610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18c36cc6-f085-4274-8802-77b42302ae4d": {"__data__": {"id_": "18c36cc6-f085-4274-8802-77b42302ae4d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "86ba549a6a6e343696ec3e557f954f31a4826ce75320024046b120ecb4fe3349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04040bb4-5ce0-40c0-9964-92d424691a82", "node_type": "1", "metadata": {}, "hash": "80d83d5e678a26e659befc17359596ca15d238c7c09999dc18861a524fcb1bb4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s41467-018-04484-2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We can determine Geq\nempirically by running matrix-matrix multiplication with some large matrix of\ninputs V and using the equation Geq = IV\u22121. In practice, we usually choose V to be\nthe N \u00d7 N identity matrix. The runtime of this calculation is proportional to N.", "mimetype": "text/plain", "start_char_idx": 10611, "end_char_idx": 10873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10.1038/s41467-018-04484-2": {"__data__": {"id_": "10.1038/s41467-018-04484-2", "embedding": null, "metadata": {"node_type": "doi_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"4": {"node_id": "root_node", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "69f46ecb-64f0-4828-be69-6fdea351df8f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0eb49248-6451-4dd4-af95-84debb4e62f2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3ef651ff-766a-4a1b-acc7-1b0fe4358d2a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a7e1eb8d-cc5e-4529-80cd-86880ab1c579", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "61b10b43-1127-46eb-9dac-29bf234c551d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6554170d-6516-4dad-9c9e-c0b3671ec8ad", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "887da0a6-a918-45f8-97f3-6eb363aec90b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4b9365ff-ecad-478d-9922-4dccf7c469f4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4e9dedfd-7b47-4cfd-9ba3-f37e71163773", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9d112cca-1060-43ff-a555-823b666028ce", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "932ded43-76d8-4843-b571-05e6af61a054", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "49c1a25c-8aa6-4512-82e5-94fe17760319", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "74adacaa-4ffd-4d51-8bb1-af16038aa398", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fffd3454-7a13-4317-9cf4-93e6e1aaf4c9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "deddc4ff-b450-4c19-a581-605ba421c0bf", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8264a7a0-17c0-4a36-b3ba-5bdb87ca5fa3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "02ffb4cf-a7cf-4921-9b13-0f04fbd1c7e9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8f757292-10ed-4d25-8c32-eb8eaa7f60e1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "11d58726-a1f4-4efd-be95-4060eb6e8311", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "93432d76-aa5d-44c5-9966-232c066a1e11", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "64fb4016-0551-46ef-9dd6-b376c71311bd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ddb2c5a2-1269-4c29-805e-009d49124ed5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f20b7187-4e07-4ee0-9f99-fa7eb00302a4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "da295ceb-8a43-4690-9fbe-b678aedf5db4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "eebd11f7-da9d-48df-8e8e-35717b84d6d6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "05b7bc04-844d-401c-aed2-1ebf29dcfe55", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "99c461c9-39dc-441d-9e20-5658778103a8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ae4d523e-e7d3-441f-a3f9-eb7c5ce6ced2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1d58ac39-68bc-45a9-9c8b-2a79aa7e726f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "44c32e15-c836-42c4-86f6-7389163cbe01", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "56c304f4-a53e-4ce6-a7cc-eb1ec11e699c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "53159cb3-d55d-4004-88e8-f2cffba985ed", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0d832b5f-a58c-4190-850c-1393f8c82908", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c6ce9951-3366-4b23-80c1-a54994cf97be", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bb99e91d-09d3-4f2c-956e-86b1fc15ea01", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cffb2891-1970-4436-ac95-293ace706841", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "916a3217-0ee1-4469-a9d1-d5470d1aae9c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "22f43c92-187b-41e0-aaf5-9fc7f4e3462b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0b24b883-1e13-4ae6-afe8-e17247b7242c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "dab7f045-53f9-4fc6-aeb0-57e68ab6b8bc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4dc6ef75-3d29-4e86-b3fc-a3a3da0bd193", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6e1ba783-6d7a-4167-bc12-3bf9d0e7c5a6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4b597f4b-6bc5-49d9-b319-9e51efece81a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1c915ba3-d40b-452e-8c5c-cca6b4e42c66", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "44658f6c-6fae-4c9c-a56f-102ff8d88f8a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0e9c3967-ccc0-4554-9aac-c88f870e3147", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "846f23c6-7d45-4eec-bdc0-7d18e1d86468", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "83f1b13d-ca80-4ecb-88f4-feabe9c81073", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "568a6c63-8443-4a31-8170-3b13bee1aab7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2a967a35-c929-4297-934d-81c7294b6836", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "31586887-2b48-43d1-a7f9-fe6b0c57d3c3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "19054bc3-9198-4776-b980-2bc762d0c322", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "356f2f45-993a-4742-b1e2-2fbebeadc6fa", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "75244e7d-095e-4d01-8953-306760bca971", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bf094086-9c4a-48d8-bd6b-11d862fd889c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4e11fc2c-92b6-436f-ae81-ba6c4d60627d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "02587a1c-08eb-41f2-92d3-e5e732fccaef", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9ce1be7a-a33b-41db-a132-2648c1b53f38", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9737dc88-cfee-4ce7-b6cb-534b1d0ae78f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a8b160eb-2ecf-433e-a204-98abb6371899", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "04bff131-cee6-4985-9ec6-33c2658f405a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b7c3291a-1802-4c9d-9c14-9cc5455b9191", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "935fddc8-4640-486f-bd2c-e9ddc05cd514", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f67f804e-a91a-43b1-864b-0573cfbc4566", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9f08e17b-1f33-4ae0-823c-b9455395cba7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6ca4ceb0-c5a7-45fe-b6d9-83e215954b9b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3f32a1a3-cb24-4ce8-b5b8-0971a7b91306", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c2dbc002-4aa9-4a30-9364-b71cee9431bf", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fcba84b0-3be7-4f64-af6f-c137ccfc06d5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "91fac5af-354c-4a14-b045-b6c98a556d31", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d666af53-ea3e-4e9e-bac4-852c89e1ca31", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "52a16434-4e96-4cf7-b124-d6119d5b727a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cea6327b-8740-46af-8a77-385dfda82840", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d8027022-cba1-4178-b36e-abe1f6bbb03c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "84293e49-2a25-46bf-bd91-10048448e84d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a54162ee-38a0-4ce1-a5d0-4beafaa4b1bf", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ce4cb413-aa31-4b76-adf3-108fb53d2098", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5d1df985-c156-40cc-bdf5-7fcf777ae171", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ba48f775-1d63-4def-8b60-fc544042eaa7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2d407c38-2a83-4e3a-8a82-c9424dbf9a5a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a27a4f6f-6d75-40e7-b12e-6f88b75acead", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "df5526b1-3875-4a25-8eb8-10ae7adfc9f5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fde06822-4570-445a-b422-4d9e65cb81fc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4fb023f4-473e-4a4d-a360-94103ee37a57", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c040cd72-3d48-4732-a5db-4bc57ad52201", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2eba354e-cfac-4c3d-bb1a-86abc71da489", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "04040bb4-5ce0-40c0-9964-92d424691a82", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "18c36cc6-f085-4274-8802-77b42302ae4d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}]}, "text": "DOI: 10.1038/s41467-018-04484-2", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "536ef344-8e87-42a9-8b3f-464e6060d2fb": {"__data__": {"id_": "536ef344-8e87-42a9-8b3f-464e6060d2fb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1185fde7-b191-4e05-95e3-574de68e57a5", "node_type": "1", "metadata": {}, "hash": "f5da24fe127931f256a8722678a4a0f75f3ba6bb03c0dcec2ea2269dc1c68dd1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "https:/\n/doi.org/10.1038/s42256-019-0089-1\n1Department of Electrical and Computer Engineering, University of Massachusetts, Amherst, MA, USA. 2Hewlett Packard Labs, Hewlett Packard \nEnterprise, Palo Alto, CA, USA. 3Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, USA. 4Information \nDirectorate, Air Force Research Laboratory, Rome, NY, USA. 5Department of Electrical and Computer Engineering, Binghamton University, Binghamton, \nNY, USA.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 483, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1185fde7-b191-4e05-95e3-574de68e57a5": {"__data__": {"id_": "1185fde7-b191-4e05-95e3-574de68e57a5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "536ef344-8e87-42a9-8b3f-464e6060d2fb", "node_type": "1", "metadata": {}, "hash": "acb09cf834e4bc44c7f365fa62819ffa0a61ef5e60fe6e3cbf3a8a37952ae1d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66e0628e-0300-434a-a4a1-1c934d2d3e75", "node_type": "1", "metadata": {}, "hash": "dc4ee1983513c2cf54b5eea12bc1e50ce951d7257601adf8dc9106d10e9cbea1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "6Institute of Microelectronics, Tsinghua University, Beijing, China. 7Department of Electrical and Computer Engineering, Texas A&M University, \nCollege Station, TX, USA. 8These authors contributed equally: Zhongrui Wang, Can Li, Peng Lin. *e-mail: qxia@umass.edu; jjyang@umass.edu\nW\neight sharing is a common feature of state-of-the-art \nneural networks that considerably reduces the number \nof free parameters needed to perform feature extractions \ncompared with densely connected networks.", "mimetype": "text/plain", "start_char_idx": 484, "end_char_idx": 975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66e0628e-0300-434a-a4a1-1c934d2d3e75": {"__data__": {"id_": "66e0628e-0300-434a-a4a1-1c934d2d3e75", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1185fde7-b191-4e05-95e3-574de68e57a5", "node_type": "1", "metadata": {}, "hash": "f5da24fe127931f256a8722678a4a0f75f3ba6bb03c0dcec2ea2269dc1c68dd1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c98cffa1-6f5f-46c6-a338-04a6ce47bcae", "node_type": "1", "metadata": {}, "hash": "ebb21a5022c3d332dfc5f800db7231dfd020fcd1a305ec8194f746be0e5cbec7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Inspired by the fact that \nthe visual cortex neurons of monkeys individually respond to small \nregions of the visual fields and the neighbouring cells have simi-\nlar partially overlapping receptive fields1, weight sharing between \nlocally connected neurons gives rise to convolutional neural net-\nworks (CNNs) that not only minimize the pre-processing to handle \ntranslation and local distortion of the inputs but also extract and \ncategorize local spatial correlations2. CNNs are now the dominant \narchitecture for analysing visual imagery, reflected in the success of \nVGG3, GoogLeNet4 and ResNet5.", "mimetype": "text/plain", "start_char_idx": 976, "end_char_idx": 1576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c98cffa1-6f5f-46c6-a338-04a6ce47bcae": {"__data__": {"id_": "c98cffa1-6f5f-46c6-a338-04a6ce47bcae", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66e0628e-0300-434a-a4a1-1c934d2d3e75", "node_type": "1", "metadata": {}, "hash": "dc4ee1983513c2cf54b5eea12bc1e50ce951d7257601adf8dc9106d10e9cbea1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "165cb3fc-631f-4a76-b259-23c8e922c438", "node_type": "1", "metadata": {}, "hash": "e695595775f82ecde74c6842f253ede7092473fc763bf7f83d2fde8f926bd376", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Weight sharing can also exploit \nboth the spatial and temporal translation invariance of patterns, lead-\ning to recurrent structures. Convolutional long short-term memory \n(ConvLSTM), which is capable of learning long-term dependence \ndue to its recurrent architecture, uses convolutional kernels to deter-\nmine both the input-to-state and state-to-state transitions, exhibit-\ning the capability of intrinsic spatio-temporal feature extraction6,7. \nConvLSTM provides natural end-to-end trainable building blocks \nfor three-dimensional inputs (such as videos) with applications \nranging from precipitation nowcasting6 to video encoding8.", "mimetype": "text/plain", "start_char_idx": 1577, "end_char_idx": 2213, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "165cb3fc-631f-4a76-b259-23c8e922c438": {"__data__": {"id_": "165cb3fc-631f-4a76-b259-23c8e922c438", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c98cffa1-6f5f-46c6-a338-04a6ce47bcae", "node_type": "1", "metadata": {}, "hash": "ebb21a5022c3d332dfc5f800db7231dfd020fcd1a305ec8194f746be0e5cbec7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4df2cc98-fc6b-478e-ad27-c14cbc113043", "node_type": "1", "metadata": {}, "hash": "b153f4b929f151d5aa76ebea2a46ca00f611adb61c6bb1b81dbdcd5d5bb55f69", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Most implementations of shared-weight structures rely on \ngraphics processing units. However, when implemented in such \nconventional digital hardware, these weight-shared networks suffer \nfrom large inference latency and high power consumption. These \nissues may become less affordable if the computing is performed at \nthe edge in the era of the Internet of Things. Application-specific \n \nintegrated circuits with optimized multiply-accumulate (MAC) \nunits, such as the tensor processing unit9, Eyeriss10 and DaDianNao11, \ncould potentially boost the area/energy efficiency.", "mimetype": "text/plain", "start_char_idx": 2214, "end_char_idx": 2790, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4df2cc98-fc6b-478e-ad27-c14cbc113043": {"__data__": {"id_": "4df2cc98-fc6b-478e-ad27-c14cbc113043", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "165cb3fc-631f-4a76-b259-23c8e922c438", "node_type": "1", "metadata": {}, "hash": "e695595775f82ecde74c6842f253ede7092473fc763bf7f83d2fde8f926bd376", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92dc3828-21d4-4749-b6f0-f58d4d16ebf6", "node_type": "1", "metadata": {}, "hash": "b3386b74c72a380183f489f58b44008f68d10de6b2e7f70a351d89624d313807", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "However, the \nvon Neumann bottleneck of such architectures and the increasingly \ncost-ineffective scaling of the transistor limit the ultimate efficiency \nof those approaches12\u201315. Therefore, fundamental changes to the \ncomputing platform and its building blocks are critical to meet the \never-growing demand for computing power.\nMemristors are emerging two-terminal electronic devices that \nshow analogue conductance, fast switching, superior scalability, long \nretention and long endurance13\u201322. In crossbars, memristor arrays \nnaturally parallelize MAC operations where the data are stored23,24.", "mimetype": "text/plain", "start_char_idx": 2791, "end_char_idx": 3389, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92dc3828-21d4-4749-b6f0-f58d4d16ebf6": {"__data__": {"id_": "92dc3828-21d4-4749-b6f0-f58d4d16ebf6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4df2cc98-fc6b-478e-ad27-c14cbc113043", "node_type": "1", "metadata": {}, "hash": "b153f4b929f151d5aa76ebea2a46ca00f611adb61c6bb1b81dbdcd5d5bb55f69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f607fd6-7d9b-4ec8-9958-9eb7e53ac6c4", "node_type": "1", "metadata": {}, "hash": "48f9180edd7058f12626ff14667a98a1514c0c8e0b1252400c583456fe4dba6e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Such analogue in-memory computing directly uses intrinsic physics \n \nlaws to obviate the large energy and time overheads incurred by \nfrequent data shuttling in von Neumann systems25\u201331. This allows \nmemristor crossbar arrays to physically embody the fully connected \nlayers of the networks21,24,32\u201340 with a substantially improved area/\nenergy efficiency. However, the practical computing systems are \n \nalways bounded in terms of computing power and memory. In \naddition, the typical datasets like images, videos and audios are of \nspatial or temporal correlations.", "mimetype": "text/plain", "start_char_idx": 3391, "end_char_idx": 3958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f607fd6-7d9b-4ec8-9958-9eb7e53ac6c4": {"__data__": {"id_": "8f607fd6-7d9b-4ec8-9958-9eb7e53ac6c4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92dc3828-21d4-4749-b6f0-f58d4d16ebf6", "node_type": "1", "metadata": {}, "hash": "b3386b74c72a380183f489f58b44008f68d10de6b2e7f70a351d89624d313807", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf706117-2f30-4d1b-8e70-4c70d699cb36", "node_type": "1", "metadata": {}, "hash": "fe627e05c437ffeaa6bd82fec3ce657f2698324e843c5388e5b4656d6222c0bb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Given the same amount of hard-\nware (memory) or alternatively the number of trainable parameters \n(degrees of freedom) of the system, multilayer perceptrons usually \nshow poor functionalities (classification accuracy, for example) \ncompared with the weight-shared networks, which limits their \npractical applications.\nAs memristors are most efficiently assembled in crossbars, which \ndiffer from the microstructures of the weight-shared artificial neural \nIn situ training of feed-forward and recurrent \nconvolutional memristor networks\nZhongrui Wang1,8, Can Li  1,2,8,", "mimetype": "text/plain", "start_char_idx": 3959, "end_char_idx": 4528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf706117-2f30-4d1b-8e70-4c70d699cb36": {"__data__": {"id_": "cf706117-2f30-4d1b-8e70-4c70d699cb36", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f607fd6-7d9b-4ec8-9958-9eb7e53ac6c4", "node_type": "1", "metadata": {}, "hash": "48f9180edd7058f12626ff14667a98a1514c0c8e0b1252400c583456fe4dba6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60295af4-15ab-4289-8fcd-15c3df4438c5", "node_type": "1", "metadata": {}, "hash": "3901444dfae8b3ac7fc355b6a88d47e4ef1b2f88d0e68c42b1a7b3dc26332029", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Peng Lin1,8, Mingyi Rao1, Yongyang Nie1, Wenhao Song1, Qinru Qiu3, \nYunning Li1, Peng Yan  1, John Paul Strachan  2, Ning Ge2, Nathan McDonald4, Qing Wu4, Miao Hu5, \nHuaqiang Wu  6, R. Stanley Williams  7, Qiangfei Xia  1* and J. Joshua Yang  1*\nThe explosive growth of machine learning is largely due to the recent advancements in hardware and architecture.", "mimetype": "text/plain", "start_char_idx": 4529, "end_char_idx": 4887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60295af4-15ab-4289-8fcd-15c3df4438c5": {"__data__": {"id_": "60295af4-15ab-4289-8fcd-15c3df4438c5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf706117-2f30-4d1b-8e70-4c70d699cb36", "node_type": "1", "metadata": {}, "hash": "fe627e05c437ffeaa6bd82fec3ce657f2698324e843c5388e5b4656d6222c0bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cde84fb6-6a66-474d-b152-d7492a2b559b", "node_type": "1", "metadata": {}, "hash": "2f8f3eaac221c420b6fe85d100a2e4be0230022b1efefba4468d95fafe0f0941", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The engi-\nneering of network structures, taking advantage of the spatial or temporal translational isometry of patterns, naturally leads \nto bio-inspired, shared-weight structures such as convolutional neural networks, which have markedly reduced the number of \nfree parameters. State-of-the-art microarchitectures commonly rely on weight-sharing techniques, but still suffer from the \nvon Neumann bottleneck of transistor-based platforms.", "mimetype": "text/plain", "start_char_idx": 4888, "end_char_idx": 5327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cde84fb6-6a66-474d-b152-d7492a2b559b": {"__data__": {"id_": "cde84fb6-6a66-474d-b152-d7492a2b559b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60295af4-15ab-4289-8fcd-15c3df4438c5", "node_type": "1", "metadata": {}, "hash": "3901444dfae8b3ac7fc355b6a88d47e4ef1b2f88d0e68c42b1a7b3dc26332029", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0c9e927-0412-4f5c-88d7-602dd668ac86", "node_type": "1", "metadata": {}, "hash": "fa360c8b035c3e9c72127be4599e99e600150577f87af798965cc794338d3441", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Here, we experimentally demonstrate the in situ training of a five-level \nconvolutional neural network that self-adapts to non-idealities of the one-transistor one-memristor array to classify the MNIST \ndataset, achieving similar accuracy to the memristor-based multilayer perceptron with a reduction in trainable parameters of \n~75% owing to the shared weights.", "mimetype": "text/plain", "start_char_idx": 5328, "end_char_idx": 5690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0c9e927-0412-4f5c-88d7-602dd668ac86": {"__data__": {"id_": "e0c9e927-0412-4f5c-88d7-602dd668ac86", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cde84fb6-6a66-474d-b152-d7492a2b559b", "node_type": "1", "metadata": {}, "hash": "2f8f3eaac221c420b6fe85d100a2e4be0230022b1efefba4468d95fafe0f0941", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c920e1de-67e5-4586-92f4-303473cc251b", "node_type": "1", "metadata": {}, "hash": "799b35d8b35c59b5a91586d5e077ef80655fc776ff7dd836701a2060495bb71e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In addition, the memristors encoded both spatial and temporal translational invariance \nsimultaneously in a convolutional long short-term memory network\u2014a memristor-based neural network with intrinsic 3D input \nprocessing\u2014which was trained in situ to classify a synthetic MNIST sequence dataset using just 850 weights. These proof-of-\nprinciple demonstrations combine the architectural advantages of weight sharing and the area/energy efficiency boost of the \nmemristors, paving the way to future edge artificial intelligence.", "mimetype": "text/plain", "start_char_idx": 5691, "end_char_idx": 6217, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c920e1de-67e5-4586-92f4-303473cc251b": {"__data__": {"id_": "c920e1de-67e5-4586-92f4-303473cc251b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0c9e927-0412-4f5c-88d7-602dd668ac86", "node_type": "1", "metadata": {}, "hash": "fa360c8b035c3e9c72127be4599e99e600150577f87af798965cc794338d3441", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b64ffab-b826-4bd6-b7eb-c7d703fb0b54", "node_type": "1", "metadata": {}, "hash": "068a0fefe547ff27ba1f830af64ac9babd79f88de258af373a007dab2b8f65db", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Nature Machine Intelligence | VOL 1 | SEPTEMBER 2019 | 434\u2013442 | www.nature.com/natmachintell\n434\nArticles\nNature Machine Intelligence\nnetworks, it is necessary to remap the high-dimensional trainable \nparameters of a weight-shared network efficiently to a 2D memris-\ntor array. In addition, such mapping should accommodate more \nstringent requirements for the accuracy of weight representation \nbecause the weight-shared architectures are much more susceptible \nto hardware non-idealities compared with fully connected networks41.", "mimetype": "text/plain", "start_char_idx": 6218, "end_char_idx": 6749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b64ffab-b826-4bd6-b7eb-c7d703fb0b54": {"__data__": {"id_": "7b64ffab-b826-4bd6-b7eb-c7d703fb0b54", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c920e1de-67e5-4586-92f4-303473cc251b", "node_type": "1", "metadata": {}, "hash": "799b35d8b35c59b5a91586d5e077ef80655fc776ff7dd836701a2060495bb71e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2dd7cdf-1e30-4fb6-aa1c-37031f2c133b", "node_type": "1", "metadata": {}, "hash": "a2c15b5b46b0f73dd0b536aef624bf9ee4230c378950d0e549693e0f536d26b3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Contemporary memristors may represent synaptic weights inaccu-\nrately with conductance because of their stochastic behaviour, which \nare associated with ionic transport and can be improved through iter-\native corrections at the expense of time and energy. This makes the \ntraining of weight-shared architectures a demanding task. Memristors \nhave thus only been exploited for fully connected networks21,24,29,32,34\u201338, \nwith temporal weight sharing just recently demonstrated in a recur-\nrent structure42.", "mimetype": "text/plain", "start_char_idx": 6751, "end_char_idx": 7256, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2dd7cdf-1e30-4fb6-aa1c-37031f2c133b": {"__data__": {"id_": "d2dd7cdf-1e30-4fb6-aa1c-37031f2c133b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b64ffab-b826-4bd6-b7eb-c7d703fb0b54", "node_type": "1", "metadata": {}, "hash": "068a0fefe547ff27ba1f830af64ac9babd79f88de258af373a007dab2b8f65db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc73a7d3-1d11-473a-a9c6-ce0c57836a45", "node_type": "1", "metadata": {}, "hash": "f8596e24d773113bd2fe86c318c4939ea09221ff4a219daa9b5a5817f7930412", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Theoretical investigations of how the convolutional \nlayers can be mapped to memristor crossbars41 and memristor CNNs \nof binary weights43 have been conducted, with the former experi-\nmentally verified on HfOx memristors25,44. Recently, MAC operations \nfor convolution with 2-bit inputs and 3-bit weights have been dem-\nonstrated on megabit binary state resistive switch macros with both \n65 nm (ref. \n38) and 55 nm (ref. \n39) CMOS logic process.", "mimetype": "text/plain", "start_char_idx": 7257, "end_char_idx": 7703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc73a7d3-1d11-473a-a9c6-ce0c57836a45": {"__data__": {"id_": "bc73a7d3-1d11-473a-a9c6-ce0c57836a45", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2dd7cdf-1e30-4fb6-aa1c-37031f2c133b", "node_type": "1", "metadata": {}, "hash": "a2c15b5b46b0f73dd0b536aef624bf9ee4230c378950d0e549693e0f536d26b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a08b3032-0ea0-4c33-9e14-cf48b649224d", "node_type": "1", "metadata": {}, "hash": "6ebebe561bc8ee44d43a61061969b49e8b5db677bce105b79da13df8cdd24671", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "However, in situ \ntraining of memristor-based spatial shared-weight architectures (such \nas CNNs) are lacking, as is the case for concurrent spatio-temporal \nweight sharing (ConvLSTM, for example) on memristor arrays.", "mimetype": "text/plain", "start_char_idx": 7704, "end_char_idx": 7921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a08b3032-0ea0-4c33-9e14-cf48b649224d": {"__data__": {"id_": "a08b3032-0ea0-4c33-9e14-cf48b649224d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc73a7d3-1d11-473a-a9c6-ce0c57836a45", "node_type": "1", "metadata": {}, "hash": "f8596e24d773113bd2fe86c318c4939ea09221ff4a219daa9b5a5817f7930412", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76540f75-6473-490d-9e0a-24bb98bd3c9f", "node_type": "1", "metadata": {}, "hash": "85fb001cca35b408a7c6ab04f2c018024fb824cdf17540a961987915e739e4ac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In situ training stores and updates the weights directly in mem-\nristors, and performs computations (for example, forward passes) at \nthe original place where the neural network parameters are stored; \nthis avoids the need to implement a duplicated system in digital \ncomputers, such as ex situ training, which substantially enhances \nthe area/energy efficiency of the system by eliminating the proces-\nsor-memory bottleneck of digital computers.", "mimetype": "text/plain", "start_char_idx": 7922, "end_char_idx": 8368, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76540f75-6473-490d-9e0a-24bb98bd3c9f": {"__data__": {"id_": "76540f75-6473-490d-9e0a-24bb98bd3c9f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a08b3032-0ea0-4c33-9e14-cf48b649224d", "node_type": "1", "metadata": {}, "hash": "6ebebe561bc8ee44d43a61061969b49e8b5db677bce105b79da13df8cdd24671", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3286bb37-a2dd-405b-a19b-eb4aa9fe19d2", "node_type": "1", "metadata": {}, "hash": "48bc163a2622278c4786497f62503caff333aee1f2b18f2df2df45677b4ab92b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "More importantly, \nin situ training with backpropagation is capable of self-adaptively \nadjusting the network parameters to minimize the impacts of the \ninevitable non-idealities of the hardware (such as wire resistance, \nanalogue peripheral asymmetry, non-responsive memristors, con-\nductance drift and variations in the conductance programming) \nwithout any prior knowledge of the hardware45, which is critical to \nshared-weight networks.", "mimetype": "text/plain", "start_char_idx": 8369, "end_char_idx": 8809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3286bb37-a2dd-405b-a19b-eb4aa9fe19d2": {"__data__": {"id_": "3286bb37-a2dd-405b-a19b-eb4aa9fe19d2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76540f75-6473-490d-9e0a-24bb98bd3c9f", "node_type": "1", "metadata": {}, "hash": "85fb001cca35b408a7c6ab04f2c018024fb824cdf17540a961987915e739e4ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "504b8543-f5e3-4956-b5b4-054aea818f56", "node_type": "1", "metadata": {}, "hash": "47ac631d107bc900184d7f22f42c872a150a9271a854a7875c1425b93c2f2d0e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Here we show that in situ training of shared-weight neural net-\nworks tolerates the hardware non-idealities of the one-transistor \none-memristor (1T1R) array using a simple dense mapping of con-\nvolution kernels to the memristor crossbar. We achieved 92.13% \naccuracy in classifying MNIST handwritten digits using just ~1,000 \nweights\u2014that is, only one-quarter of the number of a memristor \nmultilayer perceptron with similar performance.", "mimetype": "text/plain", "start_char_idx": 8810, "end_char_idx": 9248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "504b8543-f5e3-4956-b5b4-054aea818f56": {"__data__": {"id_": "504b8543-f5e3-4956-b5b4-054aea818f56", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3286bb37-a2dd-405b-a19b-eb4aa9fe19d2", "node_type": "1", "metadata": {}, "hash": "48bc163a2622278c4786497f62503caff333aee1f2b18f2df2df45677b4ab92b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc5cfc83-cc62-464d-96a9-8c0a0cd297b7", "node_type": "1", "metadata": {}, "hash": "c5a8cc7f567e174d6b23cef2f39273375332a4a791695678da5663cbe5d21fac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In addition, we \ndemonstrate that the advantages of the weight sharing can be fur-\nther extended by cascading the convolutional kernels in a recurrent \nConvLSTM network with intrinsic 3D inputs, which identified the \nspatial and temporal correlations in both input-to-state and state-\nto-state transitions. The experimentally demonstrated reduction of \ntrainable parameters to 850 by spatio-temporal memristor weight \nsharing represents a promising approach for using memristor-based \nhardware to efficiently implement advanced network topologies for \nedge applications.\nResults\n1T1R CNN with spatial weight sharing.", "mimetype": "text/plain", "start_char_idx": 9249, "end_char_idx": 9865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc5cfc83-cc62-464d-96a9-8c0a0cd297b7": {"__data__": {"id_": "fc5cfc83-cc62-464d-96a9-8c0a0cd297b7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "504b8543-f5e3-4956-b5b4-054aea818f56", "node_type": "1", "metadata": {}, "hash": "47ac631d107bc900184d7f22f42c872a150a9271a854a7875c1425b93c2f2d0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69ec8af6-47b1-4544-893c-f5b3366afb8e", "node_type": "1", "metadata": {}, "hash": "48ab31bfd3692f6a327378757974f315f053a58c12d34c8891543127b907fa9a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Figure 1a shows an optical \nmicrograph of the 1T1R memristor array, which comprises Pt/TaOx/\nTa memristors with analogue programming capability, long data reten-\ntion and large endurance due to the chemically stable Ta\u2013O phases46, \nand its partition that physically implements all trainable parameters \nof the neural network. The trainable parameters, or weights of dif-\nferent layers, are interfaced with upstream and downstream neurons \nthat are implemented by conventional transistor circuits off-chip \n \n(as depicted in Supplementary Fig. 1).", "mimetype": "text/plain", "start_char_idx": 9866, "end_char_idx": 10412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69ec8af6-47b1-4544-893c-f5b3366afb8e": {"__data__": {"id_": "69ec8af6-47b1-4544-893c-f5b3366afb8e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc5cfc83-cc62-464d-96a9-8c0a0cd297b7", "node_type": "1", "metadata": {}, "hash": "c5a8cc7f567e174d6b23cef2f39273375332a4a791695678da5663cbe5d21fac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9131e166-368c-4b73-b994-da90529f11b9", "node_type": "1", "metadata": {}, "hash": "7c3a63c54288bbca0e6ec131b67e90a2031243de2e82a7bb9ec5bdcdb7926ad5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In contrast to employing dedi-\ncated 1T1R arrays for different neural network layers, the partition-\ning of a single large array essentially shares the pre-synaptic neurons \nbetween different layers of the same network, maximizing the utility \nof the peripheral circuits and benefitting edge applications.\nThe zoomed scanning electron micrograph in Fig. 1 shows that \n1T1R cells of the same column share common sources of the tran-\nsistors, connected by the same bit lines.", "mimetype": "text/plain", "start_char_idx": 10413, "end_char_idx": 10886, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9131e166-368c-4b73-b994-da90529f11b9": {"__data__": {"id_": "9131e166-368c-4b73-b994-da90529f11b9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69ec8af6-47b1-4544-893c-f5b3366afb8e", "node_type": "1", "metadata": {}, "hash": "48ab31bfd3692f6a327378757974f315f053a58c12d34c8891543127b907fa9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47d3df59-f137-42b0-af4c-cb0db935322e", "node_type": "1", "metadata": {}, "hash": "09ddbee02b0a3904221ca3bfe1c8e4d5ae1b7dabeff271606cc3c3621c7f97ba", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Cells of the same row share \ncommon gates of the transistors, wired on the same word lines, and \ncommon top electrodes (TE) of the memristors. During the infer-\nence stage, all transistors were operated in the deep triode region \nso the 1T1R array became a pseudo-1R-crossbar capable of per-\nforming vector\u2013matrix multiplications directly using Ohm\u2019s law \nand Kirchhoff\u2019s current law, negating the need for transferring data \nback and forth between memories and processors.", "mimetype": "text/plain", "start_char_idx": 10887, "end_char_idx": 11360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47d3df59-f137-42b0-af4c-cb0db935322e": {"__data__": {"id_": "47d3df59-f137-42b0-af4c-cb0db935322e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9131e166-368c-4b73-b994-da90529f11b9", "node_type": "1", "metadata": {}, "hash": "7c3a63c54288bbca0e6ec131b67e90a2031243de2e82a7bb9ec5bdcdb7926ad5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1e90641-491e-4fb6-b147-00fe6a52a306", "node_type": "1", "metadata": {}, "hash": "3b3b12f2e8f13ae347f46c838ab6c573bc01a6b7c78f8db1b893b6e3f0fc6703", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In the weight-\nupdating phase, the transistors imposed current compliance, which \nprogrammed the memristors at an acceptable accuracy with a sin-\ngle-shot blind-update that is faster than applying a train of identical \npulses. (see Fig. 1b and Supplementary Fig. 2)\nFigure 1c illustrates the computation flow of the CNN during a \nforward pass. The network was a modified five-level CNN derived \nfrom the pioneering LeNet-5 with a reduced weight population \nto fit the 128 \u00d7 64 1T1R array2.", "mimetype": "text/plain", "start_char_idx": 11361, "end_char_idx": 11850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1e90641-491e-4fb6-b147-00fe6a52a306": {"__data__": {"id_": "f1e90641-491e-4fb6-b147-00fe6a52a306", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47d3df59-f137-42b0-af4c-cb0db935322e", "node_type": "1", "metadata": {}, "hash": "09ddbee02b0a3904221ca3bfe1c8e4d5ae1b7dabeff271606cc3c3621c7f97ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7db0e85-3af6-4128-9788-488b573a682c", "node_type": "1", "metadata": {}, "hash": "ed136644979f4591bb47141b612d368e79136f8bec810c45c40bc2eefd8d2856", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As our primary goal is to show the \nadvantage of weight sharing, this simple CNN features a relatively \nlarge ratio between the accuracy and the number of weights (see \nSupplementary Note 1 for the experimental comparison with another \nLeNet-5-like network). The example input to the network was a \nresized MNIST handwritten digit image with 8 \u00d7 8 pixels and 8-bit \nresolution (see the Methods for the generation of the resized MNIST \ndataset, and Supplementary Note 2 for the downsampling impact on \nthe network performance).", "mimetype": "text/plain", "start_char_idx": 11851, "end_char_idx": 12377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7db0e85-3af6-4128-9788-488b573a682c": {"__data__": {"id_": "c7db0e85-3af6-4128-9788-488b573a682c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1e90641-491e-4fb6-b147-00fe6a52a306", "node_type": "1", "metadata": {}, "hash": "3b3b12f2e8f13ae347f46c838ab6c573bc01a6b7c78f8db1b893b6e3f0fc6703", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54da2ae5-79b9-4447-ab32-d643dfe0f5ae", "node_type": "1", "metadata": {}, "hash": "26a382ad7b94d207eec986e3e31cee857f90ef2f5cd9ffbd23b26d527526f2f6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Here we collapsed both the input patch \nfor convolution (highlighted in the purple box) and the parameters \nof a single kernel to column vectors. The different unrolled kernels of \nthe same layer were placed horizontally, forming a parameter matrix \nthat stored all the trainable weights of a single convolutional layer.", "mimetype": "text/plain", "start_char_idx": 12378, "end_char_idx": 12698, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54da2ae5-79b9-4447-ab32-d643dfe0f5ae": {"__data__": {"id_": "54da2ae5-79b9-4447-ab32-d643dfe0f5ae", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7db0e85-3af6-4128-9788-488b573a682c", "node_type": "1", "metadata": {}, "hash": "ed136644979f4591bb47141b612d368e79136f8bec810c45c40bc2eefd8d2856", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c7f6748-59ee-43c6-9929-40cdfe9fb37f", "node_type": "1", "metadata": {}, "hash": "7843db22e6d5fb00406da355be5e419afacdc0c2e98a4f490fc442486412c95f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Such a dense mapping scheme, in contrast to the Toeplitz matrix \nor sparse mapping scheme, performs convolution by scanning the \nreceptive field over the entire input volume, leading to multiple MAC \noperations that yield the full volume of the output map, featuring \na large MAC-operation/weight-update ratio24 (the weight updates \ncannot be fully parallelized on the memristor array, and thus should \nbe performed as infrequently as possible). Alternatively, sparse map-\nping using duplicate kernels with a step shift could further parallelize \nthe convolution over different input patches.", "mimetype": "text/plain", "start_char_idx": 12700, "end_char_idx": 13292, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c7f6748-59ee-43c6-9929-40cdfe9fb37f": {"__data__": {"id_": "8c7f6748-59ee-43c6-9929-40cdfe9fb37f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54da2ae5-79b9-4447-ab32-d643dfe0f5ae", "node_type": "1", "metadata": {}, "hash": "26a382ad7b94d207eec986e3e31cee857f90ef2f5cd9ffbd23b26d527526f2f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3310836-8b30-40e7-8195-2e8c47ec8037", "node_type": "1", "metadata": {}, "hash": "be58344f4d7102ca9cec900f6c58448cde3ccc08dcc0e612cb3dfbad1a90f80b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "However, the effective \nthroughput per unit area drops rapidly as the data/kernel ratio scales \nup, in addition to the challenge of duplicating identical kernels and \nhaving ideal 0s in the sparse matrix (due to the intrinsic variability \nof memristors) and increased memristor programming energy24. We \ntherefore chose the dense mapping scheme over sparse mapping.\nThe input was first physically convolved with the 15 3 \u00d7 3 kernels \ncharacterized by zero padding and unitary stride on the memristor \narray.", "mimetype": "text/plain", "start_char_idx": 13293, "end_char_idx": 13800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3310836-8b30-40e7-8195-2e8c47ec8037": {"__data__": {"id_": "e3310836-8b30-40e7-8195-2e8c47ec8037", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c7f6748-59ee-43c6-9929-40cdfe9fb37f", "node_type": "1", "metadata": {}, "hash": "7843db22e6d5fb00406da355be5e419afacdc0c2e98a4f490fc442486412c95f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddff0ee4-5311-4775-9ea0-6138d6a11005", "node_type": "1", "metadata": {}, "hash": "c1b167a6697cc5ddb977956fcb0a7a12cefb8a33ccd71c57efdb94b5dc83a386", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The signed weight was encoded into the conductance differ-\nence between one pair or multiple pairs (here we used two pairs to \nfurther improve the programming accuracy37) of memristors on the \nsame row. The analogue convolution outputs, digitized by 16-bit ana-\nlogue-to-digital converters (ADCs), were activated by rectified linear \nunits (ReLU) in software element-wise before being sent to the next \nlayer, producing 15 feature maps of size 8 \u00d7 8 (see Methods).", "mimetype": "text/plain", "start_char_idx": 13801, "end_char_idx": 14265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddff0ee4-5311-4775-9ea0-6138d6a11005": {"__data__": {"id_": "ddff0ee4-5311-4775-9ea0-6138d6a11005", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3310836-8b30-40e7-8195-2e8c47ec8037", "node_type": "1", "metadata": {}, "hash": "be58344f4d7102ca9cec900f6c58448cde3ccc08dcc0e612cb3dfbad1a90f80b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4f96757-a6b6-410a-80c8-35247ddf2999", "node_type": "1", "metadata": {}, "hash": "a085ae8171b9a83eae402ccf9615f8b7ecbc3dcc88c722ed3f5e0bc59dbd7be7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "These \nfeature maps were again convolved by four 2 \u00d7 2 kernels followed by a \ndownsampling layer that implemented the max pooling function over \nnon-overlapping pooling windows of size 2 \u00d7 2. Finally, the feature \nmaps were flattened as the inputs into a 10-way softmax output layer.", "mimetype": "text/plain", "start_char_idx": 14266, "end_char_idx": 14549, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4f96757-a6b6-410a-80c8-35247ddf2999": {"__data__": {"id_": "c4f96757-a6b6-410a-80c8-35247ddf2999", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddff0ee4-5311-4775-9ea0-6138d6a11005", "node_type": "1", "metadata": {}, "hash": "c1b167a6697cc5ddb977956fcb0a7a12cefb8a33ccd71c57efdb94b5dc83a386", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "259bf45e-5516-4e01-b7dc-551b72742212", "node_type": "1", "metadata": {}, "hash": "d906da1ae41b502c677348ce774e012b35012c4a78b45397fea2e30b90b57bfd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The computational complexity for a convolution stride is usually \nO kdim1 \u00b4 kdim2 \u00b4 kdim3 \u00b4 knum\n\u00f0\n\u00de\nI\n where kdim1, kdim2, kdim3 and knum are the \nheight, width, depth and population of the kernels, respectively. On \nthe other hand, the complexity becomes O(1) once the weights are \nphysically mapped to the 1T1R array because all MAC operations \nwithin the same kernel and across all kernels of the entire layer are \nperformed in a single time step.", "mimetype": "text/plain", "start_char_idx": 14550, "end_char_idx": 15001, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "259bf45e-5516-4e01-b7dc-551b72742212": {"__data__": {"id_": "259bf45e-5516-4e01-b7dc-551b72742212", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4f96757-a6b6-410a-80c8-35247ddf2999", "node_type": "1", "metadata": {}, "hash": "a085ae8171b9a83eae402ccf9615f8b7ecbc3dcc88c722ed3f5e0bc59dbd7be7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e856bd5e-e4b6-4762-b2f8-ec13b0c91a12", "node_type": "1", "metadata": {}, "hash": "16c8c5cfe7fa89146240b435c5bafd4e531c00bee983fe3f7adaaccc430f3e58", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Alternatively, the first (second) con-\nNature Machine Intelligence | VOL 1 | SEPTEMBER 2019 | 434\u2013442 | www.nature.com/natmachintell\n435\nArticles\nNature Machine Intelligence\nFig. 1 | 1T1R implementation of the 5-level convolutional neural network (CNN). a, Optical and scanning electron micrographs of the 128 \u00d7 64 1T1R array \nwith probe card (black tips on the four edges) landed.", "mimetype": "text/plain", "start_char_idx": 15002, "end_char_idx": 15383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e856bd5e-e4b6-4762-b2f8-ec13b0c91a12": {"__data__": {"id_": "e856bd5e-e4b6-4762-b2f8-ec13b0c91a12", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "259bf45e-5516-4e01-b7dc-551b72742212", "node_type": "1", "metadata": {}, "hash": "d906da1ae41b502c677348ce774e012b35012c4a78b45397fea2e30b90b57bfd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f18737d2-9be6-430c-9e16-d76a6eb0c46d", "node_type": "1", "metadata": {}, "hash": "095722d076db328b5fb8bc42cc422fd844d2b6343318080f35e8739564d52e5e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The colour blocks illustrate the partitions of the 1T1R array to implement the trainable parameters of \nthe two convolutional layers (15 3 \u00d7 3 kernels of the first convolutional layer and 4 2 \u00d7 2 kernels of the second convolutional layer) and the fully connected \nlayer (weight matrix size 64 \u00d7 10). The differential conductance pairs were formed between adjacent bit lines (for example, gi,j\n+ and gi,j\n\u2013 where i and j are \nintegers). Each weight is represented by two differential pairs here.", "mimetype": "text/plain", "start_char_idx": 15384, "end_char_idx": 15878, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f18737d2-9be6-430c-9e16-d76a6eb0c46d": {"__data__": {"id_": "f18737d2-9be6-430c-9e16-d76a6eb0c46d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e856bd5e-e4b6-4762-b2f8-ec13b0c91a12", "node_type": "1", "metadata": {}, "hash": "16c8c5cfe7fa89146240b435c5bafd4e531c00bee983fe3f7adaaccc430f3e58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c71e47e0-f3fd-4832-81ad-006424afb3cf", "node_type": "1", "metadata": {}, "hash": "7ffbd49b5e68c0045547b95bdc030125958a1d8c215e72ffaf4361b50388a0e7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The scanning electron micrograph shows a zoomed 1T1R cell (right). Scale bar, 20 \u03bcm. \nb, Analogue SET and RESET for all 128 \u00d7 64 1T1R cells with linearly varying common gate voltages for all compliance transistors.", "mimetype": "text/plain", "start_char_idx": 15879, "end_char_idx": 16093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c71e47e0-f3fd-4832-81ad-006424afb3cf": {"__data__": {"id_": "c71e47e0-f3fd-4832-81ad-006424afb3cf", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f18737d2-9be6-430c-9e16-d76a6eb0c46d", "node_type": "1", "metadata": {}, "hash": "095722d076db328b5fb8bc42cc422fd844d2b6343318080f35e8739564d52e5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be01b8ee-0b58-43f9-a7c5-12f10dbb9280", "node_type": "1", "metadata": {}, "hash": "9467321eb94bbd300eca0af1bfc96c667a63e46cf13d3b92a76af19a58948fd5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "All 1T1R cells receive 15 \nconsecutive SET operations with the gate voltage linearly increasing from 1.0 V to 2.4 V, followed by 15 RESET operations with the gate voltage linearly \ndecreasing from 2.4 V to 1.0 V (for each RESET operation, a full RESET is applied before the 1T1R cell is SET again with current compliance). c, The network \nstructure of the five-level CNN for classifying MNIST handwritten digits.", "mimetype": "text/plain", "start_char_idx": 16094, "end_char_idx": 16506, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be01b8ee-0b58-43f9-a7c5-12f10dbb9280": {"__data__": {"id_": "be01b8ee-0b58-43f9-a7c5-12f10dbb9280", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c71e47e0-f3fd-4832-81ad-006424afb3cf", "node_type": "1", "metadata": {}, "hash": "7ffbd49b5e68c0045547b95bdc030125958a1d8c215e72ffaf4361b50388a0e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3be32cca-1f81-4ead-be6a-81e55c832377", "node_type": "1", "metadata": {}, "hash": "5688905db9e854cc49bc1510fb4c17a4045057b851bc56e2784250ddd18ef91b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The 8-bit grayscale input of size 8 \u00d7 8 was convolved by sliding the convolution \nregion in the purple box. The selected convolution region was unrolled (for example, column vector elements V1,1, V2,1, V3,1, V1,2, \u2026, V3,3) and fed to the \ncrossbar network of differential pairs as voltage signals (two duplicated differential pairs were used to represent a single weight in the experiment).", "mimetype": "text/plain", "start_char_idx": 16507, "end_char_idx": 16897, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3be32cca-1f81-4ead-be6a-81e55c832377": {"__data__": {"id_": "3be32cca-1f81-4ead-be6a-81e55c832377", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be01b8ee-0b58-43f9-a7c5-12f10dbb9280", "node_type": "1", "metadata": {}, "hash": "9467321eb94bbd300eca0af1bfc96c667a63e46cf13d3b92a76af19a58948fd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0b76408-e9a5-4c25-83d9-4e0b7ebf70e9", "node_type": "1", "metadata": {}, "hash": "e4ec8feddea6267dbad4e1955bb4222ad2856162a74bbee068af755530cb7746", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The \nsummed output currents I(kernel 1), I(kernel 2),\u2026, I(kernel 15) were collected for all kernels. The input image was first physically convolved by the 15 \nmemristor kernels of size 3 \u00d7 3 with zero padding and unitary stride. The analogue convolving outputs of the first convolutional layer were activated by \nthe ReLU element-wise before being sent to the next layer, producing 15 feature maps of size 8 \u00d7 8.", "mimetype": "text/plain", "start_char_idx": 16898, "end_char_idx": 17310, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0b76408-e9a5-4c25-83d9-4e0b7ebf70e9": {"__data__": {"id_": "c0b76408-e9a5-4c25-83d9-4e0b7ebf70e9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3be32cca-1f81-4ead-be6a-81e55c832377", "node_type": "1", "metadata": {}, "hash": "5688905db9e854cc49bc1510fb4c17a4045057b851bc56e2784250ddd18ef91b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6411a181-99b2-41fc-9b30-c3ab03677adb", "node_type": "1", "metadata": {}, "hash": "0b7cf1066371cdee8bba51d59d05268275495b052312720ba11897ddbce3ed01", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "These feature maps were again convolved by the four \nkernels of size 2 \u00d7 2 of the second convolutional layer, followed by a subsampling layer that implemented the max pooling function over non-overlapping \npooling windows of size 2 \u00d7 2. Then those feature maps were flattened as the inputs into a 10-way softmax output layer. The winner neuron of the output \nlayer predicted the class of the input image (see Methods). The values of the feature maps and output are as indicated by the colour bar.\n0\n\u20134.4\n\u20131.6\n0\n1.", "mimetype": "text/plain", "start_char_idx": 17311, "end_char_idx": 17824, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6411a181-99b2-41fc-9b30-c3ab03677adb": {"__data__": {"id_": "6411a181-99b2-41fc-9b30-c3ab03677adb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0b76408-e9a5-4c25-83d9-4e0b7ebf70e9", "node_type": "1", "metadata": {}, "hash": "e4ec8feddea6267dbad4e1955bb4222ad2856162a74bbee068af755530cb7746", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21b6acde-09d6-4740-82b9-80a89aa41d61", "node_type": "1", "metadata": {}, "hash": "4939a7bb51520df330a1bcec4c6b9247d22cd123da817f675dc82aecacdaad92", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1\n2.6\n2.6\n1.0\n15 8 \u00d7 8 feature maps\n4 8 \u00d7 8 feature maps\n4 4 \u00d7 4 feature maps / 64 maps\nOutput\n4 2 \u00d7 2 kernels + linear\n64 \u00d7 10 fully connected layer + softmax\n15 3 \u00d7 3 kernels + ReLU\nInput \u20188\u2019\n15 8 \u00d7 8 feature maps\n4 8 \u00d7 8 feature maps\n4 4 \u00d7 4 \nFeature maps\n64 maps\nOutput\nDigit \u20188\u2019 identified\n2 \u00d7 2 Max", "mimetype": "text/plain", "start_char_idx": 17824, "end_char_idx": 18128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21b6acde-09d6-4740-82b9-80a89aa41d61": {"__data__": {"id_": "21b6acde-09d6-4740-82b9-80a89aa41d61", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6411a181-99b2-41fc-9b30-c3ab03677adb", "node_type": "1", "metadata": {}, "hash": "0b7cf1066371cdee8bba51d59d05268275495b052312720ba11897ddbce3ed01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd9a2a7d-8b7f-402a-bd96-c8bea4a3ed32", "node_type": "1", "metadata": {}, "hash": "47631e92d2e9a01ba5feba6ce514bcd6d8c97943378c55affed367e4e26da2b2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "pooling\nFlatten\nRegion for\nconvolution\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n15 3\u00d73 kernels\ng1,1\n+\ng1,1\n\u2013\ng9,1\n+\ng9,1\n\u2013\ng1,2\n+\ng1,2\n\u2013\ng9,2\n+\ng9,2\n\u2013\ng1,15\n+\ng1,15\n\u2013\ng9,15\n+\ng9,15\n\u2013\n5V\n5V\nV1,", "mimetype": "text/plain", "start_char_idx": 18129, "end_char_idx": 18300, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd9a2a7d-8b7f-402a-bd96-c8bea4a3ed32": {"__data__": {"id_": "bd9a2a7d-8b7f-402a-bd96-c8bea4a3ed32", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21b6acde-09d6-4740-82b9-80a89aa41d61", "node_type": "1", "metadata": {}, "hash": "4939a7bb51520df330a1bcec4c6b9247d22cd123da817f675dc82aecacdaad92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c19dc867-d31f-474a-8a57-e3d1a340f2a6", "node_type": "1", "metadata": {}, "hash": "464b9f353fc73c9076041a8ca42858fdab7f4d03b8a463bb682325b690b0dcec", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1 V1,2 V1,3\nV2,1 V2,2 V2,3\nV3,1 V3,2 V3,3\n5V\nI (kernel 1)\nV1,1\nV2,1\nV3,1\nV1,2\nV2,2\nV3,2\nV1,3\nV2,3\nV3,3\nI (kernel 2)\nI (kernel 15)\nV1,1\n\u2013V1,1\nV3,3\n\u2013V3,", "mimetype": "text/plain", "start_char_idx": 18300, "end_char_idx": 18450, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c19dc867-d31f-474a-8a57-e3d1a340f2a6": {"__data__": {"id_": "c19dc867-d31f-474a-8a57-e3d1a340f2a6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd9a2a7d-8b7f-402a-bd96-c8bea4a3ed32", "node_type": "1", "metadata": {}, "hash": "47631e92d2e9a01ba5feba6ce514bcd6d8c97943378c55affed367e4e26da2b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b7ba1a6-ae1d-497e-b9e6-0a3d2caf8358", "node_type": "1", "metadata": {}, "hash": "e0fa9039579bf0aaa3ec693d65fa7659061c7038170956b1892221bcd8cfbd45", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "3\nBit line\nCycles\n10\n20\n30\n0\n1,000\nConductance (\u00b5S)\n500\nMemristor TE line\nBit line\nWord line\n4 2 \u00d7 2 kernels\n64 \u00d7 10 fully connected layer\n15 3\u00d73\nkernels\nWord lines\nBit lines\na\nb\nc\nMemristor TE lines\nWord lines\nMemristor TE line\nNature Machine Intelligence | VOL 1 | SEPTEMBER 2019 | 434\u2013442 | www.nature.", "mimetype": "text/plain", "start_char_idx": 18450, "end_char_idx": 18755, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b7ba1a6-ae1d-497e-b9e6-0a3d2caf8358": {"__data__": {"id_": "0b7ba1a6-ae1d-497e-b9e6-0a3d2caf8358", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c19dc867-d31f-474a-8a57-e3d1a340f2a6", "node_type": "1", "metadata": {}, "hash": "464b9f353fc73c9076041a8ca42858fdab7f4d03b8a463bb682325b690b0dcec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "182a9b13-3a7b-4083-8c86-a509291959a9", "node_type": "1", "metadata": {}, "hash": "2f35543a0ec16a252fea3942a4a1a668627926c730c33b6b1fc6b3b2777345e2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "com/natmachintell\n436\nArticles\nNature Machine Intelligence\nvolution layer performs 270 (480) operations (multiplications and \nadditions) per cycle (that is, one matrix\u2013vector multiplication with \na 1T1R sub-array), 64 (64) cycles per sample and 17,280 (30,720) \noperations per sample in the forward pass. Similarly, the fully con-\nnected layer performs 1,280 operations per cycle, 1 cycle per sample \n(see Supplementary Table 1).", "mimetype": "text/plain", "start_char_idx": 18755, "end_char_idx": 19184, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "182a9b13-3a7b-4083-8c86-a509291959a9": {"__data__": {"id_": "182a9b13-3a7b-4083-8c86-a509291959a9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b7ba1a6-ae1d-497e-b9e6-0a3d2caf8358", "node_type": "1", "metadata": {}, "hash": "e0fa9039579bf0aaa3ec693d65fa7659061c7038170956b1892221bcd8cfbd45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c29c70a-7ce6-472c-9d07-dff88547737b", "node_type": "1", "metadata": {}, "hash": "1258d2640cf416fb2d47984c8c92a392f344c62407434d02ecd35e93aa7af2e3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As a result, 1T1R array-based hybrid \nanalogue\u2013digital computing platforms show clear advantages in \narea and energy efficiencies over transistor-based implementations; \nthese advantages are also scalable to state-of-the-art neural networks \n(see Supplementary Notes 3 and 4 on the projected area/energy effi-\nciency of hybrid analogue\u2013digital systems based on a 128 \u00d7 64 1T1R \narray and multiple large-scale arrays for running AlexNet).\nIn situ training of the 1T1R CNN.", "mimetype": "text/plain", "start_char_idx": 19185, "end_char_idx": 19656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c29c70a-7ce6-472c-9d07-dff88547737b": {"__data__": {"id_": "2c29c70a-7ce6-472c-9d07-dff88547737b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "182a9b13-3a7b-4083-8c86-a509291959a9", "node_type": "1", "metadata": {}, "hash": "2f35543a0ec16a252fea3942a4a1a668627926c730c33b6b1fc6b3b2777345e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "323f77ef-7bc6-4196-9426-d0183681bd5e", "node_type": "1", "metadata": {}, "hash": "82cc684b351d1e27a99a34a54703050fa785b2286d414f245fb1a8f37f9beb53", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The in situ training of the net-\nwork, consisting of the forward pass, error backpropagation and \nweight update, was implemented on the hybrid analogue\u2013digital \nsystem. The training comprised 1,200 mini-batches in 2 epochs \nusing a mini-batch size of 100 for all 60,000 images in the MNIST \ntraining dataset. As shown in Fig.", "mimetype": "text/plain", "start_char_idx": 19657, "end_char_idx": 19982, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "323f77ef-7bc6-4196-9426-d0183681bd5e": {"__data__": {"id_": "323f77ef-7bc6-4196-9426-d0183681bd5e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c29c70a-7ce6-472c-9d07-dff88547737b", "node_type": "1", "metadata": {}, "hash": "1258d2640cf416fb2d47984c8c92a392f344c62407434d02ecd35e93aa7af2e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fadecbb8-3c29-4374-98f7-bc2a855fe909", "node_type": "1", "metadata": {}, "hash": "f6271005e70af8b200e00dafb4b7b4174180b6c8cacb77dfd1a7d0d975af4042", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2a, the hybrid analogue\u2013digital \nsystem combined the advantages of the area/energy efficiency of the \nmemristors in performing the computationally expensive vector\u2013\nmatrix multiplications and digital logic for implementing the rest of \nthe training algorithm (computing the cross-entropy loss and calcu-\nlating the weight change using the RMSprop optimizer, for example; \nsee Supplementary Fig. 1). The backpropagation was implemented \nin a MATLAB environment (R2018b, version 9.5) using the physi-\ncally acquired weights.", "mimetype": "text/plain", "start_char_idx": 19983, "end_char_idx": 20505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fadecbb8-3c29-4374-98f7-bc2a855fe909": {"__data__": {"id_": "fadecbb8-3c29-4374-98f7-bc2a855fe909", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "323f77ef-7bc6-4196-9426-d0183681bd5e", "node_type": "1", "metadata": {}, "hash": "82cc684b351d1e27a99a34a54703050fa785b2286d414f245fb1a8f37f9beb53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92aaea5d-b8e4-4510-b616-babdc0d59c77", "node_type": "1", "metadata": {}, "hash": "d9bfa2838218b89a56fa7364a3138f72b5a91f0bf0aa5890dd1fa8c32f79bb2f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The weight gradients were then used to cal-\nculate how the array would be updated by the RMSprop, a variant \nof stochastic gradient descent with weight- and gradient-history \ndependency47. Instead of an iterative write\u2013read scheme, we used \nthe one-shot blind-update, which was enabled by the transistor cur-\nrent compliance. The conductance was updated column by column \nwith limited parallelism O(nbitlines), where nbitlines denotes the number \nof bit lines of the 1T1R array, rather than the O(1) complexity of the \nvector\u2013matrix multiplication (see Methods).", "mimetype": "text/plain", "start_char_idx": 20506, "end_char_idx": 21068, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92aaea5d-b8e4-4510-b616-babdc0d59c77": {"__data__": {"id_": "92aaea5d-b8e4-4510-b616-babdc0d59c77", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fadecbb8-3c29-4374-98f7-bc2a855fe909", "node_type": "1", "metadata": {}, "hash": "f6271005e70af8b200e00dafb4b7b4174180b6c8cacb77dfd1a7d0d975af4042", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71f930f0-880b-4c52-ae4a-9e8fd15f5dae", "node_type": "1", "metadata": {}, "hash": "d9d9c292191bc58fda24dd84f14a686a72bfe1b6baf32d4c6d2064b1160b5907", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Smoothed training accuracy and loss over the epochs are pre-\nsented in Fig. 2b,c, with test-set accuracies of 92.13% after two \nepochs (see Supplementary Fig. 3 for the statistics of the inference \nand Supplementary Fig. 4 for the representative classification and \nmisclassification of all classes in the inference). The illustrative \nFig. 2 | In situ training of the 1T1R-based five-level CNN. a, Schematic of the hybrid analogue\u2013digital training of the CNN.", "mimetype": "text/plain", "start_char_idx": 21069, "end_char_idx": 21529, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71f930f0-880b-4c52-ae4a-9e8fd15f5dae": {"__data__": {"id_": "71f930f0-880b-4c52-ae4a-9e8fd15f5dae", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92aaea5d-b8e4-4510-b616-babdc0d59c77", "node_type": "1", "metadata": {}, "hash": "d9bfa2838218b89a56fa7364a3138f72b5a91f0bf0aa5890dd1fa8c32f79bb2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e2322e9-a46d-4f13-b539-91639ee755ba", "node_type": "1", "metadata": {}, "hash": "e7422877f85654968140fd6dc6d70bbe5fb6fed7d6bb3bc65533c54d1ec95e09", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The analogue 1T1R memristor array \nphysically performed the computationally expensive vector\u2013matrix multiplications, while the digital logic calculated the cross-entropy loss and the weight \nupdates (the new conductance matrix G) using the RMSprop optimizer. b,c, The smoothed experimental in-batch accuracy increased (b) and loss decresed \n(c) over the course of in situ training. The experimental curves are indistinguishable from the simulation that includes programming noise, closely following \nthe defect-free simulation with a ~4% gap in accuracy during the second epoch of the training.", "mimetype": "text/plain", "start_char_idx": 21530, "end_char_idx": 22124, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e2322e9-a46d-4f13-b539-91639ee755ba": {"__data__": {"id_": "0e2322e9-a46d-4f13-b539-91639ee755ba", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71f930f0-880b-4c52-ae4a-9e8fd15f5dae", "node_type": "1", "metadata": {}, "hash": "d9d9c292191bc58fda24dd84f14a686a72bfe1b6baf32d4c6d2064b1160b5907", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acea037c-6e2d-4b7b-a44b-1d1756ff2902", "node_type": "1", "metadata": {}, "hash": "8a3754e2bf3a742a258697499c68aaede2fb6e7ffe5a95277243be187f0f0de7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "d\u2013f, The distribution of weights before and after the in situ \ntraining of the 15 3 \u00d7 3 kernels (d), the four 2 \u00d7 2 kernels (e) and the fully connected layer (f). The Gaussian-like distributions were broadened by the training.\n\u20135\nWeight\n0\n5\n\u20135\nWeight\n0\n5\n\u20135\nWeight\n0\n5\nPre-training\nPost-training\nPre-training\nPost-training\nPre-training\nPost-training\nProbability density (%)\nProbability density (%)\nProbability density", "mimetype": "text/plain", "start_char_idx": 22125, "end_char_idx": 22542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acea037c-6e2d-4b7b-a44b-1d1756ff2902": {"__data__": {"id_": "acea037c-6e2d-4b7b-a44b-1d1756ff2902", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e2322e9-a46d-4f13-b539-91639ee755ba", "node_type": "1", "metadata": {}, "hash": "e7422877f85654968140fd6dc6d70bbe5fb6fed7d6bb3bc65533c54d1ec95e09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79dfc1a5-b3f8-4ac4-b467-e023bd6f2abb", "node_type": "1", "metadata": {}, "hash": "b14b5d1a0068b81c6ef434da3ea75c116712cabe51e6096ef1a8b2bb8a5f35c9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(%)\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n100\n101\n102\n103\n10\u20132\n10\u20131\n100\n101\nAveraged loss\nNoise-free simulation\nSimulation with programming noise\nExperiment\nNumber of mini-batches\n0\n200\n400\n600\n800\n1,", "mimetype": "text/plain", "start_char_idx": 22543, "end_char_idx": 22798, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79dfc1a5-b3f8-4ac4-b467-e023bd6f2abb": {"__data__": {"id_": "79dfc1a5-b3f8-4ac4-b467-e023bd6f2abb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acea037c-6e2d-4b7b-a44b-1d1756ff2902", "node_type": "1", "metadata": {}, "hash": "8a3754e2bf3a742a258697499c68aaede2fb6e7ffe5a95277243be187f0f0de7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36ec4795-2693-4d92-93fe-b884bd1d14d7", "node_type": "1", "metadata": {}, "hash": "32f77b6fcf4a4cb5f704a9bc33fbb1c004cab7f8d2a9d9c438be95881ea26c9b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "000\n1,200\nNumber of mini-batches\n0\n20\n40\n60\n80\n100\nAveraged training\naccuracy (%)\nNoise-free simulation\nSimulation with programming noise\nExperiment\nSample training\nimages and labels\nLayer-by-layer\nbatch forward pass\nCompute cross-\nentropy loss\nError\nbackpropagation\nCalculate \u0394G\nwith RMSprop\nOne-shot update\nweights G\nLast mini-batch?", "mimetype": "text/plain", "start_char_idx": 22798, "end_char_idx": 23133, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36ec4795-2693-4d92-93fe-b884bd1d14d7": {"__data__": {"id_": "36ec4795-2693-4d92-93fe-b884bd1d14d7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79dfc1a5-b3f8-4ac4-b467-e023bd6f2abb", "node_type": "1", "metadata": {}, "hash": "b14b5d1a0068b81c6ef434da3ea75c116712cabe51e6096ef1a8b2bb8a5f35c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff108c4f-bcef-4bf5-b316-47c86e4507d6", "node_type": "1", "metadata": {}, "hash": "b613f74c665e3a746465af3b9465c3be5a0cbb3375ad4e01d288017416eae73c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Hardware vector\u2013\nmatrix multiplication\nPhysical weights\n G acquisition\nNo\nAnalogue computing\nDigital computing\na\nb\nd\ne\nf\nc\nNature Machine Intelligence | VOL 1 | SEPTEMBER 2019 | 434\u2013442 | www.nature.com/natmachintell\n437\nArticles\nNature Machine Intelligence\ntraining and inference are shown in the Supplementary Videos 1 \nand 2. The inaccuracy of memristor programming (the limiting fac-\ntor for the performance) could be improved by further engineering \nthe memristor material stack48.", "mimetype": "text/plain", "start_char_idx": 23134, "end_char_idx": 23620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff108c4f-bcef-4bf5-b316-47c86e4507d6": {"__data__": {"id_": "ff108c4f-bcef-4bf5-b316-47c86e4507d6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36ec4795-2693-4d92-93fe-b884bd1d14d7", "node_type": "1", "metadata": {}, "hash": "32f77b6fcf4a4cb5f704a9bc33fbb1c004cab7f8d2a9d9c438be95881ea26c9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da9c94a9-8bd5-49e3-bd4e-2abd6582c42c", "node_type": "1", "metadata": {}, "hash": "61dc391f0a1e01bb1bbffc3d5296d3c8742c13ceed396b9f58d238e0927d81f6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "However, stochastic programming \nof the memristors may naturally mitigate overfitting as adding \nnoise to weights49,50 is one of the widely used methods of suppress-\ning the generalization error associated with small datasets (see \nSupplementary Fig. 5 for the simulated impact of the programming \nnoise over the generalization error).\nTo capture the impact of inevitable device imperfections on pro-\ngramming, we simulated the in situ training by presuming cycle-to-\ncycle and device-to-device variation in memristor programming.", "mimetype": "text/plain", "start_char_idx": 23621, "end_char_idx": 24151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da9c94a9-8bd5-49e3-bd4e-2abd6582c42c": {"__data__": {"id_": "da9c94a9-8bd5-49e3-bd4e-2abd6582c42c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff108c4f-bcef-4bf5-b316-47c86e4507d6", "node_type": "1", "metadata": {}, "hash": "b613f74c665e3a746465af3b9465c3be5a0cbb3375ad4e01d288017416eae73c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "288fcfa9-ad7b-4df6-842c-87bba2c7fe9f", "node_type": "1", "metadata": {}, "hash": "e7fd7f56977cadd4c4501b9bec42b00b0badbd3132bd04c0d21754366a437923", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The programming noise was set to follow a normal distribution \nwith a standard deviation of 10 \u00b5S in the simulated update of mem-\nristor conductance. The experimental curve, particularly in the \nsecond epoch, is nearly indistinguishable from the simulated curve \nwith the assumed programming noise (see Fig. 2b,c).", "mimetype": "text/plain", "start_char_idx": 24153, "end_char_idx": 24467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "288fcfa9-ad7b-4df6-842c-87bba2c7fe9f": {"__data__": {"id_": "288fcfa9-ad7b-4df6-842c-87bba2c7fe9f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da9c94a9-8bd5-49e3-bd4e-2abd6582c42c", "node_type": "1", "metadata": {}, "hash": "61dc391f0a1e01bb1bbffc3d5296d3c8742c13ceed396b9f58d238e0927d81f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e37d045-1339-4f70-893e-74a7b3eb9f18", "node_type": "1", "metadata": {}, "hash": "15f7608b0ae8a91c96eba7e04edd5a1417739a49573cf198a30fd31eaff57556", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Moreover, we \nobserved a consistent trend in the training of noise-free memris-\ntors (or equivalently double-precision floating point simulations), \nwith an in-batch accuracy gap of around 4% in the second epoch \nof the training and a test-set accuracy difference of around 4%. \nUsing just ~1,000 weights, this accuracy was equivalent to our pre-\nviously reported multilayer perceptron using ~4,000 weights, with \n11% non-responsive devices in both cases45, illustrating the robust \nlearning capability of the hybrid analogue\u2013digital computing plat-\nform.", "mimetype": "text/plain", "start_char_idx": 24468, "end_char_idx": 25023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e37d045-1339-4f70-893e-74a7b3eb9f18": {"__data__": {"id_": "4e37d045-1339-4f70-893e-74a7b3eb9f18", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "288fcfa9-ad7b-4df6-842c-87bba2c7fe9f", "node_type": "1", "metadata": {}, "hash": "e7fd7f56977cadd4c4501b9bec42b00b0badbd3132bd04c0d21754366a437923", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca62d585-1b19-400d-86b4-61a818687be5", "node_type": "1", "metadata": {}, "hash": "74b57f132a64cbd4d22405fd90bc0aa0524ded0f560458196ea4c31d245e5213", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The reduction of the free parameters without compromising \nperformance hinges on the shared weights of the memristor-based \nCNN. The performance of the hardware network can approach \nthat of the software with improved memristor fabrication yield and \nprogramming accuracy, which are being actively pursued by the \nmaterials community48.\nFigure 2d\u2013f shows the weight distributions of the convolu-\ntional and fully connected layers before and after training (see \nSupplementary Fig. 6 for the conductance and weights after train-\ning).", "mimetype": "text/plain", "start_char_idx": 25024, "end_char_idx": 25557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca62d585-1b19-400d-86b4-61a818687be5": {"__data__": {"id_": "ca62d585-1b19-400d-86b4-61a818687be5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e37d045-1339-4f70-893e-74a7b3eb9f18", "node_type": "1", "metadata": {}, "hash": "15f7608b0ae8a91c96eba7e04edd5a1417739a49573cf198a30fd31eaff57556", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87c50d04-f39e-4ca9-8228-a8bea1179753", "node_type": "1", "metadata": {}, "hash": "2bc61a3f73afd7df6a5c950d4509a136db95a6f32b6f7e7271a07c41e91dbabe", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The distribution of the weights is nearly normal, as the train-\ning algorithm placed no limitation on the weight values. The weight \ndistributions broadened as the training proceeded, verifying robust \nin situ training without requiring extra regularizing techniques.\n1T1R ConvLSTM with spatio-temporal weight sharing. In addi-\ntion to spatial weight sharing, the 1T1R array could concurrently \nimplement spatio-temporal weight sharing\u2014with a ConvLSTM \nnetwork, for example.", "mimetype": "text/plain", "start_char_idx": 25558, "end_char_idx": 26032, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87c50d04-f39e-4ca9-8228-a8bea1179753": {"__data__": {"id_": "87c50d04-f39e-4ca9-8228-a8bea1179753", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca62d585-1b19-400d-86b4-61a818687be5", "node_type": "1", "metadata": {}, "hash": "74b57f132a64cbd4d22405fd90bc0aa0524ded0f560458196ea4c31d245e5213", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a81a973d-e24f-40a7-8bc0-1dc20fade299", "node_type": "1", "metadata": {}, "hash": "78bb87b479644b117aa4a6c7ddf2e001b56ea19ce554a323c614b8838a265674", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A ConvLSTM block is essentially a layer with \ntwo multidimensional inputs: in our case, the frame of the synthetic \nMNIST-sequence video of size 8 \u00d7 8 and the recurrent input of size \n6 \u00d7 6 (\u00d75), based on the output of the last time step (see Methods \nfor the generation of the MNIST-sequence dataset). The recurrent \nnature, or temporal structure, of ConvLSTM was expressed by a \nsequence of feed-forward operations, as shown in Fig. 3a, linking \none time step to the next.", "mimetype": "text/plain", "start_char_idx": 26033, "end_char_idx": 26507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a81a973d-e24f-40a7-8bc0-1dc20fade299": {"__data__": {"id_": "a81a973d-e24f-40a7-8bc0-1dc20fade299", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87c50d04-f39e-4ca9-8228-a8bea1179753", "node_type": "1", "metadata": {}, "hash": "2bc61a3f73afd7df6a5c950d4509a136db95a6f32b6f7e7271a07c41e91dbabe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbc44d52-c54b-4635-a402-78c57db51d24", "node_type": "1", "metadata": {}, "hash": "2126ca19c4ab7256a9d921c719ea8deb3206f7fed09194689a45a6accae24137", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Sharing of the trainable parameters occurs \nbetween the time steps as in an LSTM, effectively reusing the same \nweights across all time steps of the calculation. In implementing \nspatial correlations by convolving both inputs, ConvLSTM shares \nthe weights both spatially and temporally, which could model gen-\neral-purpose spatio-temporal patterns in 3D inputs6.\nBoth of the inputs were subsequently convolved by the self-\nparameterized cell input and control gates of the ConvLSTM. The \ncell input and control gates were mapped to the memristor arrays \nas shown in Fig.", "mimetype": "text/plain", "start_char_idx": 26508, "end_char_idx": 27078, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbc44d52-c54b-4635-a402-78c57db51d24": {"__data__": {"id_": "cbc44d52-c54b-4635-a402-78c57db51d24", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a81a973d-e24f-40a7-8bc0-1dc20fade299", "node_type": "1", "metadata": {}, "hash": "78bb87b479644b117aa4a6c7ddf2e001b56ea19ce554a323c614b8838a265674", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7f02b11-382b-4227-99a7-b687d909489d", "node_type": "1", "metadata": {}, "hash": "7a94585081a7b64cd6ed8b51d8574690ac9155e096809529089c53cd63108577", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "3b. A new frame of the input arrived with each \ntime step, and its information was incorporated into the cell under \nthe control of the gates. The previous cell status would be partially \nremoved in this process if the forget gate was on. Whether the latest \ncell output propagates to the next time frame was determined by \nthe output gate. Before the first frame of the input, all the states of \nthe ConvLSTM layer were initialized to zero, corresponding to \u2018total \nignorance\u2019 of the future6.", "mimetype": "text/plain", "start_char_idx": 27079, "end_char_idx": 27572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7f02b11-382b-4227-99a7-b687d909489d": {"__data__": {"id_": "b7f02b11-382b-4227-99a7-b687d909489d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbc44d52-c54b-4635-a402-78c57db51d24", "node_type": "1", "metadata": {}, "hash": "2126ca19c4ab7256a9d921c719ea8deb3206f7fed09194689a45a6accae24137", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffd3229e-9f7d-4d64-8474-80f5f682a9ee", "node_type": "1", "metadata": {}, "hash": "61228e6bcd4c08bf2cbfdabca651dcd7bfd4b4b836806a6eb3ff60a90cbd08ef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The final output, five 6 \u00d7 6 feature maps of the ConvLSTM layer, \nwas downsampled by a max pooling layer. The pooled feature maps \nwere then flattened as the inputs to a six-neuron softmax layer, to \nclassify the six classes of the MNIST-sequences (see Methods).", "mimetype": "text/plain", "start_char_idx": 27573, "end_char_idx": 27835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffd3229e-9f7d-4d64-8474-80f5f682a9ee": {"__data__": {"id_": "ffd3229e-9f7d-4d64-8474-80f5f682a9ee", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7f02b11-382b-4227-99a7-b687d909489d", "node_type": "1", "metadata": {}, "hash": "7a94585081a7b64cd6ed8b51d8574690ac9155e096809529089c53cd63108577", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdda81c6-7282-4b9d-9cf8-f0f7438de133", "node_type": "1", "metadata": {}, "hash": "156c2535e436d7889cca3982ed101f709b2cabb29f2c67b2a458c27a5ed30e8c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A single convolution stride of ConvLSTM has com-\nputational \ncomplexity \nO kidim1 \u00b4 kidim2 \u00b4 kidim3 \u00b4 kinum\n\u00f0\n\u00de\nI\n \n\u00feO krdim1 \u00b4 krdim2 \u00b4 krdim3 \u00b4 krnum\n\u00f0\n\u00de\nI\n, where variables ki and kr cor-\nrespond to the kernels of the input and recurrent input, respectively. \nSimilar to the CNN case, the complexity was reduced to O(1) by \nmapping the weights to the 1T1R array.", "mimetype": "text/plain", "start_char_idx": 27836, "end_char_idx": 28201, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdda81c6-7282-4b9d-9cf8-f0f7438de133": {"__data__": {"id_": "bdda81c6-7282-4b9d-9cf8-f0f7438de133", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffd3229e-9f7d-4d64-8474-80f5f682a9ee", "node_type": "1", "metadata": {}, "hash": "61228e6bcd4c08bf2cbfdabca651dcd7bfd4b4b836806a6eb3ff60a90cbd08ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf9d4265-4d94-4829-91a9-78d555b55418", "node_type": "1", "metadata": {}, "hash": "f9f89e4877f949e18b6b6ca318a8315effea2b2f35d80630491118f8e48e7121", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The ConvLSTM layer per-\nforms 1,160 operations per cycle, 108 cycles per sample in the for-\nward pass, which consists of 3 time steps, and 125,280 operations \nper sample. Similarly, the fully connected layer performs 540 opera-\ntions per cycle, 1 cycle per sample (see Supplementary Table 2). \n \nAll other computations shown in the computational graph were ele-\nment-wise operations performed infrequently, once per completion \nof all strides of a convolution, using the conventional logic circuitry \nof the hybrid analogue\u2013digital system.", "mimetype": "text/plain", "start_char_idx": 28202, "end_char_idx": 28741, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf9d4265-4d94-4829-91a9-78d555b55418": {"__data__": {"id_": "cf9d4265-4d94-4829-91a9-78d555b55418", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdda81c6-7282-4b9d-9cf8-f0f7438de133", "node_type": "1", "metadata": {}, "hash": "156c2535e436d7889cca3982ed101f709b2cabb29f2c67b2a458c27a5ed30e8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c9885da-d240-4dc5-8dab-71a7fe893431", "node_type": "1", "metadata": {}, "hash": "6da228529363c6aaab91dc9188e8430f455135c83a1b222269fe0e9b38804011", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In situ training of the 1T1R ConvLSTM. The temporal recurrence \nof ConvLSTM makes the error signal backpropagate both spatially \n(within the same ConvLSTM block) and temporally (across all \nConvLSTM blocks at different times). Extra memory is thus needed \nto record the neural activities of each time step in a forward pass, \nwhich differs from the training of a feed-forward network in Fig. 2a.", "mimetype": "text/plain", "start_char_idx": 28742, "end_char_idx": 29137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c9885da-d240-4dc5-8dab-71a7fe893431": {"__data__": {"id_": "3c9885da-d240-4dc5-8dab-71a7fe893431", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf9d4265-4d94-4829-91a9-78d555b55418", "node_type": "1", "metadata": {}, "hash": "f9f89e4877f949e18b6b6ca318a8315effea2b2f35d80630491118f8e48e7121", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "812d1682-2766-4ee2-ae8d-de8f8c573c84", "node_type": "1", "metadata": {}, "hash": "2c3e16ba965def7d2a12b9b52be6c6896c58545abd44f13a212325e5308e4f56", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Such a memory was implemented digitally here, but potential ana-\nlogue sample and hold memories could be employed to develop fast \nbackpropagation in the analogue domain.\nThe animated training and inference are shown in Supplementary \nVideos 3 and 4. Similar to the case in Fig. 2b, the intrinsic variations \nof the memristors lead to small differences between the training \n(95.97%) and test-set (96.44%) accuracies, confirming the built-in \nregularizing capability of the memristor array (see Supplementary \nFig. 7 for the statistics of the inference) .", "mimetype": "text/plain", "start_char_idx": 29141, "end_char_idx": 29696, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "812d1682-2766-4ee2-ae8d-de8f8c573c84": {"__data__": {"id_": "812d1682-2766-4ee2-ae8d-de8f8c573c84", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c9885da-d240-4dc5-8dab-71a7fe893431", "node_type": "1", "metadata": {}, "hash": "6da228529363c6aaab91dc9188e8430f455135c83a1b222269fe0e9b38804011", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52c140fb-0bb9-4b80-8cbf-21f9900257b6", "node_type": "1", "metadata": {}, "hash": "4ad69dafd57861f2e19d1eae0091bc3aac79f1d673d5878687ae3469b2d14dad", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This proof-of-concept \nvideo classification used only 850 weights, in contrast to the 3,675 \nweights of a functional equivalent with a five-kernel convolution \nlayer followed by five LSTM units and a dense layer, manifesting the \ndrastic parameter reduction of the spatio-temporal weight sharing \nof ConvLSTM. The experimental accuracy reveals small differences \nbetween both predictions, assuming ideal and noisy memristor pro-\ngramming, as shown in Fig. 4a,b.", "mimetype": "text/plain", "start_char_idx": 29697, "end_char_idx": 30158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52c140fb-0bb9-4b80-8cbf-21f9900257b6": {"__data__": {"id_": "52c140fb-0bb9-4b80-8cbf-21f9900257b6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "812d1682-2766-4ee2-ae8d-de8f8c573c84", "node_type": "1", "metadata": {}, "hash": "2c3e16ba965def7d2a12b9b52be6c6896c58545abd44f13a212325e5308e4f56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "922eca02-2805-4715-8c99-033021baf0b9", "node_type": "1", "metadata": {}, "hash": "31fa93c8fe16744c9edeacd1b1d54ca726939ce74b6d649c9ef8e541e3957b4b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Figure 4c,d shows the broadening of the Gaussian-like weight \ndistributions of the ConvLSTM and fully connected layers before \nand after training, demonstrating robust in situ training with \n \nsimultaneous \nspatial \nand \ntemporal \nweight \nsharing \n(see \nSupplementary Fig. 8 for the conductance and weights after train-\ning). Figure 4e depicts representative examples of valid and invalid \nclassifications of the MNIST-sequences. The invalid classifications \nmay also be challenging for human beings to identify at the down-\nsampled resolution (see Supplementary Fig.", "mimetype": "text/plain", "start_char_idx": 30159, "end_char_idx": 30726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "922eca02-2805-4715-8c99-033021baf0b9": {"__data__": {"id_": "922eca02-2805-4715-8c99-033021baf0b9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52c140fb-0bb9-4b80-8cbf-21f9900257b6", "node_type": "1", "metadata": {}, "hash": "4ad69dafd57861f2e19d1eae0091bc3aac79f1d673d5878687ae3469b2d14dad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b87c2e4-e1ba-4c3c-b6e8-5423cfeb7ab7", "node_type": "1", "metadata": {}, "hash": "b906086d4dab19064e0448433a3670194ec2ea2c8c73be3a760f7604a13ddd21", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "9 for the representative \nclassification and misclassification of all classes in the inference).\nDiscussion\nIn summary, we demonstrated in situ training of both the spa-\ntial shared-weight CNN and the spatio-temporal shared-weight \nConvLSTM using memristor crossbar arrays. The in situ training \nself-adaptively mitigated the impact of hardware non-idealities, \nwhich is vital to meeting the stringent convergence requirements of \nweight-sharing structures.", "mimetype": "text/plain", "start_char_idx": 30727, "end_char_idx": 31184, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b87c2e4-e1ba-4c3c-b6e8-5423cfeb7ab7": {"__data__": {"id_": "5b87c2e4-e1ba-4c3c-b6e8-5423cfeb7ab7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "922eca02-2805-4715-8c99-033021baf0b9", "node_type": "1", "metadata": {}, "hash": "31fa93c8fe16744c9edeacd1b1d54ca726939ce74b6d649c9ef8e541e3957b4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9274bfc9-e30e-498a-bb77-2d987c9fab45", "node_type": "1", "metadata": {}, "hash": "9d13bfc201dd860d082dbaee90a7f7146083fe7b33d85d63f9f686d1f6d21c88", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The accuracy was equivalent to that of a \nfully connected memristive neural network in classifying MNIST \nhandwritten digits with CNN, but required only ~25% of the weights. \nIn addition, we demonstrated simultaneous spatial and temporal \nweight sharing, for example, in the ConvLSTM network that features \nintrinsic 3D input processing for classifying the MNIST-sequence \nvideos.", "mimetype": "text/plain", "start_char_idx": 31185, "end_char_idx": 31565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9274bfc9-e30e-498a-bb77-2d987c9fab45": {"__data__": {"id_": "9274bfc9-e30e-498a-bb77-2d987c9fab45", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b87c2e4-e1ba-4c3c-b6e8-5423cfeb7ab7", "node_type": "1", "metadata": {}, "hash": "b906086d4dab19064e0448433a3670194ec2ea2c8c73be3a760f7604a13ddd21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff5c10cc-032d-427b-92ed-c18e28956e1b", "node_type": "1", "metadata": {}, "hash": "ebb7fc30a12e6218696ee6184616aeccbf729b79fed3f1da91eaa924108c9488", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The reduction in the number of trainable parameters as a \nresult of weight sharing, as well as the area/energy efficiency boost \nNature Machine Intelligence | VOL 1 | SEPTEMBER 2019 | 434\u2013442 | www.nature.com/natmachintell\n438\nArticles\nNature Machine Intelligence\nfrom analogue in-memory computing, illustrates the potential of \nmemristor-based computing platforms for implementing advanced \ntopologies of neural networks directed towards edge applications.", "mimetype": "text/plain", "start_char_idx": 31566, "end_char_idx": 32023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff5c10cc-032d-427b-92ed-c18e28956e1b": {"__data__": {"id_": "ff5c10cc-032d-427b-92ed-c18e28956e1b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9274bfc9-e30e-498a-bb77-2d987c9fab45", "node_type": "1", "metadata": {}, "hash": "9d13bfc201dd860d082dbaee90a7f7146083fe7b33d85d63f9f686d1f6d21c88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2d97ba5-52ed-4a25-9c95-0509f6204908", "node_type": "1", "metadata": {}, "hash": "91e878f3a30faf95df8d0497e895d371df61d413e431203335df48b8c9fc1098", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The proof-of-concept in situ training of a neural network using \nthe spatial or spatio-temporal weight-sharing technique on emerg-\ning memories not only demonstrated that the advantages of the \nFig. 3 | 1T1R implementation of the ConvLSTM network. a, The network structure of ConvLSTM for classifying the MNIST sequence. The temporal recurrence \nis expanded as the operations proceed in sequence from one time step (t) to the next.", "mimetype": "text/plain", "start_char_idx": 32024, "end_char_idx": 32455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2d97ba5-52ed-4a25-9c95-0509f6204908": {"__data__": {"id_": "d2d97ba5-52ed-4a25-9c95-0509f6204908", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff5c10cc-032d-427b-92ed-c18e28956e1b", "node_type": "1", "metadata": {}, "hash": "ebb7fc30a12e6218696ee6184616aeccbf729b79fed3f1da91eaa924108c9488", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd13a164-d1ed-40b7-8e48-7fce828b8be6", "node_type": "1", "metadata": {}, "hash": "81d39cf17e46c9685eaa740ea16fb891625d81712ba550a5c747717ed11e7721", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The 8-bit grayscale frame input (I) of size 8 \u00d7 8 was fed to the ConvLSTM \nlayer at each time step, together with the recurrent input (RI) based on the output of the same layer in the last time step. Both inputs were physically convolved \nby the 20 kernels of the self-parameterized cell input and controlling gates (for example input gate, forget gate and output gate), which were mapped to the \nmemristor array.", "mimetype": "text/plain", "start_char_idx": 32456, "end_char_idx": 32869, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd13a164-d1ed-40b7-8e48-7fce828b8be6": {"__data__": {"id_": "cd13a164-d1ed-40b7-8e48-7fce828b8be6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2d97ba5-52ed-4a25-9c95-0509f6204908", "node_type": "1", "metadata": {}, "hash": "91e878f3a30faf95df8d0497e895d371df61d413e431203335df48b8c9fc1098", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7375df0-64e7-4bb3-8f68-49d58f32ecbb", "node_type": "1", "metadata": {}, "hash": "57065b055eabbca6bbb96e64704e17288dfa3e5ed38d6ff1df183a1d069c4f5c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The single cell input or gate consisted of five input kernels of size 3 \u00d7 3 and five recurrent input kernels of size 2 \u00d7 2. The 1T1R output currents \nwere then nonlinearly activated and gated to update the cell and yield the output. The output of the current time step would serve as one of the inputs to the \nsame ConvLSTM layer in the next time step. Similarly, the cell of the current time step would be passed to the next time step to be gated by the forget gate \nof the same ConvLSTM layer.", "mimetype": "text/plain", "start_char_idx": 32870, "end_char_idx": 33365, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7375df0-64e7-4bb3-8f68-49d58f32ecbb": {"__data__": {"id_": "c7375df0-64e7-4bb3-8f68-49d58f32ecbb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd13a164-d1ed-40b7-8e48-7fce828b8be6", "node_type": "1", "metadata": {}, "hash": "81d39cf17e46c9685eaa740ea16fb891625d81712ba550a5c747717ed11e7721", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "358a3ab9-d84d-4f31-8068-0e2ae55137b5", "node_type": "1", "metadata": {}, "hash": "a7b1620c259ad27191574a75a5d5a1f21fd6cfd6f43f75a37d56f60307fdc236", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Note that all the states of the ConvLSTM were initialized to zero before the first input, corresponding to a total ignorance of the \nfuture. The output of the last time step of the ConvLSTM, five feature maps of size 6 \u00d7 6, was downsampled by a max pooling layer. The pooled feature maps \nwere then flattened as the inputs to a six-neuron softmax layer, to classify the six possible MNIST sequences (see Methods).", "mimetype": "text/plain", "start_char_idx": 33366, "end_char_idx": 33779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "358a3ab9-d84d-4f31-8068-0e2ae55137b5": {"__data__": {"id_": "358a3ab9-d84d-4f31-8068-0e2ae55137b5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7375df0-64e7-4bb3-8f68-49d58f32ecbb", "node_type": "1", "metadata": {}, "hash": "57065b055eabbca6bbb96e64704e17288dfa3e5ed38d6ff1df183a1d069c4f5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03281470-88d8-4425-ae40-9c2d14940f75", "node_type": "1", "metadata": {}, "hash": "696d1ca9eefacfecb98ed8049215dd8b703037c58f3a1b15771c6ffe31c71cbc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "All trainable parameters of the \nConvLSTM layer are shared spatially between different convolution regions, besides the weights are also effectively reused across all time steps.  \nb, Optical micrograph of the 128 \u00d7 64 1T1R array with colour blocks illustrating the partitions of the 1T1R array used to implement the trainable parameters of \nthe ConvLSTM layer and the fully connected layer (weight matrix size 45 \u00d7 6). The values of the feature maps and output are as indicated by the colour bar.\nWord lines\nBit lines\nMemristor TE lines\n45 \u00d7 6 fully connected", "mimetype": "text/plain", "start_char_idx": 33780, "end_char_idx": 34340, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03281470-88d8-4425-ae40-9c2d14940f75": {"__data__": {"id_": "03281470-88d8-4425-ae40-9c2d14940f75", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "358a3ab9-d84d-4f31-8068-0e2ae55137b5", "node_type": "1", "metadata": {}, "hash": "a7b1620c259ad27191574a75a5d5a1f21fd6cfd6f43f75a37d56f60307fdc236", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ad2433a-caf2-4cd7-bd7d-b6613918c6de", "node_type": "1", "metadata": {}, "hash": "059c9c32d247e1349c341ed43c70567328d93249dfabb0d93d96187536dc3509", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "layer\nCell input 5 3 \u00d7 3 and 5 2 \u00d7 2 kernels\nInput gate 5 3 \u00d7 3 and 5 2 \u00d7 2 kernels\nForget gate 5 3 \u00d7 3 and 5 2 \u00d7 2 kernels\nOutput gate 5 3 \u00d7 3 and 5 2 \u00d7 2 kernels\nCell input 5 3 \u00d7 3 and 5 2 \u00d7 2 kernels\nInput gate 5 3 \u00d7 3 and 5 2 \u00d7 2 kernels\nForget gate 5 3 \u00d7 3 and 5", "mimetype": "text/plain", "start_char_idx": 34341, "end_char_idx": 34608, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ad2433a-caf2-4cd7-bd7d-b6613918c6de": {"__data__": {"id_": "8ad2433a-caf2-4cd7-bd7d-b6613918c6de", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03281470-88d8-4425-ae40-9c2d14940f75", "node_type": "1", "metadata": {}, "hash": "696d1ca9eefacfecb98ed8049215dd8b703037c58f3a1b15771c6ffe31c71cbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "967f9469-77fd-4076-bc9d-a8858767f772", "node_type": "1", "metadata": {}, "hash": "2b99230bae8411d4221f2f2b2d6e6897a6b520dc96e63fe96e1682f91d16390e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2 \u00d7 2 kernels\nOutput gate 5 3 \u00d7 3 and 5 2 \u00d7 2 kernels\n\u20131\n0\n1\n1\nRecurrent input/cell input/5 6 \u00d7 6 feature maps\n5 3 \u00d7 3 feature maps/64 maps\nInput gate/forget gate/output gate/output\n0\n+\n+\n2 \u00d7 2 max pooling\n45 \u00d7 6 fully connected layer + softmax\nSequence\n\u20181\u20132\u20133 \u2019\nidentified\nFlatten\n5 6 \u00d7 6 feature maps\n5 3 \u00d7 3 \nfeature maps\n64", "mimetype": "text/plain", "start_char_idx": 34609, "end_char_idx": 34936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "967f9469-77fd-4076-bc9d-a8858767f772": {"__data__": {"id_": "967f9469-77fd-4076-bc9d-a8858767f772", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ad2433a-caf2-4cd7-bd7d-b6613918c6de", "node_type": "1", "metadata": {}, "hash": "059c9c32d247e1349c341ed43c70567328d93249dfabb0d93d96187536dc3509", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5bd77f8-aa6b-471d-9ff0-314f33009f44", "node_type": "1", "metadata": {}, "hash": "7cfd1aa1cf84ed6321d6f0ac38ca59623da814399acc4b707971aac40f8aca5d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "maps\nOutput\nt = 2\nt = 3\nt = 1\nInput\nRecurrent input\nCell input\n5 3 \u00d7 3 kernels (I)\n5 2 \u00d7 2 kernels (RI)\nInput gate\n5 3 \u00d7 3 kernels (I)\n5 2 \u00d7 2 kernels (RI)\nForget gate\n5 3 \u00d7 3 kernels (I)\n5 2 \u00d7 2 kernels (RI)\nOutput gate\n5 3 \u00d7 3 kernels (I)\n5 2 \u00d7 2 kernels (RI)\nCell input\n5 3 \u00d7 3 kernels", "mimetype": "text/plain", "start_char_idx": 34937, "end_char_idx": 35225, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5bd77f8-aa6b-471d-9ff0-314f33009f44": {"__data__": {"id_": "d5bd77f8-aa6b-471d-9ff0-314f33009f44", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "967f9469-77fd-4076-bc9d-a8858767f772", "node_type": "1", "metadata": {}, "hash": "2b99230bae8411d4221f2f2b2d6e6897a6b520dc96e63fe96e1682f91d16390e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95b963d8-770e-404f-8ff9-8adc0248cac1", "node_type": "1", "metadata": {}, "hash": "97596e299d8bd56d27e2d2ae5a2e98359175686280c396829c253c68ae4b7a0f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(I)\n5 2 \u00d7 2 kernels (RI)\nInput gate\n5 3 \u00d7 3 kernels (I)\n5 2 \u00d7 2 kernels (RI)\nForget gate\n5 3 \u00d7 3 kernels (I)\n5 2 \u00d7 2 kernels (RI)\nOutput gate\n5 3 \u00d7 3 kernels (I)\n5 2 \u00d7 2 kernels (RI)\nCell input\n5 3 \u00d7 3 kernels (I)\n5 2 \u00d7 2 kernels (RI)\nInput gate\n5 3 \u00d7 3 kernels (I)\n5 2 \u00d7 2 kernels", "mimetype": "text/plain", "start_char_idx": 35016, "end_char_idx": 35297, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95b963d8-770e-404f-8ff9-8adc0248cac1": {"__data__": {"id_": "95b963d8-770e-404f-8ff9-8adc0248cac1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5bd77f8-aa6b-471d-9ff0-314f33009f44", "node_type": "1", "metadata": {}, "hash": "7cfd1aa1cf84ed6321d6f0ac38ca59623da814399acc4b707971aac40f8aca5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23b05784-67a8-41cb-a160-5ff6aebf8f83", "node_type": "1", "metadata": {}, "hash": "0bca750261fa2b03bab3a6a7aa201fc9e027de71dffdf1905e76ecf6d750339b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(RI)\nForget gate\n5 3 \u00d7 3 kernels (I)\n5 2 \u00d7 2 kernels (RI)\nOutput gate\n5 3 \u00d7 3 kernels (I)\n5 2 \u00d7 2 kernels (RI)\nCell\n1\u20132\u20133\n1\u20133\u20132\n2\u20131\u20133\n2\u20133\u20131\n3\u20131\u20132\n3\u20132\u20131\na\nb\nNature Machine Intelligence | VOL 1 | SEPTEMBER 2019 | 434\u2013442 | www.nature.", "mimetype": "text/plain", "start_char_idx": 35508, "end_char_idx": 35740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23b05784-67a8-41cb-a160-5ff6aebf8f83": {"__data__": {"id_": "23b05784-67a8-41cb-a160-5ff6aebf8f83", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "48e4dd8ed1e0bcda7081cb4f0c228d1731e57d3e88c8ae6c60505a64e96e7f5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95b963d8-770e-404f-8ff9-8adc0248cac1", "node_type": "1", "metadata": {}, "hash": "97596e299d8bd56d27e2d2ae5a2e98359175686280c396829c253c68ae4b7a0f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "com/natmachintell\n439\nArticles\nNature Machine Intelligence\nweight sharing could be combined with the area/energy efficiency \nof memristor crossbars for post-von Neuman computing, but also \nrevealed that the inherent stochasticity of memristor programming \ndoes not always negatively impact the in situ training because rea-\nsonable amount of stochasticity could serve as a natural regulariza-\ntion to avoid overfitting.", "mimetype": "text/plain", "start_char_idx": 35740, "end_char_idx": 36159, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9315cfbb-c202-43c8-9ffe-649de42faeac": {"__data__": {"id_": "9315cfbb-c202-43c8-9ffe-649de42faeac", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27d30242-4c4d-4aa8-b422-37bcb0a3265b", "node_type": "1", "metadata": {}, "hash": "b4ce7d88b81f396c48f4d3b3191bc257366fd6ffd725d7a4da1f4c9fc994dc45", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Methods\n1T1R array integration. The 1T1R arrays were fabricated by integrating Pt/TaOx/\nTa memristors on the transistor array. The front-end and part of the back-end \nprocesses to build the transistor arrays, word lines, bit lines and the memristor \nTE lines were done in a commercial fab using the 2 \u00b5m technology node. The \ntransistors were used as selecting devices to mitigate the sneak path currents \nin programming the memristors and to enable precise conductance tuning by \nlimiting the current.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27d30242-4c4d-4aa8-b422-37bcb0a3265b": {"__data__": {"id_": "27d30242-4c4d-4aa8-b422-37bcb0a3265b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9315cfbb-c202-43c8-9ffe-649de42faeac", "node_type": "1", "metadata": {}, "hash": "7256365f9e53496e7f7d9d9011ffd4a4d2e1ce19ab6bb34a55649207f5a7c051", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c56bc08-cdc6-4738-8d7a-ef17a91a3747", "node_type": "1", "metadata": {}, "hash": "21c838ecccbb304c978a6d7cf5f5d02381d243cc289ebc9e92e6fb02fc2051f3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The memristor integration began with Ar plasma etching of \nthe native oxide of the metal vias, followed by the deposition of 8 nm Ag, 2 nm Ti \nand 200 nm Pd in sequence using a sputter with Ar ambience. The chip was then \nannealed at 300 \u00b0C for 36 minutes to improve the metallic contacts with memristor \nelectrodes. The bottom electrode of memristors consisted of 5 nm Ta and 60 nm Pt \ndeposited by sputtering in sequence.", "mimetype": "text/plain", "start_char_idx": 503, "end_char_idx": 926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c56bc08-cdc6-4738-8d7a-ef17a91a3747": {"__data__": {"id_": "2c56bc08-cdc6-4738-8d7a-ef17a91a3747", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27d30242-4c4d-4aa8-b422-37bcb0a3265b", "node_type": "1", "metadata": {}, "hash": "b4ce7d88b81f396c48f4d3b3191bc257366fd6ffd725d7a4da1f4c9fc994dc45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ae09193-1dc8-49e9-abb4-0b2a232d034f", "node_type": "1", "metadata": {}, "hash": "99c508c186bbbe3e788186eebaa17ceeb2a20bbb0d5e9d31d420348ea696c73f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The 10 nm TaOx switching layer was deposited \nby sputtering a Ta2O5 target at room temperature in Ar plasma, and patterned by \nreactive ion etching using CHF3 and O2. The memristor top electrodes comprised \n50 nm Ta and 20 nm Pd, deposited by sputtering in sequence. The memristor \nintegration was done in our lab. All metals deposited in the lab were patterned by \nlift-off, with a memristor junction size of 4 \u00d7 4 \u03bcm2.\nThe hybrid analogue\u2013digital platform.", "mimetype": "text/plain", "start_char_idx": 927, "end_char_idx": 1385, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ae09193-1dc8-49e9-abb4-0b2a232d034f": {"__data__": {"id_": "2ae09193-1dc8-49e9-abb4-0b2a232d034f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c56bc08-cdc6-4738-8d7a-ef17a91a3747", "node_type": "1", "metadata": {}, "hash": "21c838ecccbb304c978a6d7cf5f5d02381d243cc289ebc9e92e6fb02fc2051f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10d9dc49-bb2d-4eef-973c-f863b9f8728f", "node_type": "1", "metadata": {}, "hash": "0965c97f59f032d95d91ffb401d7156066db2b2d4fdb0113b850abce0c85571c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A customized hybrid analogue\u2013digital \nplatform was used to electrically read and write the 1T1R chip24,25,51. The system \nprovides 128 + 64 + 64 ways of concurrent analogue voltages with a minimum pulse \nwidth of ~100 ns and an amplitude of up to \u00b110 V. It could also sense 64 ways  \nFig. 4 | In situ training of the 1T1R based ConvLSTM.", "mimetype": "text/plain", "start_char_idx": 1386, "end_char_idx": 1723, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10d9dc49-bb2d-4eef-973c-f863b9f8728f": {"__data__": {"id_": "10d9dc49-bb2d-4eef-973c-f863b9f8728f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ae09193-1dc8-49e9-abb4-0b2a232d034f", "node_type": "1", "metadata": {}, "hash": "99c508c186bbbe3e788186eebaa17ceeb2a20bbb0d5e9d31d420348ea696c73f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3d022ba-01bb-462d-a949-c537174c1bba", "node_type": "1", "metadata": {}, "hash": "21514be7fc7a180ddcd6130e8ccf5db9a52546c89c8a883e585113fca1e599f1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "a,b, The smoothed experimental mini-batch accuracy increased (a) and loss decreased (b) over the \ncourse of in situ training in classifying the MNIST sequence. The experimental curves, the simulation with programming noise and the ideal programming \nsimulation are close to each other. c,d, The Gaussian-like distribution of weights before and after the in situ training. The distributions were broadened \nby the training of the ConvLSTM (c) and fully connected layers (d), consistent with those shown in Fig. 2d\u2013f.", "mimetype": "text/plain", "start_char_idx": 1724, "end_char_idx": 2239, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3d022ba-01bb-462d-a949-c537174c1bba": {"__data__": {"id_": "a3d022ba-01bb-462d-a949-c537174c1bba", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10d9dc49-bb2d-4eef-973c-f863b9f8728f", "node_type": "1", "metadata": {}, "hash": "0965c97f59f032d95d91ffb401d7156066db2b2d4fdb0113b850abce0c85571c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecf94287-c9ad-47f3-9563-9570446d9dce", "node_type": "1", "metadata": {}, "hash": "90df5f4025c3d96afe1d46c5a22a79148825ef3ec42a86d655b86f10963016a7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "e, Representative inference examples of \nthe correctly classified sequence (top) and misclassified sequence (bottom) after the in situ training. The middle panels show the raw currents collected \nby the six output neurons (blue, red and orange bars correspond to time steps 1, 2 and 3, respectively). The right panels show the associated Bayesian \nprobabilities calculated by the softmax activation (of the last time step only). All inference examples are with Supplementary Video 4.", "mimetype": "text/plain", "start_char_idx": 2240, "end_char_idx": 2723, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecf94287-c9ad-47f3-9563-9570446d9dce": {"__data__": {"id_": "ecf94287-c9ad-47f3-9563-9570446d9dce", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3d022ba-01bb-462d-a949-c537174c1bba", "node_type": "1", "metadata": {}, "hash": "21514be7fc7a180ddcd6130e8ccf5db9a52546c89c8a883e585113fca1e599f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd8f4cce-e318-4425-8b99-74a073449928", "node_type": "1", "metadata": {}, "hash": "982b6172090c218281a2e7a0bae18ccd19b13c8b5a9c25c89639bb11c38c03ad", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "0\n1\n0\n100\n0\n1\n0\n100\n1\u20132\u20133\n1\u20133\u20132\n2\u20131\u20133\n2\u20133\u20131\n3\u20131\u20132\n3\u20132\u20131\nNeurons\n1\u20132\u20133\n1\u20133\u20132\n2\u20131\u20133\n2\u20133\u20131\n3\u20131\u20132\n3\u20132\u20131\nNeurons\nProbability\nCurrent (uA)\nProbability density (%)\n\u20132\nWeight\n0\n2\n4\n\u20134\nProbability density", "mimetype": "text/plain", "start_char_idx": 2724, "end_char_idx": 2919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd8f4cce-e318-4425-8b99-74a073449928": {"__data__": {"id_": "dd8f4cce-e318-4425-8b99-74a073449928", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecf94287-c9ad-47f3-9563-9570446d9dce", "node_type": "1", "metadata": {}, "hash": "90df5f4025c3d96afe1d46c5a22a79148825ef3ec42a86d655b86f10963016a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1004139-76ac-4192-8ab2-db2e5335eadc", "node_type": "1", "metadata": {}, "hash": "6c6cddabcbc20dd9614a9b728d4ca89e8924bc4b9bae057ba3b3395e8af452d4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(%)\n\u20132\nWeight\n0\n2\n4\n\u20134\nPre-training\nPost-training\nPre-training\nPost-training\n0\n5\n10\n15\n20\n25\n0\n5\n10\n15\n20\n25\n10\n\u20132\n10\n0\n10\n1\n10\n2\n10\n\u20131\n10\n0\n10\n1\nAveraged loss\nMini-batches\nNoise-free simulation\nSimulation with programming noise\nExperiment\n0\n100\n200\nMini-batches\n0\n20\n40\n60\n80\n100\nAveraged training", "mimetype": "text/plain", "start_char_idx": 2920, "end_char_idx": 3218, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1004139-76ac-4192-8ab2-db2e5335eadc": {"__data__": {"id_": "f1004139-76ac-4192-8ab2-db2e5335eadc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd8f4cce-e318-4425-8b99-74a073449928", "node_type": "1", "metadata": {}, "hash": "982b6172090c218281a2e7a0bae18ccd19b13c8b5a9c25c89639bb11c38c03ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e1eae16-775e-408e-95b8-11187a140a24", "node_type": "1", "metadata": {}, "hash": "89a3c6d415fea0af804e94a4bdcbf2e83c5376edebe75da131e2df2d2693d6ee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "accuracy (%)\nNoise-free simulation\nSimulation with programming noise\nExperiment\na\nb\nc\nd\ne\nNature Machine Intelligence | VOL 1 | SEPTEMBER 2019 | 434\u2013442 | www.nature.com/natmachintell\n440\nArticles\nNature Machine Intelligence\nof analogue current signals at the same time (see Supplementary Fig. 1 for the \nfunctional block diagram of the system).\nEach bit line or word line could be independently configured for voltage \nbiasing, ground or high impedance.", "mimetype": "text/plain", "start_char_idx": 3219, "end_char_idx": 3673, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e1eae16-775e-408e-95b8-11187a140a24": {"__data__": {"id_": "9e1eae16-775e-408e-95b8-11187a140a24", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1004139-76ac-4192-8ab2-db2e5335eadc", "node_type": "1", "metadata": {}, "hash": "6c6cddabcbc20dd9614a9b728d4ca89e8924bc4b9bae057ba3b3395e8af452d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0959c3a-c953-4561-8247-19e33ccf2761", "node_type": "1", "metadata": {}, "hash": "780b0b73db7fca3c3c9211fb8589bf961b42319a2028c0493f4b6714d026fa62", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For voltage biasing, each line was driven by \na channel of a digital-to-analogue converter (LTC2668, Analog Devices, 16-bit \nresolution), which communicated with the microcontroller via digital I/O ports.\nEach memristor TE line could be independently configured for voltage \nbiasing, ground, high-impedance or current sensing. The voltage biasing was \nimplemented in the same way with that of the bit or word lines.", "mimetype": "text/plain", "start_char_idx": 3674, "end_char_idx": 4089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0959c3a-c953-4561-8247-19e33ccf2761": {"__data__": {"id_": "c0959c3a-c953-4561-8247-19e33ccf2761", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e1eae16-775e-408e-95b8-11187a140a24", "node_type": "1", "metadata": {}, "hash": "89a3c6d415fea0af804e94a4bdcbf2e83c5376edebe75da131e2df2d2693d6ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bbaa51b-e138-417b-9958-9c9a69e0da00", "node_type": "1", "metadata": {}, "hash": "e2a9e7bbe973a5d74582c139bb04dab50106a895e964748b876271f898c61ab6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For current \nsensing, each memristor TE line connected to one of the four transimpedance \namplifiers (LTC6268, Analog Devices) with different gains. The voltage outputs \nof the transimpedance amplifiers were read by the analogue-to-digital converters \n(MAX11046, Maxim Integrated, 16-bit resolution) and retrieved by the \nmicrocontroller.\nBasic 1T1R operations. The basic electrical operations of the 1T1R memristor \narray include analogue SET programming, conductance readout and vector\u2013matrix \nmultiplication.", "mimetype": "text/plain", "start_char_idx": 4090, "end_char_idx": 4601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bbaa51b-e138-417b-9958-9c9a69e0da00": {"__data__": {"id_": "9bbaa51b-e138-417b-9958-9c9a69e0da00", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0959c3a-c953-4561-8247-19e33ccf2761", "node_type": "1", "metadata": {}, "hash": "780b0b73db7fca3c3c9211fb8589bf961b42319a2028c0493f4b6714d026fa62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "258a74f5-76f2-47b7-ac21-46fc4e1309bd", "node_type": "1", "metadata": {}, "hash": "d6a4699b41ed55cb9de69a91d2cd453672849251c6eb995eaf7ce29cc84b40f5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For analogue SET programming, the 1T1R array was programmed row-by-row \n(see the flowchart in Supplementary Fig. 10a). To program a selected row, all bit \nlines were floating, except the selected one, which was grounded. Each word line \nwas assigned a different voltage on the basis of the targeted conductance.  \nAll memristor TE lines were biased with the same VSET voltages. The analogue \nSET programming of memristors of the same row is parallel.", "mimetype": "text/plain", "start_char_idx": 4602, "end_char_idx": 5052, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "258a74f5-76f2-47b7-ac21-46fc4e1309bd": {"__data__": {"id_": "258a74f5-76f2-47b7-ac21-46fc4e1309bd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bbaa51b-e138-417b-9958-9c9a69e0da00", "node_type": "1", "metadata": {}, "hash": "e2a9e7bbe973a5d74582c139bb04dab50106a895e964748b876271f898c61ab6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f64707f-be8e-4102-aaf5-2d5cb0eda120", "node_type": "1", "metadata": {}, "hash": "686c3de4c14eef370c3a425ecb492406f36d305346e923a9e9191e2b1c1612da", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For weight readout, \nthe row electrodes output a scaled identity voltage matrix in 128 time steps (that \nis, the only non-zero input VRead at time step i is with the ith bit line). Within each \ntime step, the currents of all column wires were read by the microcontroller (see \nthe flowchart in Supplementary Fig. 10b). For vector\u2013matrix multiplications, the \nbit lines output the vector V . The currents of all column wires were read by the \nmicrocontroller (see the flowchart in Supplementary Fig. 10c).", "mimetype": "text/plain", "start_char_idx": 5053, "end_char_idx": 5557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f64707f-be8e-4102-aaf5-2d5cb0eda120": {"__data__": {"id_": "5f64707f-be8e-4102-aaf5-2d5cb0eda120", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "258a74f5-76f2-47b7-ac21-46fc4e1309bd", "node_type": "1", "metadata": {}, "hash": "d6a4699b41ed55cb9de69a91d2cd453672849251c6eb995eaf7ce29cc84b40f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7655e661-672a-421d-9b4e-b152784cd4c9", "node_type": "1", "metadata": {}, "hash": "8ee2d80347c93f6694655b9f778aa5d5bbe15ade6a9e438dc3c426c6a1ee0ce8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "More advanced \nfunctions such as hidden neurons were implemented by employing multiple basic \n1T1R operations (see Supplementary Fig. 1).\nTraining method. For both the CNN and the ConvLSTM networks, the  \nforward passes were physically implemented on the hybrid analogue\u2013digital \ncomputing platforms (see Supplementary Fig. 11). The cross-entropy loss \nfunction was used to define the error.", "mimetype": "text/plain", "start_char_idx": 5558, "end_char_idx": 5949, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7655e661-672a-421d-9b4e-b152784cd4c9": {"__data__": {"id_": "7655e661-672a-421d-9b4e-b152784cd4c9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f64707f-be8e-4102-aaf5-2d5cb0eda120", "node_type": "1", "metadata": {}, "hash": "686c3de4c14eef370c3a425ecb492406f36d305346e923a9e9191e2b1c1612da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c7a3f89-0ade-4ade-8648-c33d8b8c0dfb", "node_type": "1", "metadata": {}, "hash": "028780974dea353c57e7aa98ea8f8e25ae2c399c90736459b76bee39d08dfd89", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To minimize the error on a set of  \ntraining samples, the error derivatives with the trainable parameters \nwere computed by the standard backpropagation52 (for CNN) or iterative \nbackpropagation through time53 (for ConvLSTM) for each sample and \naccumulated over the mini-batch. The error derivatives over the weights \nwere used to calculate the required change in each weight using the RMSprop \noptimizer47 for every mini-batch.\nThe calculated weight update was first mapped to the changes of the \nmemristor conductance by multiplying a constant factor Rgw (see Table 1).", "mimetype": "text/plain", "start_char_idx": 5950, "end_char_idx": 6522, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c7a3f89-0ade-4ade-8648-c33d8b8c0dfb": {"__data__": {"id_": "2c7a3f89-0ade-4ade-8648-c33d8b8c0dfb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7655e661-672a-421d-9b4e-b152784cd4c9", "node_type": "1", "metadata": {}, "hash": "8ee2d80347c93f6694655b9f778aa5d5bbe15ade6a9e438dc3c426c6a1ee0ce8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be30074a-f899-4826-b5cc-484370ce5dd5", "node_type": "1", "metadata": {}, "hash": "cf41af029021b94601c6368e382bab44d7c90b7abf063fdca483cbbcaa12d683", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Each \nweight, (g+ \u2013 g\u2212)/Rgw, is proportional to the mean conductance difference (g+ \u2013 g\u2212) \nbetween one or more pairs of memristors (here we used two pairs of memristors \nfrom the same row to further improve the programming accuracy37). Note that \nthe conductance changes in the two memristors in the same differential pair \nare of the same magnitude but opposite sign.", "mimetype": "text/plain", "start_char_idx": 6523, "end_char_idx": 6891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be30074a-f899-4826-b5cc-484370ce5dd5": {"__data__": {"id_": "be30074a-f899-4826-b5cc-484370ce5dd5", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c7a3f89-0ade-4ade-8648-c33d8b8c0dfb", "node_type": "1", "metadata": {}, "hash": "028780974dea353c57e7aa98ea8f8e25ae2c399c90736459b76bee39d08dfd89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d112972-09cb-4a3d-8e7e-8ab58bc4ab90", "node_type": "1", "metadata": {}, "hash": "daa2b458c36dc9fc4303ed281efa31ae9dee85d0c9b723688e1d1b7651d1afe0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The changes in the memristor \nconductance were then converted to the changes in the transistor gate voltages \nby multiplying by the constant factor Rvg (see Table 1). The new gate voltages  \nfor SET programming were the sums of the calculated changes of the gate \nvoltages and the very last gate voltages, bounded to the interval [Vg,min, Vg,max] \n(see Table 1). The new gate voltages were subsequently used to program the \n1T1R memristor array.\nResized MNIST dataset and the MNIST sequence dataset.", "mimetype": "text/plain", "start_char_idx": 6892, "end_char_idx": 7391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d112972-09cb-4a3d-8e7e-8ab58bc4ab90": {"__data__": {"id_": "7d112972-09cb-4a3d-8e7e-8ab58bc4ab90", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be30074a-f899-4826-b5cc-484370ce5dd5", "node_type": "1", "metadata": {}, "hash": "cf41af029021b94601c6368e382bab44d7c90b7abf063fdca483cbbcaa12d683", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f208bd4-4dbb-4411-bce7-827a842a29e1", "node_type": "1", "metadata": {}, "hash": "e921ce402a7705e32bec89adb8a94bc58543fd973dfc6f420185ff83d501ad9c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To fit the dimension \nof the 1T1R array, the original MNIST dataset images were resized from 28 \u00d7 28 to \n20 \u00d7 20 pixels by trimming the edges of the original images and keeping the central \nzones. The chopped images were then downsampled to 8 \u00d7 8 pixels using bicubic \ninterpolation for all 60,000 training samples and 10,000 test samples.", "mimetype": "text/plain", "start_char_idx": 7392, "end_char_idx": 7731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f208bd4-4dbb-4411-bce7-827a842a29e1": {"__data__": {"id_": "4f208bd4-4dbb-4411-bce7-827a842a29e1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d112972-09cb-4a3d-8e7e-8ab58bc4ab90", "node_type": "1", "metadata": {}, "hash": "daa2b458c36dc9fc4303ed281efa31ae9dee85d0c9b723688e1d1b7651d1afe0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13663ab3-c81f-4cb9-9074-7784689f73c3", "node_type": "1", "metadata": {}, "hash": "692ed40e179de13df63895310d5023af3890cdadb51c15ef6c6a362cda0b9b66", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The MNIST-sequence was generated by first creating a random sequence of \nthree digits 1, 2 or 3 (that is, one of the following six sequences: 1\u20132\u20133, 1\u20133\u20132, 2\u20131\u2013\n3, 2\u20133\u20131, 3\u20131\u20132, and 3\u20132\u20131). The first frame would be the next unused resized \nMNIST image corresponding to the first digit in the generated sequence. The same \nprocess would apply to the generation of the second and the third frames.", "mimetype": "text/plain", "start_char_idx": 7732, "end_char_idx": 8127, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13663ab3-c81f-4cb9-9074-7784689f73c3": {"__data__": {"id_": "13663ab3-c81f-4cb9-9074-7784689f73c3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f208bd4-4dbb-4411-bce7-827a842a29e1", "node_type": "1", "metadata": {}, "hash": "e921ce402a7705e32bec89adb8a94bc58543fd973dfc6f420185ff83d501ad9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b297d62-4349-4174-921b-e1631ae968e7", "node_type": "1", "metadata": {}, "hash": "34589e9c82f1f89cfaa282834be07b06cc4f4ff19867d0ecf44f49a1f9cea499", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The \nused images were not reused, so each sequence comprised exclusive frames. This \ngeneration process was repeated until the images of a certain digit were used up, \nresulting in an MNIST-sequence dataset with 5,958 training sequences and 1,010 \ntesting sequences.\nAll input images are linearly mapped to the voltage range [\u22120.2 V, 0.2 V]; these \nvoltages were applied to the bit lines of the 1T1R array.\nHyperparameters.", "mimetype": "text/plain", "start_char_idx": 8128, "end_char_idx": 8551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b297d62-4349-4174-921b-e1631ae968e7": {"__data__": {"id_": "6b297d62-4349-4174-921b-e1631ae968e7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods", "node_type": "4", "metadata": {}, "hash": "0abfbd9f2adf65f286d3b36997a57b5595fa372b21e0b094f5bc92ff9a37a8a1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13663ab3-c81f-4cb9-9074-7784689f73c3", "node_type": "1", "metadata": {}, "hash": "692ed40e179de13df63895310d5023af3890cdadb51c15ef6c6a362cda0b9b66", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1038/s42256-019-0089-1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Table 1 shows the hyperparameters used for the in situ \ntraining, including the hyperparameters for the neural network and the hardware \nparameters for 1T1R crossbar operation.", "mimetype": "text/plain", "start_char_idx": 8552, "end_char_idx": 8728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10.1038/s42256-019-0089-1": {"__data__": {"id_": "10.1038/s42256-019-0089-1", "embedding": null, "metadata": {"node_type": "doi_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"4": {"node_id": "root_node", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "536ef344-8e87-42a9-8b3f-464e6060d2fb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1185fde7-b191-4e05-95e3-574de68e57a5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "66e0628e-0300-434a-a4a1-1c934d2d3e75", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c98cffa1-6f5f-46c6-a338-04a6ce47bcae", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "165cb3fc-631f-4a76-b259-23c8e922c438", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4df2cc98-fc6b-478e-ad27-c14cbc113043", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "92dc3828-21d4-4749-b6f0-f58d4d16ebf6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8f607fd6-7d9b-4ec8-9958-9eb7e53ac6c4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cf706117-2f30-4d1b-8e70-4c70d699cb36", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "60295af4-15ab-4289-8fcd-15c3df4438c5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cde84fb6-6a66-474d-b152-d7492a2b559b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e0c9e927-0412-4f5c-88d7-602dd668ac86", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c920e1de-67e5-4586-92f4-303473cc251b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7b64ffab-b826-4bd6-b7eb-c7d703fb0b54", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d2dd7cdf-1e30-4fb6-aa1c-37031f2c133b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bc73a7d3-1d11-473a-a9c6-ce0c57836a45", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a08b3032-0ea0-4c33-9e14-cf48b649224d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "76540f75-6473-490d-9e0a-24bb98bd3c9f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3286bb37-a2dd-405b-a19b-eb4aa9fe19d2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "504b8543-f5e3-4956-b5b4-054aea818f56", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fc5cfc83-cc62-464d-96a9-8c0a0cd297b7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "69ec8af6-47b1-4544-893c-f5b3366afb8e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9131e166-368c-4b73-b994-da90529f11b9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "47d3df59-f137-42b0-af4c-cb0db935322e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f1e90641-491e-4fb6-b147-00fe6a52a306", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c7db0e85-3af6-4128-9788-488b573a682c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "54da2ae5-79b9-4447-ab32-d643dfe0f5ae", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8c7f6748-59ee-43c6-9929-40cdfe9fb37f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e3310836-8b30-40e7-8195-2e8c47ec8037", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ddff0ee4-5311-4775-9ea0-6138d6a11005", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c4f96757-a6b6-410a-80c8-35247ddf2999", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "259bf45e-5516-4e01-b7dc-551b72742212", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e856bd5e-e4b6-4762-b2f8-ec13b0c91a12", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f18737d2-9be6-430c-9e16-d76a6eb0c46d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c71e47e0-f3fd-4832-81ad-006424afb3cf", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "be01b8ee-0b58-43f9-a7c5-12f10dbb9280", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3be32cca-1f81-4ead-be6a-81e55c832377", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c0b76408-e9a5-4c25-83d9-4e0b7ebf70e9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6411a181-99b2-41fc-9b30-c3ab03677adb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "21b6acde-09d6-4740-82b9-80a89aa41d61", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bd9a2a7d-8b7f-402a-bd96-c8bea4a3ed32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c19dc867-d31f-474a-8a57-e3d1a340f2a6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0b7ba1a6-ae1d-497e-b9e6-0a3d2caf8358", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "182a9b13-3a7b-4083-8c86-a509291959a9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2c29c70a-7ce6-472c-9d07-dff88547737b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "323f77ef-7bc6-4196-9426-d0183681bd5e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fadecbb8-3c29-4374-98f7-bc2a855fe909", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "92aaea5d-b8e4-4510-b616-babdc0d59c77", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "71f930f0-880b-4c52-ae4a-9e8fd15f5dae", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0e2322e9-a46d-4f13-b539-91639ee755ba", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "acea037c-6e2d-4b7b-a44b-1d1756ff2902", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "79dfc1a5-b3f8-4ac4-b467-e023bd6f2abb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "36ec4795-2693-4d92-93fe-b884bd1d14d7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ff108c4f-bcef-4bf5-b316-47c86e4507d6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "da9c94a9-8bd5-49e3-bd4e-2abd6582c42c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "288fcfa9-ad7b-4df6-842c-87bba2c7fe9f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4e37d045-1339-4f70-893e-74a7b3eb9f18", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ca62d585-1b19-400d-86b4-61a818687be5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "87c50d04-f39e-4ca9-8228-a8bea1179753", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a81a973d-e24f-40a7-8bc0-1dc20fade299", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cbc44d52-c54b-4635-a402-78c57db51d24", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b7f02b11-382b-4227-99a7-b687d909489d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ffd3229e-9f7d-4d64-8474-80f5f682a9ee", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bdda81c6-7282-4b9d-9cf8-f0f7438de133", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cf9d4265-4d94-4829-91a9-78d555b55418", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3c9885da-d240-4dc5-8dab-71a7fe893431", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "812d1682-2766-4ee2-ae8d-de8f8c573c84", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "52c140fb-0bb9-4b80-8cbf-21f9900257b6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "922eca02-2805-4715-8c99-033021baf0b9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5b87c2e4-e1ba-4c3c-b6e8-5423cfeb7ab7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9274bfc9-e30e-498a-bb77-2d987c9fab45", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ff5c10cc-032d-427b-92ed-c18e28956e1b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d2d97ba5-52ed-4a25-9c95-0509f6204908", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cd13a164-d1ed-40b7-8e48-7fce828b8be6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c7375df0-64e7-4bb3-8f68-49d58f32ecbb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "358a3ab9-d84d-4f31-8068-0e2ae55137b5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "03281470-88d8-4425-ae40-9c2d14940f75", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8ad2433a-caf2-4cd7-bd7d-b6613918c6de", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "967f9469-77fd-4076-bc9d-a8858767f772", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d5bd77f8-aa6b-471d-9ff0-314f33009f44", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "95b963d8-770e-404f-8ff9-8adc0248cac1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "23b05784-67a8-41cb-a160-5ff6aebf8f83", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9315cfbb-c202-43c8-9ffe-649de42faeac", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "27d30242-4c4d-4aa8-b422-37bcb0a3265b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2c56bc08-cdc6-4738-8d7a-ef17a91a3747", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2ae09193-1dc8-49e9-abb4-0b2a232d034f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "10d9dc49-bb2d-4eef-973c-f863b9f8728f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a3d022ba-01bb-462d-a949-c537174c1bba", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ecf94287-c9ad-47f3-9563-9570446d9dce", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "dd8f4cce-e318-4425-8b99-74a073449928", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f1004139-76ac-4192-8ab2-db2e5335eadc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9e1eae16-775e-408e-95b8-11187a140a24", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c0959c3a-c953-4561-8247-19e33ccf2761", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9bbaa51b-e138-417b-9958-9c9a69e0da00", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "258a74f5-76f2-47b7-ac21-46fc4e1309bd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5f64707f-be8e-4102-aaf5-2d5cb0eda120", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7655e661-672a-421d-9b4e-b152784cd4c9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2c7a3f89-0ade-4ade-8648-c33d8b8c0dfb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "be30074a-f899-4826-b5cc-484370ce5dd5", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7d112972-09cb-4a3d-8e7e-8ab58bc4ab90", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4f208bd4-4dbb-4411-bce7-827a842a29e1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "13663ab3-c81f-4cb9-9074-7784689f73c3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6b297d62-4349-4174-921b-e1631ae968e7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}]}, "text": "DOI: 10.1038/s42256-019-0089-1", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "653d640e-3b26-4d0a-a02c-540557ceb834": {"__data__": {"id_": "653d640e-3b26-4d0a-a02c-540557ceb834", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd2537c7-0e85-4c8b-983f-e8ad539e4230", "node_type": "1", "metadata": {}, "hash": "df8283f99a2251578dd3e2fb5696138199c76922de20fa3d05d77b25295a9359", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "References\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al.\nTensorflow: Large-scale machine learning on heterogeneous\nsystems, 2015. Software available from tensorflow. org, 1,\n2015.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 251, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd2537c7-0e85-4c8b-983f-e8ad539e4230": {"__data__": {"id_": "bd2537c7-0e85-4c8b-983f-e8ad539e4230", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "653d640e-3b26-4d0a-a02c-540557ceb834", "node_type": "1", "metadata": {}, "hash": "63e954eb41c2a9f3d936a922aa9e1762261e5884b81d1828ecfc07d6d5064031", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f917288b-5cfc-4872-b640-40623ce72f6c", "node_type": "1", "metadata": {}, "hash": "26769c83f1412e195d082b3bce7fb64709abac5520ade8ed221018e2bd1c0d04", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5, 11, 12, 13\n[2] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and\nY. Chen.\nCompressing neural networks with the hashing\ntrick. CoRR, abs/1504.04788, 2015. 1\n[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. Imagenet: A large-scale hierarchical image database.", "mimetype": "text/plain", "start_char_idx": 252, "end_char_idx": 535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f917288b-5cfc-4872-b640-40623ce72f6c": {"__data__": {"id_": "f917288b-5cfc-4872-b640-40623ce72f6c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd2537c7-0e85-4c8b-983f-e8ad539e4230", "node_type": "1", "metadata": {}, "hash": "df8283f99a2251578dd3e2fb5696138199c76922de20fa3d05d77b25295a9359", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f6bbcc6-e4c7-422e-acfb-62438c84b885", "node_type": "1", "metadata": {}, "hash": "ef2f0eb2f85e485b6c5cab19ebff1601f5426a67c75298cf4d9164dcf7097c1d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In Computer Vision and Pattern Recognition, 2009. CVPR\n2009. IEEE Conference on, pages 248\u2013255. IEEE, 2009. 2,\n11\n[4] Y. Gong, L. Liu, M. Yang, and L. Bourdev.\nCompress-\ning deep convolutional networks using vector quantization.\narXiv preprint arXiv:1412.6115, 2014. 1\n[5] Google.\nTensorFlow\nLite.\nhttps://www.\ntensorflow.org/mobile/tflite.", "mimetype": "text/plain", "start_char_idx": 536, "end_char_idx": 876, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f6bbcc6-e4c7-422e-acfb-62438c84b885": {"__data__": {"id_": "8f6bbcc6-e4c7-422e-acfb-62438c84b885", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f917288b-5cfc-4872-b640-40623ce72f6c", "node_type": "1", "metadata": {}, "hash": "26769c83f1412e195d082b3bce7fb64709abac5520ade8ed221018e2bd1c0d04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee6ca9d3-bce1-4235-8990-2726bd2659b8", "node_type": "1", "metadata": {}, "hash": "dd88ff5db609f3b3e4cbfc0bef97352ede97457ab347d83a8034a85211381e13", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2, 3, 4, 11\n[6] G. Guennebaud, B. Jacob, et al.\nEigen v3.\nhttp://\neigen.tuxfamily.org. 6\n[7] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan.\nDeep learning with limited numerical precision.\nIn Pro-\nceedings of the 32nd International Conference on Machine\nLearning (ICML-15), pages 1737\u20131746, 2015.", "mimetype": "text/plain", "start_char_idx": 877, "end_char_idx": 1184, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee6ca9d3-bce1-4235-8990-2726bd2659b8": {"__data__": {"id_": "ee6ca9d3-bce1-4235-8990-2726bd2659b8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f6bbcc6-e4c7-422e-acfb-62438c84b885", "node_type": "1", "metadata": {}, "hash": "ef2f0eb2f85e485b6c5cab19ebff1601f5426a67c75298cf4d9164dcf7097c1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b43bc75-11ea-4abe-b7dd-ddb3074a2255", "node_type": "1", "metadata": {}, "hash": "21d581d75d371e2730e17530a9b680f46ef4a52359fec6f0211cc604b3248066", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2\n[8] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-\npressing deep neural network with pruning, trained quantiza-\ntion and huffman coding. CoRR, abs/1510.00149, 2, 2015.\n1\n[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n770\u2013778, 2016.", "mimetype": "text/plain", "start_char_idx": 1185, "end_char_idx": 1559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b43bc75-11ea-4abe-b7dd-ddb3074a2255": {"__data__": {"id_": "2b43bc75-11ea-4abe-b7dd-ddb3074a2255", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee6ca9d3-bce1-4235-8990-2726bd2659b8", "node_type": "1", "metadata": {}, "hash": "dd88ff5db609f3b3e4cbfc0bef97352ede97457ab347d83a8034a85211381e13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fbb6874-d5b4-4ab0-90cc-0ac8f1558ca2", "node_type": "1", "metadata": {}, "hash": "3d3a53ae7996202e03059df2e06b4e8884c168f545755b612709138bee96cd34", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "6\n[10] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam. Mobilenets: Effi-\ncient convolutional neural networks for mobile vision appli-\ncations. CoRR, abs/1704.04861, 2017.", "mimetype": "text/plain", "start_char_idx": 1560, "end_char_idx": 1777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fbb6874-d5b4-4ab0-90cc-0ac8f1558ca2": {"__data__": {"id_": "5fbb6874-d5b4-4ab0-90cc-0ac8f1558ca2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b43bc75-11ea-4abe-b7dd-ddb3074a2255", "node_type": "1", "metadata": {}, "hash": "21d581d75d371e2730e17530a9b680f46ef4a52359fec6f0211cc604b3248066", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65c79469-0da8-4033-a9f8-47ed14779a78", "node_type": "1", "metadata": {}, "hash": "72f1864fdc2dd8db8d97f94468eeadc481391321c42c294b47467417bf83d3bd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1, 2, 7, 8, 12, 13\n[11] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Wein-\nberger. Densely connected convolutional networks. In The\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), July 2017. 1\n[12] J. Huang, V. Rathod, D. Chow, C. Sun, and M. Zhu. Tensor-\nflow object detection api, 2017.", "mimetype": "text/plain", "start_char_idx": 1778, "end_char_idx": 2089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65c79469-0da8-4033-a9f8-47ed14779a78": {"__data__": {"id_": "65c79469-0da8-4033-a9f8-47ed14779a78", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fbb6874-d5b4-4ab0-90cc-0ac8f1558ca2", "node_type": "1", "metadata": {}, "hash": "3d3a53ae7996202e03059df2e06b4e8884c168f545755b612709138bee96cd34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37013a73-33c7-4234-ad58-5384ea2073b4", "node_type": "1", "metadata": {}, "hash": "7c127525907c19c8394d4b660c876d26e088fedd6848a6bdce671c2f5159ccc1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "7\n[13] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,\nA. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al.\nSpeed/accuracy trade-offs for modern convolutional object\ndetectors. arXiv preprint arXiv:1611.10012, 2016.", "mimetype": "text/plain", "start_char_idx": 2090, "end_char_idx": 2320, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37013a73-33c7-4234-ad58-5384ea2073b4": {"__data__": {"id_": "37013a73-33c7-4234-ad58-5384ea2073b4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65c79469-0da8-4033-a9f8-47ed14779a78", "node_type": "1", "metadata": {}, "hash": "72f1864fdc2dd8db8d97f94468eeadc481391321c42c294b47467417bf83d3bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66a6267c-c7d1-40c5-b758-a556fbd81f81", "node_type": "1", "metadata": {}, "hash": "35dd2629b6a55076cef1c19bc422df39dd595da33e25362ab8f12f4e26ce2890", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "12\n[14] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and\nY. Bengio. Binarized neural networks. In Advances in neural\ninformation processing systems, pages 4107\u20134115, 2016. 1,\n2\n[15] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and\nY. Bengio. Quantized neural networks: Training neural net-\nworks with low precision weights and activations.", "mimetype": "text/plain", "start_char_idx": 2321, "end_char_idx": 2672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66a6267c-c7d1-40c5-b758-a556fbd81f81": {"__data__": {"id_": "66a6267c-c7d1-40c5-b758-a556fbd81f81", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37013a73-33c7-4234-ad58-5384ea2073b4", "node_type": "1", "metadata": {}, "hash": "7c127525907c19c8394d4b660c876d26e088fedd6848a6bdce671c2f5159ccc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6aefbcb-2b38-465d-a642-bc3904f2a37e", "node_type": "1", "metadata": {}, "hash": "26bff22cbf38d0a173e183cab5a44e2cefeba0d0e2565ad291aeda0bedb9007b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "arXiv\npreprint arXiv:1609.07061, 2016. 6\n[16] F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J.\nDally, and K. Keutzer. Squeezenet: Alexnet-level accuracy\nwith 50x fewer parameters and\u00a1 1mb model size.\narXiv\npreprint arXiv:1602.07360, 2016.", "mimetype": "text/plain", "start_char_idx": 2673, "end_char_idx": 2919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6aefbcb-2b38-465d-a642-bc3904f2a37e": {"__data__": {"id_": "c6aefbcb-2b38-465d-a642-bc3904f2a37e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66a6267c-c7d1-40c5-b758-a556fbd81f81", "node_type": "1", "metadata": {}, "hash": "35dd2629b6a55076cef1c19bc422df39dd595da33e25362ab8f12f4e26ce2890", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20e24256-4f4b-495d-834f-827477ab4680", "node_type": "1", "metadata": {}, "hash": "faa5d01542a7f83f67ee07ee31fe5e917177eee4b24342534574ead6a55c73d1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1\n[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\nIn Proceedings of the 32Nd International Conference on In-\nternational Conference on Machine Learning - Volume 37,\nICML\u201915, pages 448\u2013456. JMLR.org, 2015. 5, 6\n[18] B. Jacob, P. Warden, et al. gemmlowp: a small self-contained\nlow-precision gemm library.", "mimetype": "text/plain", "start_char_idx": 2920, "end_char_idx": 3299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20e24256-4f4b-495d-834f-827477ab4680": {"__data__": {"id_": "20e24256-4f4b-495d-834f-827477ab4680", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6aefbcb-2b38-465d-a642-bc3904f2a37e", "node_type": "1", "metadata": {}, "hash": "26bff22cbf38d0a173e183cab5a44e2cefeba0d0e2565ad291aeda0bedb9007b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f034d87-f9b3-4f17-a85f-62484b63ca3a", "node_type": "1", "metadata": {}, "hash": "bbcb8e9abe55a5aedb938c92845c6c3ca47043da54a42281b3b69e55fd3f1018", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "https://github.com/\ngoogle/gemmlowp. 2, 4, 6, 11\n[19] S. Kligys, S. Sivakumar, et al. Tensorflow quantized training\nsupport.\nhttps://github.com/tensorflow/\ntensorflow/tree/master/tensorflow/\ncontrib/quantize. 5, 6\n[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassification with deep convolutional neural networks.", "mimetype": "text/plain", "start_char_idx": 3300, "end_char_idx": 3630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f034d87-f9b3-4f17-a85f-62484b63ca3a": {"__data__": {"id_": "6f034d87-f9b3-4f17-a85f-62484b63ca3a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20e24256-4f4b-495d-834f-827477ab4680", "node_type": "1", "metadata": {}, "hash": "faa5d01542a7f83f67ee07ee31fe5e917177eee4b24342534574ead6a55c73d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbf1f363-f629-4d0a-9f84-5ea2b7e46d95", "node_type": "1", "metadata": {}, "hash": "24cc590d5bce52d752870186f0ffbda7696cace88a0ea610146d92713b7d7651", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In\nAdvances in neural information processing systems, pages\n1097\u20131105, 2012. 1\n[21] C. Leng, H. Li, S. Zhu, and R. Jin. Extremely low bit neural\nnetwork: Squeeze the last bit out with admm. arXiv preprint\narXiv:1707.09870, 2017. 1, 6\n[22] F. Li, B. Zhang, and B. Liu. Ternary weight networks.", "mimetype": "text/plain", "start_char_idx": 3631, "end_char_idx": 3923, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbf1f363-f629-4d0a-9f84-5ea2b7e46d95": {"__data__": {"id_": "bbf1f363-f629-4d0a-9f84-5ea2b7e46d95", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f034d87-f9b3-4f17-a85f-62484b63ca3a", "node_type": "1", "metadata": {}, "hash": "bbcb8e9abe55a5aedb938c92845c6c3ca47043da54a42281b3b69e55fd3f1018", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c59ba73a-a77e-49a2-a465-40aaf8c49915", "node_type": "1", "metadata": {}, "hash": "d8f7805331b9fb1cda628ce1b289888703a75cfc79c2df911d9011664f4999aa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "arXiv\npreprint arXiv:1605.04711, 2016. 1, 6\n[23] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll\u00b4\nar, and C. L. Zitnick. Microsoft coco: Com-\nmon objects in context. In European conference on computer\nvision, pages 740\u2013755. Springer, 2014. 2\n[24] T.-Y.", "mimetype": "text/plain", "start_char_idx": 3924, "end_char_idx": 4204, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c59ba73a-a77e-49a2-a465-40aaf8c49915": {"__data__": {"id_": "c59ba73a-a77e-49a2-a465-40aaf8c49915", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbf1f363-f629-4d0a-9f84-5ea2b7e46d95", "node_type": "1", "metadata": {}, "hash": "24cc590d5bce52d752870186f0ffbda7696cace88a0ea610146d92713b7d7651", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7143612-55a0-481c-a46d-6878df23305d", "node_type": "1", "metadata": {}, "hash": "ac5a79179a53b1fa16b75843c93474072d0fd5bd2d253d745a374649e8fd0f04", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll\u00b4\nar, and C. L. Zitnick. Microsoft COCO: Com-\nmon objects in context. In ECCV, 2014. 7\n[25] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed.\nSsd:\nSingle shot multibox detector.", "mimetype": "text/plain", "start_char_idx": 4205, "end_char_idx": 4457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7143612-55a0-481c-a46d-6878df23305d": {"__data__": {"id_": "e7143612-55a0-481c-a46d-6878df23305d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c59ba73a-a77e-49a2-a465-40aaf8c49915", "node_type": "1", "metadata": {}, "hash": "d8f7805331b9fb1cda628ce1b289888703a75cfc79c2df911d9011664f4999aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "711754cc-08f9-4303-bea0-da807b32be97", "node_type": "1", "metadata": {}, "hash": "c5dc7303e16b92d8f9cd088ae3150c69a5d0978ba0b0cc8d635ced76cad533e3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint\narXiv:1512.02325, 2015. 7\n[26] N. Mellempudi, A. Kundu, D. Mudigere, D. Das, B. Kaul,\nand P. Dubey.\nTernary neural networks with fine-grained\nquantization. arXiv preprint arXiv:1705.01462, 2017. 1, 6\n[27] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi.", "mimetype": "text/plain", "start_char_idx": 4458, "end_char_idx": 4730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "711754cc-08f9-4303-bea0-da807b32be97": {"__data__": {"id_": "711754cc-08f9-4303-bea0-da807b32be97", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7143612-55a0-481c-a46d-6878df23305d", "node_type": "1", "metadata": {}, "hash": "ac5a79179a53b1fa16b75843c93474072d0fd5bd2d253d745a374649e8fd0f04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8cad45bb-ee52-442e-ace9-99b8a2fddd24", "node_type": "1", "metadata": {}, "hash": "fc20a6f29aa1e02e781224624f70db9fcd2b6bd89b70860cb0195ad704cf8004", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Xnor-\nnet: Imagenet classification using binary convolutional neu-\nral networks. arXiv preprint arXiv:1603.05279, 2016. 1, 2\n[28] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.", "mimetype": "text/plain", "start_char_idx": 4731, "end_char_idx": 4996, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8cad45bb-ee52-442e-ace9-99b8a2fddd24": {"__data__": {"id_": "8cad45bb-ee52-442e-ace9-99b8a2fddd24", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "711754cc-08f9-4303-bea0-da807b32be97", "node_type": "1", "metadata": {}, "hash": "c5dc7303e16b92d8f9cd088ae3150c69a5d0978ba0b0cc8d635ced76cad533e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e6df688-51de-4237-8883-c5fbceb88279", "node_type": "1", "metadata": {}, "hash": "fdf32a39f874843c4382100ae10cfc250117b82b53707380bccd552dee242580", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1\n[29] C. Szegedy,\nW. Liu,\nY. Jia,\nP. Sermanet,\nS. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npages 1\u20139, 2015. 1\n[30] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.", "mimetype": "text/plain", "start_char_idx": 4997, "end_char_idx": 5308, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e6df688-51de-4237-8883-c5fbceb88279": {"__data__": {"id_": "8e6df688-51de-4237-8883-c5fbceb88279", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8cad45bb-ee52-442e-ace9-99b8a2fddd24", "node_type": "1", "metadata": {}, "hash": "fc20a6f29aa1e02e781224624f70db9fcd2b6bd89b70860cb0195ad704cf8004", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "667a099a-d403-4699-b667-eed09ba6cd24", "node_type": "1", "metadata": {}, "hash": "375ec9bb85b4b734cdc917fe8a37208a1ec8c5937b26d466337ff82a1b3ee431", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Rethinking the inception architecture for computer vision.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2818\u20132826, 2016. 6\n[31] V. Vanhoucke, A. Senior, and M. Z. Mao. Improving the\nspeed of neural networks on cpus. In Proc. Deep Learning\nand Unsupervised Feature Learning NIPS Workshop, vol-\nume 1, page 4, 2011. 2\n[32] X. Zhang, X. Zhou, M. Lin, and J. Sun.", "mimetype": "text/plain", "start_char_idx": 5309, "end_char_idx": 5711, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "667a099a-d403-4699-b667-eed09ba6cd24": {"__data__": {"id_": "667a099a-d403-4699-b667-eed09ba6cd24", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e6df688-51de-4237-8883-c5fbceb88279", "node_type": "1", "metadata": {}, "hash": "fdf32a39f874843c4382100ae10cfc250117b82b53707380bccd552dee242580", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4dccc2d-1b59-4438-b656-96c99e766059", "node_type": "1", "metadata": {}, "hash": "d28d9ddac88957480a2e2f22267dbf88cb03c03d7df9509c4dcf472dace5df7c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Shufflenet: An\nextremely efficient convolutional neural network for mobile\ndevices. CoRR, abs/1707.01083, 2017. 1\n2712\nAuthorized licensed use limited to: Nanjing University. Downloaded on August 14,2023 at 15:45:48 UTC from IEEE Xplore.  Restrictions apply. \n[33] A. Zhou, A. Yao, Y. Guo, L. Xu, and Y. Chen. Incremen-\ntal network quantization: Towards lossless cnns with low-\nprecision weights.", "mimetype": "text/plain", "start_char_idx": 5712, "end_char_idx": 6108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4dccc2d-1b59-4438-b656-96c99e766059": {"__data__": {"id_": "c4dccc2d-1b59-4438-b656-96c99e766059", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "667a099a-d403-4699-b667-eed09ba6cd24", "node_type": "1", "metadata": {}, "hash": "375ec9bb85b4b734cdc917fe8a37208a1ec8c5937b26d466337ff82a1b3ee431", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d3e14b0-0f20-4535-9380-b09c4592ff35", "node_type": "1", "metadata": {}, "hash": "bded35f9be2080f63c05ee700980796678f310fdc6161cb353eba272704d3ef0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1702.03044, 2017.\n1, 6\n[34] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou.\nDorefa-net:\nTraining low bitwidth convolutional neural\nnetworks with low bitwidth gradients.\narXiv preprint\narXiv:1606.06160, 2016. 1, 2\n[35] C. Zhu, S. Han, H. Mao, and W. J. Dally.", "mimetype": "text/plain", "start_char_idx": 6109, "end_char_idx": 6392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d3e14b0-0f20-4535-9380-b09c4592ff35": {"__data__": {"id_": "9d3e14b0-0f20-4535-9380-b09c4592ff35", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "a07685f1acb3655c89d57d864b0439bee42b8b0e09a3ef0081c4c2115111f153", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4dccc2d-1b59-4438-b656-96c99e766059", "node_type": "1", "metadata": {}, "hash": "d28d9ddac88957480a2e2f22267dbf88cb03c03d7df9509c4dcf472dace5df7c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/cvpr.2018.00286", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Trained ternary\nquantization. arXiv preprint arXiv:1612.01064, 2016. 1\n2713\nAuthorized licensed use limited to: Nanjing University. Downloaded on August 14,2023 at 15:45:48 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 6393, "end_char_idx": 6608, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10.1109/cvpr.2018.00286": {"__data__": {"id_": "10.1109/cvpr.2018.00286", "embedding": null, "metadata": {"node_type": "doi_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"4": {"node_id": "root_node", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "653d640e-3b26-4d0a-a02c-540557ceb834", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bd2537c7-0e85-4c8b-983f-e8ad539e4230", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f917288b-5cfc-4872-b640-40623ce72f6c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8f6bbcc6-e4c7-422e-acfb-62438c84b885", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ee6ca9d3-bce1-4235-8990-2726bd2659b8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2b43bc75-11ea-4abe-b7dd-ddb3074a2255", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5fbb6874-d5b4-4ab0-90cc-0ac8f1558ca2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "65c79469-0da8-4033-a9f8-47ed14779a78", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "37013a73-33c7-4234-ad58-5384ea2073b4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "66a6267c-c7d1-40c5-b758-a556fbd81f81", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c6aefbcb-2b38-465d-a642-bc3904f2a37e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "20e24256-4f4b-495d-834f-827477ab4680", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6f034d87-f9b3-4f17-a85f-62484b63ca3a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bbf1f363-f629-4d0a-9f84-5ea2b7e46d95", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c59ba73a-a77e-49a2-a465-40aaf8c49915", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e7143612-55a0-481c-a46d-6878df23305d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "711754cc-08f9-4303-bea0-da807b32be97", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8cad45bb-ee52-442e-ace9-99b8a2fddd24", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8e6df688-51de-4237-8883-c5fbceb88279", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "667a099a-d403-4699-b667-eed09ba6cd24", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c4dccc2d-1b59-4438-b656-96c99e766059", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9d3e14b0-0f20-4535-9380-b09c4592ff35", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}]}, "text": "DOI: 10.1109/cvpr.2018.00286", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b38de36-948e-4614-99da-9f15b9629e60": {"__data__": {"id_": "8b38de36-948e-4614-99da-9f15b9629e60", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45414ee3-3cc9-40e9-b846-d8c2a93d0d31", "node_type": "1", "metadata": {}, "hash": "b75b46d92147090385acf25e55d1b0e7b1345888fbbadde61d960f2650d836b9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "References\n[1] Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,\nAndy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-\nmawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur,\nJosh Levenberg, Rajat Monga, Sherry Moore, Derek G. Mur-\nray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete War-\nden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Ten-\nsorflow: A system for large-scale machine learning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45414ee3-3cc9-40e9-b846-d8c2a93d0d31": {"__data__": {"id_": "45414ee3-3cc9-40e9-b846-d8c2a93d0d31", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b38de36-948e-4614-99da-9f15b9629e60", "node_type": "1", "metadata": {}, "hash": "0cee5e4927d64dbba3d0e30d8dda547a44202687bb3183be7edb935cb9c54eeb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ab3370c-75b2-4479-8a10-d9d83081db08", "node_type": "1", "metadata": {}, "hash": "a6e39fa516df56d84d6a406413ea0fffb926b5892b726bb29ed6ed4b9838366e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In 12th\nUSENIX Symposium on Operating Systems Design and Im-\nplementation (OSDI 16), pages 265\u2013283, 2016.\n[2] Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry.\nPost-training 4-bit quantization of convolution networks for\nrapid-deployment. arXiv preprint arXiv:1810.05723, 2018.", "mimetype": "text/plain", "start_char_idx": 405, "end_char_idx": 690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ab3370c-75b2-4479-8a10-d9d83081db08": {"__data__": {"id_": "7ab3370c-75b2-4479-8a10-d9d83081db08", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45414ee3-3cc9-40e9-b846-d8c2a93d0d31", "node_type": "1", "metadata": {}, "hash": "b75b46d92147090385acf25e55d1b0e7b1345888fbbadde61d960f2650d836b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b37585ad-1e7c-46de-916f-52c239038c42", "node_type": "1", "metadata": {}, "hash": "86661e0d7a6cc5df18edc7d080a8218f8b461880938a7d1c4827f94330a09435", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[3] Chaim Baskin, Eli Schwartz, Evgenii Zheltonozhskii, Natan\nLiss, Raja Giryes, Alex M. Bronstein, and Avi Mendelson.\nUniq: Uniform noise injection for non-uniform quantization\nof neural networks. arXiv preprint arXiv:1804.10969, 2018.\n[4] Yoshua Bengio, Nicholas Lonard, and Aaron Courville.\nEstimating or propagating gradients through stochastic\nneurons for conditional computation.", "mimetype": "text/plain", "start_char_idx": 691, "end_char_idx": 1076, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b37585ad-1e7c-46de-916f-52c239038c42": {"__data__": {"id_": "b37585ad-1e7c-46de-916f-52c239038c42", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ab3370c-75b2-4479-8a10-d9d83081db08", "node_type": "1", "metadata": {}, "hash": "a6e39fa516df56d84d6a406413ea0fffb926b5892b726bb29ed6ed4b9838366e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d1a2099-19be-4747-b251-a655a6b55164", "node_type": "1", "metadata": {}, "hash": "c515bcd22a9db3e1b880c7dd4feda229a0b44bdace89e1dcc9617931623b20f6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint\narXiv:1308.3432, 2013.\n[5] Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconce-\nlos. Deep learning with low precision by half-wave gaussian\nquantization. 2017 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), Jul 2017.", "mimetype": "text/plain", "start_char_idx": 1077, "end_char_idx": 1329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d1a2099-19be-4747-b251-a655a6b55164": {"__data__": {"id_": "8d1a2099-19be-4747-b251-a655a6b55164", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b37585ad-1e7c-46de-916f-52c239038c42", "node_type": "1", "metadata": {}, "hash": "86661e0d7a6cc5df18edc7d080a8218f8b461880938a7d1c4827f94330a09435", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2f4d688-296e-4551-aac3-d8afab3b182a", "node_type": "1", "metadata": {}, "hash": "b9a76942dc65ea3c3a2433a38cc96a948e00367eb46e88712fedf6b1602626dc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[6] Jungwook Choi, Pierce I-Jen Chuang, Zhuo Wang, Swa-\ngath Venkataramani, Vijayalakshmi Srinivasan, and Kailash\nGopalakrishnan.\nBridging the accuracy gap for 2-\nbit quantized neural networks (qnn).\narXiv preprint\narXiv:1807.06964, 2018.", "mimetype": "text/plain", "start_char_idx": 1330, "end_char_idx": 1568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2f4d688-296e-4551-aac3-d8afab3b182a": {"__data__": {"id_": "c2f4d688-296e-4551-aac3-d8afab3b182a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d1a2099-19be-4747-b251-a655a6b55164", "node_type": "1", "metadata": {}, "hash": "c515bcd22a9db3e1b880c7dd4feda229a0b44bdace89e1dcc9617931623b20f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fac5e889-761e-49d4-bbb1-31693cf7d5e3", "node_type": "1", "metadata": {}, "hash": "e57d91687ee24cf4549ec2ddf395908ea4511d707f71bf9778ec29eef2f2c03e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[7] Jungwook Choi, Zhuo Wang, Swagath Venkataramani,\nPierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash\nGopalakrishnan.\nPact:\nParameterized clipping activa-\ntion for quantized neural networks.\narXiv preprint\narXiv:1805.06085, 2018.\n[8] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre\nDavid.\nBinaryconnect:\nTraining deep neural networks\nwith binary weights during propagations.", "mimetype": "text/plain", "start_char_idx": 1569, "end_char_idx": 1958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fac5e889-761e-49d4-bbb1-31693cf7d5e3": {"__data__": {"id_": "fac5e889-761e-49d4-bbb1-31693cf7d5e3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2f4d688-296e-4551-aac3-d8afab3b182a", "node_type": "1", "metadata": {}, "hash": "b9a76942dc65ea3c3a2433a38cc96a948e00367eb46e88712fedf6b1602626dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79b56430-ee52-43f4-84f5-fa57cb3dee54", "node_type": "1", "metadata": {}, "hash": "c62051c0bf21d44e1b740847c9b60f8c3573db8b06ae998b9f6ce9eae21dc366", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint\narXiv:1511.00363, 2015.\n[9] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran\nEl-Yaniv, and Yoshua Bengio. Binarized neural networks:\nTraining deep neural networks with weights and activations\nconstrained to +1 or -1. arXiv preprint arXiv:1602.02830,\n2016.\n[10] J. Deng, W. Dong, R. Socher, L.-J.", "mimetype": "text/plain", "start_char_idx": 1959, "end_char_idx": 2274, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79b56430-ee52-43f4-84f5-fa57cb3dee54": {"__data__": {"id_": "79b56430-ee52-43f4-84f5-fa57cb3dee54", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fac5e889-761e-49d4-bbb1-31693cf7d5e3", "node_type": "1", "metadata": {}, "hash": "e57d91687ee24cf4549ec2ddf395908ea4511d707f71bf9778ec29eef2f2c03e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4565c392-9058-45dc-89b9-d22d3b261983", "node_type": "1", "metadata": {}, "hash": "fb385ffee3939925cccb60ea90c6dcfd814059eca5a2dbc4750b9cc9b6c2f82c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Li, K. Li, and L. Fei-\nFei. ImageNet: A Large-Scale Hierarchical Image Database.\n2009 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), Jul 2009.\n[11] Jiong Gong, Haihao Shen, Guoming Zhang, Xiaoli Liu,\nShane Li, Ge Jin, Niharika Maheshwari, Evarist Fomenko,\nand Eden Segal.\nHighly efficient 8-bit low precision in-\nference of convolutional neural networks with intelcaffe.", "mimetype": "text/plain", "start_char_idx": 2275, "end_char_idx": 2665, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4565c392-9058-45dc-89b9-d22d3b261983": {"__data__": {"id_": "4565c392-9058-45dc-89b9-d22d3b261983", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79b56430-ee52-43f4-84f5-fa57cb3dee54", "node_type": "1", "metadata": {}, "hash": "c62051c0bf21d44e1b740847c9b60f8c3573db8b06ae998b9f6ce9eae21dc366", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3769e20d-58e0-4c7c-a3b0-ff460a883fa2", "node_type": "1", "metadata": {}, "hash": "2c54216613e2f7061e6ede7202133aa7ce47461b0705a4db2595faa7deb16493", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Proceedings of the 1st on Reproducible Quality-Efficient\nSystems Tournament on Co-designing Pareto-efficient Deep\nLearning - ReQuEST 18, 2018.\n[12] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pe-\ndram, Mark A. Horowitz, and William J. Dally. Eie. ACM\nSIGARCH Computer Architecture News, 44(3):243254, Jun\n2016.\n[13] Song Han, Huizi Mao, and William J. Dally.", "mimetype": "text/plain", "start_char_idx": 2666, "end_char_idx": 3030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3769e20d-58e0-4c7c-a3b0-ff460a883fa2": {"__data__": {"id_": "3769e20d-58e0-4c7c-a3b0-ff460a883fa2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4565c392-9058-45dc-89b9-d22d3b261983", "node_type": "1", "metadata": {}, "hash": "fb385ffee3939925cccb60ea90c6dcfd814059eca5a2dbc4750b9cc9b6c2f82c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c87102f3-c101-43c6-8bf4-3c4a782d59fb", "node_type": "1", "metadata": {}, "hash": "9d8f60fd09bdccd93a8f39217a76de90e7c93ad4e09a8b3dd49a77730b4e2fa8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Deep com-\npression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding.\narXiv preprint\narXiv:1510.00149, 2015.\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. 2016 IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), Jun 2016.\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks.", "mimetype": "text/plain", "start_char_idx": 3031, "end_char_idx": 3470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c87102f3-c101-43c6-8bf4-3c4a782d59fb": {"__data__": {"id_": "c87102f3-c101-43c6-8bf4-3c4a782d59fb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3769e20d-58e0-4c7c-a3b0-ff460a883fa2", "node_type": "1", "metadata": {}, "hash": "2c54216613e2f7061e6ede7202133aa7ce47461b0705a4db2595faa7deb16493", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f6585b8-6463-4e59-9e00-bbadd8d8dff3", "node_type": "1", "metadata": {}, "hash": "e0b8fe30328bda6fa5fa2331b958d5508cd7aad1cc4db512d1607cf93b68219d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Lecture Notes\nin Computer Science, page 630645, 2016.\n[16] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran\nEl-Yaniv, and Yoshua Bengio. Binarized neural networks.\nIn Advances in Neural Information Processing Systems 29,\npages 4107\u20134115. Curran Associates, Inc., 2016.\n[17] Benoit Jacob et al. gemmlowp: a small self-contained low-\nprecision gemm library.(2017), 2017.", "mimetype": "text/plain", "start_char_idx": 3471, "end_char_idx": 3844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f6585b8-6463-4e59-9e00-bbadd8d8dff3": {"__data__": {"id_": "3f6585b8-6463-4e59-9e00-bbadd8d8dff3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c87102f3-c101-43c6-8bf4-3c4a782d59fb", "node_type": "1", "metadata": {}, "hash": "9d8f60fd09bdccd93a8f39217a76de90e7c93ad4e09a8b3dd49a77730b4e2fa8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ace8c8e8-c109-432b-87c6-c96dc89c67f0", "node_type": "1", "metadata": {}, "hash": "33f1d203631215084b78a4147855fe182b659b848ecb0d3c56ba2bbc3a0afb9e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[18] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,\nMatthew Tang, Andrew Howard, Hartwig Adam, and Dmitry\nKalenichenko. Quantization and training of neural networks\nfor efficient integer-arithmetic-only inference. 2018 IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), Jun 2018.\n[19] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey\nKarayev, Jonathan Long, Ross Girshick, Sergio Guadarrama,\nand Trevor Darrell.", "mimetype": "text/plain", "start_char_idx": 3845, "end_char_idx": 4283, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ace8c8e8-c109-432b-87c6-c96dc89c67f0": {"__data__": {"id_": "ace8c8e8-c109-432b-87c6-c96dc89c67f0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f6585b8-6463-4e59-9e00-bbadd8d8dff3", "node_type": "1", "metadata": {}, "hash": "e0b8fe30328bda6fa5fa2331b958d5508cd7aad1cc4db512d1607cf93b68219d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d13c6c7-3d6f-4d9e-8313-a6924a50c1df", "node_type": "1", "metadata": {}, "hash": "b98eb4eec1fa33d152435e26737d089a67852ab694cac3407bfd9ed8c0349c23", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Caffe: Convolutional architecture for fast\nfeature embedding. arXiv preprint arXiv:1408.5093, 2014.\n[20] Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son,\nYoungjun Kwak, Jae-Joon Han, Sung Ju Hwang, and\nChangkyu Choi. Learning to quantize deep networks by op-\ntimizing quantization intervals with task loss. arXiv preprint\narXiv:1808.05779, 2018.", "mimetype": "text/plain", "start_char_idx": 4284, "end_char_idx": 4634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d13c6c7-3d6f-4d9e-8313-a6924a50c1df": {"__data__": {"id_": "0d13c6c7-3d6f-4d9e-8313-a6924a50c1df", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ace8c8e8-c109-432b-87c6-c96dc89c67f0", "node_type": "1", "metadata": {}, "hash": "33f1d203631215084b78a4147855fe182b659b848ecb0d3c56ba2bbc3a0afb9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1cedde8-107a-4825-92c1-4ffce42e8681", "node_type": "1", "metadata": {}, "hash": "52eff423e8acd9704872504321a198bf9a405fb5cb8557cd4ec66700209f98fd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[21] Raghuraman Krishnamoorthi.\nQuantizing deep convolu-\ntional networks for efficient inference: A whitepaper. arXiv\npreprint arXiv:1806.08342, 2018.\n[22] Alex Krizhevsky,\nVinod Nair,\nand Geoffrey Hinton.\nThe cifar-10 dataset.\nonline:\nhttp://www. cs. toronto.\nedu/kriz/cifar. html, page 4, 2014.\n[23] Fengfu Li, Bo Zhang, and Bin Liu.", "mimetype": "text/plain", "start_char_idx": 4635, "end_char_idx": 4970, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1cedde8-107a-4825-92c1-4ffce42e8681": {"__data__": {"id_": "d1cedde8-107a-4825-92c1-4ffce42e8681", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d13c6c7-3d6f-4d9e-8313-a6924a50c1df", "node_type": "1", "metadata": {}, "hash": "b98eb4eec1fa33d152435e26737d089a67852ab694cac3407bfd9ed8c0349c23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb26e56a-9512-4569-a49d-9e4b5d688040", "node_type": "1", "metadata": {}, "hash": "99a36de58d01fa28033dd1b92a5c8dc54cdee6b4316888c035ab78fae3b6c253", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Ternary weight networks.\narXiv preprint arXiv:1605.04711, 2016.\n[24] Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan\nSamet, and Tom Goldstein.\nTraining quantized nets: A\ndeeper understanding.\narXiv preprint arXiv:1706.02379,\n2017.\n[25] Xiaofan Lin, Cong Zhao, and Wei Pan.\nTowards accu-\nrate binary convolutional neural network.", "mimetype": "text/plain", "start_char_idx": 4971, "end_char_idx": 5303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb26e56a-9512-4569-a49d-9e4b5d688040": {"__data__": {"id_": "cb26e56a-9512-4569-a49d-9e4b5d688040", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1cedde8-107a-4825-92c1-4ffce42e8681", "node_type": "1", "metadata": {}, "hash": "52eff423e8acd9704872504321a198bf9a405fb5cb8557cd4ec66700209f98fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efc2fa00-4b18-474a-9839-6adaaaf173f3", "node_type": "1", "metadata": {}, "hash": "ce5579eada53390da802f6020f779c77943ec9383a628e68f5479cec4914fbce", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neural Infor-\nmation Processing Systems 30, pages 345\u2013353. Curran As-\nsociates, Inc., 2017.", "mimetype": "text/plain", "start_char_idx": 5304, "end_char_idx": 5513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "efc2fa00-4b18-474a-9839-6adaaaf173f3": {"__data__": {"id_": "efc2fa00-4b18-474a-9839-6adaaaf173f3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb26e56a-9512-4569-a49d-9e4b5d688040", "node_type": "1", "metadata": {}, "hash": "99a36de58d01fa28033dd1b92a5c8dc54cdee6b4316888c035ab78fae3b6c253", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62bff64a-b30b-4d7c-b871-a7555a617676", "node_type": "1", "metadata": {}, "hash": "a85fcb3195321ed51bf1d79d8f930db685e7346092cb6d13ca97c5d87a29d716", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[26] Jeffrey L. McKinstry, Steven K. Esser, Rathinakumar Ap-\npuswamy, Deepika Bablani, John V. Arthur, Izzet B. Yildiz,\nand Dharmendra S. Modha. Discovering low-precision net-\nworks close to full-precision networks for efficient embed-\nded inference. arXiv preprint arXiv:1809.04191, 2018.\n4860\n[27] Szymon Migacz. 8-bit inference with tensorrt.", "mimetype": "text/plain", "start_char_idx": 5514, "end_char_idx": 5859, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62bff64a-b30b-4d7c-b871-a7555a617676": {"__data__": {"id_": "62bff64a-b30b-4d7c-b871-a7555a617676", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efc2fa00-4b18-474a-9839-6adaaaf173f3", "node_type": "1", "metadata": {}, "hash": "ce5579eada53390da802f6020f779c77943ec9383a628e68f5479cec4914fbce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad6fbd61-69c5-4df4-97f1-d5e8142c7c8d", "node_type": "1", "metadata": {}, "hash": "da9265ad7849b6068260337a5c4332cbc89c518d77524bd3056580dc7ee459c4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In GPU Tech-\nnology Conference, 2017.\n[28] Asit Mishra and Debbie Marr. Apprentice: Using knowledge\ndistillation techniques to improve low-precision network ac-\ncuracy. arXiv preprint arXiv:1711.05852, 2017.\n[29] Asit Mishra, Eriko Nurvitadhi, Jeffrey J Cook, and Deb-\nbie Marr. Wrpn: Wide reduced-precision networks. arXiv\npreprint arXiv:1709.01134, 2017.", "mimetype": "text/plain", "start_char_idx": 5860, "end_char_idx": 6216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad6fbd61-69c5-4df4-97f1-d5e8142c7c8d": {"__data__": {"id_": "ad6fbd61-69c5-4df4-97f1-d5e8142c7c8d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62bff64a-b30b-4d7c-b871-a7555a617676", "node_type": "1", "metadata": {}, "hash": "a85fcb3195321ed51bf1d79d8f930db685e7346092cb6d13ca97c5d87a29d716", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0747f83-f727-4f75-9b9a-2fbd94c61dcb", "node_type": "1", "metadata": {}, "hash": "72b399dba096685cf0875946c3d3c984d79f6f010b534f82068823052304ddf4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[30] Daisuke Miyashita, Edward H. Lee, and Boris Murmann.\nConvolutional neural networks using logarithmic data rep-\nresentation. arXiv preprint arXiv:1603.01025, 2016.\n[31] nihui et al. Ncnn. https://github.com/Tencent/\nncnn, 2017.", "mimetype": "text/plain", "start_char_idx": 6217, "end_char_idx": 6448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0747f83-f727-4f75-9b9a-2fbd94c61dcb": {"__data__": {"id_": "a0747f83-f727-4f75-9b9a-2fbd94c61dcb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad6fbd61-69c5-4df4-97f1-d5e8142c7c8d", "node_type": "1", "metadata": {}, "hash": "da9265ad7849b6068260337a5c4332cbc89c518d77524bd3056580dc7ee459c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "057a69ee-cc37-4a44-8ab5-17759486636d", "node_type": "1", "metadata": {}, "hash": "da8d219f199f323bf131ea357f2f52911c837a7dfac87e241c9a48ff3a6fabc4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[32] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban\nDesmaison, Luca Antiga, and Adam Lerer. Automatic dif-\nferentiation in PyTorch. In NIPS Autodiff Workshop, 2017.\n[33] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,\nand Ali Farhadi. Xnor-net: Imagenet classification using bi-\nnary convolutional neural networks.", "mimetype": "text/plain", "start_char_idx": 6449, "end_char_idx": 6825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "057a69ee-cc37-4a44-8ab5-17759486636d": {"__data__": {"id_": "057a69ee-cc37-4a44-8ab5-17759486636d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0747f83-f727-4f75-9b9a-2fbd94c61dcb", "node_type": "1", "metadata": {}, "hash": "72b399dba096685cf0875946c3d3c984d79f6f010b534f82068823052304ddf4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0be1da4-24c3-459a-a68b-ab0afc354bfa", "node_type": "1", "metadata": {}, "hash": "8d86f0188e2ce5bea9bb4a72d42db1b7a0431fb6772464d9aec2698f7384ac19", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Lecture Notes in Com-\nputer Science, page 525542, 2016.\n[34] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen.\nMobilenetv2: Inverted\nresiduals and linear bottlenecks. In The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June\n2018.\n[35] Frederick Tung and Greg Mori. Clip-q: Deep network com-\npression learning by in-parallel pruning-quantization.", "mimetype": "text/plain", "start_char_idx": 6826, "end_char_idx": 7228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0be1da4-24c3-459a-a68b-ab0afc354bfa": {"__data__": {"id_": "a0be1da4-24c3-459a-a68b-ab0afc354bfa", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "057a69ee-cc37-4a44-8ab5-17759486636d", "node_type": "1", "metadata": {}, "hash": "da8d219f199f323bf131ea357f2f52911c837a7dfac87e241c9a48ff3a6fabc4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b6f05d2-d2ac-42d3-833c-4406fd359c30", "node_type": "1", "metadata": {}, "hash": "4dbc115e0c0113f09786064a6d82d8db04563160e685f0ef14831e8a14fd8650", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2018\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), June 2018.\n[36] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song\nHan. Haq: Hardware-aware automated quantization. arXiv\npreprint arXiv:1811.08886, 2018.\n[37] Peisong Wang, Qinghao Hu, Yifan Zhang, Chunjie Zhang,\nYang Liu, and Jian Cheng. Two-step quantization for low-bit\nneural networks.", "mimetype": "text/plain", "start_char_idx": 7229, "end_char_idx": 7594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b6f05d2-d2ac-42d3-833c-4406fd359c30": {"__data__": {"id_": "8b6f05d2-d2ac-42d3-833c-4406fd359c30", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0be1da4-24c3-459a-a68b-ab0afc354bfa", "node_type": "1", "metadata": {}, "hash": "8d86f0188e2ce5bea9bb4a72d42db1b7a0431fb6772464d9aec2698f7384ac19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a2fe741-2ac4-444c-9594-ff5556bc5d49", "node_type": "1", "metadata": {}, "hash": "391c1e25059ad1838a570eafcba4e45a9500a1230a9ac4a806f1da475ce62562", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "2018 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2018.\n[38] Yuhui Xu, Yongzhuang Wang, Aojun Zhou, Weiyao Lin, and\nHongkai Xiong. Deep neural network compression with sin-\ngle and multiple level quantization. In AAAI, 2018.\n[39] Amir Yazdanbakhsh, Ahmed T. Elthakeb, Prannoy Pil-\nligundla,\nFatemehSadat Mireshghallah,\nand Hadi Es-\nmaeilzadeh.", "mimetype": "text/plain", "start_char_idx": 7595, "end_char_idx": 7965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a2fe741-2ac4-444c-9594-ff5556bc5d49": {"__data__": {"id_": "8a2fe741-2ac4-444c-9594-ff5556bc5d49", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b6f05d2-d2ac-42d3-833c-4406fd359c30", "node_type": "1", "metadata": {}, "hash": "4dbc115e0c0113f09786064a6d82d8db04563160e685f0ef14831e8a14fd8650", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5855de4-e6ee-4b3a-be83-26e3edd84711", "node_type": "1", "metadata": {}, "hash": "58f644df235cbdc1325f44b6a60c2afd95b206357cddddad16357c38e71c453f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Releq: An automatic reinforcement learning\napproach for deep quantization of neural networks. arXiv\npreprint arXiv:1811.01704, 2018.\n[40] Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher,\nYingyong Qi, and Jack Xin. Blended coarse gradient descent\nfor full quantization of deep neural networks. Research in the\nMathematical Sciences, 6(1):14, 2019.", "mimetype": "text/plain", "start_char_idx": 7966, "end_char_idx": 8320, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5855de4-e6ee-4b3a-be83-26e3edd84711": {"__data__": {"id_": "f5855de4-e6ee-4b3a-be83-26e3edd84711", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a2fe741-2ac4-444c-9594-ff5556bc5d49", "node_type": "1", "metadata": {}, "hash": "391c1e25059ad1838a570eafcba4e45a9500a1230a9ac4a806f1da475ce62562", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b883d14-bc1e-440e-8ccb-deea7b93d18e", "node_type": "1", "metadata": {}, "hash": "5aa7ef007b963301871cfa1193f65319fe5e27e0c57228b450f1363d30f7d463", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "[41] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang\nHua. Lq-nets: Learned quantization for highly accurate and\ncompact deep neural networks. In The European Conference\non Computer Vision (ECCV), September 2018.\n[42] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and\nYurong Chen. Incremental network quantization: Towards\nlossless cnns with low-precision weights.", "mimetype": "text/plain", "start_char_idx": 8321, "end_char_idx": 8689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b883d14-bc1e-440e-8ccb-deea7b93d18e": {"__data__": {"id_": "3b883d14-bc1e-440e-8ccb-deea7b93d18e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5855de4-e6ee-4b3a-be83-26e3edd84711", "node_type": "1", "metadata": {}, "hash": "58f644df235cbdc1325f44b6a60c2afd95b206357cddddad16357c38e71c453f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27d9d5aa-7553-41ac-81c8-9945ed56cefc", "node_type": "1", "metadata": {}, "hash": "13df07ff23f7fc11dc3c855ba80bab55f7af5159703836d66823ee5d9fc8e435", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint\narXiv:1702.03044, 2017.\n[43] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen,\nand Yuheng Zou. Dorefa-net: Training low bitwidth convo-\nlutional neural networks with low bitwidth gradients. arXiv\npreprint arXiv:1606.06160, 2016.\n[44] Shu-Chang Zhou, Yu-Zhi Wang, He Wen, Qin-Yao He, and\nYu-Heng Zou.", "mimetype": "text/plain", "start_char_idx": 8690, "end_char_idx": 9009, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27d9d5aa-7553-41ac-81c8-9945ed56cefc": {"__data__": {"id_": "27d9d5aa-7553-41ac-81c8-9945ed56cefc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b883d14-bc1e-440e-8ccb-deea7b93d18e", "node_type": "1", "metadata": {}, "hash": "5aa7ef007b963301871cfa1193f65319fe5e27e0c57228b450f1363d30f7d463", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a20e9cb2-ae72-48cb-89c1-ff4a78e825ba", "node_type": "1", "metadata": {}, "hash": "880b5ee4724cc6326c124562faf412dc418137c4f7f50e8e449cea01e2f19989", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Balanced quantization: An effective and ef-\nficient approach to quantized neural networks. Journal of\nComputer Science and Technology, 32(4):667682, Jul 2017.\n[45] Chenzhuo Zhu, Song Han, Huizi Mao, and William J.\nDally.\nTrained ternary quantization.\narXiv preprint\narXiv:1612.01064, 2016.\n[46] Xiaotian Zhu, Wengang Zhou, and Houqiang Li.", "mimetype": "text/plain", "start_char_idx": 9010, "end_char_idx": 9349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a20e9cb2-ae72-48cb-89c1-ff4a78e825ba": {"__data__": {"id_": "a20e9cb2-ae72-48cb-89c1-ff4a78e825ba", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "9d2c75077a052eb6d0dd6f41f9600150b74cf2d51e9bfa5b2f46ecf9465a5450", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27d9d5aa-7553-41ac-81c8-9945ed56cefc", "node_type": "1", "metadata": {}, "hash": "13df07ff23f7fc11dc3c855ba80bab55f7af5159703836d66823ee5d9fc8e435", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1109/iccv.2019.00495", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Adap-\ntive layerwise quantization for deep neural network compres-\nsion. 2018 IEEE International Conference on Multimedia\nand Expo (ICME), pages 1\u20136, 2018.\n4861", "mimetype": "text/plain", "start_char_idx": 9350, "end_char_idx": 9510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10.1109/iccv.2019.00495": {"__data__": {"id_": "10.1109/iccv.2019.00495", "embedding": null, "metadata": {"node_type": "doi_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"4": {"node_id": "root_node", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8b38de36-948e-4614-99da-9f15b9629e60", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "45414ee3-3cc9-40e9-b846-d8c2a93d0d31", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7ab3370c-75b2-4479-8a10-d9d83081db08", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b37585ad-1e7c-46de-916f-52c239038c42", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8d1a2099-19be-4747-b251-a655a6b55164", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c2f4d688-296e-4551-aac3-d8afab3b182a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fac5e889-761e-49d4-bbb1-31693cf7d5e3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "79b56430-ee52-43f4-84f5-fa57cb3dee54", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4565c392-9058-45dc-89b9-d22d3b261983", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3769e20d-58e0-4c7c-a3b0-ff460a883fa2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c87102f3-c101-43c6-8bf4-3c4a782d59fb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3f6585b8-6463-4e59-9e00-bbadd8d8dff3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ace8c8e8-c109-432b-87c6-c96dc89c67f0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0d13c6c7-3d6f-4d9e-8313-a6924a50c1df", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d1cedde8-107a-4825-92c1-4ffce42e8681", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "cb26e56a-9512-4569-a49d-9e4b5d688040", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "efc2fa00-4b18-474a-9839-6adaaaf173f3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "62bff64a-b30b-4d7c-b871-a7555a617676", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ad6fbd61-69c5-4df4-97f1-d5e8142c7c8d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a0747f83-f727-4f75-9b9a-2fbd94c61dcb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "057a69ee-cc37-4a44-8ab5-17759486636d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a0be1da4-24c3-459a-a68b-ab0afc354bfa", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8b6f05d2-d2ac-42d3-833c-4406fd359c30", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8a2fe741-2ac4-444c-9594-ff5556bc5d49", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f5855de4-e6ee-4b3a-be83-26e3edd84711", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3b883d14-bc1e-440e-8ccb-deea7b93d18e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "27d9d5aa-7553-41ac-81c8-9945ed56cefc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a20e9cb2-ae72-48cb-89c1-ff4a78e825ba", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}]}, "text": "DOI: 10.1109/iccv.2019.00495", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "faf6e3e1-b949-4e5b-8f66-1a4e4cf4665c": {"__data__": {"id_": "faf6e3e1-b949-4e5b-8f66-1a4e4cf4665c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fdd2b41-08ae-42c1-a646-a78abd8d4423", "node_type": "1", "metadata": {}, "hash": "d94e9eea72811fa1158fe9e6021b430b8ef2ed5ee3fa10ddf3282b550cae857b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1\nIntroduction\nDeep neural networks (DNN) have shown significant improvements in several applica-\ntion domains including computer vision and speech recognition. In computer vision, a\nparticular type of DNN, known as Convolutional Neural Networks (CNN), have demon-\nstrated state-of-the-art results in object recognition [1,2,3,4] and detection [5,6,7].\nConvolutional neural networks show reliable results on object recognition and de-\ntection that are useful in real world applications.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fdd2b41-08ae-42c1-a646-a78abd8d4423": {"__data__": {"id_": "9fdd2b41-08ae-42c1-a646-a78abd8d4423", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "faf6e3e1-b949-4e5b-8f66-1a4e4cf4665c", "node_type": "1", "metadata": {}, "hash": "1888f7cee52dcdb0493f2de9b3c703671bb1da0869be468d48e5f73b34bb77cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4af9d98f-6d4a-413b-a65d-ef8634b0435e", "node_type": "1", "metadata": {}, "hash": "aa0e2c6a76a531b7fc6b6ebcc31e5f705de77bfc35803a18d9debe7c6c795413", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Concurrent to the recent progress in\nrecognition, interesting advancements have been happening in virtual reality (VR by\nOculus) [8], augmented reality (AR by HoloLens) [9], and smart wearable devices.\nPutting these two pieces together, we argue that it is the right time to equip smart\nportable devices with the power of state-of-the-art recognition systems. However, CNN-\nbased recognition systems need large amounts of memory and computational power.\nWhile they perform well on expensive, GPU-based machines, they are often unsuitable\nfor smaller devices like cell phones and embedded electronics.", "mimetype": "text/plain", "start_char_idx": 487, "end_char_idx": 1087, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4af9d98f-6d4a-413b-a65d-ef8634b0435e": {"__data__": {"id_": "4af9d98f-6d4a-413b-a65d-ef8634b0435e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fdd2b41-08ae-42c1-a646-a78abd8d4423", "node_type": "1", "metadata": {}, "hash": "d94e9eea72811fa1158fe9e6021b430b8ef2ed5ee3fa10ddf3282b550cae857b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff3cc678-711e-459e-8766-64add4a5b79b", "node_type": "1", "metadata": {}, "hash": "4f20e4801df2209692c593f176b20f15b60e941e5273b780ac96ec55cd850eb2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "For example, AlexNet[1] has 61M parameters (249MB of memory) and performs\n1.5B high precision operations to classify one image. These numbers are even higher for\ndeeper CNNs e.g.,VGG [2] (see section 4.1). These models quickly overtax the limited\nstorage, battery power, and compute capabilities of smaller devices like cell phones.\narXiv:1603.05279v4  [cs.CV]  2 Aug 2016\n2\nRastegari et al.\n  . . .", "mimetype": "text/plain", "start_char_idx": 1088, "end_char_idx": 1487, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff3cc678-711e-459e-8766-64add4a5b79b": {"__data__": {"id_": "ff3cc678-711e-459e-8766-64add4a5b79b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4af9d98f-6d4a-413b-a65d-ef8634b0435e", "node_type": "1", "metadata": {}, "hash": "aa0e2c6a76a531b7fc6b6ebcc31e5f705de77bfc35803a18d9debe7c6c795413", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b546bc00-f93e-48a7-93ac-786f239a61c7", "node_type": "1", "metadata": {}, "hash": "31576f0edc12c17be2e8c899d7dfd3c9349ae8b5e944a9823360ec97c3a00626", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": ". . .  \nc \nwin \nhin \nw \nh \nInput \nWeight \n!\"#$%&'()*&+*,%-.(\n/0\"&*,%-.(\n1.\"2(+-(\n3%-4%51,%-(\n6\"7%&8(\n9*4+-:(\n;<-=\"&\"->\"?(\nComputation\n( Saving(         \n;<-=\"&\"->\"?(\nC>>1&*>8(%-(\n<7*:\"!\"#(\n;C5\"D!\"#?", "mimetype": "text/plain", "start_char_idx": 1492, "end_char_idx": 1690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b546bc00-f93e-48a7-93ac-786f239a61c7": {"__data__": {"id_": "b546bc00-f93e-48a7-93ac-786f239a61c7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff3cc678-711e-459e-8766-64add4a5b79b", "node_type": "1", "metadata": {}, "hash": "4f20e4801df2209692c593f176b20f15b60e941e5273b780ac96ec55cd850eb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa28498f-7e78-4bd0-a63b-67347822d054", "node_type": "1", "metadata": {}, "hash": "c3b7fdb1d4ea9a1327b56a5c4b334c074c70502b42e9be460029abb52a2e50df", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(\n9#*-2*&2(\n3%-4%51,%-(\n(\nE(F(G(F(H(\nID(\nID(\nJKLMN(\nO+-*&8(P\"+:Q#(\nE(F(G(\nRLSD(\nRTD(\nJK6MV(\nO+-*&8P\"+:Q#((\nO+-*&8(<-01#(\n;!\"#$%\"&'?", "mimetype": "text/plain", "start_char_idx": 1690, "end_char_idx": 1821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa28498f-7e78-4bd0-a63b-67347822d054": {"__data__": {"id_": "aa28498f-7e78-4bd0-a63b-67347822d054", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b546bc00-f93e-48a7-93ac-786f239a61c7", "node_type": "1", "metadata": {}, "hash": "31576f0edc12c17be2e8c899d7dfd3c9349ae8b5e944a9823360ec97c3a00626", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5f0c874-9fea-4ed4-8ad3-353dc53704d4", "node_type": "1", "metadata": {}, "hash": "4796d5257382eec2318fa624b2c333fb78cfdd534a2f3365ad2a3243f5af0279", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(\nW!/X(F(\nY+#>%1-#(\nRLSD(\nRKVD(\nJSSMT(\n0.11 -0.21 ... -0.34 \n-0.25 0.61 ...  0.52\nReal-Value Weights \nReal-Value Inputs \n! \n! \n! \n!\n! \n! \n0.11 -0.21 ... -0.34 \n-0.25 0.61 ...  0.52\nBinary Weights \nReal-Value Inputs \n! \n! \n! \n!\n! \n!", "mimetype": "text/plain", "start_char_idx": 1821, "end_char_idx": 2052, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5f0c874-9fea-4ed4-8ad3-353dc53704d4": {"__data__": {"id_": "f5f0c874-9fea-4ed4-8ad3-353dc53704d4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa28498f-7e78-4bd0-a63b-67347822d054", "node_type": "1", "metadata": {}, "hash": "c3b7fdb1d4ea9a1327b56a5c4b334c074c70502b42e9be460029abb52a2e50df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "856900f2-2a59-4eb8-b43a-b3655765c66e", "node_type": "1", "metadata": {}, "hash": "53886adeec7fa187e21d1fdef6a4d8ceb44c529e65795eb8111cd1d0a175025c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1  -1 ... -1 \n-1  1 ...  1\nBinary Weights \nBinary Inputs \n! \n! \n! \n!\n! \n! \n32x\n32x\nFig. 1: We propose two efficient variations of convolutional neural networks. Binary-\nWeight-Networks, when the weight filters contains binary values. XNOR-Networks,\nwhen both weigh and input have binary values. These networks are very efficient in\nterms of memory and computation, while being very accurate in natural image classifi-\ncation.", "mimetype": "text/plain", "start_char_idx": 2054, "end_char_idx": 2479, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "856900f2-2a59-4eb8-b43a-b3655765c66e": {"__data__": {"id_": "856900f2-2a59-4eb8-b43a-b3655765c66e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5f0c874-9fea-4ed4-8ad3-353dc53704d4", "node_type": "1", "metadata": {}, "hash": "4796d5257382eec2318fa624b2c333fb78cfdd534a2f3365ad2a3243f5af0279", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "442a996d-489a-4fc9-8512-8aab841141a0", "node_type": "1", "metadata": {}, "hash": "c7733e0a33555dc4096b81125504f633bc6005fdc9e4d2f57cb1038e642e4f70", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This offers the possibility of using accurate vision techniques in portable devices\nwith limited resources.\nIn this paper, we introduce simple, efficient, and accurate approximations to CNNs\nby binarizing the weights and even the intermediate representations in convolutional\nneural networks. Our binarization method aims at finding the best approximations of the\nconvolutions using binary operations. We demonstrate that our way of binarizing neural\nnetworks results in ImageNet classification accuracy numbers that are comparable to\nstandard full precision networks while requiring a significantly less memory and fewer\nfloating point operations.", "mimetype": "text/plain", "start_char_idx": 2480, "end_char_idx": 3128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "442a996d-489a-4fc9-8512-8aab841141a0": {"__data__": {"id_": "442a996d-489a-4fc9-8512-8aab841141a0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "856900f2-2a59-4eb8-b43a-b3655765c66e", "node_type": "1", "metadata": {}, "hash": "53886adeec7fa187e21d1fdef6a4d8ceb44c529e65795eb8111cd1d0a175025c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b58461df-014e-400f-8a3b-b7fcc899b4b8", "node_type": "1", "metadata": {}, "hash": "9e39dc422071d9968ccdeb83d1679c59df62fd3e912c4028334c40b12958da2b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We study two approximations: Neural networks with binary weights and XNOR-\nNetworks. In Binary-Weight-Networks all the weight values are approximated with bi-\nnary values. A convolutional neural network with binary weights is significantly smaller\n(\u223c32\u00d7) than an equivalent network with single-precision weight values. In addition,\nwhen weight values are binary, convolutions can be estimated by only addition and\nsubtraction (without multiplication), resulting in \u223c2\u00d7 speed up.", "mimetype": "text/plain", "start_char_idx": 3129, "end_char_idx": 3607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b58461df-014e-400f-8a3b-b7fcc899b4b8": {"__data__": {"id_": "b58461df-014e-400f-8a3b-b7fcc899b4b8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "442a996d-489a-4fc9-8512-8aab841141a0", "node_type": "1", "metadata": {}, "hash": "c7733e0a33555dc4096b81125504f633bc6005fdc9e4d2f57cb1038e642e4f70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e065c41a-5c38-4da7-8316-3a0eb1f37f4a", "node_type": "1", "metadata": {}, "hash": "33f9bf4da2fe4247085ee78746bf02de698ad318f3964f5224cfb86a29a312c0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Binary-weight ap-\nproximations of large CNNs can fit into the memory of even small, portable devices\nwhile maintaining the same level of accuracy (See Section 4.1 and 4.2).\nTo take this idea further, we introduce XNOR-Networks where both the weights\nand the inputs to the convolutional and fully connected layers are approximated with\nbinary values1. Binary weights and binary inputs allow an efficient way of implement-\ning convolutional operations.", "mimetype": "text/plain", "start_char_idx": 3608, "end_char_idx": 4058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e065c41a-5c38-4da7-8316-3a0eb1f37f4a": {"__data__": {"id_": "e065c41a-5c38-4da7-8316-3a0eb1f37f4a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b58461df-014e-400f-8a3b-b7fcc899b4b8", "node_type": "1", "metadata": {}, "hash": "9e39dc422071d9968ccdeb83d1679c59df62fd3e912c4028334c40b12958da2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ae8975d-4301-43bb-a39d-f9e02fc18f1b", "node_type": "1", "metadata": {}, "hash": "914884aca1f230e35a4982dcd5bfb03541fb782e538fbea9d91cb3390620b9ff", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "If all of the operands of the convolutions are binary, then\nthe convolutions can be estimated by XNOR and bitcounting operations [11]. XNOR-\nNets result in accurate approximation of CNNs while offering \u223c58\u00d7 speed up in CPUs\n(in terms of number of the high precision operations). This means that XNOR-Nets can\nenable real-time inference in devices with small memory and no GPUs (Inference in\nXNOR-Nets can be done very efficiently on CPUs).", "mimetype": "text/plain", "start_char_idx": 4059, "end_char_idx": 4498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ae8975d-4301-43bb-a39d-f9e02fc18f1b": {"__data__": {"id_": "8ae8975d-4301-43bb-a39d-f9e02fc18f1b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e065c41a-5c38-4da7-8316-3a0eb1f37f4a", "node_type": "1", "metadata": {}, "hash": "33f9bf4da2fe4247085ee78746bf02de698ad318f3964f5224cfb86a29a312c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91c75266-d293-4741-8856-db890e62e5db", "node_type": "1", "metadata": {}, "hash": "5e2ea4f54da82915b3c05ccb59dfa4ef359814a27d3cd6ecd58940f5e60090da", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To the best of our knowledge this paper is the first attempt to present an evalua-\ntion of binary neural networks on large-scale datasets like ImageNet. Our experimental\n1 fully connected layers can be implemented by convolution, therefore, in the rest of the paper,\nwe refer to them also as convolutional layers [10].", "mimetype": "text/plain", "start_char_idx": 4499, "end_char_idx": 4817, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91c75266-d293-4741-8856-db890e62e5db": {"__data__": {"id_": "91c75266-d293-4741-8856-db890e62e5db", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ae8975d-4301-43bb-a39d-f9e02fc18f1b", "node_type": "1", "metadata": {}, "hash": "914884aca1f230e35a4982dcd5bfb03541fb782e538fbea9d91cb3390620b9ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a586f64-69e3-4ed5-b217-246748cade34", "node_type": "1", "metadata": {}, "hash": "99ec2d0b57d079fd6d9ac5a143a4eaa9a4430975b3dcdacc401095bf5e630262", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\n3\nresults show that our proposed method for binarizing convolutional neural networks\noutperforms the state-of-the-art network binarization method of [11] by a large margin\n(16.3%) on top-1 image classification in the ImageNet challenge ILSVRC2012. Our\ncontribution is two-fold: First, we introduce a new way of binarizing the weight val-\nues in convolutional neural networks and show the advantage of our solution compared\nto state-of-the-art solutions.", "mimetype": "text/plain", "start_char_idx": 4818, "end_char_idx": 5348, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a586f64-69e3-4ed5-b217-246748cade34": {"__data__": {"id_": "0a586f64-69e3-4ed5-b217-246748cade34", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91c75266-d293-4741-8856-db890e62e5db", "node_type": "1", "metadata": {}, "hash": "5e2ea4f54da82915b3c05ccb59dfa4ef359814a27d3cd6ecd58940f5e60090da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5556efd5-34f7-47f7-9c0d-cd10088a77da", "node_type": "1", "metadata": {}, "hash": "832060b3a51777077d2e24e6dd270f915cd686daf0dafafd8cea3f4e0da2c4b6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Second, we introduce XNOR-Nets, a deep neural network\nmodel with binary weights and binary inputs and show that XNOR-Nets can obtain sim-\nilar classification accuracies compared to standard networks while being significantly\nmore efficient. Our code is available at: http://allenai.org/plato/xnornet\n2\nRelated Work\nDeep neural networks often suffer from over-parametrization and large amounts of re-\ndundancy in their models. This typically results in inefficient computation and memory\nusage[12].", "mimetype": "text/plain", "start_char_idx": 5349, "end_char_idx": 5846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5556efd5-34f7-47f7-9c0d-cd10088a77da": {"__data__": {"id_": "5556efd5-34f7-47f7-9c0d-cd10088a77da", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a586f64-69e3-4ed5-b217-246748cade34", "node_type": "1", "metadata": {}, "hash": "99ec2d0b57d079fd6d9ac5a143a4eaa9a4430975b3dcdacc401095bf5e630262", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db041908-6a1b-49de-9e04-7158a955bbf6", "node_type": "1", "metadata": {}, "hash": "aa2d671e4565d509d06b54e77f66568131c83a8e58628f6f1df072e513bd3317", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Several methods have been proposed to address efficient training and infer-\nence in deep neural networks.\nShallow networks: Estimating a deep neural network with a shallower model re-\nduces the size of a network. Early theoretical work by Cybenko shows that a network\nwith a large enough single hidden layer of sigmoid units can approximate any decision\nboundary [13]. In several areas (e.g.,vision and speech), however, shallow networks\ncannot compete with deep models [14]. [15] trains a shallow network on SIFT features\nto classify the ImageNet dataset.", "mimetype": "text/plain", "start_char_idx": 5847, "end_char_idx": 6403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db041908-6a1b-49de-9e04-7158a955bbf6": {"__data__": {"id_": "db041908-6a1b-49de-9e04-7158a955bbf6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5556efd5-34f7-47f7-9c0d-cd10088a77da", "node_type": "1", "metadata": {}, "hash": "832060b3a51777077d2e24e6dd270f915cd686daf0dafafd8cea3f4e0da2c4b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa56bd0b-aeee-4add-85fd-f1fddafeb944", "node_type": "1", "metadata": {}, "hash": "d91d481ea524110100242016dbc730e6085fa69c71a9207e2c348bce433a01dc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "They show it is difficult to train shallow networks\nwith large number of parameters. [16] provides empirical evidence on small datasets\n(e.g.,CIFAR-10) that shallow nets are capable of learning the same functions as deep\nnets. In order to get the similar accuracy, the number of parameters in the shallow net-\nwork must be close to the number of parameters in the deep network. They do this by\nfirst training a state-of-the-art deep model, and then training a shallow model to mimic\nthe deep model.", "mimetype": "text/plain", "start_char_idx": 6404, "end_char_idx": 6902, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa56bd0b-aeee-4add-85fd-f1fddafeb944": {"__data__": {"id_": "fa56bd0b-aeee-4add-85fd-f1fddafeb944", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db041908-6a1b-49de-9e04-7158a955bbf6", "node_type": "1", "metadata": {}, "hash": "aa2d671e4565d509d06b54e77f66568131c83a8e58628f6f1df072e513bd3317", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "994c769d-f908-4f83-bdc5-6e732cfa53ad", "node_type": "1", "metadata": {}, "hash": "c25ccdabefefa925578616b5f9fd40c7a2b99fa017357f21d52733bcd7639e64", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "These methods are different from our approach because we use the\nstandard deep architectures not the shallow estimations.\nCompressing pre-trained deep networks: Pruning redundant, non-informative\nweights in a previously trained network reduces the size of the network at inference\ntime. Weight decay [17] was an early method for pruning a network. Optimal Brain\nDamage [18] and Optimal Brain Surgeon [19] use the Hessian of the loss function to\nprune a network by reducing the number of connections.", "mimetype": "text/plain", "start_char_idx": 6903, "end_char_idx": 7402, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "994c769d-f908-4f83-bdc5-6e732cfa53ad": {"__data__": {"id_": "994c769d-f908-4f83-bdc5-6e732cfa53ad", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa56bd0b-aeee-4add-85fd-f1fddafeb944", "node_type": "1", "metadata": {}, "hash": "d91d481ea524110100242016dbc730e6085fa69c71a9207e2c348bce433a01dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97b6008a-66c1-4b50-bef0-2e8e54861b75", "node_type": "1", "metadata": {}, "hash": "fbbea0b074d96bd0f67e8711845afee8f781dc131f5e280c3c844324dad705e7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Recently [20] reduced the\nnumber of parameters by an order of magnitude in several state-of-the-art neural net-\nworks by pruning. [21] proposed to reduce the number of activations for compression\nand acceleration. Deep compression [22] reduces the storage and energy required to run\ninference on large networks so they can be deployed on mobile devices. They remove\nthe redundant connections and quantize weights so that multiple connections share the\nsame weight, and then they use Huffman coding to compress the weights.", "mimetype": "text/plain", "start_char_idx": 7403, "end_char_idx": 7925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97b6008a-66c1-4b50-bef0-2e8e54861b75": {"__data__": {"id_": "97b6008a-66c1-4b50-bef0-2e8e54861b75", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "994c769d-f908-4f83-bdc5-6e732cfa53ad", "node_type": "1", "metadata": {}, "hash": "c25ccdabefefa925578616b5f9fd40c7a2b99fa017357f21d52733bcd7639e64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0954f18-142f-4c95-85c5-1e4596235197", "node_type": "1", "metadata": {}, "hash": "7c778a9d778f4572f38a044b3424e2fca93d94590876d09f304b3b1f0bdaf159", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "HashedNets\n[23] uses a hash function to reduce model size by randomly grouping the weights, such\nthat connections in a hash bucket use a single parameter value. Matrix factorization has\nbeen used by [24,25]. We are different from these approaches because we do not use a\npretrained network. We train binary networks from scratch.\n4\nRastegari et al.\nDesigning compact layers: Designing compact blocks at each layer of a deep net-\nwork can help to save memory and computational costs.", "mimetype": "text/plain", "start_char_idx": 7926, "end_char_idx": 8408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0954f18-142f-4c95-85c5-1e4596235197": {"__data__": {"id_": "f0954f18-142f-4c95-85c5-1e4596235197", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97b6008a-66c1-4b50-bef0-2e8e54861b75", "node_type": "1", "metadata": {}, "hash": "fbbea0b074d96bd0f67e8711845afee8f781dc131f5e280c3c844324dad705e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ed6baf4-44a7-4373-a63f-5f128f098efc", "node_type": "1", "metadata": {}, "hash": "4659b0099489c470615729bc9445c5d1a1677dde999cdd35048246fcb1728944", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Replacing the fully connected\nlayer with global average pooling was examined in the Network in Network architec-\nture [26], GoogLenet[3] and Residual-Net[4], which achieved state-of-the-art results\non several benchmarks. The bottleneck structure in Residual-Net [4] has been proposed\nto reduce the number of parameters and improve speed. Decomposing 3 \u00d7 3 convo-\nlutions with two 1 \u00d7 1 is used in [27] and resulted in state-of-the-art performance on\nobject recognition.", "mimetype": "text/plain", "start_char_idx": 8409, "end_char_idx": 8878, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ed6baf4-44a7-4373-a63f-5f128f098efc": {"__data__": {"id_": "1ed6baf4-44a7-4373-a63f-5f128f098efc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0954f18-142f-4c95-85c5-1e4596235197", "node_type": "1", "metadata": {}, "hash": "7c778a9d778f4572f38a044b3424e2fca93d94590876d09f304b3b1f0bdaf159", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df88479b-51f3-4218-8af7-478303a1dacb", "node_type": "1", "metadata": {}, "hash": "0d00921ee6843ef33f2b348f51646a6972ad58ce675daf0df62c9cc2c67852b4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Replacing 3 \u00d7 3 convolutions with 1 \u00d7 1 convolutions is used in\n[28] to create a very compact neural network that can achieve \u223c50\u00d7 reduction in the\nnumber of parameters while obtaining high accuracy. Our method is different from this\nline of work because we use the full network (not the compact version) but with binary\nparameters.\nQuantizing parameters: High precision parameters are not very important in achiev-\ning high performance in deep networks. [29] proposed to quantize the weights of fully\nconnected layers in a deep network by vector quantization techniques.", "mimetype": "text/plain", "start_char_idx": 8879, "end_char_idx": 9450, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df88479b-51f3-4218-8af7-478303a1dacb": {"__data__": {"id_": "df88479b-51f3-4218-8af7-478303a1dacb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ed6baf4-44a7-4373-a63f-5f128f098efc", "node_type": "1", "metadata": {}, "hash": "4659b0099489c470615729bc9445c5d1a1677dde999cdd35048246fcb1728944", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "256d878d-b29d-4ce1-97ae-f144999fe5e3", "node_type": "1", "metadata": {}, "hash": "a1b170a9e6853e2066640b534ececbf439508e71205a7bde530a8b139c3494f6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "They showed just\nthresholding the weight values at zero only decreases the top-1 accuracy on ILSVRC2012\nby less than %10. [30] proposed a provably polynomial time algorithm for training a\nsparse networks with +1/0/-1 weights. A fixed-point implementation of 8-bit integer\nwas compared with 32-bit floating point activations in [31]. Another fixed-point net-\nwork with ternary weights and 3-bits activations was presented by [32].", "mimetype": "text/plain", "start_char_idx": 9451, "end_char_idx": 9880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "256d878d-b29d-4ce1-97ae-f144999fe5e3": {"__data__": {"id_": "256d878d-b29d-4ce1-97ae-f144999fe5e3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df88479b-51f3-4218-8af7-478303a1dacb", "node_type": "1", "metadata": {}, "hash": "0d00921ee6843ef33f2b348f51646a6972ad58ce675daf0df62c9cc2c67852b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f5148c2-980e-4cce-9dc2-f06f3990764d", "node_type": "1", "metadata": {}, "hash": "8f58b8a1e9a8d04c9a267c80f988159162f345efceb8356b327b3d2f526c778e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Quantizing a\nnetwork with L2 error minimization achieved better accuracy on MNIST and CIFAR-10\ndatasets in [33]. [34] proposed a back-propagation process by quantizing the represen-\ntations at each layer of the network. To convert some of the remaining multiplications\ninto binary shifts the neurons get restricted values of power-of-two integers. In [34]\nthey carry the full precision weights during the test phase, and only quantize the neu-\nrons during the back-propagation process, and not during the forward-propagation.", "mimetype": "text/plain", "start_char_idx": 9881, "end_char_idx": 10406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f5148c2-980e-4cce-9dc2-f06f3990764d": {"__data__": {"id_": "1f5148c2-980e-4cce-9dc2-f06f3990764d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "256d878d-b29d-4ce1-97ae-f144999fe5e3", "node_type": "1", "metadata": {}, "hash": "a1b170a9e6853e2066640b534ececbf439508e71205a7bde530a8b139c3494f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4347006-e63b-4773-a42a-25cdbda90770", "node_type": "1", "metadata": {}, "hash": "6c12285e341edabae05ddcd5fadaaeaea85d216d0a07af7bfd007dc9da5d72cd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Our\nwork is similar to these methods since we are quantizing the parameters in the network.\nBut our quantization is the extreme scenario +1,-1.\nNetwork binarization: These works are the most related to our approach. Several\nmethods attempt to binarize the weights and the activations in neural networks.The per-\nformance of highly quantized networks (e.g.,binarized) were believed to be very poor\ndue to the destructive property of binary quantization [35].", "mimetype": "text/plain", "start_char_idx": 10407, "end_char_idx": 10864, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4347006-e63b-4773-a42a-25cdbda90770": {"__data__": {"id_": "a4347006-e63b-4773-a42a-25cdbda90770", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f5148c2-980e-4cce-9dc2-f06f3990764d", "node_type": "1", "metadata": {}, "hash": "8f58b8a1e9a8d04c9a267c80f988159162f345efceb8356b327b3d2f526c778e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8650cb1-e3fc-461a-805c-329c9560f394", "node_type": "1", "metadata": {}, "hash": "f3be1719d4717c2732712f82bdfe2eb5d9e1bc2335ac50b40526fbf3b3e19fe0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Expectation BackPropaga-\ntion (EBP) in [36] showed high performance can be achieved by a network with binary\nweights and binary activations. This is done by a variational Bayesian approach, that\ninfers networks with binary weights and neurons. A fully binary network at run time\npresented in [37] using a similar approach to EBP, showing significant improvement in\nenergy efficiency. In EBP the binarized parameters were only used during inference. Bi-\nnaryConnect [38] extended the probablistic idea behind EBP.", "mimetype": "text/plain", "start_char_idx": 10865, "end_char_idx": 11377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8650cb1-e3fc-461a-805c-329c9560f394": {"__data__": {"id_": "d8650cb1-e3fc-461a-805c-329c9560f394", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4347006-e63b-4773-a42a-25cdbda90770", "node_type": "1", "metadata": {}, "hash": "6c12285e341edabae05ddcd5fadaaeaea85d216d0a07af7bfd007dc9da5d72cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7cbdd57-6a2b-43c3-bb41-9b55d76c0f0b", "node_type": "1", "metadata": {}, "hash": "84c889f8f857cf9005b3462f2fe090ccea3f34270a2d4d87213f857334d0f68e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Similar to our approach,\nBinaryConnect uses the real-valued version of the weights as a key reference for the\nbinarization process. The real-valued weight updated using the back propagated error\nby simply ignoring the binarization in the update. BinaryConnect achieved state-of-the-\nart results on small datasets (e.g.,CIFAR-10, SVHN). Our experiments shows that this\nmethod is not very successful on large-scale datsets (e.g.,ImageNet).", "mimetype": "text/plain", "start_char_idx": 11378, "end_char_idx": 11815, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7cbdd57-6a2b-43c3-bb41-9b55d76c0f0b": {"__data__": {"id_": "c7cbdd57-6a2b-43c3-bb41-9b55d76c0f0b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8650cb1-e3fc-461a-805c-329c9560f394", "node_type": "1", "metadata": {}, "hash": "f3be1719d4717c2732712f82bdfe2eb5d9e1bc2335ac50b40526fbf3b3e19fe0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd9bc8e1-56e3-4a3e-9e0e-0ba5a42765e4", "node_type": "1", "metadata": {}, "hash": "65d34a5a52043ba8a80fda0e64b403cfc6bcc7cbeeb031793991f6e193071f56", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "BinaryNet[11]\npropose an extention of BinaryConnect, where both weights and activations are bi-\nnarized. Our method is different from them in the binarization method and the net-\nXNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\n5\nwork structure. We also compare our method with BinaryNet on ImageNet, and our\nmethod outperforms BinaryNet by a large margin.[39] argued that the noise introduced\nby weight binarization provides a form of regularization, which could help to improve\ntest accuracy.", "mimetype": "text/plain", "start_char_idx": 11816, "end_char_idx": 12338, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd9bc8e1-56e3-4a3e-9e0e-0ba5a42765e4": {"__data__": {"id_": "fd9bc8e1-56e3-4a3e-9e0e-0ba5a42765e4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7cbdd57-6a2b-43c3-bb41-9b55d76c0f0b", "node_type": "1", "metadata": {}, "hash": "84c889f8f857cf9005b3462f2fe090ccea3f34270a2d4d87213f857334d0f68e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36fe0d16-5f90-415d-97bb-8207ca42a491", "node_type": "1", "metadata": {}, "hash": "3e4f8cd75f77d9c3e31096bbd0b59bf4c50fb0d0a15585ad89ad75fb7d445f16", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This method binarizes weights while maintaining full precision activa-\ntion. [40] proposed fully binary training and testing in an array of committee machines\nwith randomized input. [41] retraine a previously trained neural network with binary\nweights and binary inputs.\n3\nBinary Convolutional Neural Network\nWe represent an L-layer CNN architecture with a triplet \u27e8I, W, \u2217\u27e9. I is a set of ten-\nsors, where each element I = Il(l=1,...,L) is the input tensor for the lth layer of CNN\n(Green cubes in figure 1).", "mimetype": "text/plain", "start_char_idx": 12339, "end_char_idx": 12848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36fe0d16-5f90-415d-97bb-8207ca42a491": {"__data__": {"id_": "36fe0d16-5f90-415d-97bb-8207ca42a491", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd9bc8e1-56e3-4a3e-9e0e-0ba5a42765e4", "node_type": "1", "metadata": {}, "hash": "65d34a5a52043ba8a80fda0e64b403cfc6bcc7cbeeb031793991f6e193071f56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1ddb1d8-d66a-443d-bb7e-f27adb0569d3", "node_type": "1", "metadata": {}, "hash": "08bceec66edb1d2a4ad6ac82325d6b40e49797ac8f0dd83394c33329fec5f1ee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "W is a set of tensors, where each element in this set W =\nWlk(k=1,...,Kl) is the kth weight filter in the lth layer of the CNN. Kl is the number of\nweight filters in the lth layer of the CNN. \u2217represents a convolutional operation with\nI and W as its operands2. I \u2208Rc\u00d7win\u00d7hin, where (c, win, hin) represents channels,\nwidth and height respectively.W \u2208Rc\u00d7w\u00d7h, where w \u2264win, h \u2264hin.", "mimetype": "text/plain", "start_char_idx": 12849, "end_char_idx": 13228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1ddb1d8-d66a-443d-bb7e-f27adb0569d3": {"__data__": {"id_": "a1ddb1d8-d66a-443d-bb7e-f27adb0569d3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36fe0d16-5f90-415d-97bb-8207ca42a491", "node_type": "1", "metadata": {}, "hash": "3e4f8cd75f77d9c3e31096bbd0b59bf4c50fb0d0a15585ad89ad75fb7d445f16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e1c208e-d993-46e0-a0e5-848afa168ce9", "node_type": "1", "metadata": {}, "hash": "a0103adbe9a5fea83f11726a4c92553915cf223bb6339cc432fc6b0b1e934533", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We propose\ntwo variations of binary CNN: Binary-weights, where the elements of W are binary\ntensors and XNOR-Networks, where elements of both I and W are binary tensors.\n3.1\nBinary-Weight-Networks\nIn order to constrain a convolutional neural network \u27e8I, W, \u2217\u27e9to have binary weights,\nwe estimate the real-value weight filter W \u2208W using a binary filter B \u2208{+1, \u22121}c\u00d7w\u00d7h\nand a scaling factor \u03b1 \u2208R+ such that W \u2248\u03b1B.", "mimetype": "text/plain", "start_char_idx": 13229, "end_char_idx": 13640, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e1c208e-d993-46e0-a0e5-848afa168ce9": {"__data__": {"id_": "9e1c208e-d993-46e0-a0e5-848afa168ce9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1ddb1d8-d66a-443d-bb7e-f27adb0569d3", "node_type": "1", "metadata": {}, "hash": "08bceec66edb1d2a4ad6ac82325d6b40e49797ac8f0dd83394c33329fec5f1ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a65598f-d600-409b-b08c-4f3879639fd1", "node_type": "1", "metadata": {}, "hash": "e13d2088030956e360f78e17cb341467a059dcae52df7f7b474aae0264a60be9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A convolutional operation can be ap-\npriximated by:\nI \u2217W \u2248(I \u2295B) \u03b1\n(1)\nwhere, \u2295indicates a convolution without any multiplication. Since the weight values\nare binary, we can implement the convolution with additions and subtractions. The bi-\nnary weight filters reduce memory usage by a factor of \u223c32\u00d7 compared to single-\nprecision filters. We represent a CNN with binary weights by \u27e8I, B, A, \u2295\u27e9, where B is\na set of binary tensors and A is a set of positive real scalars,", "mimetype": "text/plain", "start_char_idx": 13641, "end_char_idx": 14112, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a65598f-d600-409b-b08c-4f3879639fd1": {"__data__": {"id_": "3a65598f-d600-409b-b08c-4f3879639fd1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e1c208e-d993-46e0-a0e5-848afa168ce9", "node_type": "1", "metadata": {}, "hash": "a0103adbe9a5fea83f11726a4c92553915cf223bb6339cc432fc6b0b1e934533", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee9dc51b-cb69-4e2f-88b3-03eb228aea79", "node_type": "1", "metadata": {}, "hash": "23259df04f2560f9a3da52c2289d349cf98277aecc991b9004d4cd4554450e69", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "such that B = Blk is a\nbinary filter and \u03b1 = Alk is an scaling factor and Wlk \u2248AlkBlk\nEstimating binary weights: Without loss of generality we assume W, B are vectors\nin Rn, where n = c \u00d7 w \u00d7 h. To find an optimal estimation for W \u2248\u03b1B, we solve the\nfollowing optimization:\nJ(B, \u03b1) = \u2225W \u2212\u03b1B\u22252\n\u03b1\u2217, B\u2217= argmin\n\u03b1,B\nJ(B,", "mimetype": "text/plain", "start_char_idx": 14113, "end_char_idx": 14428, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee9dc51b-cb69-4e2f-88b3-03eb228aea79": {"__data__": {"id_": "ee9dc51b-cb69-4e2f-88b3-03eb228aea79", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a65598f-d600-409b-b08c-4f3879639fd1", "node_type": "1", "metadata": {}, "hash": "e13d2088030956e360f78e17cb341467a059dcae52df7f7b474aae0264a60be9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed16d59b-d284-411b-af81-dac6b38d8325", "node_type": "1", "metadata": {}, "hash": "46d6082d04b753f9ddfd2100672b0265a6f567aafa79ab73b4be457173d189fa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "\u03b1)\n(2)\n2 In this paper we assume convolutional filters do not have bias terms\n6\nRastegari et al.\nby expanding equation 2, we have\nJ(B, \u03b1) = \u03b12BTB \u22122\u03b1WTB + WTW\n(3)\nsince B \u2208{+1, \u22121}n, BTB = n is a constant . WTW is also a constant because\nW is a known variable. Lets define c = WTW.", "mimetype": "text/plain", "start_char_idx": 14429, "end_char_idx": 14710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed16d59b-d284-411b-af81-dac6b38d8325": {"__data__": {"id_": "ed16d59b-d284-411b-af81-dac6b38d8325", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee9dc51b-cb69-4e2f-88b3-03eb228aea79", "node_type": "1", "metadata": {}, "hash": "23259df04f2560f9a3da52c2289d349cf98277aecc991b9004d4cd4554450e69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b54be825-f95e-4cf8-b261-51dd9f7cea01", "node_type": "1", "metadata": {}, "hash": "ec0c23e4b84741748182cf2c7c415124c793cf281bd643c74f1c9be5c283b911", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Now, we can rewrite the equation 3 as\nfollow: J(B, \u03b1) = \u03b12n \u22122\u03b1WTB + c. The optimal solution for B can be achieved\nby maximizing the following constrained optimization: (note that \u03b1 is a positive value\nin equation 2, therefore it can be ignored in the maximization)\nB\u2217= argmax\nB\n{WTB} s.t.", "mimetype": "text/plain", "start_char_idx": 14711, "end_char_idx": 15000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b54be825-f95e-4cf8-b261-51dd9f7cea01": {"__data__": {"id_": "b54be825-f95e-4cf8-b261-51dd9f7cea01", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed16d59b-d284-411b-af81-dac6b38d8325", "node_type": "1", "metadata": {}, "hash": "46d6082d04b753f9ddfd2100672b0265a6f567aafa79ab73b4be457173d189fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51f9ac20-bfcd-4b4e-8a64-e687f205f918", "node_type": "1", "metadata": {}, "hash": "b0b49451c3ed4f7c83cf53ee85d5939b959cab320246c81a025fd740cee6f563", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "B \u2208{+1, \u22121}n\n(4)\nThis optimization can be solved by assigning Bi = +1 if Wi \u22650 and Bi = \u22121 if\nWi < 0, therefore the optimal solution is B\u2217= sign(W).", "mimetype": "text/plain", "start_char_idx": 15001, "end_char_idx": 15149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51f9ac20-bfcd-4b4e-8a64-e687f205f918": {"__data__": {"id_": "51f9ac20-bfcd-4b4e-8a64-e687f205f918", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b54be825-f95e-4cf8-b261-51dd9f7cea01", "node_type": "1", "metadata": {}, "hash": "ec0c23e4b84741748182cf2c7c415124c793cf281bd643c74f1c9be5c283b911", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd3c46c6-0611-4a40-b81e-7ca3240ad115", "node_type": "1", "metadata": {}, "hash": "cd4ed16bd7ada6d37dd1df6fc2e124acc952dd0b2e2eb1365c20ef6a538e7053", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In order to find the optimal\nvalue for the scaling factor \u03b1\u2217, we take the derivative of J with respect to \u03b1 and set it\nto zero:\n\u03b1\u2217= WTB\u2217\nn\n(5)\nBy replacing B\u2217with sign(W)\n\u03b1\u2217= WT sign(W)\nn\n=\n\ufffd|Wi|\nn\n= 1\nn\u2225W\u2225\u21131\n(6)\ntherefore, the optimal estimation of a binary weight filter can be simply achieved by\ntaking the sign of weight values. The optimal scaling factor is the average of absolute\nweight values.", "mimetype": "text/plain", "start_char_idx": 15150, "end_char_idx": 15551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd3c46c6-0611-4a40-b81e-7ca3240ad115": {"__data__": {"id_": "bd3c46c6-0611-4a40-b81e-7ca3240ad115", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51f9ac20-bfcd-4b4e-8a64-e687f205f918", "node_type": "1", "metadata": {}, "hash": "b0b49451c3ed4f7c83cf53ee85d5939b959cab320246c81a025fd740cee6f563", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d90622b2-db1e-4595-b45a-88021548e613", "node_type": "1", "metadata": {}, "hash": "1e78b50ff198a6b4d05a9b00d788cd49e5bf8ba2d36b53dac23ae3d7e9f47bec", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Training Binary-Weights-Networks: Each iteration of training a CNN involves three\nsteps; forward pass, backward pass and parameters update. To train a CNN with binary\nweights (in convolutional layers), we only binarize the weights during the forward pass\nand backward propagation. To compute the gradient for sign function sign(r), we fol-\nlow the same approach as [11], where \u2202sign\n\u2202r\n= r1|r|\u22641.", "mimetype": "text/plain", "start_char_idx": 15552, "end_char_idx": 15948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d90622b2-db1e-4595-b45a-88021548e613": {"__data__": {"id_": "d90622b2-db1e-4595-b45a-88021548e613", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd3c46c6-0611-4a40-b81e-7ca3240ad115", "node_type": "1", "metadata": {}, "hash": "cd4ed16bd7ada6d37dd1df6fc2e124acc952dd0b2e2eb1365c20ef6a538e7053", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3aa7adb4-b40f-4a03-9b10-5410ae180273", "node_type": "1", "metadata": {}, "hash": "7b82daee23752ed040d6f7477cc80583bfb23515970881748805f54539f85538", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The gradient in backward after\nthe scaled sign function is\n\u2202C\n\u2202Wi = \u2202C\n\ufffd\nWi ( 1\nn + \u2202sign\n\u2202Wi \u03b1). For updating the parameters, we\nuse the high precision (real-value) weights. Because, in gradient descend the parameter\nchanges are tiny, binarization after updating the parameters ignores these changes and\nthe training objective can not be improved. [11,38] also employed this strategy to train\na binary network.\nAlgorithm 1 demonstrates our procedure for training a CNN with binary weights.", "mimetype": "text/plain", "start_char_idx": 15949, "end_char_idx": 16439, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3aa7adb4-b40f-4a03-9b10-5410ae180273": {"__data__": {"id_": "3aa7adb4-b40f-4a03-9b10-5410ae180273", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d90622b2-db1e-4595-b45a-88021548e613", "node_type": "1", "metadata": {}, "hash": "1e78b50ff198a6b4d05a9b00d788cd49e5bf8ba2d36b53dac23ae3d7e9f47bec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2926afe7-549a-42c9-b99f-7a235508a9af", "node_type": "1", "metadata": {}, "hash": "0f655245534f1f2b6647ec5ff22673a21f27873be5b51a9bfe87b26ef4f3581e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "First, we binarize the weight filters at each layer by computing B and A. Then we call\nforward propagation using binary weights and its corresponding scaling factors, where\nall the convolutional operations are carried out by equation 1. Then, we call backward\npropagation, where the gradients are computed with respect to the estimated weight\nfilters \ufffd\nW. Lastly, the parameters and the learning rate gets updated by an update rule\ne.g.,SGD update with momentum or ADAM [42].\nOnce the training finished, there is no need to keep the real-value weights.", "mimetype": "text/plain", "start_char_idx": 16440, "end_char_idx": 16992, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2926afe7-549a-42c9-b99f-7a235508a9af": {"__data__": {"id_": "2926afe7-549a-42c9-b99f-7a235508a9af", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3aa7adb4-b40f-4a03-9b10-5410ae180273", "node_type": "1", "metadata": {}, "hash": "7b82daee23752ed040d6f7477cc80583bfb23515970881748805f54539f85538", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7cdaf08-647a-4d36-b6b2-35b4bd5ab93b", "node_type": "1", "metadata": {}, "hash": "5eea8beb0fcd916a31d25ecd43caeba792ca91cb39f0e7168db033e57717b7f7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Because,\nat inference we only perform forward propagation with the binarized weights.\nXNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\n7\nAlgorithm 1 Training an L-layers CNN with binary weights:\nInput: A minibatch of inputs and targets (I, Y), cost function C(Y, \u02c6\nY), current weight Wt and\ncurrent learning rate \u03b7t.\nOutput: updated weight Wt+1 and updated learning rate \u03b7t+1.", "mimetype": "text/plain", "start_char_idx": 16993, "end_char_idx": 17397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7cdaf08-647a-4d36-b6b2-35b4bd5ab93b": {"__data__": {"id_": "f7cdaf08-647a-4d36-b6b2-35b4bd5ab93b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2926afe7-549a-42c9-b99f-7a235508a9af", "node_type": "1", "metadata": {}, "hash": "0f655245534f1f2b6647ec5ff22673a21f27873be5b51a9bfe87b26ef4f3581e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c41a97a-bce7-43b4-afb6-8b20b267aee1", "node_type": "1", "metadata": {}, "hash": "103b8563cdf77fd3097bca324cf6d364567c7edeab1f4eb76d7b7f8eff6fc6ab", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "1: Binarizing weight filters:\n2: for l = 1 to L do\n3:\nfor kth filter in lth layer do\n4:\nAlk = 1\nn\u2225Wt\nlk\u2225\u21131\n5:\nBlk = sign(Wt\nlk)\n6:\n\ufffd\nWlk = AlkBlk\n7: \u02c6\nY =\nBinaryForward(I, B,", "mimetype": "text/plain", "start_char_idx": 17398, "end_char_idx": 17572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c41a97a-bce7-43b4-afb6-8b20b267aee1": {"__data__": {"id_": "6c41a97a-bce7-43b4-afb6-8b20b267aee1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7cdaf08-647a-4d36-b6b2-35b4bd5ab93b", "node_type": "1", "metadata": {}, "hash": "5eea8beb0fcd916a31d25ecd43caeba792ca91cb39f0e7168db033e57717b7f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79c5db2a-a4ec-4b81-aed2-97905f7968bc", "node_type": "1", "metadata": {}, "hash": "4e86ce0d4928e64f16f96e5878f148f291f7536c87aa1190cf2bbf35aa67f45b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "A) // standard forward propagation except that convolutions are computed\nusing equation 1 or 11\n8:\n\u2202C\n\u2202\ufffd\nW = BinaryBackward( \u2202C\n\u2202\u02c6\nY , \ufffd\nW)\n// standard backward propagation except that gradients are computed\nusing \ufffd\nW instead of Wt\n9: Wt+1 = UpdateParameters(Wt, \u2202C\n\u2202\ufffd\nW , \u03b7t) // Any update rules (e.g.SGD or ADAM)\n10: \u03b7t+1 = UpdateLearningrate(\u03b7t,", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79c5db2a-a4ec-4b81-aed2-97905f7968bc": {"__data__": {"id_": "79c5db2a-a4ec-4b81-aed2-97905f7968bc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c41a97a-bce7-43b4-afb6-8b20b267aee1", "node_type": "1", "metadata": {}, "hash": "103b8563cdf77fd3097bca324cf6d364567c7edeab1f4eb76d7b7f8eff6fc6ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "849f070a-a09f-4577-9e94-b57a991af1c7", "node_type": "1", "metadata": {}, "hash": "8684983217ef5f0343d76f702894d9465c9b6395a63c232c045e7eeeae1a342c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "t) // Any learning rate scheduling function\n3.2\nXNOR-Networks\nSo far, we managed to find binary weights and a scaling factor to estimate the real-\nvalue weights. The inputs to the convolutional layers are still real-value tensors. Now,\nwe explain how to binarize both weigths and inputs, so convolutions can be imple-\nmented efficiently using XNOR and bitcounting operations. This is the key element of\nour XNOR-Networks.", "mimetype": "text/plain", "start_char_idx": 17923, "end_char_idx": 18344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "849f070a-a09f-4577-9e94-b57a991af1c7": {"__data__": {"id_": "849f070a-a09f-4577-9e94-b57a991af1c7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79c5db2a-a4ec-4b81-aed2-97905f7968bc", "node_type": "1", "metadata": {}, "hash": "4e86ce0d4928e64f16f96e5878f148f291f7536c87aa1190cf2bbf35aa67f45b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a50d1ab-c431-4d4b-90a6-2b1107ee143e", "node_type": "1", "metadata": {}, "hash": "7599e12525c92e8407da63c6857af708ff176539ae75ec3e4037f703c702c5f0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In order to constrain a convolutional neural network \u27e8I, W, \u2217\u27e9\nto have binary weights and binary inputs, we need to enforce binary operands at each\nstep of the convolutional operation. A convolution consist of repeating a shift operation\nand a dot product. Shift operation moves the weight filter over the input and the dot\nproduct performs element-wise multiplications between the values of the weight filter\nand the corresponding part of the input. If we express dot product in terms of binary\noperations, convolution can be approximated using binary operations.", "mimetype": "text/plain", "start_char_idx": 18345, "end_char_idx": 18909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a50d1ab-c431-4d4b-90a6-2b1107ee143e": {"__data__": {"id_": "2a50d1ab-c431-4d4b-90a6-2b1107ee143e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "849f070a-a09f-4577-9e94-b57a991af1c7", "node_type": "1", "metadata": {}, "hash": "8684983217ef5f0343d76f702894d9465c9b6395a63c232c045e7eeeae1a342c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7cf8d3cf-e074-4c78-8769-f7254411e3c0", "node_type": "1", "metadata": {}, "hash": "2b929a7de838573980e6fc5f9969c02fdd4e3df731b42a407dae2063c7ca55eb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Dot product be-\ntween two binary vectors can be implemented by XNOR-Bitcounting operations [11].\nIn this section, we explain how to approximate the dot product between two vectors in\nRn by a dot product between two vectors in {+1, \u22121}n. Next, we demonstrate how to\nuse this approximation for estimating a convolutional operation between two tensors.", "mimetype": "text/plain", "start_char_idx": 18910, "end_char_idx": 19259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7cf8d3cf-e074-4c78-8769-f7254411e3c0": {"__data__": {"id_": "7cf8d3cf-e074-4c78-8769-f7254411e3c0", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a50d1ab-c431-4d4b-90a6-2b1107ee143e", "node_type": "1", "metadata": {}, "hash": "7599e12525c92e8407da63c6857af708ff176539ae75ec3e4037f703c702c5f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d9bd29d-615c-4477-aa4b-4832f9fa14a4", "node_type": "1", "metadata": {}, "hash": "aa5018cef5d686bbe7968dcb4014fe9500582fbe2636d53671aa38e1f7d7e784", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Binary Dot Product: To approximate the dot product between X, W \u2208Rn such that\nXTW \u2248\u03b2HT\u03b1B, where H, B \u2208{+1, \u22121}n and \u03b2, \u03b1 \u2208R+, we solve the following\noptimization:\n\u03b1\u2217, B\u2217, \u03b2\u2217, H\u2217= argmin\n\u03b1,B,\u03b2,H\n\u2225X \u2299W \u2212\u03b2\u03b1H \u2299B\u2225\n(7)\nwhere \u2299indicates element-wise product.", "mimetype": "text/plain", "start_char_idx": 19260, "end_char_idx": 19511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d9bd29d-615c-4477-aa4b-4832f9fa14a4": {"__data__": {"id_": "5d9bd29d-615c-4477-aa4b-4832f9fa14a4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7cf8d3cf-e074-4c78-8769-f7254411e3c0", "node_type": "1", "metadata": {}, "hash": "2b929a7de838573980e6fc5f9969c02fdd4e3df731b42a407dae2063c7ca55eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "987df21d-13c3-4b91-970c-94a91a2f82ae", "node_type": "1", "metadata": {}, "hash": "7509b537578de8c2beb80fe6e4d1c0299aa69761127882318bc96f7727df5c7d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We define Y \u2208Rn such that Yi = XiWi,\nC \u2208{+1, \u22121}n such that Ci = HiBi and \u03b3 \u2208R+ such that \u03b3 = \u03b2\u03b1. The equation 7\ncan be written as:\n\u03b3\u2217, C\u2217= argmin\n\u03b3,C\n\u2225Y \u2212\u03b3C\u2225\n(8)\n8\nRastegari et al.\n-1.4  0.5 \u2026  0.2   2 \n-1  1 \u2026  1   1 \nRedundant computations in overlapping areas", "mimetype": "text/plain", "start_char_idx": 19512, "end_char_idx": 19775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "987df21d-13c3-4b91-970c-94a91a2f82ae": {"__data__": {"id_": "987df21d-13c3-4b91-970c-94a91a2f82ae", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d9bd29d-615c-4477-aa4b-4832f9fa14a4", "node_type": "1", "metadata": {}, "hash": "aa5018cef5d686bbe7968dcb4014fe9500582fbe2636d53671aa38e1f7d7e784", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6642526-3af2-433a-82ab-f978c2caa3f6", "node_type": "1", "metadata": {}, "hash": "8eca972808a2bf5320bdacd5b96133b1286398303663cb8d4543e296944ba377", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "= \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \nc \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2248 \n= \n= \n= \n= \n(1)  Binarizing Weight \n(2) Binarizing Input \n(4) Convolution with XNOR-Bitcount \n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\u2026 \nInefficient", "mimetype": "text/plain", "start_char_idx": 19777, "end_char_idx": 20008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6642526-3af2-433a-82ab-f978c2caa3f6": {"__data__": {"id_": "d6642526-3af2-433a-82ab-f978c2caa3f6", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "987df21d-13c3-4b91-970c-94a91a2f82ae", "node_type": "1", "metadata": {}, "hash": "7509b537578de8c2beb80fe6e4d1c0299aa69761127882318bc96f7727df5c7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6d4c5f4-3cdc-403a-9c3a-66ed10ce59a2", "node_type": "1", "metadata": {}, "hash": "2250f4803604b2ee0690bba02edecd03281a5647da3bf42a6f75a1ef51fcf190", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Efficient \n= \n= \n(3) Binarizing Input \nFig. 2: This figure illustrates the procedure explained in section 3.2 for approximating a convo-\nlution using binary operations.\nthe optimal solutions can be achieved from equation 2 as follow\nC\u2217= sign(Y) = sign(X) \u2299sign(W) = H\u2217\u2299B\u2217\n(9)\nSince |Xi|, |Wi| are independent, knowing that Yi = XiWi then,", "mimetype": "text/plain", "start_char_idx": 20010, "end_char_idx": 20348, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6d4c5f4-3cdc-403a-9c3a-66ed10ce59a2": {"__data__": {"id_": "e6d4c5f4-3cdc-403a-9c3a-66ed10ce59a2", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6642526-3af2-433a-82ab-f978c2caa3f6", "node_type": "1", "metadata": {}, "hash": "8eca972808a2bf5320bdacd5b96133b1286398303663cb8d4543e296944ba377", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5cc0672-8f70-412d-af6e-7ca91249b848", "node_type": "1", "metadata": {}, "hash": "7b07531253005623d9962bb75f56fa82ca5d558ba79745f22fea9b1150119006", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "E [|Yi|] = E [|Xi||Wi|] = E [|Xi|] E [|Wi|] therefore,\n\u03b3\u2217=\n\ufffd|Yi|\nn\n=\n\ufffd|Xi||Wi|\nn\n\u2248\n\ufffd1\nn\u2225X\u2225\u21131\n\ufffd\ufffd1\nn\u2225W\u2225\u21131\n\ufffd\n= \u03b2\u2217\u03b1\u2217\n(10)\nBinary Convolution: Convolving weight filter W \u2208Rc\u00d7w\u00d7h (where win \u226bw,", "mimetype": "text/plain", "start_char_idx": 20349, "end_char_idx": 20536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5cc0672-8f70-412d-af6e-7ca91249b848": {"__data__": {"id_": "c5cc0672-8f70-412d-af6e-7ca91249b848", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6d4c5f4-3cdc-403a-9c3a-66ed10ce59a2", "node_type": "1", "metadata": {}, "hash": "2250f4803604b2ee0690bba02edecd03281a5647da3bf42a6f75a1ef51fcf190", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9867c00-0d52-406c-b6aa-4c4487e64bff", "node_type": "1", "metadata": {}, "hash": "005330905dedba5351ad6578ba4072a20e2f6d06c26483361c5da9bee3de9645", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "hin \u226b\nh) with the input tensor I \u2208Rc\u00d7win\u00d7hin requires computing the scaling factor \u03b2 for all\npossible sub-tensors in I with same size as W. Two of these sub-tensors are illustrated\nin figure 2 (second row) by X1 and X2. Due to overlaps between subtensors, comput-\ning \u03b2 for all possible sub-tensors leads to a large number of redundant computations.", "mimetype": "text/plain", "start_char_idx": 20537, "end_char_idx": 20886, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9867c00-0d52-406c-b6aa-4c4487e64bff": {"__data__": {"id_": "c9867c00-0d52-406c-b6aa-4c4487e64bff", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5cc0672-8f70-412d-af6e-7ca91249b848", "node_type": "1", "metadata": {}, "hash": "7b07531253005623d9962bb75f56fa82ca5d558ba79745f22fea9b1150119006", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3ff5b87-96c1-45ce-b887-03ca029cbe9c", "node_type": "1", "metadata": {}, "hash": "da60c613834a7113de9010507e2393fba3b0f34fabbec18d64605e1fb228ec1d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To overcome this redundancy, first, we compute a matrix A =\n\ufffd|I:,:,i|\nc\n, which is the\naverage over absolute values of the elements in the input I across the channel. Then\nwe convolve A with a 2D filter k \u2208Rw\u00d7h, K = A \u2217k, where \u2200ij kij =\n1\nw\u00d7h. K\ncontains scaling factors \u03b2 for all sub-tensors in the input I. Kij corresponds to \u03b2 for\na sub-tensor centered at the location ij (across width and height).", "mimetype": "text/plain", "start_char_idx": 20887, "end_char_idx": 21289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3ff5b87-96c1-45ce-b887-03ca029cbe9c": {"__data__": {"id_": "d3ff5b87-96c1-45ce-b887-03ca029cbe9c", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9867c00-0d52-406c-b6aa-4c4487e64bff", "node_type": "1", "metadata": {}, "hash": "005330905dedba5351ad6578ba4072a20e2f6d06c26483361c5da9bee3de9645", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8bc747e-7478-409e-8b53-b16648b4e708", "node_type": "1", "metadata": {}, "hash": "6559d100ee13b4b92011d5803be0c90b78a247ca10b2d6aa9d287eb1fb59d400", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This procedure is\nshown in the third row of figure 2. Once we obtained the scaling factor \u03b1 for the weight\nand \u03b2 for all sub-tensors in I (denoted by K), we can approximate the convolution\nbetween input I and weight filter W mainly using binary operations:\nI \u2217W \u2248(sign(I) \u229bsign(W)) \u2299K\u03b1\n(11)\nwhere \u229bindicates a convolutional operation using XNOR and bitcount operations. This\nis illustrated in the last row in figure 2.", "mimetype": "text/plain", "start_char_idx": 21290, "end_char_idx": 21708, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8bc747e-7478-409e-8b53-b16648b4e708": {"__data__": {"id_": "d8bc747e-7478-409e-8b53-b16648b4e708", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3ff5b87-96c1-45ce-b887-03ca029cbe9c", "node_type": "1", "metadata": {}, "hash": "da60c613834a7113de9010507e2393fba3b0f34fabbec18d64605e1fb228ec1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f3e12e4-a8da-464a-8f39-3607ca063b9f", "node_type": "1", "metadata": {}, "hash": "422a876e1bcddadd76590eb44ed442710df38bed2fc71d9c8581f77a69f2a7bc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Note that the number of non-binary operations\nis very small compared to binary operations.\nXNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\n9\nA typical block in CNN \nA block in XNOR-Net \nPool \r  \nBinConv \r  \n \r  \nBNorm \r  \nBinAc/v \r  \nBNorm \r  \nAc/v \r  \nPool \r  \nConv \r  \n \r  \nFig. 3: This figure contrasts the block structure in our XNOR-Network (right) with a typical\nCNN (left).", "mimetype": "text/plain", "start_char_idx": 21709, "end_char_idx": 22118, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f3e12e4-a8da-464a-8f39-3607ca063b9f": {"__data__": {"id_": "7f3e12e4-a8da-464a-8f39-3607ca063b9f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8bc747e-7478-409e-8b53-b16648b4e708", "node_type": "1", "metadata": {}, "hash": "6559d100ee13b4b92011d5803be0c90b78a247ca10b2d6aa9d287eb1fb59d400", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b94611cd-0027-4160-a6c8-46b243d25ab8", "node_type": "1", "metadata": {}, "hash": "368dd9c124141a147a27b22b2db8079cc83160c6a3ef6faba6d693515a4eb0a5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Training XNOR-Networks: A typical block in CNN contains several different layers.\nFigure 3 (left) illustrates a typical block in a CNN. This block has four layers in the\nfollowing order: 1-Convolutional, 2-Batch Normalization, 3-Activation and 4-Pooling.\nBatch Normalization layer[43] normalizes the input batch by its mean and variance.\nThe activation is an element-wise non-linear function (e.g.,Sigmoid, ReLU).", "mimetype": "text/plain", "start_char_idx": 22119, "end_char_idx": 22532, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b94611cd-0027-4160-a6c8-46b243d25ab8": {"__data__": {"id_": "b94611cd-0027-4160-a6c8-46b243d25ab8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f3e12e4-a8da-464a-8f39-3607ca063b9f", "node_type": "1", "metadata": {}, "hash": "422a876e1bcddadd76590eb44ed442710df38bed2fc71d9c8581f77a69f2a7bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5879588-e82d-4d60-8598-36520dc89449", "node_type": "1", "metadata": {}, "hash": "dfeb1f520a84fd7740be19d97a7c968be312af070d0f9b0f8d6bd8973e03845a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The pool-\ning layer applies any type of pooling (e.g.,max,min or average) on the input batch.\nApplying pooling on binary input results in significant loss of information. For exam-\nple, max-pooling on binary input returns a tensor that most of its elements are equal to\n+1. Therefore, we put the pooling layer after the convolution. To further decrease the\ninformation loss due to binarization, we normalize the input before binarization.", "mimetype": "text/plain", "start_char_idx": 22533, "end_char_idx": 22971, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5879588-e82d-4d60-8598-36520dc89449": {"__data__": {"id_": "b5879588-e82d-4d60-8598-36520dc89449", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b94611cd-0027-4160-a6c8-46b243d25ab8", "node_type": "1", "metadata": {}, "hash": "368dd9c124141a147a27b22b2db8079cc83160c6a3ef6faba6d693515a4eb0a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dc2e943-b385-43cb-a390-4db54b5eb8fc", "node_type": "1", "metadata": {}, "hash": "76bebb31a7483be1f01efa291b967badf5690cdeaa321676d7e78191f1ac7afd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This\nensures the data to hold zero mean, therefore, thresholding at zero leads to less quanti-\nzation error. The order of layers in a block of binary CNN is shown in Figure 3(right).\nThe binary activation layer(BinActiv) computes K and sign(I) as explained in sec-\ntion 3.2. In the next layer (BinConv), given K and sign(I), we compute binary convo-\nlution by equation 11. Then at the last layer (Pool), we apply the pooling operations.", "mimetype": "text/plain", "start_char_idx": 22972, "end_char_idx": 23408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3dc2e943-b385-43cb-a390-4db54b5eb8fc": {"__data__": {"id_": "3dc2e943-b385-43cb-a390-4db54b5eb8fc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5879588-e82d-4d60-8598-36520dc89449", "node_type": "1", "metadata": {}, "hash": "dfeb1f520a84fd7740be19d97a7c968be312af070d0f9b0f8d6bd8973e03845a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c528384-6c68-4c6a-9a11-6de467c656fc", "node_type": "1", "metadata": {}, "hash": "d50b8c5fe29ca65db71cc573793897819a8810e72dfe636ca81a7d0e8795a556", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We can insert a non-binary activation(e.g.,ReLU) after binary convolution. This helps\nwhen we use state-of-the-art networks (e.g.,AlexNet or VGG).\nOnce we have the binary CNN structure, the training algorithm would be the same\nas algorithm 1.\nBinary Gradient: The computational bottleneck in the backward pass at each layer\nis computing a convolution between weight filters(w) and the gradients with respect of\nthe inputs (gin). Similar to binarization in the forward pass, we can binarize gin in the\nbackward pass.", "mimetype": "text/plain", "start_char_idx": 23409, "end_char_idx": 23924, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c528384-6c68-4c6a-9a11-6de467c656fc": {"__data__": {"id_": "2c528384-6c68-4c6a-9a11-6de467c656fc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dc2e943-b385-43cb-a390-4db54b5eb8fc", "node_type": "1", "metadata": {}, "hash": "76bebb31a7483be1f01efa291b967badf5690cdeaa321676d7e78191f1ac7afd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81a437ce-4410-4a79-bdbf-d39059d0eead", "node_type": "1", "metadata": {}, "hash": "8a3f4b01cbe3350dcb964f08861b828cda4dd15069d5df0f2972809f24a0840a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "This leads to a very efficient training procedure using binary operations.\nNote that if we use equation 6 to compute the scaling factor for gin, the direction of\nmaximum change for SGD would be diminished. To preserve the maximum change in\nall dimensions, we use maxi(|gin\ni |) as the scaling factor.\nk-bit Quantization: So far, we showed 1-bit quantization of weights and inputs\nusing sign(x) function.", "mimetype": "text/plain", "start_char_idx": 23925, "end_char_idx": 24328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81a437ce-4410-4a79-bdbf-d39059d0eead": {"__data__": {"id_": "81a437ce-4410-4a79-bdbf-d39059d0eead", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c528384-6c68-4c6a-9a11-6de467c656fc", "node_type": "1", "metadata": {}, "hash": "d50b8c5fe29ca65db71cc573793897819a8810e72dfe636ca81a7d0e8795a556", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4feffabc-f8b8-4783-b457-15644ca8de40", "node_type": "1", "metadata": {}, "hash": "b3bbfd3fe1cf27414e947b4d2f20741ccc5d59060e3c74ea29b6218ba2802491", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "One can easily extend the quantization level to k-bits by using\nqk(x) = 2( [(2k\u22121)( x+1\n2\n)]\n2k\u22121\n\u22121\n2) instead of the sign function. Where [.] indicates rounding\noperation and x \u2208[\u22121, 1].\n4\nExperiments\nWe evaluate our method by analyzing its efficiency and accuracy. We measure the ef-\nficiency by computing the computational speedup (in terms of number of high preci-\nsion operation) achieved by our binary convolution vs. standard convolution.", "mimetype": "text/plain", "start_char_idx": 24329, "end_char_idx": 24775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4feffabc-f8b8-4783-b457-15644ca8de40": {"__data__": {"id_": "4feffabc-f8b8-4783-b457-15644ca8de40", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81a437ce-4410-4a79-bdbf-d39059d0eead", "node_type": "1", "metadata": {}, "hash": "8a3f4b01cbe3350dcb964f08861b828cda4dd15069d5df0f2972809f24a0840a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b12b2e54-0c86-4c24-8791-746706842a2e", "node_type": "1", "metadata": {}, "hash": "072cbbd528892912faf36cb4aecf4d9ca3fb76a79670d7fd1aab3bc027c9777b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "To mea-\n10\nRastegari et al.\n0 \r  \n200 \r  \n400 \r  \n600 \r  \n800 \r  \n1000 \r  \n1200 \r  \nVGG-\u00ad\u201019 \r  \nResNet-\u00ad\u201018 \r  \nAlexNet \r  \nDouble \r  Precision \r  \nBinary \r  Precision \r  \n16MB \n1GB \n1.5MB \n100MB \n475MB \n7.", "mimetype": "text/plain", "start_char_idx": 24776, "end_char_idx": 24983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b12b2e54-0c86-4c24-8791-746706842a2e": {"__data__": {"id_": "b12b2e54-0c86-4c24-8791-746706842a2e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4feffabc-f8b8-4783-b457-15644ca8de40", "node_type": "1", "metadata": {}, "hash": "b3bbfd3fe1cf27414e947b4d2f20741ccc5d59060e3c74ea29b6218ba2802491", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12b7697b-71a7-4392-951d-24cd554528b8", "node_type": "1", "metadata": {}, "hash": "78554b06839e7dd2d04947f15bf23e9f32cabed1b46df78382dab2965884071d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4MB \n(a)\n1   \n32  \n1024\nnumber of channels\n0x\n20x\n40x\n60x\n80x\nSpeedup by varying channel size\n(b)\n0x0\n10x10\n20x20\nfilter size\n50x\n55x\n60x\n65x\nSpeedup by varying filter size\n(c)\nFig. 4: This figure shows the efficiency of binary convolutions in terms of memory(a) and\ncomputation(b-c).", "mimetype": "text/plain", "start_char_idx": 24983, "end_char_idx": 25267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12b7697b-71a7-4392-951d-24cd554528b8": {"__data__": {"id_": "12b7697b-71a7-4392-951d-24cd554528b8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b12b2e54-0c86-4c24-8791-746706842a2e", "node_type": "1", "metadata": {}, "hash": "072cbbd528892912faf36cb4aecf4d9ca3fb76a79670d7fd1aab3bc027c9777b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1a84cae-a62d-45e0-acad-09c984d7a470", "node_type": "1", "metadata": {}, "hash": "fc615eb6abe2ea2ca30d8afb995266a17b0fe25761327237dfae27453e3e0aed", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(a) is contrasting the required memory for binary and double precision weights\nin three different architectures(AlexNet, ResNet-18 and VGG-19). (b,c) Show speedup gained by\nbinary convolution under (b)-different number of channels and (c)-different filter size\nsure accuracy, we perform image classification on the large-scale ImageNet dataset.\nThis paper is the first work that evaluates binary neural networks on the ImageNet\ndataset. Our binarization technique is general, we can use any CNN architecture. We\nevaluate AlexNet [1] and two deeper architectures in our experiments.", "mimetype": "text/plain", "start_char_idx": 25268, "end_char_idx": 25849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1a84cae-a62d-45e0-acad-09c984d7a470": {"__data__": {"id_": "a1a84cae-a62d-45e0-acad-09c984d7a470", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12b7697b-71a7-4392-951d-24cd554528b8", "node_type": "1", "metadata": {}, "hash": "78554b06839e7dd2d04947f15bf23e9f32cabed1b46df78382dab2965884071d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a52ec5d-18c3-4107-94c7-660e459a6692", "node_type": "1", "metadata": {}, "hash": "daa1f18eb212acbc092b5cd80e1cca76ef572f10ee1117e4e7754f303f790bb4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We compare our\nmethod with two recent works on binarizing neural networks; BinaryConnect [38] and\nBinaryNet [11]. The classification accuracy of our binary-weight-network version of\nAlexNet is as accurate as the full precision version of AlexNet. This classification ac-\ncuracy outperforms competitors on binary neural networks by a large margin. We also\npresent an ablation study, where we evaluate the key elements of our proposed method;\ncomputing scaling factors and our block structure for binary CNN. We shows that our\nmethod of computing the scaling factors is important to reach high accuracy.", "mimetype": "text/plain", "start_char_idx": 25850, "end_char_idx": 26451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a52ec5d-18c3-4107-94c7-660e459a6692": {"__data__": {"id_": "5a52ec5d-18c3-4107-94c7-660e459a6692", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1a84cae-a62d-45e0-acad-09c984d7a470", "node_type": "1", "metadata": {}, "hash": "fc615eb6abe2ea2ca30d8afb995266a17b0fe25761327237dfae27453e3e0aed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc021ad9-7922-4548-8e04-f17492f46b7a", "node_type": "1", "metadata": {}, "hash": "bc4e95e47d404af0240a46ca1df26ecd96a532b31443dcef7f0f7edb81420df7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4.1\nEfficiency Analysis\nIn an standard convolution, the total number of operations is cNWNI, where c is the\nnumber of channels, NW = wh and NI = winhin. Note that some modern CPUs can\nfuse the multiplication and addition as a single cycle operation. On those CPUs, Binary-\nWeight-Networks does not deliver speed up. Our binary approximation of convolution\n(equation 11) has cNWNI binary operations and NI non-binary operations.", "mimetype": "text/plain", "start_char_idx": 26452, "end_char_idx": 26879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc021ad9-7922-4548-8e04-f17492f46b7a": {"__data__": {"id_": "dc021ad9-7922-4548-8e04-f17492f46b7a", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a52ec5d-18c3-4107-94c7-660e459a6692", "node_type": "1", "metadata": {}, "hash": "daa1f18eb212acbc092b5cd80e1cca76ef572f10ee1117e4e7754f303f790bb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e99832ef-4ee2-4207-80ac-3ed6607927ce", "node_type": "1", "metadata": {}, "hash": "46440b935cfcefb76018e381734801fcf0a70497df8f454213974734f3d2c288", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "With the\ncurrent generation of CPUs, we can perform 64 binary operations in one clock of CPU,\ntherefore the speedup can be computed by S =\ncNWNI\n1\n64 cNWNI+NI =\n64cNW\ncNW+64.\nThe speedup depends on the channel size and filter size but not the input size. In fig-\nure 4-(b-c) we illustrate the speedup achieved by changing the number of channels and\nfilter size.", "mimetype": "text/plain", "start_char_idx": 26880, "end_char_idx": 27241, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e99832ef-4ee2-4207-80ac-3ed6607927ce": {"__data__": {"id_": "e99832ef-4ee2-4207-80ac-3ed6607927ce", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc021ad9-7922-4548-8e04-f17492f46b7a", "node_type": "1", "metadata": {}, "hash": "bc4e95e47d404af0240a46ca1df26ecd96a532b31443dcef7f0f7edb81420df7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cc45d1c-43df-4b9c-9462-f4dfd3e37555", "node_type": "1", "metadata": {}, "hash": "7cf1a81aa64cc3bfe870f91a6e7c5c808c33eb4a75ee3b4e0e896cfc242fafe2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "While changing one parameter, we fix other parameters as follows: c = 256,\nnI = 142 and nW = 32 (majority of convolutions in ResNet[4] architecture have this\nstructure). Using our approximation of convolution we gain 62.27\u00d7 theoretical speed\nup, but in our CPU implementation with all of the overheads, we achieve 58x speed\nup in one convolution (Excluding the process for memory allocation and memory ac-\ncess).", "mimetype": "text/plain", "start_char_idx": 27242, "end_char_idx": 27654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6cc45d1c-43df-4b9c-9462-f4dfd3e37555": {"__data__": {"id_": "6cc45d1c-43df-4b9c-9462-f4dfd3e37555", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e99832ef-4ee2-4207-80ac-3ed6607927ce", "node_type": "1", "metadata": {}, "hash": "46440b935cfcefb76018e381734801fcf0a70497df8f454213974734f3d2c288", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "908a880e-de47-4c12-ab9c-e57b74a70e64", "node_type": "1", "metadata": {}, "hash": "538a2e7128c253a667376137960235ad8623e64b7f6f384f89b467660d41d430", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "With the small channel size (c = 3) and filter size (NW = 1 \u00d7 1) the speedup\nis not considerably high. This motivates us to avoid binarization at the first and last\nXNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\n11\nlayer of a CNN. In the first layer the chanel size is 3 and in the last layer the filter size\nis 1 \u00d7 1. A similar strategy was used in [11].", "mimetype": "text/plain", "start_char_idx": 27655, "end_char_idx": 28040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "908a880e-de47-4c12-ab9c-e57b74a70e64": {"__data__": {"id_": "908a880e-de47-4c12-ab9c-e57b74a70e64", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cc45d1c-43df-4b9c-9462-f4dfd3e37555", "node_type": "1", "metadata": {}, "hash": "7cf1a81aa64cc3bfe870f91a6e7c5c808c33eb4a75ee3b4e0e896cfc242fafe2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8d55467-1365-4c2c-a007-ac7303027784", "node_type": "1", "metadata": {}, "hash": "6d6d18888ed69ee70e504ca72d066528150882fb76febc118ed01699e6c83251", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Figure 4-a shows the required memory\nfor three different CNN architectures(AlexNet, VGG-19, ResNet-18) with binary and\ndouble precision weights. Binary-weight-networks are so small that can be easily fitted\ninto portable devices. BinaryNet [11] is in the same order of memory and computation\nefficiency as our method. In Figure 4, we show an analysis of computation and memory\ncost for a binary convolution. The same analysis is valid for BinaryNet and BinaryCon-\nnect.", "mimetype": "text/plain", "start_char_idx": 28041, "end_char_idx": 28510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8d55467-1365-4c2c-a007-ac7303027784": {"__data__": {"id_": "a8d55467-1365-4c2c-a007-ac7303027784", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "908a880e-de47-4c12-ab9c-e57b74a70e64", "node_type": "1", "metadata": {}, "hash": "538a2e7128c253a667376137960235ad8623e64b7f6f384f89b467660d41d430", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80591771-6e15-4c26-8c9f-200b0b14708e", "node_type": "1", "metadata": {}, "hash": "bd928de35f5ada0f7b3af180ea23fea356975500d337a14d477e5b1f407e6e53", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "The key difference of our method is using a scaling-factor, which does not change\nthe order of efficiency while providing a significant improvement in accuracy.\n4.2\nImage Classification\nWe evaluate the performance of our proposed approach on the task of natural im-\nage classification. So far, in the literature, binary neural network methods have pre-\nsented their evaluations on either limited domain or simplified datasets e.g.,CIFAR-10,\nMNIST, SVHN. To compare with state-of-the-art vision, we evaluate our method on\nImageNet (ILSVRC2012).", "mimetype": "text/plain", "start_char_idx": 28511, "end_char_idx": 29054, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80591771-6e15-4c26-8c9f-200b0b14708e": {"__data__": {"id_": "80591771-6e15-4c26-8c9f-200b0b14708e", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8d55467-1365-4c2c-a007-ac7303027784", "node_type": "1", "metadata": {}, "hash": "6d6d18888ed69ee70e504ca72d066528150882fb76febc118ed01699e6c83251", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78bfa7e3-a4f9-4760-a73c-714c181f6716", "node_type": "1", "metadata": {}, "hash": "b7670731a0d24aeaa690d7b6763c6b8e349d7344a26d9b31c6e1dc78cfd4050c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "ImageNet has \u223c1.2M train images from 1K categories and\n50K validation images. The images in this dataset are natural images with reasonably\nhigh resolution compared to the CIFAR and MNIST dataset, which have relatively small\nimages. We report our classification performance using Top-1 and Top-5 accuracies.", "mimetype": "text/plain", "start_char_idx": 29055, "end_char_idx": 29362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78bfa7e3-a4f9-4760-a73c-714c181f6716": {"__data__": {"id_": "78bfa7e3-a4f9-4760-a73c-714c181f6716", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80591771-6e15-4c26-8c9f-200b0b14708e", "node_type": "1", "metadata": {}, "hash": "bd928de35f5ada0f7b3af180ea23fea356975500d337a14d477e5b1f407e6e53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d94de39-fa51-4cfe-bbb0-d287f46359a4", "node_type": "1", "metadata": {}, "hash": "76960d9f92b1647b80308aae81b7e70189a4a3c37b49d334f578e9873d7c1513", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We adopt three different CNN architectures as our base architectures for binarization:\nAlexNet [1], Residual Networks (known as ResNet) [4], and a variant of GoogLenet\n[3].We compare our Binary-weight-network (BWN) with BinaryConnect(BC) [38] and\nour XNOR-Networks(XNOR-Net) with BinaryNeuralNet(BNN) [11]. BinaryConnect(BC)\nis a method for training a deep neural network with binary weights during forward\nand backward propagations.", "mimetype": "text/plain", "start_char_idx": 29363, "end_char_idx": 29796, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d94de39-fa51-4cfe-bbb0-d287f46359a4": {"__data__": {"id_": "4d94de39-fa51-4cfe-bbb0-d287f46359a4", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78bfa7e3-a4f9-4760-a73c-714c181f6716", "node_type": "1", "metadata": {}, "hash": "b7670731a0d24aeaa690d7b6763c6b8e349d7344a26d9b31c6e1dc78cfd4050c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45dc4261-9b7f-479b-8128-002f34667159", "node_type": "1", "metadata": {}, "hash": "1dde9e29d8dd52aea1f42bff0ff776a656c4dfc998b0fe8f676f6c3001d74603", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Similar to our approach, they keep the real-value weights\nduring the updating parameters step. Our binarization is different from BC. The bina-\nrization in BC can be either deterministic or stochastic. We use the deterministic bina-\nrization for BC in our comparisons because the stochastic binarization is not efficient.\nThe same evaluation settings have been used and discussed in [11]. BinaryNeural-\nNet(BNN) [11] is a neural network with binary weights and activations during infer-\nence and gradient computation in training.", "mimetype": "text/plain", "start_char_idx": 29797, "end_char_idx": 30326, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45dc4261-9b7f-479b-8128-002f34667159": {"__data__": {"id_": "45dc4261-9b7f-479b-8128-002f34667159", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d94de39-fa51-4cfe-bbb0-d287f46359a4", "node_type": "1", "metadata": {}, "hash": "76960d9f92b1647b80308aae81b7e70189a4a3c37b49d334f578e9873d7c1513", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fdb303f-f149-4dba-a3af-cb1e43bedeab", "node_type": "1", "metadata": {}, "hash": "8fd63d586f869db589be6e0767d865cc193d37fb56796804c2150145fa8669e5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "In concept, this is a similar approach to our\nXNOR-Network but the binarization method and the network structure in BNN is dif-\nferent from ours. Their training algorithm is similar to BC and they used deterministic\nbinarization in their evaluations.\nCIFAR-10 : BC and BNN showed near state-of-the-art performance on CIFAR-\n10, MNIST, and SVHN dataset.", "mimetype": "text/plain", "start_char_idx": 30327, "end_char_idx": 30679, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7fdb303f-f149-4dba-a3af-cb1e43bedeab": {"__data__": {"id_": "7fdb303f-f149-4dba-a3af-cb1e43bedeab", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45dc4261-9b7f-479b-8128-002f34667159", "node_type": "1", "metadata": {}, "hash": "1dde9e29d8dd52aea1f42bff0ff776a656c4dfc998b0fe8f676f6c3001d74603", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6294cc3d-8b02-42e2-b564-accd8aea215b", "node_type": "1", "metadata": {}, "hash": "3f655509e52649f428e7bfc06ae007dd09c684bced45ca2feb5f61bdcdba9a6a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "BWN and XNOR-Net on CIFAR-10 using the same\nnetwork architecture as BC and BNN achieve the error rate of 9.88% and 10.17% re-\nspectively. In this paper we explore the possibility of obtaining near state-of-the-art\nresults on a much larger and more challenging dataset (ImageNet).\nAlexNet:\n[1] is a CNN architecture with 5 convolutional layers and two fully-\nconnected layers. This architecture was the first CNN architecture that showed to be\nsuccessful on ImageNet classification task. This network has 61M parameters.", "mimetype": "text/plain", "start_char_idx": 30680, "end_char_idx": 31199, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6294cc3d-8b02-42e2-b564-accd8aea215b": {"__data__": {"id_": "6294cc3d-8b02-42e2-b564-accd8aea215b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fdb303f-f149-4dba-a3af-cb1e43bedeab", "node_type": "1", "metadata": {}, "hash": "8fd63d586f869db589be6e0767d865cc193d37fb56796804c2150145fa8669e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5024c39-42e3-4b44-9a73-0d64a77e8bb8", "node_type": "1", "metadata": {}, "hash": "514c8ac3352afe0c4735adfcda24687c43eb80bfbc1ffbeec4052397bad62bfa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We use\nAlexNet coupled with batch normalization layers [43].\nTrain: In each iteration of training, images are resized to have 256 pixel at their\nsmaller dimension and then a random crop of 224\u00d7224 is selected for training. We run\n12\nRastegari et al.\n0\n10\n20\nNumber of epochs\n20\n40\n60\nAccuray(%)\nTop-1,", "mimetype": "text/plain", "start_char_idx": 31200, "end_char_idx": 31501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5024c39-42e3-4b44-9a73-0d64a77e8bb8": {"__data__": {"id_": "b5024c39-42e3-4b44-9a73-0d64a77e8bb8", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6294cc3d-8b02-42e2-b564-accd8aea215b", "node_type": "1", "metadata": {}, "hash": "3f655509e52649f428e7bfc06ae007dd09c684bced45ca2feb5f61bdcdba9a6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69fd70be-78c0-49cf-baf1-2f0dade331d3", "node_type": "1", "metadata": {}, "hash": "f9d76616b35021c43ab53319bd8d0e7d69887787d796e5cec33a72a82c1944b4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Binary-Weight\nBWN-train\nBWN-val\nBC-train\nBC-val\n0\n10\n20\nNumber of epochs\n20\n40\n60\nAccuray(%)\nTop-1, Binary-Weight-Input\nXNOR-Net-train\nXNOR-Net-val\nBNN-train\nBNN-val\n0\n10\n20\nNumber of epochs\n20\n40\n60\n80\nAccuray(%)\nTop-5,", "mimetype": "text/plain", "start_char_idx": 31502, "end_char_idx": 31722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69fd70be-78c0-49cf-baf1-2f0dade331d3": {"__data__": {"id_": "69fd70be-78c0-49cf-baf1-2f0dade331d3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5024c39-42e3-4b44-9a73-0d64a77e8bb8", "node_type": "1", "metadata": {}, "hash": "514c8ac3352afe0c4735adfcda24687c43eb80bfbc1ffbeec4052397bad62bfa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d76b9d13-2d70-4d35-80bc-64031c2e783b", "node_type": "1", "metadata": {}, "hash": "884af1badcac69076da04bb2678559eff726b296d5e5f3cd9258e19e902c0661", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Binary-Weight\nBWN-train\nBWN-val\nBC-train\nBC-val\n0\n10\n20\nNumber of epochs\n20\n40\n60\n80\nAccuray(%)\nTop-5, Binary-Weight-Input\nXNOR-Net-train\nXNOR-Net-val\nBC-train\nBC-val\nFig. 5: This figure compares the imagenet classification accuracy on Top-1 and Top-5 across\ntraining epochs.", "mimetype": "text/plain", "start_char_idx": 31723, "end_char_idx": 31998, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d76b9d13-2d70-4d35-80bc-64031c2e783b": {"__data__": {"id_": "d76b9d13-2d70-4d35-80bc-64031c2e783b", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69fd70be-78c0-49cf-baf1-2f0dade331d3", "node_type": "1", "metadata": {}, "hash": "f9d76616b35021c43ab53319bd8d0e7d69887787d796e5cec33a72a82c1944b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ec54057-8962-4387-90e3-dc4fbad587fd", "node_type": "1", "metadata": {}, "hash": "6999d6e1c56ba42bf586f884e3ec9a55cb95c32f358d9b2e61763ed655a056bd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Our approaches BWN and XNOR-Net outperform BinaryConnect(BC) and Bi-\nnaryNet(BNN) in all the epochs by large margin(\u223c17%).\nClassification Accuracy(%)\nBinary-Weight\nBinary-Input-Binary-Weight Full-Precision\nBWN\nBC[11]\nXNOR-Net\nBNN[11]\nAlexNet[1]\nTop-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1\nTop-5\nTop-1 Top-5\n56.8\n79.4\n35.4\n61.", "mimetype": "text/plain", "start_char_idx": 31999, "end_char_idx": 32322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ec54057-8962-4387-90e3-dc4fbad587fd": {"__data__": {"id_": "6ec54057-8962-4387-90e3-dc4fbad587fd", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d76b9d13-2d70-4d35-80bc-64031c2e783b", "node_type": "1", "metadata": {}, "hash": "884af1badcac69076da04bb2678559eff726b296d5e5f3cd9258e19e902c0661", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45e7927a-ab5e-4f00-b4d4-7dfc96d8a5aa", "node_type": "1", "metadata": {}, "hash": "d9a3e3798d273802477cd10e1c14525f37158d85e34b8588e315f8864be97cb1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "0\n44.2\n69.2\n27.9\n50.42\n56.6\n80.2\nTable 1: This table compares the final accuracies (Top1 - Top5) of the full precision network with\nour binary precision networks; Binary-Weight-Networks(BWN) and XNOR-Networks(XNOR-\nNet) and the competitor methods; BinaryConnect(BC) and BinaryNet(BNN).\nthe training algorithm for 16 epochs with batche size equal to 512.", "mimetype": "text/plain", "start_char_idx": 32322, "end_char_idx": 32675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45e7927a-ab5e-4f00-b4d4-7dfc96d8a5aa": {"__data__": {"id_": "45e7927a-ab5e-4f00-b4d4-7dfc96d8a5aa", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ec54057-8962-4387-90e3-dc4fbad587fd", "node_type": "1", "metadata": {}, "hash": "6999d6e1c56ba42bf586f884e3ec9a55cb95c32f358d9b2e61763ed655a056bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5b334f2-f58f-49f8-9a54-e2582d5890d1", "node_type": "1", "metadata": {}, "hash": "b14dba7b9d60bf066b04cfa7e3e08d2a574c687cc207940962585f862f9c8c2c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We use negative-log-\nlikelihood over the soft-max of the outputs as our classification loss function. In our\nimplementation of AlexNet we do not use the Local-Response-Normalization(LRN)\nlayer3. We use SGD with momentum=0.9 for updating parameters in BWN and BC.\nFor XNOR-Net and BNN we used ADAM [42]. ADAM converges faster and usually\nachieves better accuracy for binary inputs [11]. The learning rate starts at 0.1 and we\napply a learning-rate-decay=0.01 every 4 epochs.", "mimetype": "text/plain", "start_char_idx": 32676, "end_char_idx": 33149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5b334f2-f58f-49f8-9a54-e2582d5890d1": {"__data__": {"id_": "d5b334f2-f58f-49f8-9a54-e2582d5890d1", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45e7927a-ab5e-4f00-b4d4-7dfc96d8a5aa", "node_type": "1", "metadata": {}, "hash": "d9a3e3798d273802477cd10e1c14525f37158d85e34b8588e315f8864be97cb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd03f1d7-9675-4e52-b49c-561cafc0d49d", "node_type": "1", "metadata": {}, "hash": "af73366f381334adf529edc652f6221a581ad907de166d0c1c113928daf54386", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Test: At inference time, we use the 224 \u00d7 224 center crop for forward propagation.\nFigure 5 demonstrates the classification accuracy for training and inference along\nthe training epochs for top-1 and top-5 scores. The dashed lines represent training ac-\ncuracy and solid lines shows the validation accuracy. In all of the epochs our method\noutperforms BC and BNN by large margin (\u223c17%). Table 1 compares our final accu-\nracy with BC and BNN.", "mimetype": "text/plain", "start_char_idx": 33150, "end_char_idx": 33591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd03f1d7-9675-4e52-b49c-561cafc0d49d": {"__data__": {"id_": "fd03f1d7-9675-4e52-b49c-561cafc0d49d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5b334f2-f58f-49f8-9a54-e2582d5890d1", "node_type": "1", "metadata": {}, "hash": "b14dba7b9d60bf066b04cfa7e3e08d2a574c687cc207940962585f862f9c8c2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51cd05e9-61cb-47c8-91a2-5a20f9d639a9", "node_type": "1", "metadata": {}, "hash": "e7a14d481d72560e0c400780830842d0c80773df10556e24496365540ef739c1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We found that the scaling factors for the weights (\u03b1) is much\nmore effective than the scaling factors for the inputs (\u03b2). Removing \u03b2 reduces the ac-\ncuracy by a small margin (less than 1% top-1 alexnet).\nBinary Gradient: Using XNOR-Net with binary gradient the accuracy of top-1 will\ndrop only by 1.4%.", "mimetype": "text/plain", "start_char_idx": 33592, "end_char_idx": 33894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51cd05e9-61cb-47c8-91a2-5a20f9d639a9": {"__data__": {"id_": "51cd05e9-61cb-47c8-91a2-5a20f9d639a9", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd03f1d7-9675-4e52-b49c-561cafc0d49d", "node_type": "1", "metadata": {}, "hash": "af73366f381334adf529edc652f6221a581ad907de166d0c1c113928daf54386", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f766b16-d587-4d22-8e6d-57a8b6fc7d1f", "node_type": "1", "metadata": {}, "hash": "7866d33ba117d4417dc71e201657b140e566cd7c118c71c2f462545cf48bca5d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Residual Net : We use the ResNet-18 proposed in [4] with short-cut type B.4\nTrain: In each training iteration, images are resized randomly between 256 and\n480 pixel on the smaller dimension and then a random crop of 224 \u00d7 224 is selected\nfor training. We run the training algorithm for 58 epochs with batch size equal to 256\n3 Our implementation is followed by https://gist.github.com/szagoruyko/dd032c529048492630fc\n4 We used the Torch implementation in https://github.com/facebook/fb.resnet.", "mimetype": "text/plain", "start_char_idx": 33895, "end_char_idx": 34388, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f766b16-d587-4d22-8e6d-57a8b6fc7d1f": {"__data__": {"id_": "6f766b16-d587-4d22-8e6d-57a8b6fc7d1f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51cd05e9-61cb-47c8-91a2-5a20f9d639a9", "node_type": "1", "metadata": {}, "hash": "e7a14d481d72560e0c400780830842d0c80773df10556e24496365540ef739c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bca7f5d9-6240-460b-8830-4ed286911cb3", "node_type": "1", "metadata": {}, "hash": "73eea564166f95fcf8d252a6dae7ca79035bdd73d75d81a3f08a150696dc1e89", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "torch\nXNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\n13\n0\n20\n40\n60\nNumber of epochs\n20\n40\n60\n80\nAccuray(%)\nResNet, Top-1\nBWN-train\nBWN-val\nXNOR-Net-train\nXNOR-Net-val\n(a)\n0\n20\n40\n60\nNumber of epochs\n20\n40\n60\n80\nAccuray(%)\nResNet,", "mimetype": "text/plain", "start_char_idx": 34388, "end_char_idx": 34647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bca7f5d9-6240-460b-8830-4ed286911cb3": {"__data__": {"id_": "bca7f5d9-6240-460b-8830-4ed286911cb3", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f766b16-d587-4d22-8e6d-57a8b6fc7d1f", "node_type": "1", "metadata": {}, "hash": "7866d33ba117d4417dc71e201657b140e566cd7c118c71c2f462545cf48bca5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f8cbf6c-781b-4a56-b30b-fe46b799bf79", "node_type": "1", "metadata": {}, "hash": "0117829fe8c9f884b7b9dfbaf3c6ab7eba873571244e31da9af76e3445a55fa4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Top-5\nBWN-train\nBWN-val\nXNOR-Net-train\nXNOR-Net-val\n(b)\nFig. 6: This figure shows the classification accuracy; (a)Top-1 and (b)Top-5 measures across\nthe training epochs on ImageNet dataset by Binary-Weight-Network and XNOR-Network using\nResNet-18.\nResNet-18\nGoogLenet\nNetwork Variations\ntop-1\ntop-5\ntop-1\ntop-5\nBinary-Weight-Network\n60.8\n83.", "mimetype": "text/plain", "start_char_idx": 34648, "end_char_idx": 34989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f8cbf6c-781b-4a56-b30b-fe46b799bf79": {"__data__": {"id_": "5f8cbf6c-781b-4a56-b30b-fe46b799bf79", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bca7f5d9-6240-460b-8830-4ed286911cb3", "node_type": "1", "metadata": {}, "hash": "73eea564166f95fcf8d252a6dae7ca79035bdd73d75d81a3f08a150696dc1e89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63a49a88-263d-44ac-ba3d-248447fb392f", "node_type": "1", "metadata": {}, "hash": "99d678503fd53cac7d1d94e2c7016c1e9169af29349c42396c18a8af04011242", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "0\n65.5\n86.1\nXNOR-Network\n51.2\n73.2\nN/A\nN/A\nFull-Precision-Network\n69.3\n89.2\n71.3\n90.0\nTable 2: This table compares the final classification accuracy achieved by our binary precision\nnetworks with the full precision network in ResNet-18 and GoogLenet architectures.\nimages. The learning rate starts at 0.1 and we use the learning-rate-decay equal to 0.01\nat epochs number 30 and 40.", "mimetype": "text/plain", "start_char_idx": 34989, "end_char_idx": 35370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63a49a88-263d-44ac-ba3d-248447fb392f": {"__data__": {"id_": "63a49a88-263d-44ac-ba3d-248447fb392f", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f8cbf6c-781b-4a56-b30b-fe46b799bf79", "node_type": "1", "metadata": {}, "hash": "0117829fe8c9f884b7b9dfbaf3c6ab7eba873571244e31da9af76e3445a55fa4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e73f9de-3297-4b2b-87e9-1851a348105d", "node_type": "1", "metadata": {}, "hash": "bf19ef06abdef0e48b8edb309e73f13d7be2c7524aed00a6057c4cd8b12fe2fb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Test: At inference time, we use the 224 \u00d7 224 center crop for forward propagation.\nFigure 6 demonstrates the classification accuracy (Top-1 and Top-5) along the epochs\nfor training and inference. The dashed lines represent training and the solid lines repre-\nsent inference. Table 2 shows our final accuracy by BWN and XNOR-Net.\nGoogLenet Variant : We experiment with a variant of GoogLenet [3] that uses a\nsimilar number of parameters and connections but only straightforward convolutions,\nno branching5.", "mimetype": "text/plain", "start_char_idx": 35371, "end_char_idx": 35876, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e73f9de-3297-4b2b-87e9-1851a348105d": {"__data__": {"id_": "7e73f9de-3297-4b2b-87e9-1851a348105d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63a49a88-263d-44ac-ba3d-248447fb392f", "node_type": "1", "metadata": {}, "hash": "99d678503fd53cac7d1d94e2c7016c1e9169af29349c42396c18a8af04011242", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ac298ca-3662-40af-9534-e324e48d3a11", "node_type": "1", "metadata": {}, "hash": "d679246bd73d902546ed00d48e992d32d1d071e0263aa2b6384802df3bc1c83d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "It has 21 convolutional layers with filter sizes alternating between 1 \u00d7 1\nand 3 \u00d7 3.\nTrain: Images are resized randomly between 256 and 320 pixel on the smaller di-\nmension and then a random crop of 224 \u00d7 224 is selected for training. We run the\ntraining algorithm for 80 epochs with batch size of 128. The learning rate starts at 0.1\nand we use polynomial rate decay, \u03b2 = 4.\nTest: At inference time, we use a center crop of 224 \u00d7 224.", "mimetype": "text/plain", "start_char_idx": 35877, "end_char_idx": 36313, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ac298ca-3662-40af-9534-e324e48d3a11": {"__data__": {"id_": "4ac298ca-3662-40af-9534-e324e48d3a11", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e73f9de-3297-4b2b-87e9-1851a348105d", "node_type": "1", "metadata": {}, "hash": "bf19ef06abdef0e48b8edb309e73f13d7be2c7524aed00a6057c4cd8b12fe2fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92d31c41-a3f7-4951-9db8-5dcec36a74cc", "node_type": "1", "metadata": {}, "hash": "d37d90c982774cfb5291c9e2d7d044cf89a2dc2b5f36bdcdd1c9794b1693cad9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "4.3\nAblation Studies\nThere are two key differences between our method and the previous network binariaza-\ntion methods; the binararization technique and the block structure in our binary CNN.\n5 We used the Darknet [44] implementation: http://pjreddie.com/darknet/imagenet/#extraction\n14\nRastegari et al.\nBinary-Weight-Network\nStrategy for computing \u03b1\ntop-1\ntop-5\nUsing equation 6\n56.8\n79.4\nUsing a separate layer\n46.2\n69.", "mimetype": "text/plain", "start_char_idx": 36314, "end_char_idx": 36735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92d31c41-a3f7-4951-9db8-5dcec36a74cc": {"__data__": {"id_": "92d31c41-a3f7-4951-9db8-5dcec36a74cc", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ac298ca-3662-40af-9534-e324e48d3a11", "node_type": "1", "metadata": {}, "hash": "d679246bd73d902546ed00d48e992d32d1d071e0263aa2b6384802df3bc1c83d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d890d1e5-b8fd-426d-9597-93e0af0b44b7", "node_type": "1", "metadata": {}, "hash": "290b34f850d5d464990e82e15af4790bfc8ab24eeff4bd4f8c253166540ed1da", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "5\n(a)\nXNOR-Network\nBlock Structure\ntop-1\ntop-5\nC-B-A-P\n30.3\n57.5\nB-A-C-P\n44.2\n69.2\n(b)\nTable 3: In this table, we evaluate two key elements of our approach; computing the optimal\nscaling factors and specifying the right order for layers in a block of CNN with binary input.", "mimetype": "text/plain", "start_char_idx": 36735, "end_char_idx": 37008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d890d1e5-b8fd-426d-9597-93e0af0b44b7": {"__data__": {"id_": "d890d1e5-b8fd-426d-9597-93e0af0b44b7", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92d31c41-a3f7-4951-9db8-5dcec36a74cc", "node_type": "1", "metadata": {}, "hash": "d37d90c982774cfb5291c9e2d7d044cf89a2dc2b5f36bdcdd1c9794b1693cad9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "735065a3-b136-48fa-93ad-ab0ed1a35b90", "node_type": "1", "metadata": {}, "hash": "27f5532f1920d244079715cc8052ea0c834c430da4033af4ccfbca4437b15020", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "(a) demonstrates the importance of the scaling factor in training binary-weight-networks and (b)\nshows that our way of ordering the layers in a block of CNN is crucial for training XNOR-\nNetworks. C,B,A,P stands for Convolutional, BatchNormalization, Acive function (here binary\nactivation), and Pooling respectively.\nFor binarization, we find the optimal scaling factors at each iteration of training. For\nthe block structure, we order the layers in a block in a way that decreases the quantiza-\ntion loss for training XNOR-Net.", "mimetype": "text/plain", "start_char_idx": 37009, "end_char_idx": 37538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "735065a3-b136-48fa-93ad-ab0ed1a35b90": {"__data__": {"id_": "735065a3-b136-48fa-93ad-ab0ed1a35b90", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d890d1e5-b8fd-426d-9597-93e0af0b44b7", "node_type": "1", "metadata": {}, "hash": "290b34f850d5d464990e82e15af4790bfc8ab24eeff4bd4f8c253166540ed1da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bae563e-1d68-4634-b4fa-48292d321a02", "node_type": "1", "metadata": {}, "hash": "137e67a0828670c69937995dd685981dfc4b81a2df22b99bc606747849cf91c2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Here, we evaluate the effect of each of these elements\nin the performance of the binary networks. Instead of computing the scaling factor \u03b1\nusing equation 6, one can consider \u03b1 as a network parameter. In other words, a layer\nafter binary convolution multiplies the output of convolution by an scalar parameter for\neach filter. This is similar to computing the affine parameters in batch normalization.\nTable 3-a compares the performance of a binary network with two ways of computing\nthe scaling factors.", "mimetype": "text/plain", "start_char_idx": 37539, "end_char_idx": 38043, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bae563e-1d68-4634-b4fa-48292d321a02": {"__data__": {"id_": "6bae563e-1d68-4634-b4fa-48292d321a02", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "735065a3-b136-48fa-93ad-ab0ed1a35b90", "node_type": "1", "metadata": {}, "hash": "27f5532f1920d244079715cc8052ea0c834c430da4033af4ccfbca4437b15020", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67fe8c25-e9aa-4693-b753-b9d402bc7bbb", "node_type": "1", "metadata": {}, "hash": "ca2553246116f2c88bb000915d9860e16e5dd88cf927e7bcc746284986f206ba", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "As we mentioned in section 3.2 the typical block structure in CNN is\nnot suitable for binarization. Table 3-b compares the standard block structure C-B-A-P\n(Convolution, Batch Normalization, Activation, Pooling) with our structure B-A-C-P.\n(A, is binary activation).\n5\nConclusion\nWe introduce simple, efficient, and accurate binary approximations for neural networks.", "mimetype": "text/plain", "start_char_idx": 38044, "end_char_idx": 38411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67fe8c25-e9aa-4693-b753-b9d402bc7bbb": {"__data__": {"id_": "67fe8c25-e9aa-4693-b753-b9d402bc7bbb", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bae563e-1d68-4634-b4fa-48292d321a02", "node_type": "1", "metadata": {}, "hash": "137e67a0828670c69937995dd685981dfc4b81a2df22b99bc606747849cf91c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e1cb92f-1d7d-4352-a448-7a32f8d86b6d", "node_type": "1", "metadata": {}, "hash": "7489a5138a3bbe0088164a450e0d83b78ff394c0f3c525eb7f2721580c20a002", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "We train a neural network that learns to find binary values for weights, which reduces\nthe size of network by \u223c32\u00d7 and provide the possibility of loading very deep neural\nnetworks into portable devices with limited memory. We also propose an architecture,\nXNOR-Net, that uses mostly bitwise operations to approximate convolutions. This pro-\nvides \u223c58\u00d7 speed up and enables the possibility of running the inference of state of\nthe art deep neural network on CPU (rather than GPU) in real-time.", "mimetype": "text/plain", "start_char_idx": 38412, "end_char_idx": 38904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e1cb92f-1d7d-4352-a448-7a32f8d86b6d": {"__data__": {"id_": "8e1cb92f-1d7d-4352-a448-7a32f8d86b6d", "embedding": null, "metadata": {"node_type": "chunk_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText", "node_type": "4", "metadata": {}, "hash": "59f701800df0a3b3db8f77e90e1b39792a20bd8d8fd596020f18d007b80bad67", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67fe8c25-e9aa-4693-b753-b9d402bc7bbb", "node_type": "1", "metadata": {}, "hash": "ca2553246116f2c88bb000915d9860e16e5dd88cf927e7bcc746284986f206ba", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10.1007/978-3-319-46493-0_32", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}}, "text": "Acknowledgements\nThis work is in part supported by ONR N00014-13-1-0720, NSF IIS- 1338054, Allen\nDistinguished Investigator Award, and the Allen Institute for Artificial Intelligence.\nXNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\n15", "mimetype": "text/plain", "start_char_idx": 38905, "end_char_idx": 39168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10.1007/978-3-319-46493-0_32": {"__data__": {"id_": "10.1007/978-3-319-46493-0_32", "embedding": null, "metadata": {"node_type": "doi_node"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"4": {"node_id": "root_node", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "faf6e3e1-b949-4e5b-8f66-1a4e4cf4665c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9fdd2b41-08ae-42c1-a646-a78abd8d4423", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4af9d98f-6d4a-413b-a65d-ef8634b0435e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ff3cc678-711e-459e-8766-64add4a5b79b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b546bc00-f93e-48a7-93ac-786f239a61c7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "aa28498f-7e78-4bd0-a63b-67347822d054", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f5f0c874-9fea-4ed4-8ad3-353dc53704d4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "856900f2-2a59-4eb8-b43a-b3655765c66e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "442a996d-489a-4fc9-8512-8aab841141a0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b58461df-014e-400f-8a3b-b7fcc899b4b8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e065c41a-5c38-4da7-8316-3a0eb1f37f4a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8ae8975d-4301-43bb-a39d-f9e02fc18f1b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "91c75266-d293-4741-8856-db890e62e5db", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "0a586f64-69e3-4ed5-b217-246748cade34", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5556efd5-34f7-47f7-9c0d-cd10088a77da", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "db041908-6a1b-49de-9e04-7158a955bbf6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fa56bd0b-aeee-4add-85fd-f1fddafeb944", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "994c769d-f908-4f83-bdc5-6e732cfa53ad", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "97b6008a-66c1-4b50-bef0-2e8e54861b75", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f0954f18-142f-4c95-85c5-1e4596235197", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1ed6baf4-44a7-4373-a63f-5f128f098efc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "df88479b-51f3-4218-8af7-478303a1dacb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "256d878d-b29d-4ce1-97ae-f144999fe5e3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "1f5148c2-980e-4cce-9dc2-f06f3990764d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a4347006-e63b-4773-a42a-25cdbda90770", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d8650cb1-e3fc-461a-805c-329c9560f394", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c7cbdd57-6a2b-43c3-bb41-9b55d76c0f0b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fd9bc8e1-56e3-4a3e-9e0e-0ba5a42765e4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "36fe0d16-5f90-415d-97bb-8207ca42a491", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a1ddb1d8-d66a-443d-bb7e-f27adb0569d3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "9e1c208e-d993-46e0-a0e5-848afa168ce9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3a65598f-d600-409b-b08c-4f3879639fd1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ee9dc51b-cb69-4e2f-88b3-03eb228aea79", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "ed16d59b-d284-411b-af81-dac6b38d8325", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b54be825-f95e-4cf8-b261-51dd9f7cea01", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "51f9ac20-bfcd-4b4e-8a64-e687f205f918", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bd3c46c6-0611-4a40-b81e-7ca3240ad115", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d90622b2-db1e-4595-b45a-88021548e613", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3aa7adb4-b40f-4a03-9b10-5410ae180273", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2926afe7-549a-42c9-b99f-7a235508a9af", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "f7cdaf08-647a-4d36-b6b2-35b4bd5ab93b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6c41a97a-bce7-43b4-afb6-8b20b267aee1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "79c5db2a-a4ec-4b81-aed2-97905f7968bc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "849f070a-a09f-4577-9e94-b57a991af1c7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2a50d1ab-c431-4d4b-90a6-2b1107ee143e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7cf8d3cf-e074-4c78-8769-f7254411e3c0", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5d9bd29d-615c-4477-aa4b-4832f9fa14a4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "987df21d-13c3-4b91-970c-94a91a2f82ae", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d6642526-3af2-433a-82ab-f978c2caa3f6", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e6d4c5f4-3cdc-403a-9c3a-66ed10ce59a2", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c5cc0672-8f70-412d-af6e-7ca91249b848", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "c9867c00-0d52-406c-b6aa-4c4487e64bff", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d3ff5b87-96c1-45ce-b887-03ca029cbe9c", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d8bc747e-7478-409e-8b53-b16648b4e708", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7f3e12e4-a8da-464a-8f39-3607ca063b9f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b94611cd-0027-4160-a6c8-46b243d25ab8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b5879588-e82d-4d60-8598-36520dc89449", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "3dc2e943-b385-43cb-a390-4db54b5eb8fc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "2c528384-6c68-4c6a-9a11-6de467c656fc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "81a437ce-4410-4a79-bdbf-d39059d0eead", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4feffabc-f8b8-4783-b457-15644ca8de40", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b12b2e54-0c86-4c24-8791-746706842a2e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "12b7697b-71a7-4392-951d-24cd554528b8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a1a84cae-a62d-45e0-acad-09c984d7a470", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5a52ec5d-18c3-4107-94c7-660e459a6692", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "dc021ad9-7922-4548-8e04-f17492f46b7a", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "e99832ef-4ee2-4207-80ac-3ed6607927ce", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6cc45d1c-43df-4b9c-9462-f4dfd3e37555", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "908a880e-de47-4c12-ab9c-e57b74a70e64", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "a8d55467-1365-4c2c-a007-ac7303027784", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "80591771-6e15-4c26-8c9f-200b0b14708e", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "78bfa7e3-a4f9-4760-a73c-714c181f6716", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4d94de39-fa51-4cfe-bbb0-d287f46359a4", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "45dc4261-9b7f-479b-8128-002f34667159", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7fdb303f-f149-4dba-a3af-cb1e43bedeab", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6294cc3d-8b02-42e2-b564-accd8aea215b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "b5024c39-42e3-4b44-9a73-0d64a77e8bb8", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "69fd70be-78c0-49cf-baf1-2f0dade331d3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d76b9d13-2d70-4d35-80bc-64031c2e783b", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6ec54057-8962-4387-90e3-dc4fbad587fd", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "45e7927a-ab5e-4f00-b4d4-7dfc96d8a5aa", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d5b334f2-f58f-49f8-9a54-e2582d5890d1", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "fd03f1d7-9675-4e52-b49c-561cafc0d49d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "51cd05e9-61cb-47c8-91a2-5a20f9d639a9", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6f766b16-d587-4d22-8e6d-57a8b6fc7d1f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "bca7f5d9-6240-460b-8830-4ed286911cb3", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "5f8cbf6c-781b-4a56-b30b-fe46b799bf79", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "63a49a88-263d-44ac-ba3d-248447fb392f", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "7e73f9de-3297-4b2b-87e9-1851a348105d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "4ac298ca-3662-40af-9534-e324e48d3a11", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "92d31c41-a3f7-4951-9db8-5dcec36a74cc", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "d890d1e5-b8fd-426d-9597-93e0af0b44b7", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "735065a3-b136-48fa-93ad-ab0ed1a35b90", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "6bae563e-1d68-4634-b4fa-48292d321a02", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "67fe8c25-e9aa-4693-b753-b9d402bc7bbb", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}, {"node_id": "8e1cb92f-1d7d-4352-a448-7a32f8d86b6d", "node_type": null, "metadata": {}, "hash": null, "class_name": "RelatedNodeInfo"}]}, "text": "DOI: 10.1007/978-3-319-46493-0_32", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"root_node": {"doc_hash": "b1406c989293b696c8deeb683bf214202cb3c9d42f3b6da0688dc66842635709"}, "a0caafa2-03a1-459a-89e1-708df0be7966": {"doc_hash": "9541db243cb14fd9e6151f093b77a9722f8729588224ec0b69c7ce1b005bbc07", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "2929254c-844e-4c67-b6d5-6e8bd409bee6": {"doc_hash": "c6d335753dc2f07c81d1266db067a9562b2697f1bbaabd0a2addafe44727421f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "8e0761f7-06e8-4475-acd5-025aeb072fb7": {"doc_hash": "c0dc8268b691a4673f3d098d0103e5b27802a1af5c8f7bb4cfc27efcbd3318d2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "6bfc782a-4235-49bc-966f-0b279ec4e6d1": {"doc_hash": "cdcec3a925dd10fc2a521f78b707d649710225cfb4822d23fef8230814e96769", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "04666a9b-f40b-4843-804c-80d445aedceb": {"doc_hash": "16489d7d57a3b2898698bc00899fcd3582fdd621a13e78d9808096d41112d4b8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "c051e5fe-53c2-49b2-93be-866ba68773ac": {"doc_hash": "f8812742c6789e8a4a983163821659d8b0c5585fd9774edc6f8493264731ea5c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "62315ef2-ad40-479b-9888-6c077637caec": {"doc_hash": "a1ab233eed1749376ce6d3fa1b6efeed8b5770907c1e81002d738ca0b1c9f280", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "bb403b59-c170-4877-8a45-6fcdb3416f3d": {"doc_hash": "5001735b6272f7942044b82074073da33b53b597468183460dde9a21fa47e4f1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "4aa3c1bb-2890-427e-9444-efbf4752b9fc": {"doc_hash": "5fbe339f02b3df1cfd151d925e2514612b0b5b5b618dec42abc889b43d492cf6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "9a7a7070-481f-44e4-b23e-931b8fb814ac": {"doc_hash": "40bf485646f8c33f56da1662651c6402a454a266e98840636fc57ac77a778f73", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "3bb61a03-40f8-4bf3-ac28-9e1b5f349e87": {"doc_hash": "53576ecb3455ffe166446000f8f2ace17b8252e49796f951683a176bf98b02c6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "084f6aa2-2a95-4e36-ae95-7b8bdb900fca": {"doc_hash": "dad41d06f4c84aba340649e59ade2908238ce91bdca834b28ae32aa7f6cc469c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "89fa1b5b-ec50-470c-882f-f81ffeacaaa6": {"doc_hash": "a7b1efbb458accbf0c7de4b2bc75cf05ce1837564349762ef38e802c4728a0d5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "5c0bba9e-7133-4636-ba81-f84c210bc32c": {"doc_hash": "c61a9c351340c0069725d79d0770585618e0fda84e675b4333ce267177880c39", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "eac61337-ab4b-43c4-b972-ce0b9ed21ca8": {"doc_hash": "8aceda094702dd7508e1934b339daaf0c80c8f6122c962fdf55783fd81022b85", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "a922a896-544c-41b9-aae5-36ed21bd1c2e": {"doc_hash": "61d400680f6c86921f0f3889c9bc208a4a76da3f456f67b1544f81d0c50ce9e7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "eb0b5c96-3abd-4026-8d8a-bc0fd325aca8": {"doc_hash": "bb5fef0e32b63971d66985ad76f0db7d47df3134c096610c934060e231390264", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "7f715b14-44dc-46d3-b48c-439335722505": {"doc_hash": "4001f27fe4e532bd61abfb29f908f23e0ea088f0e1395ddf4adb6721a940046c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "6ea6e0be-82dd-4b21-8f07-8679f9c3cd63": {"doc_hash": "00a316a59dcafcdc2e4257299f56a30ebe5c29373a6bcd4077c4b1589f6fec35", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "f7c3a139-ad6b-47e7-8515-6595d9d30330": {"doc_hash": "ec8fd22fc7565d246cd56daa215de3be65e9e7f6d4363c93f1c99d3b43c4c358", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "7a5aae2a-3bea-4c7a-b7d1-fe0148de4301": {"doc_hash": "b72efb74350b0ece9ad333c3c81062451a95ea4f8fe92f8ca5c183272aaee5b0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "16598399-70e9-4bec-bf7e-c884323b1790": {"doc_hash": "12b56a5b367d342d49130d383c4b9736c4174c383efdea26e9dd60c14bb4792d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "ec79c6a9-2dd3-4c95-aec9-3949c24e085f": {"doc_hash": "b1a2d6f93e67dcc4d208d61f2a1b2f8e3b7c26322d4074d425114e8194186b94", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "12501e2c-d18e-45d0-b4df-b04b171ac8df": {"doc_hash": "b6de46d3af674682f37b7284729773c4dbfb16357935cb29206302e8b18e9b81", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "a666b670-4745-4078-a0c7-27a7e13a0565": {"doc_hash": "e92d5c8a573fd6042354f8750cffb3e0f558dc20a3cfd00e3628453a37ac8f17", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "11f6fd5b-1ffd-4c0f-a81d-385b5f9a29ea": {"doc_hash": "147810d08b8b9bebcdd7909a38ee12f0ea51efd68c5b138547d107a006372168", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "80a0e15b-3324-483b-83cf-05ba7c5a709c": {"doc_hash": "dcf287fe15642baac818e6a784650e8ad47ccb30d3381cc1fd90788be4086db6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "25a0d61f-165b-4bef-be9c-2ae95bf4b427": {"doc_hash": "8a5b3cd6b01e6655caa67e48f2c8574666814126dc5d30712da29ab9f14569cc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "a2b8cfaa-16dd-4daa-a610-4fe00cdbc1f6": {"doc_hash": "813472d28896f581e804bfa9dc949c0c258ddb642632e7a21fd87f4aab2cf8a8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "80f926a7-357e-4b05-8df5-0811620a625e": {"doc_hash": "38a23b7e15071a928da5aeb2763e6f9414f46a6916715f9cd7743d4fe1db39ed", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "7d957296-e747-4823-8909-6284d238ef93": {"doc_hash": "92d7b322ab8332b5665aa7de23d22a67810c446f259147f9894fd4986b43ca44", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "27a0b647-3732-4092-8b98-1658cf568749": {"doc_hash": "0657baf8642f85b453f06777d80ffbf09569ea6c7431000e33b1fdb1a1c37a26", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "9d4af09d-ff11-4898-904d-d6ec9fb04372": {"doc_hash": "0a1444af54d99f0147baf898ae08080733cb0364d25784fb3ca31a29c625b9e2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "c62002c7-d883-438c-aa54-5c80f7c1515f": {"doc_hash": "a828d4592b71a2395a05699d3ea98445577b4cdfc0941edfa4fdf4d67fe53630", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "a044f4c4-f33d-49f8-8ae9-00c8dc0366ab": {"doc_hash": "fd466d67f677c4d7a9caeb85e2c6e72a06c800678d10e6f200cab1949fc75e7b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "56a890c1-62f2-4950-96ec-ef865a7028da": {"doc_hash": "ac09cd57b8a8f2a4755837f0cc8ed7c1b04914612f61f57d2f0fe9999832d597", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "d47d970d-a799-488f-8c02-ef44d3547b43": {"doc_hash": "1369457bc81426c7636067255c86389f1ade62f6d854f3f5a95d418543724dca", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "c286534f-0ba3-42de-9e00-271c4aae63ff": {"doc_hash": "3ff0ed50893478c2e344e5b59daa5a5a50acdc4c6c39f998cf78bd1afbc592de", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "a5645c50-741d-46fa-8745-425d711dd8b6": {"doc_hash": "0f163cff27f063e075aa07526c4a75b4a8b71cc5e6d5e7615feacec9e420df29", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "8b29399e-6a43-447a-9b97-3ffc55928232": {"doc_hash": "f4e12630f3fde94d16234b528904918f8938f90f46d49d74518e830c0884642b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "3c096904-122c-4e70-8ea0-f9522b7f06a6": {"doc_hash": "08ddc16079d89af63f9698543118d8548edb20d47e9a052af31fe23f2ce1f778", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "b2628aef-f602-4142-ab19-51b9c291faf2": {"doc_hash": "0a29e70a931471ec1880c4905ec1634f6be8f17f6a3de21f36c5506297add1c9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "a699796d-44eb-4ede-8c66-3b790c2117eb": {"doc_hash": "8eef5311cd2a45352b5c30043a43e95b705e675caa2795d1704699721d08511d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "72e1ac06-19c5-48ae-bf7c-821a8a508093": {"doc_hash": "50b2b4ad6ead5210b248fd239f4578b63f82d0f54ef3dc66cc4d1c49db9f9a75", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "d4789cdc-b420-4538-9a12-7363e468e8ee": {"doc_hash": "1388c1c61c11884281eecbeafd403fae7d39a338a8e882f204dadeaf0e952bf3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "0189774e-86b0-4277-aaf5-98cffdfda3e0": {"doc_hash": "610adeff4d0726c4e0b76d9dea3f5ceb787469c658c88c586d4bc2bb855d3f1d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "383cdbf4-b658-4f92-94e4-c1bdf1d79a9b": {"doc_hash": "8279cc897a5d751b3ee47882a02961204e566507897ff92dafafffa138034ede", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "7ec73f66-494f-4a41-9ba6-0d7a9cde8a8c": {"doc_hash": "742db9032ace936de84632b1bb6e6060fd9d54ac8851a8a28f6e4d2b47fb9d1c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "2952169b-ea4d-4e3a-b1f9-a8deb75d3136": {"doc_hash": "39d1fd060775e55d7bae90a50f2a1a1f0358698a5263c83c489f897862c121ad", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "f3d71d6c-29bf-47fd-828b-99ac385c81c3": {"doc_hash": "a18bd377a00da86d95659b2e156ca7abfc330147839b62155dd6b04b5a3049fd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "29fc2a74-fbc7-40c2-a0fc-84d8c7d55e07": {"doc_hash": "3549aefed54aaa729d8da33933414aa226b318ecdaadd32537e420ccd0c70a1b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "d88c2feb-d935-41e5-b5a0-21e8444ed341": {"doc_hash": "03381525eb10329a1c84bfaffe3f0220a436089631270a8e3387617088215fe7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText"}, "10.1109/bigcomp57234.2023.00015": {"doc_hash": "50f73f0b044932dbbbe33d8e27c9e1512d4d645a6b33aec33b8fed5f4a87266b"}, "ffdeaf3b-c475-43e3-a2c0-0e0472671663": {"doc_hash": "e564e47d360538ada24a9bd5fe51d9dc6404d1b5e78e060dabf01f86d448d5ab", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "3f463d6a-73b7-4257-a0c2-a03497302442": {"doc_hash": "9fc46e1b8d7b336722968c704a05482059aa22139674c77fdc740a5ce2b984b0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "4d9c9c93-9028-4a08-8da6-6d1ab15344cc": {"doc_hash": "17d019d9c5d45ebabaff03d6ca9dab721067d584a011fd3b296e1d977d6fe304", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "25f5135a-4134-4374-9d01-24c9a8465078": {"doc_hash": "ec3137403130a42765ee15f7a2dd6f0106ff62f22bd3a52227cd360d72911bc6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "ea2469e4-fcb7-47c1-adf3-357653468b6b": {"doc_hash": "967f84b990f320ff7da9b11a5200d505fd2b8d025d00feb767a831ca408bb564", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "620ea7d9-283d-4ca6-b2e4-cca42871147c": {"doc_hash": "c1d44a08ede35ebd6842719c82abd7cb6a332f91ead637bef407f89cccb47770", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "b67bea5a-52e5-462f-a205-bbbf673641ac": {"doc_hash": "b0a70d8ad6ec8e8ac880932d039d7aef6307bd8a13e14a90e8050430370b4b15", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "d23e8c7d-60bf-4541-b639-33eec97ead8c": {"doc_hash": "ec5825a545d1da80f9dbb270f84c065734dfb5170ca77f70bca461139d83dd6d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "ecb2a8c5-dc92-4170-bb37-e2a825ed9e31": {"doc_hash": "9ad6291cbaee1afb780a43fabfc947a06198d7deb5fdef3ab2b216720894a594", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "0b153da7-104b-4f01-833c-78e94149c4e0": {"doc_hash": "184d926ff1c24d5c20c3970f8b4b78450165309ff7b1325506bbe6c289374c19", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "d08751ae-097a-4b6a-90e2-c855556061ce": {"doc_hash": "721487edfbd1b3b1b2fe5cfd9b78b547ff764634cb06fc2405515d14a0207264", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "b2c584ff-b29b-4966-97bf-dd610fb95927": {"doc_hash": "b3b2e20252bddb0e766f5f1d72ab5904c5bbcbe99b661a2d16b2c2ef3629f8b1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "ae6f136c-33d0-43c8-8155-0e69e26e20d2": {"doc_hash": "4084c7da08c0b120c557e448829e8aaa04ab792f54d1e15b77a2c2d571217a9a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "48ac672e-3193-42e8-968c-f0703a53f97f": {"doc_hash": "c216aed6fbae3d3ffb9cdd55f028d50cceb93efa10569993e0cc10b4ee281a30", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "baca1f34-761d-476b-8849-eea3e8d49820": {"doc_hash": "3836d1ca61fc4e20975f8a0a5a5c391de89cec47b30a217531d1515788a7b0a2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "a64459c0-9aa7-4d65-b338-01a91a5f07a1": {"doc_hash": "4ddb833b5f19a6f32058442718eebd32aad2571137b26478ef292b28834bd0c6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "8853a2dc-0d1d-43f3-bcc6-ebbcc09d66a3": {"doc_hash": "8c60a94b355a86d334c4c813b133bac0fa38a48d9c679d20c01f148435b74296", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "c05cf077-62c9-4238-bf7c-4f75ed74523f": {"doc_hash": "679181e7cfe0c3e9c29d9fc5d3701589d6342baafae34996902d0d693328eef7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "547c03f7-4cc7-47a8-9830-22151ce042c6": {"doc_hash": "ca3203ea5e5168a31e7bbf7f54be3f3b818695e9ed9ae81a5c892fdab6f342f0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "b032fa00-a193-41da-a194-7776be2d46fd": {"doc_hash": "c72da5e3e8ee7fdd81876dc5413169f344f60c6efa242872c2328b91d1bdf1d6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "6152888b-1e9f-4ec1-bf9a-39bd5ecf6a19": {"doc_hash": "04ec046760874538227f453536e95b23691c2a1226e0ca6aae64a3f62581345c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "68a75f01-dabb-4d9b-885d-143a437b0d1a": {"doc_hash": "f4867830f319b897c6f5aae458d1f3d3ed7d398a62620420562334e07f074279", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "e125a4bd-9498-41af-b170-87ac268881a5": {"doc_hash": "470c7c4cab13d4c7159bd86e41681f80df56e83f1e88e5976d29c05e748dcf80", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "50601c31-e74d-4160-b2e6-214aed85138b": {"doc_hash": "cca73592db38db5d9439eca66a4e0b42e94c2f65988a3921f9a6511d7cb2478c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "e09bab72-b099-484b-8dae-b70e1b650332": {"doc_hash": "56b6628babcee6c5b806518c49b69fab5efe8201e4fd4f07348a6ac82933965b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "9ca02669-dabf-4246-9691-62ed76bd22d8": {"doc_hash": "370613a99e2b8f6630b068fe4f7acd08e8a59f4a67f1d0ef1c6a1e90000478cb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "3470e867-a066-418e-8705-fbd1a4b89481": {"doc_hash": "6b33d8043e82f437ffb42b7fd8c8d96b43c9afd3d691dfe494117a098e7ed047", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "8505cb06-bca1-4b0b-9832-fdaf92dfb907": {"doc_hash": "0a8f01d7a0237b38d3e3dd3894adead246230cafa0fd16cd6c7101325e51177b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "711dacfd-483b-45af-8ade-1864aa723de5": {"doc_hash": "c11685b4340531341144ba5fcb7660b174d1b781dc2a580f997e2137eb362d28", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "a3d40b0c-3805-4d3b-8d23-d4dd4f2deccd": {"doc_hash": "d332ae7aa6bca8a0df0abb5a26a876c0d3506ead4cecec1ff8fd928b6e63d556", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "7796bbc4-a338-43bd-bd86-5369a20e7c54": {"doc_hash": "b3edf58e13da7b85e747344a64d64d2fff7fed79608fe3ce9c8d03677b955a6d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "a05685f1-eb4c-4e53-a231-f63d381fa96f": {"doc_hash": "528c67882ae8e7b4f7e1c5f27dfff5fa1e86156e36cf72cc259b40fcc3b27218", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "eb1b185a-6e04-4913-bbbb-318f66778f2e": {"doc_hash": "f25ecd384734f5846433e7de0b1c0fd90d150743f42b0acce52344f30f330277", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "9e6e0720-d784-4adf-a0ec-4585d5a44b5d": {"doc_hash": "fcd70f48d43ca2a801902fb40ebe3129d2f6994a413fb6c15a83e0d0be9fa2ea", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "e18a3d2b-d41d-4df9-a484-058727ff9c07": {"doc_hash": "ca0724133eff870b5087f5a834b91977ca910fe84c3ba9991dbbae72e21a1a68", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "25cc674e-c95e-4e1a-973b-e04f9abfa371": {"doc_hash": "8b6a75b02c9f205a071df85e570c929bcc0d8ac806114550126cadbb7e708afa", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "ab10ee88-e59f-4c36-a5d7-904ed0a9d327": {"doc_hash": "44f97ada11f2d21f4225b2ae20a3224e4239a8fb7681fe78826c9023d16ecf22", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "43914e07-f635-4db0-b54c-4a66dc2d1a6a": {"doc_hash": "5d2ac057e44c6d96f60cca62562fa488cd67a209fea835ed83a89cd937219c99", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "d282d820-b8fc-4b98-b9ea-35ffcd3a322c": {"doc_hash": "bdd96fb15a05476163d6582c1404fe0e503ea0cc5e3adafd1a2258b3cc4286be", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "0acfc550-8ace-443a-9130-de2731b81860": {"doc_hash": "7b7bcbd0b1a3a573ca2777217aa1d833146bb5f9e68a2bb8c306626ba89d5cd5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "ffbdc075-f38f-4c62-a564-8608463c752c": {"doc_hash": "4b611e9a14a2064e4d7802d87916270e00efcc2495c191f1e853ed9a4c11698c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "2b38ed1d-3ff4-4abb-a672-c426bb82ada8": {"doc_hash": "bdfbbdda25c6aa231bf401cefe2f20924bbe9feeb4a726906aec8f164894de16", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "d24b05d3-94b4-4b7a-b890-8ffba811e88f": {"doc_hash": "0785ab5dbc1ce7df21d6b9076638f689a1b93825ea92102f76e72b7f0f5171c7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "9458e849-1aec-43ec-b3a4-7464f384192e": {"doc_hash": "82a5a23e2d5d340074c5828cc1f6ba2e3aaa65da4c371a4417d20612a42f6d7b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "bb16379e-43d3-421a-ba36-953e693b4d50": {"doc_hash": "bfd8140af233cbf32781cfbc99fae5f359c106b6950e3cf5c90f476b94f1f4d3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "1669afa0-c886-43d1-8cfb-16eca718dbaf": {"doc_hash": "8973a0026b27f1e0c17e24c45ce7ad42a0dddd49bd83916f091330d93195579c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "f757e0d3-1100-40f0-9301-b235b47072d9": {"doc_hash": "70c9e2b382c985b2b950da60c980609306a3b7ba0b47c650fb28169c7ad6ebe7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "8c75c395-bb32-4e4d-b4fa-defe48874fdf": {"doc_hash": "523371709487150d8e2f9da5ab71d7b62c49f788ffc16db5d0595e6f99a77206", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "9011f739-c3ca-4321-b88d-00e36e4548ad": {"doc_hash": "7a9ec3fc0dd69f08f06969148a133cf936d55fd34aa29cb4c1087986ff6ab163", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "fd2db383-b0c5-41f9-8aa8-2f303690613d": {"doc_hash": "2864a1cdb9ee340b738005e6e5a0d26663bfa4504714abdbaf7bb70bb2d464af", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "a0f91275-eefe-48be-95ac-2f2cc648f1e1": {"doc_hash": "1c2271a636f7d11d08cc19138dfb4c02d3d413476a30a394caf014ce829ffdd3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "0f07ca8e-d4a5-4db8-a318-40c946baac6c": {"doc_hash": "d06961e5196e62b4dd4c605811e130e5a3a85c9a79ec3a4e63a7304b3801d022", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "9279c0f9-a48e-4994-aaed-f2fd7ebb2952": {"doc_hash": "8aff26212778d30a3542d23b4e332081bba2a2224ad5b565f14b2df4a7fb7c24", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "44452aae-48f9-4dbe-9f02-a0c337763b66": {"doc_hash": "e9754a2ebed6c921edd260e77d52fa72f5f5e4ac60de343b07cfeb5b36cff557", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "28ded4bf-d587-48c8-90fc-c32fc5afda76": {"doc_hash": "c6417331eba61947859a4722fa4045908c4556baa0476b1861c3c531ccca0d91", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "7e208ca0-6c5b-4703-84af-8c4caf5cc87a": {"doc_hash": "fb4c54d70528dfeffad6fa1e17b47e68d973d7bc91a269e1b930fb738da2b53c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "85cdb7f4-646a-4e90-bd6b-c32044d60bbd": {"doc_hash": "1a4ae88657460bf269e07753f4848d2e8026e64d43e76b0e928bdb80512f0e49", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "908332bf-44e8-4dec-b0f9-36b2e1add3dc": {"doc_hash": "c1fd933b924e0ade23466e478e52a7c9f58155b77ce48f855f5f7abbb4f6e66b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "abbd5bc3-434c-4649-b569-73a6f38ffd3b": {"doc_hash": "2751e69bf60443cefc151383f3655705f1af1ad65b42caa685e7dafa73e27c2b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "cd0a8396-0811-4e4c-b5c8-4d758f48924b": {"doc_hash": "57c81ded532c71b73b85084594422cef00ee6d1f472fd433bfaa900027cd27fc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "b318501a-bd7c-4e9d-9772-98754952167c": {"doc_hash": "c1d0afbc524163941d7754a452f613f9b4d3f14c5be50f88fea39ada9547034d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "34297c71-1f08-4ea1-9408-279b2a29b126": {"doc_hash": "d4ae4e7928419bd93a7a3f6a9ae1bada9a88601150e06bcb25674352294c4e90", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "704aa060-2129-4118-80c7-6465ef9ea9a1": {"doc_hash": "a92c2086b40782ddf361f5fc5dc19aab2ed1341a3ecc11badeaa490e869ab5d3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "5fb83834-55d4-4f6b-9327-73830378823b": {"doc_hash": "c5ab18cc38bafd83d7948940224c02f6596984827d7e63a5c0f89e35728d356c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "65744212-880c-4a95-b85d-b31f439da4ba": {"doc_hash": "747b37ef64c969bea486da664d7aa66eddf7b231c29cbc56b26fd8e74be2e79b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "7e451cf6-7fde-48d0-849a-bf867df138a1": {"doc_hash": "04a5fb4cdf44ba92523a1cb1502bc86490bd8405454f239f1f6113a68b12bc0c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "18e70dd0-d18b-406f-b432-7b645699e39a": {"doc_hash": "ab49f5da9cfb0e9acc4c552ea207cd9ee0465eb6719d23d88f5b826a5f037594", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "0d331d13-7298-4e53-b43b-8e246e25c6c8": {"doc_hash": "a93342517ba3737136be4d9dda4cc3feabc49a132bb3175bcfd3987348911d0a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "0dc8c9af-7ef0-4190-a630-e23b6a30b4d6": {"doc_hash": "80a1a492dee6e9dbe3342a23ca824f7a1088866bb60f8f5a7bed854481463d19", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "33d33faf-7308-4dfb-b22d-f2009ca99fcf": {"doc_hash": "fefba635ac14d177a853b47a62b6888f25dd20761d152045c18b8cee836d42e7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "12749957-743b-40a6-83be-6e31eac48e2c": {"doc_hash": "3076aad7203d34077dea49f55e17afca6e32dc09d851a43495693e61d44b63d8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "60f10906-a74a-4861-b1fe-54a075b91b58": {"doc_hash": "edd425d8f9a3da18bc1315d59c32f4a69326575b6f2ecd1825fdaf499ba13dd9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "6a2d98f3-08ca-4c89-a5ca-c21641b8da20": {"doc_hash": "3fad49c084e96b2c3cbef95dec555466bcca7b1a42fbcfde67019092377f0207", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "18b9c7c3-c535-423b-a086-2de36ae1cc7f": {"doc_hash": "ed52e45203a3f5558c12df07c188466cc7953a08c50bdcb3f12003c8a93336a6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "b566e6e7-c750-4be2-809a-debc38438160": {"doc_hash": "fd3f0222a23ca28669ac53a1deaa2461ca84e031487141b65ca1bd763ff385bc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "b0e9f081-2ead-40c5-9c38-974442a13794": {"doc_hash": "f898e851838e5cf51939027b8af5a3169d432be283bc568389aa24f3f0c1f94c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "49cab8c8-6676-4019-bf25-f352c2ac6415": {"doc_hash": "2dce1d0d948e9a773ad3f11e78a29f072c83f4b1dcb416f284e8d8a63ab506ba", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "f5a34c06-45e5-4e83-ba35-eb06b7a29214": {"doc_hash": "51b221ae4e8337532547b004f635c32e28bc570c30c3155a9da6b5221812388f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "ad760e92-7e57-427b-9caf-23d430443131": {"doc_hash": "2b96ebe46a65d0ea98d3b4c7c8a9519a994726766e9d80baebc77eaed0e1eedd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "80c9f6b5-456f-43d2-9cde-399ea2abd084": {"doc_hash": "6824bef817e0f1a4b5fcedfb6ff8ec9c3e93eaa956accba8ab7a2bb05d8d4dfe", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "be070e71-e7e9-41dd-ab09-e70c95c061f3": {"doc_hash": "1b59de45b2aa84ef35faa8a34f90a0e1addf5a1f320194676ed712f43cc491f2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "aed3a0b0-a415-4baa-a2d0-caa4f979f836": {"doc_hash": "bb0753ff7ecb7e1765e4477dba9c27ac1bd33471e19bd7e7beeb4020d720c6db", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "b7a827f5-de3e-49bd-8df0-0d3ea48e7ec2": {"doc_hash": "8dc0a65d2c7c6e6e51e88d5bfb0cc8b2705e2f0dda1baf0c0e001c9c7e790f87", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "1fb5cd21-691f-4418-bc6f-9bc31ccd8f36": {"doc_hash": "7712e8c51c9ffdf132cb137c2611d3c51cc4137b9ff5dc0dd0acfe8406f559a9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "ab5145ef-8416-4b97-ab7a-9ceebd3ceeb9": {"doc_hash": "d73de0e28d2874adad7ac7e5698e656e332a10d76cc4804ccce9be6c55b12de8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "a107c5f5-57c0-4377-8fcf-9a7d171222fa": {"doc_hash": "c3729a8441fccc9ff64fe0ce0b7ef024527011436d3b0bfdc1b8ddc0ae20128a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "1a4d7fe6-6afd-4b2a-bdfa-30b2cb79dd58": {"doc_hash": "3c2ebafb34bdd9e37f4e94d5c1efb5162b2a4c6bef77c3d459a14bdfab136cf1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "46ff6c09-cfa3-45f7-9472-cf71c94da6bd": {"doc_hash": "244257001dfbe90ddff7a3183272534076612e9d6bab794fc819a31d26e66f07", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "90817541-872b-40c9-a477-10c2d47d5844": {"doc_hash": "fd3da0ecf54e09dacce26d58b13fd611a8f782664201e0d6717d1ec959934ac4", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "e87ce1b6-9745-4143-bd3e-8ecc6a9fee72": {"doc_hash": "e858e3884154ea4bb17f7e375e655b82f6a45df28732a53111009ae873089f63", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "20bd3397-fcac-4188-86d8-8ff1e8881b72": {"doc_hash": "201d05b97b9f9d354aa8b0abe57905c766e19a72d41695c64e070d4fd4a9a5c9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "cf2f0faa-19f8-4803-9053-60d8768afbd4": {"doc_hash": "2e50b3244d863c0dbe215e81364a5324c6539764e97df48ffaff87a5e51c11c4", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "62e536d3-ca89-41ae-8450-a5ebec2a54d0": {"doc_hash": "4822bce9bca14b7eb79e1273f85e022c0b9711cd6bd19102f73129b8f82a1c04", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "d8503468-4dbf-4347-af65-57090e565847": {"doc_hash": "bb3c4ad923fb73be2ab2f28b1fcb5c14f1e93485e4a6a675c22d685d0c44bbe5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "2e6de388-701b-47ed-b2db-98d488f6c476": {"doc_hash": "de362d9e73877c26636c94fb77da70dc800af58f404e44a895491f42114df3e0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "60ffd116-52a2-4465-bcab-6410b5c213a0": {"doc_hash": "91d6db7f3b9437ca218c5aaa6c2e0e8a4f0a9e0330dbbeca121bc9c75077b0dc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "96e74dd4-4384-4020-ad96-0c16b28bbce0": {"doc_hash": "39b5db8fde8f022ce2244be6875a5073b2b484e37e1e5fc9155a6c42014b81de", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "6b9e58dd-8372-4df9-ab45-d9bfe068896b": {"doc_hash": "8ab0ada66e096da503eb52ddf80f9391da1b932ac8ca649ee702e28c00417eaf", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "a988e371-88e7-4e72-9501-fa11de5c7221": {"doc_hash": "9c775bca0825227c2865fc477bccf2904ffbbb0994fc62637127e1afb13ecc50", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "1cbf4c53-ddf2-4426-bc9a-9f2a4299c63f": {"doc_hash": "0adf6a3fc48e23dd9505f16c4b9cfd0e0b08ca2e7bff7b24bf4da612ecff6081", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "c8adb01c-fc25-4324-a599-5d5e419f3a17": {"doc_hash": "422ac1765fd736bf123f4b1f89e5f3cd99e8b6499ee683d8851f6b9f2769281a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "fcfa2fc7-6fa2-47ca-8483-dbee871c3c9b": {"doc_hash": "56aa0a5fade9b2266cc28a94bd146d300e5aa70948f5cc12a4925b6df151d10c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "a7243164-eb89-4d64-9bce-e55a6e4b99d9": {"doc_hash": "5f8998146f06830a5664633136611decbc17fedab99df6eaa53a389b20b86ab1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "9b68904b-d736-4f97-a746-8d7162ccb147": {"doc_hash": "09dcf8901dec01c61e6d2fd5fcd860ae08eac057db8910f9362f544ce8058af8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "922d55dd-3249-47b3-b670-22a4e15af335": {"doc_hash": "79250ae64f436da2e1931bb478bb32828499ec2021a66766e5cb7bc2dfbd6b5b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "19eb1d3d-2306-48a4-baa9-ee9d888b7345": {"doc_hash": "0072953d8d60a85126350d56062dd686819bb937e4affdaa63e6963418fd83ac", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "423abd74-22b7-458c-8f92-d5d3bf8d9580": {"doc_hash": "9f483411573aca961722e8a2bdf74d856744593e38ec0e8f402b7ea88426c89b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "ceefa223-c93b-4b61-b17c-c54857a4ce1a": {"doc_hash": "34399ed6584c454815de4da53f4e33d0ca90ba39afe65faa97f5f2a08ec5ef53", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "e14016dc-448b-46e4-8e69-5255990910a4": {"doc_hash": "a7f35d7d4ec20d2abaa9950b76ec7b7a7d3ae3e24e80f4ffccc9f2d5cefd20a5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "00638092-5fc8-4fe5-8b09-d53b7eae4283": {"doc_hash": "26b29349ff5c2a39a6cfb906752c11de7c7b0c24496f0f891f1254c98c045f21", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "0953a4c1-2cdb-4f6f-b52e-18ac5e0ec5ff": {"doc_hash": "d5e16f7a3b678ddf8bd0818a13ae732ea9a82c50e2c31fc50fc8fcd2d715b27f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "9b479b7d-27d8-4e26-8939-6fc026af37c9": {"doc_hash": "49d7032520ab65433fa94a95d26c3aa5ce7322891939f1481b5eb414385134cc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "600b05d3-80a2-4779-9a67-7324ba7a5f43": {"doc_hash": "a081bb7ac046dfbd306bc1eef2729c2529d6390b5ae30327268dcaa362d84d42", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "463f2f3f-a4c9-42a7-aafb-79cb9b243eaa": {"doc_hash": "e4c02dc7b4d7b46b6b0a7d588343d46cc5cfc01b8c53e99a9705e8c9a65db702", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "698fff3f-eff1-4dc6-b379-316790cbe937": {"doc_hash": "29900dd17723fbe639c3e68f43a76265a1b293a0801514eb696a255a6b9e9578", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText"}, "10.1109/icess.2019.8782480": {"doc_hash": "92f6bd366a5cf71eb005a01900f71b9215ff41cefd8aba767756786b45450bd8"}, "a00ab085-4161-40c6-8c30-e3ad607b89fd": {"doc_hash": "6cacf5c4257e43da918c406eead602ff55a43bfaa43b549ae71300538cf8c8fb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "a3096672-db53-482c-a009-95e6b036cef1": {"doc_hash": "d15d3fb8417218126152135b91219e1078fd930a25cbc4c9d8d48698d79d4bef", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "b6e222c2-08e1-434d-884b-ac775dc6bd98": {"doc_hash": "63e83da0e9d558adde2ba19d983b1061a6e9c7f6395715b20e65e4fc96935b20", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "e3a6e9ad-0024-4a81-ad9c-73838f7602ac": {"doc_hash": "61e50060e0136dcae24c698c066a3d1920bc2b0f2948d52dd8d3d47cda2a8304", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "6cb7b791-e7c9-4c19-91b8-544737658d2f": {"doc_hash": "30187d32f1743276097d0b89eaf46c30a43892a2b6006a2ce5bc8400f4c645ce", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "a9525ac8-4de6-4526-a69a-ceaa039ec724": {"doc_hash": "dbd7e711ecc7bcbcd36e1c98ded5e276e60a2ccd2854e61c0242e8c2b275f2e9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "b9c24d88-1cc9-4fe4-939c-050cedd3510b": {"doc_hash": "11984c8920c28f8e761ca5c525d286135a67f9d455943a2afa1d5e7193b81653", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "2c98efe5-1ded-4666-a266-a0634c77a03e": {"doc_hash": "dcc1851dd70b5849cd3154f6573c426da8b53b6abcdbee044a2c347f4235ee86", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "284a3228-e38d-4e17-80db-d78802d8f847": {"doc_hash": "2369846bccac467e6c93843f7c34765db9da5053fe22e261606b00a54cef8f7c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "e2f67fa7-b73b-4c7b-b178-d04c434b2f1a": {"doc_hash": "1b2405a85600131bab039767560520192130e5c26170e44f1425f6bbe590754c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "f4517e9c-4f7f-47a6-817a-4197c697ca82": {"doc_hash": "d15f3b0ba3064b5d61670af4eec6953b1135bb959617f6c2f2d245c1d0cd03ed", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "f9e854b5-4acf-403a-b8ce-86ab39c90784": {"doc_hash": "e49801d1ca0906a4afcb037dad63000e79e57faab5e9e10c84a90f3deb4ea704", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "d73297d6-32ce-4901-824c-8fce79c77571": {"doc_hash": "752c828d366b1a27fc4352cb05f79eb7c2324654f9d3738289852148a30b1e77", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "c89d7d84-6dda-4677-9adf-0cc318efdff9": {"doc_hash": "21c029c0d404356a843a17dfc34e3016f344715e9d62f313471dbc5e60c0ac87", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "9c0552e2-bfc9-4a44-9212-048333f21e01": {"doc_hash": "5a41bbc22c916605d16cf7ea77e0b25414411c8227904b545cf3748bd33aeb8c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "90643c3e-7f32-4875-8584-2071d08b76cc": {"doc_hash": "7b57c27c4fb3c336a0aa02dd99e267fbfc0fe157b01cf3b7d5276d42c96b8c5f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "b5f4fba5-3d2f-4fcc-8de2-a1b3369330dc": {"doc_hash": "5abee1ac35f39a2d403bf5529cca52ecec4824485466942148820ff2d720a66d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "2106144d-642e-4960-b425-558691a5c921": {"doc_hash": "2a4b7ed81558d338b633653c63751f7122d3a3da74dab03f5d1f37801a07d640", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "c5b89651-6775-409f-a4cd-6b7e9b7335c3": {"doc_hash": "198c17677d856a728c71769ccc7c0fb000a95f252665e10ae7f47aa9c78d3024", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "e3cbb9b3-fb5c-43fa-85b6-ad6758f7041b": {"doc_hash": "39671d671310141e4af4bdf8cd021387b699bd1ad50538e315cd4e8a3cc14472", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "59ce017a-b229-4751-ae99-2b35be85a5dd": {"doc_hash": "6facfeaa2fd20a1f8264638d8191071eb435bf32371084009f504f19f9881181", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "60db44b8-0543-4e04-a8e5-983e194d5060": {"doc_hash": "7db8c2a9eeaacce819159c41c8bde68551a55a94ce80f7f763095a0b437e0e30", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "b91e7ef7-2c54-44fa-888d-bc700ea556d0": {"doc_hash": "7f51e5da097f0da4ac1d8e891fe671bd934415986d40f13b6cec9693f8db3c7d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "21b8ae8b-bb04-436a-add1-104cb8dd909d": {"doc_hash": "f2f6d1b725fd1a0d32cbedf76ceadbd2e48f40a08a6f3986f4425a8fcf384999", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "777dcbac-c3e2-443f-91fd-d02d920f17e5": {"doc_hash": "cf2c635464136f107ce23d1dfba9911ff34dace10928873b39990414c7a1013c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "b86ae0c3-0e53-44ec-a9e8-8e2ca25b0a30": {"doc_hash": "dd2cbf9b2a06c958b31763a6ddddd7d742a106c6926819028d3ce06c0d34f7b5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "502ef66e-6dd2-4559-9c21-51caf7bca8c6": {"doc_hash": "1685370f11b73643cd40abedcd1277f10785f30d1d1be21274b5d57b16f77305", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "a248eb7d-7045-48f9-b481-3521d40ad33f": {"doc_hash": "e0ec567d617b2dbb19cfe9f55b7db44f06a163613efaea4f33536b94b1ec9a9d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "fe131852-5ce7-4d09-9faa-715544830cb0": {"doc_hash": "ee0f5ff0ef1c275152dba31f469c720380057280ca7bcb222201f816cede6ba1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "c8e6dd4c-902a-489f-911f-fc016ac3b818": {"doc_hash": "1378e29d7abedf65b30586515b532534f850a3d8da4bd0bd74e67c13d4e6f2bb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "56afba75-0e7c-41f6-8310-43fb22925ca6": {"doc_hash": "c79232cd9b6ffd603135d0f9cd3df1153061b9eceed5da49fd545922d95a9158", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "1e6022f1-114a-44b0-b4f8-070f850231cb": {"doc_hash": "97c3871a5aaa36d32446368ff0b384789dbdb50b78fae864bc5c06c4cffe3260", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "dc7673ad-6e8a-41ab-86ff-a347ac735b2b": {"doc_hash": "42bb6431935a2015657d569abedaf12d48e8e4b455de436c6c9b212cddc2d985", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "ce91620e-7eb7-4b78-9e15-53c97f6f9f5d": {"doc_hash": "044d2283ece25ad0e46df7f4ab85a557d7ad906d0d2b627f4191ad3e2c4ade73", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "54e970ed-7a06-43b4-bf52-ca552d9a7b8b": {"doc_hash": "f205c63da695818755a12fdceee661b62dbe64ed350bd933e866217bb3f63dfa", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "2b6aaf40-171d-4c09-bd70-a207e25cb551": {"doc_hash": "624c5a83ead161d2f8a180ed75ff6168b822362d78abf5d9d61885de1ffbf416", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "2c8d0d18-2d69-4ab7-9e31-420aea81192f": {"doc_hash": "3dc4cfa80050d7a7dd51f9c7741e4562d4f3e4b8acd2828fe10c97aba5758301", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "3c096f8d-52cd-48ab-8802-7364d73cafa2": {"doc_hash": "353cdd14ea27f5dc1ca25685b11c176d92a63909cee8e78a70ef6ca48b46b5a6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "c2df2b22-d8be-40e2-8dd1-90773fc1c113": {"doc_hash": "b0262bfd8598b02be3301b60e39d7540cdaf14e515b300edf985197242ab8bf0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "0f50e789-1e08-4641-8cd4-b28d5f6b3460": {"doc_hash": "0ffdfc324ae04b1b5bd4c234f970827a985fa845ee8a6caf322539ea43ce79ac", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "53e5906f-de74-465a-90c1-75e65dd77205": {"doc_hash": "6123cf49b34a85d0efa939ee666428401a3cf6b19894d5c2165f485b8a2912f9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "f4629a79-99fd-4883-ae92-e054b85facc5": {"doc_hash": "613b4c9a73b2dfed040d4ec48a2b697f09c725062f0433e1711c58d9f73483a6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "6222df82-1d98-47ff-a0c9-db189f8c6937": {"doc_hash": "476ff961f98d1dd3fd0bc268346f15a245ce645264aa9e9805cef2765d7eded9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "8cfadd58-f42a-4802-b49e-3d3a2ed8aaea": {"doc_hash": "9e8fe3802de66c7ab1b79f82de259120f7c886831c98af2d71b1b0693c7c7546", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "04a5abff-9386-4cbd-b72d-85593cd205a3": {"doc_hash": "1c54ec177548477749ecf3b5c5d9c8a7d3824b6755158bde3a5b56958954539e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "2e8528e1-bf1b-4bf5-81da-28e65910d1b6": {"doc_hash": "f1b4718413a2a9cdd40a170d466e331ccc4d8318afb6d9c09856d4f4d08bcea2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "a76354e5-746f-4b01-becf-8353e6f0d923": {"doc_hash": "a085ba5515d646d58da61a8d1cdeadf3532ecf3983adfab433031a6b102c37c2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "1b2e36ba-aa82-4011-8a43-ad89f8807cf4": {"doc_hash": "b44e254eacd9d87797832761db7665cae63abb0a286d81a85d59ed9784b92d00", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "8f57d00d-c394-42c0-a3e8-7b29cf00fd12": {"doc_hash": "bb4bf13863b3e35c89bd30ff97e1c9971ce8cef0859950e2735cb4ba919afc5b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "beddeded-41a9-41e2-b0d7-4d68fbef9c8a": {"doc_hash": "ff158d0fc3609ce45c515ce3fa9358aab33b4176cec5f5359ae1b65fdc51e1c2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "fe4ff507-003c-478d-8d23-7c649516db2b": {"doc_hash": "35ff6c525d59692cf615a6b906dc8a3ac31ecdb54ee55641f9d0533046009afd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "f2f79ecd-d00e-4320-bb3d-53171ac5ed78": {"doc_hash": "2cc842fe6671ac9e94daf25238f98efeccb193a52578fc73a415757bbbbbaa25", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "e5a01699-da84-4666-a2e6-f3696cc085bd": {"doc_hash": "74fc42a7e8381298db2b2f1c5987f8a1e80b90cbbe19679d6b914852cadb0ba6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "8af28542-ba25-47f0-ba2f-6bf41ab2e18e": {"doc_hash": "63c2465c2380acde267300d141d81984a9fa9c703bdc29ea1b2a0d482db34549", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "7c04abd6-cef9-4c1c-8754-b992b75de0a9": {"doc_hash": "c212fbfec5c96166386e75bdaf912a2feb50c5e001fe7fc6aa881b4f02a43d2c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "f33879f1-570c-4f85-9e46-5e17602302a4": {"doc_hash": "3ac3d782f3f28d3e827841801fe2a2e9c05abeec21db67a46dc333bc2c0b101c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "49b1d941-ef85-4e88-8b11-d1834a0ced47": {"doc_hash": "43816900a50d8b1cb0ae4af1e2ebd94af0f864796aacf40364cefc0aedc5f5b0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "065d50ae-16a0-428e-95e2-756112bd9d65": {"doc_hash": "188208d3e8859614ec20317f00c7a7f70328c23e2598b520fdd14a95aae6c1c5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "518c0cd7-d67a-4f81-827c-d5d132d6ad34": {"doc_hash": "cae5e4b19dd841a54a1302ac5ecbcc3ec5b913dae26753a24a6f5e45685b708b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "9fef4540-a8f1-45bb-98fd-1c5cfabc8776": {"doc_hash": "76c23bd07c835c40ad1b4a2e2b84cc99ac8560e89ab8498de96e57eda88fd5ba", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "0bc940a3-bd3f-49df-adfa-b3977e4a8a35": {"doc_hash": "cd6620fb4f59d6a8b9fbfcb69613b84c0d3b91a3fc08daa7b8a40321ebcda018", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "0ad79ff6-aeef-4ff5-a3bd-312024872057": {"doc_hash": "dfa1f1efb68918c9c08b6684e00a9a5f348cb3e1b76b763165a581f196c9534b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "0f83664e-f4e5-4c94-a082-2a48f14fb158": {"doc_hash": "3840f94425051d2335c38e88324b6d398c2f01911a9d67ee76c62cc4f11a7d29", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "c83150f1-456a-4ea4-bf4b-7ded0d77cefa": {"doc_hash": "456ed22b157f4758221376cd695c947c8487ade895924bdc67bc77494bf9e48b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "28973d5e-e798-45b9-a244-8d8a2e33c7aa": {"doc_hash": "4787b649cf7fdcbb98b80d5ec3bba4b6bcd954f586eed664d1e63486840b6819", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "73de1ec2-b29c-4505-8822-68e4e0215bdf": {"doc_hash": "9054450401534f96716c7a97d9058f41b6ef74628e2413fb0ac71c0ccf6efe13", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "dcf1dd35-d48b-453a-a2a8-e0c57f97bffc": {"doc_hash": "dc58b917cb2ce29b1aefa731a1920806730f7e34987d577ae5dc3f27af100f4e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "c62a6ac3-6952-44eb-97c7-9f869b5fa962": {"doc_hash": "be68211bdd3ad4087d62eb88b704bcd45ca90ace18d5801693c486cac8d2cfa5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "882bf157-6082-4418-a2d6-bdcd1b788533": {"doc_hash": "eb8c2485f7be31cdbed84ad3993a570036a0ab12886edb3784b5a4f1cd20e7fd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "95fdd86a-5a07-446b-ae20-0fb1ab62c1b2": {"doc_hash": "ed0cb71edfcd9236ab9c65c0063d2da64a351db6051e1357dcb87244a9f43fd8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "78eeca5b-10d3-4bd2-beba-6c6331f17a3c": {"doc_hash": "5f21ffb58c71e07951ac953163e10657155d2c900ba6c659d3a951b258832172", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "254e8095-883b-4a57-8435-68a9c5ab72ce": {"doc_hash": "586d71cd8c268db607b5fc19624573a0d77939c1a7e33454124c649ae1f06846", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "92dd26ee-30fe-4f33-92f6-5c8b12d4867a": {"doc_hash": "cae86bb1d698b2c310641380554c82d818ceb366c41f5459d16edd01e2286caf", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "472c7f79-4cfc-43b2-9f2f-0827a8962d77": {"doc_hash": "58a1a7ccdfd8bde638ca84647874aad1d46c76f71211a5845f525c794f12df36", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "7ba2b0c0-174f-4b19-baeb-84ad56a59329": {"doc_hash": "e6f29565ffb6d097f292589b95db848e799e0279199bc1d2d769e52bd43e5cb8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "9437fd62-8259-41e4-8953-57e2c81f337e": {"doc_hash": "d8d97252b824aa81917f7e332363c68b81ccd706ca72ab65fa81d34019c0b2f4", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "745a34b2-9238-4ca9-9859-a19cb95e6d63": {"doc_hash": "ff30ea48e4cf6b7d6906584cad7c4125ccb122a85d70f26bc1ec6064626caf03", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "c2f806ed-9172-4a16-a400-d4b6c18ebabc": {"doc_hash": "e328e949b0a7ced7662ff50ddd05ae190209dd3ce5451d70a448c1a09ad0bb25", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "51021f82-ef0a-4d15-8304-d82b3548a861": {"doc_hash": "b97e8a6a799873e1e0dafe30dbc0a6f6462e37f41b4edcfe7f7d2633e57ccf78", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "3bb4d99d-65e9-4760-9542-7d74f8ee7545": {"doc_hash": "63e53b62eb446e372b35f39d0a02c7c6e6f5734b239eb9edc9afc70b42198007", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "173b70b7-1aac-4dcd-b90f-ac71891ce5c7": {"doc_hash": "a9b6139dc1822b8739f21c76fc5dc5136cd4d861804771341ef56deda675330d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "2764cd02-33d1-4750-9efd-8cdc99f927ad": {"doc_hash": "b3189beec149626c3f81050385bc66a9d2218b72c59319baf2cf46936c400136", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "2bbf3931-5239-4fdf-814f-e1b7cc3b3e80": {"doc_hash": "052b0ccf4111c542ff821b6cc26cc9dd54896e11e82fa671272e8dfe1f432ce2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "023eaf60-3166-4103-a4f3-26084e42f378": {"doc_hash": "b844e9313f31329e3462f8aae14fc46ef35c884f1b3cca1aa3148e118cf91f21", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "4348e3f2-7220-4f7a-aa3e-9ccf9abab9d9": {"doc_hash": "6d1777b9738f9815c5ed672eb992b8540b89a8207202446869e904298c4ea6f2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "df172485-81b4-4ca1-aa82-75297446df18": {"doc_hash": "5bcbbd646198e4c89523673e30275bf36b809270f73485c955a497355e17ee81", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "ea8821e7-a465-479c-9ede-fcda348b98a2": {"doc_hash": "5e7066810f10e4855f1fee5f7be2bbc77b9cf91043028a85c12e3ce2b0cfa0d9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "63871b78-8b39-4786-a4b2-5743a0192158": {"doc_hash": "4f2917717260486ecf5e879b57c652c076761c4bc5df0c0f9c1e3cb639b117ad", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "d4618e8f-9429-46ce-b8a2-aabb65355644": {"doc_hash": "cc2230a414206ee4809ab04f255e238db98d85f9f5e9ff887a1061899422af13", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "3386c002-adce-43cb-9c7d-552cfa71796b": {"doc_hash": "c0df0ff42ae74a76ce045d824da27b7a4395de73e84f472b2b37134379dcc845", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "afde5e1f-9ae6-40f7-b8df-b9f44ec544c9": {"doc_hash": "619f305420b966e1c930c583436dd167d41665536cde7c8a7c8472f1bb4cad38", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "38bbec5d-8e77-4b46-8e10-2211408f8c2f": {"doc_hash": "e420eea2af3c9357c5e9f91029c2fedf4cabda368665f00f5c1da414e397f513", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "0f9b1487-2428-4aa3-9f47-be1d6b3a0d8d": {"doc_hash": "a63e970cab5b8b32493168bb6f234793fc9a3da802dd15b3eeca6e9a3fd4c031", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "312ecd15-f42a-48b9-8a82-6600e5fa0e28": {"doc_hash": "50cacf794ee6d71d53c14b5b360916d77eb0b9591efcceab1e50ac93f600dc7b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "bd24967b-5200-4fcd-8140-16e238331f6d": {"doc_hash": "f89a363b8aa8cc286b2937c43a3a7cbdbe22c720b791d65b3d44c4162939db05", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "13d5c9a8-541d-4f57-98c4-1dfb90d00fb4": {"doc_hash": "49a1cdbca9c6160c460fd7078b004f869270fa75d3b0306a032557cb65cda04b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "af37f0aa-1182-4f6f-b2b3-2d18bb75f0f9": {"doc_hash": "58e8d2e6575c8494cf57578a45e64d891b4f962466e62d0950e32a0d9bf91f1e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "fe7f1b9e-16c1-4b57-8361-5961c1311cd9": {"doc_hash": "96f5635b2ddc9c90e1d6f1fa337d76f6c88256b67fdb84f263602f742b38b79f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "45830ee6-afa4-4456-8ebe-1b7b71ebf662": {"doc_hash": "5b1d10f6b0b8fa837a50453db53e31569caec9e58da8530ce81a89bc0ac9503d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "6edcc081-a266-4073-ac92-28cf9302839c": {"doc_hash": "98669f3117eb9149c0b45ede3a478f1355e58a3b8fb8908a114962cee2e99fc2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "cd50d53b-a36b-4f88-897b-1ecec7961fb4": {"doc_hash": "dfab7578b8018e56beaa84aef40113de8dc10dd122932aedf35ece0a8647a2b8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "9afea05f-7150-406a-8608-a0b60157f561": {"doc_hash": "b05cd306a8d6804810f13e7e6d0998385ba341133705e67be02278bd88e58533", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "c15efbc1-a2fb-4250-8b40-62fd9eaa2825": {"doc_hash": "832e7025d8611279d98d908bf8f3b718274617dd8c13238f9ba1bd234bbb98b4", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "814aaeac-2eaf-4f8a-bd9d-03e74536fda0": {"doc_hash": "ff73fb7f2fb8fb93f29a9822129f3af46aa9be8c1d73d636f5dc6878dbbb5da9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "276029eb-491e-44af-b8fb-5090ddced02d": {"doc_hash": "fde63f25654990d27cd269ea29df8b635a86aa33a1cd225c20074460b1185fa9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "7cfe1085-38de-41b5-99cd-dfbfdb1fa048": {"doc_hash": "7c1ee977345cebe42e45bae47c5d6c04862c822c9ceca000f22367e1830274ae", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "c9d0931c-9237-4625-a349-323e48ff9912": {"doc_hash": "96c8acbe7d9de5796035ccf7de9cf01569353dcf25f49ceb2daccff25b128a7e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "e358112d-4b6c-4589-ab6c-13424a213b33": {"doc_hash": "6d4a35144c3330b208e577cf80a5b1f645b974dd9b92d14d124b7c3bd16e768f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "615beb3e-5614-4779-8bad-52087f6013ec": {"doc_hash": "6b8d4d697278088134f2ce4af7e2ef26364c537366d7be2ba25f1ab3ab655a92", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "99c84cf4-d116-4591-9175-4c43cf62995d": {"doc_hash": "9ab1d24b1270c44ebe2037f8fba124952ed45c09c64ed4ae26edd6448560bfa8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "7c718b15-d4f3-49ea-9a12-924fcdaf6ce8": {"doc_hash": "aa02a92c467aab38b518f10ea3efa19eecfc5ec5dd39da190b1e92e13dab7bff", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "28ded052-665f-45cb-a075-5352cda84962": {"doc_hash": "2d2e5004be44471aacb7bc491d1aac0f140c5bea70c3a4d8236ba3974b623349", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "e762895a-426d-4d44-9a17-154b0151c079": {"doc_hash": "ebc097810c09426e64a61a95cd0ec9cf0020b46b4dd0093e3b67bbcb75b8627a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "5ba8ee52-d15a-4ce9-bec0-c8a43e165b4f": {"doc_hash": "a9273374a6ecb2bafb1a178212cdb3223023f6e72b926cd60c95efa0c2ff4f15", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "cb1b4cf7-a033-4e60-aa94-ff4f9541db7b": {"doc_hash": "dc0f0a5363915ccca0c186e3aed88bcc9bc72d331f427cb84c554565067d61fb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "37917e4f-cf59-47fd-aa5f-4e4db1ba43a5": {"doc_hash": "53f4840f4d7429dd6cf57931bfbcb5db158c6021ad8e9dd9d687fb46bc49b359", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "ef343a51-c03b-45ac-a28b-d9a6c8a0458f": {"doc_hash": "16ed8a743a6f1b70da6445b4c3ec77f77203f2b528ba16142eae3969b2f0d0af", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "6ce86b6c-571b-4270-815a-a28425a2eea7": {"doc_hash": "892a863dd32b5df2b7f1523fca79c008fa3a6de9a55866ff58d4a52a5c305161", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "bc4a02e8-2566-4e40-abb9-0ba20265675a": {"doc_hash": "c906e1f56b64f3e8f6a0abc08dbe21e05e42df1f8cf97a6f0f4665035c8a900d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "715d2610-c061-497e-bc9d-b973c002d5f7": {"doc_hash": "3500666167266916bb15a278794fccd91ddb0835c4a2b3bb76e6782937e92008", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "f51fcedb-a6e4-42b1-9a11-8b4f128b36f4": {"doc_hash": "c5d88a443e2aafd82aa875050f39a936375db0a691acf8e477ffe652f5d9309f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "627382b4-1d6a-4334-8028-bfe8ef264439": {"doc_hash": "d0f8261d680b184fba0bedf948ea108741cf18ac34791fb40f590660941e140e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "4be65253-c5c1-49d7-a368-c4b338318f7b": {"doc_hash": "2d4b5e50c99c8894eeaf165b80bc8cff714e5b6bd2b0a245c5a3b9e99da9a991", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "17ffb171-d7a1-4ff9-9478-ae0082cac502": {"doc_hash": "7918341b08a8b4a4543f3bf9a7c677fd060acc823fe155f610eb1f1bfb0edfbd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "c68b5b8b-e902-47cd-a80a-1fbc0110520b": {"doc_hash": "9aed6498217dae95f75683a77d8ec4a7a907b9689c9b3f603fb291adaddb8ab9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "cc0512d4-eb35-4c5f-80cc-caec67869c9a": {"doc_hash": "2926987a4d078e882c044b50443b702622dedd7460a629be2bc46e7e5eeadb77", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "c731faed-4857-4b22-97c8-a79b7fc5542b": {"doc_hash": "7b238f0e610ef62d0d9c6494d9d7c8f76fd6cb1f118424c606fd28d2b833763b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "afdfe7a3-0f5f-49e7-bf57-11645500f841": {"doc_hash": "f7e2c32cf8e2b61bbe9502eed08467a3125f4b6b738a13b10c0e46d9fed24339", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "13c63c38-4afd-4575-b896-d877cf9a9127": {"doc_hash": "30da82ca64225e3f065a9febd5c78458fcd913b8ea559daba632dffe28e18f1a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "bbc28024-fbaf-4bd5-b331-9ffa2063d3d9": {"doc_hash": "9525faec27a1d4d002f1630abe59b428e2f3b10e6ec1bc7766ce7b7872b1578b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "9d8995fe-d161-459b-8043-ef90a0bd76b2": {"doc_hash": "07adad5e96cf857c99cca44ad80f411a8c3c16a6a6499b39b42d13b62db6953f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "52a7d3ee-c29b-446c-9f34-d35a5008d881": {"doc_hash": "7cda531a6afc2d2f46f8dfe1a3a7206710521e41ec9fd77a8b18454cc6707815", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "4dd37d7c-a5f5-4964-9439-703ead11d469": {"doc_hash": "5d8574d2a58b2fbb72b63681452719c700bdd2aa20dfc40fc5f6373eee525415", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "7d9832d2-c9d8-4bf9-afa7-5b685f590008": {"doc_hash": "0b7cf81b6563462281573bc1a7bec9b93b5ffe75b9e67dc8f4e6320714f0d88f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "ffef8535-8176-45dd-a5b9-d413f0dda380": {"doc_hash": "bb3a9973ad63b07aa225d197e3d4923da426869869e62a364f24e20af74c71a8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "cc1b8cf2-ebd7-40c3-9605-af705de9b9b0": {"doc_hash": "9bb5d11829e94516ca2f54d0433ca5b67d754a9a7e8ec192c07ca5c5a9f66cb8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "9893d134-8dbb-4656-9e92-45ac7b4fc86e": {"doc_hash": "d5259f492aee90bc8f260f891a3e8e48561f5e31b525152ff4e662699c0758cf", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "6ee982fc-1e1e-4676-b822-26a01c688c4e": {"doc_hash": "5833d38f62d497a9e274e33c1eff6d70239f86642c3257790b64f8e1d0837939", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "517ef586-e04b-405f-a324-524868745a04": {"doc_hash": "59ccd8f1b85e6968f6ead52a7edd3526f04587c7b9c4cb21b7eb33541c66b196", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "f6b5e5df-534c-4e80-a530-35cc84be6677": {"doc_hash": "82ecd006289f4e659316a386238a19c60964f427673eaec966f044cfd6165615", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "47cb6eea-6fdf-417d-9e32-62c7539645cb": {"doc_hash": "d503f064957b4bcfc8f6320805adfae02cc69dfaa073ddb05a8e94b9efd32ea3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "80d5b376-a62a-4664-be13-bc4b7fa94323": {"doc_hash": "2145477a51c6e91411455fa3043fa833b14203e3b0009b853b573e4e6cda3b0b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "58c87ddd-19e4-4729-8481-0bd4a49e897d": {"doc_hash": "c9ea867eff171a4605f7f1208c9c030dcae71b12d990a9571d29ef6d644d778c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "1d2e6e11-b2bd-4d29-bed7-b13000ba1c97": {"doc_hash": "0d4b22408253a384d48441055cbd870d7e790682eea55d5fbf784a732844ce8d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "ab6d6655-7946-44f1-acab-626e95181823": {"doc_hash": "e61418cd27d7db81fbe5b178897426f4350d4d19979a92c729ef8868bb3f42c7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText"}, "10.1145/3617686": {"doc_hash": "c99bad8db08443a39019755695c04e76afe34921f51ae2c865f3bad8315973b3"}, "ac150157-944e-4376-ab03-a2adc0711b9e": {"doc_hash": "5003b874adfebaf08ca0129eddf35025a5cfb79009685603b743344f42b68b2c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "4036cd26-4a0c-47e6-a38c-5580571b84f2": {"doc_hash": "2eaaa5c5d2d7946e9143fddcf3afb5f8a23e3ef25b7b06f56c14370b77ffe334", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "ddba2af4-55ac-4b35-bdd8-6b0cc7aa140c": {"doc_hash": "a7103fd346192cdb4a64a44c8c08ca63af50e4bafe0e2b94db443b136546a20f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "d69edfbc-1900-404e-85b4-19517f3146ec": {"doc_hash": "a12971f0991d305ea261c656a8e24fce3bf01401986f541855bab4b9625f30f5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "74605057-79af-413a-aef5-6bdb141f3201": {"doc_hash": "21c408781ee6e26127cf7faa5f0f45dd6b6923e3acbb1ea835dcdb892c78242e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "801ea761-ccc4-44d3-a49c-c99345bb67af": {"doc_hash": "9e8a0db0d119777a0bf1f40ba7af7fa42fc02dc4ec19cacde63536cd1f711950", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "7ee3e216-e1f3-450a-9106-c4afa90a077d": {"doc_hash": "cc05d021f26cc01d2e9c07f8ed15543d9eb603792ce7a563c641dd465b2d4b61", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "a22e22a2-782d-48cf-9c42-1b77bc109742": {"doc_hash": "bfb0d3e7f5bb4dc61ef7636a89ba33940e7e35435136c7bb85e95fa0f086c000", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "263be616-50ef-4b9c-a9fe-364ebc3c4be4": {"doc_hash": "62dde8bf5b2b5741becc2a40d639ddb36addfbec11a1ca7caa01372aa7fb0b03", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "5e4f548d-4156-4f8d-9ca3-7edc20df8b89": {"doc_hash": "4fef3ee07e12b161d06d796e077d305c8d2bff1ef86f9d08a56211e0c0246e34", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "654b133a-b06b-419f-b9db-64712eb02a5c": {"doc_hash": "d8a44b68277ad463a8cab16e45b96be14f8346fe36f0404373ced01acd7f3c0c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "9f7f2548-c016-49f9-bc8b-368200b3bfc4": {"doc_hash": "5725108acf392d7af051e8808f6b7c4ae96652d8f0928947056f481164b90679", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "54372f18-4b51-4b04-b92c-018f74c8ddba": {"doc_hash": "f3484777e34bad6a8c93d4d1d5c05a01e13d4f0f5369e4ef7669a915145eb847", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "c1e03c35-08e2-4e0f-8f85-0e108d31c967": {"doc_hash": "e6c0241ffbfd6d0df5632cae4de12910028361e9bc4c5161933350649a5b7014", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "3737d0ab-d82a-4fa2-b916-7b6c2e2e57f0": {"doc_hash": "a618e2414937ace52fe0f7c1234e8e1b3230a0543fc59e86aff3964d457a0574", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "a097ab96-b7e9-43da-b1d6-2c581306db21": {"doc_hash": "9371460d2ff6741e4ce5469ed94ad53584d057c80d3ddb098fe1f7300783ceda", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "a168720f-4e90-4475-9b55-2318ab043243": {"doc_hash": "31057da07ec6b28f7baf810f7d37725c0e8981034f0366e43d1c92959caf8cc3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "a69f7089-ec4c-4cd8-ac38-91c8c79bea9d": {"doc_hash": "5e34e5594d6dd9b7adc483f46deca506ee4c00f04710d8ece4e60ae2897b7a51", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "043ede7b-f2b6-48b7-b4c8-0d01b515cef3": {"doc_hash": "f455ab6cc4536b8665b03687f2098b0123c9b2fc80e74be1443b6c486299125f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "f2c53940-fad3-4ef6-b6fb-6621fe630357": {"doc_hash": "483319f44d60c482d39d6186ba38cbe5b0f4c1ba51e81364d311d69f64d71b8b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "af55d924-20e3-4c1a-b472-e94d5620e5bf": {"doc_hash": "90c38d04150d02e5497dc0a2b95a3698c5ae88be958013642b8890feca3b3f89", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "4d0e0363-8e64-47b2-876e-a4a2e7208c27": {"doc_hash": "764ccd9723494c7d20ce8b09dc0f2cd47aab5e1f6fa9bd5fcf20dd30bd06fd38", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "b2a448bd-c0fe-48a2-bd2e-9948218f8176": {"doc_hash": "c0fa3b5291d36c4ba5756f079ba3bd931c74ff466afdf4a930febf6d4aa9c2ad", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "77d6e95b-5746-409a-889a-f2b033a180c0": {"doc_hash": "39b17a82e7b0fc3234644fd89d83d3ca81f1d12a55dfaa8e7d3011427397e3f0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "b54e23e9-b27c-4647-8365-5473311fa6c2": {"doc_hash": "8daaf5f8cfcfa74874260025f6faaac234666bee2e9e776a26a97b5224cbd27f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "125f6f54-7a86-4e57-b17f-c5d742dc864b": {"doc_hash": "fc27a2e9e1249556268ec12e09a945752be88c24b0eebb25b4f6490f549c11be", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "3aa1a0b9-c4cb-43f4-b481-6949a4976ed7": {"doc_hash": "e51624fe5de94b4af80cef99cba7879d123bf04a2c80bca59a0f9d6109749593", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "4261e0fa-e9a9-4175-b22d-931e66dc3b3e": {"doc_hash": "2a8af5e24acadec127633b8717b1ff9f5a58601f61e4a305afb9d5ce02163dd9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "e6a4388e-24df-4d21-9f4f-f14035297a6e": {"doc_hash": "58451e54c752089e770ed41bd44fcf59bed8f5eaf3da4fef9fc6016e12fcf129", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "6303729d-8d4b-43f5-8b43-0a530b7e4445": {"doc_hash": "656646b40cfccd1533760eef399c1e722909927a254dcd6b14e353c14a1f7d9e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "a293441b-a94c-445e-810e-1bbeea180301": {"doc_hash": "af8122eb95b448a3edf7d7f99d560e7d10757aa932d794f2ac4ec4597783fc5d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "155cdde0-2686-4bf5-addb-697f5a7f296a": {"doc_hash": "e31a7b492922326c794ff9c064bd054fd84c5680d48abcbdbbc8d709bc57b757", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "256e98f6-ecbb-45fc-b674-6f7f2eba332e": {"doc_hash": "97e8940ca2e2ad4aeed8b2c50d4625f75a3fa7ca559e491d871e67d2e4440236", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "29084e2f-3623-4886-aa67-cb7f33768cae": {"doc_hash": "b61ff2528c3ec5f4428d5fcd343f63c658f25e795d5e795d9479f22359285023", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "049171c9-1891-471d-ba6e-4e172cb9c0e9": {"doc_hash": "5a5a0cc110849e7748efb215df171e764abd0d5c43872e72d75e79f78ab6e36e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "077a973e-2710-4c6a-9f9d-b31c3b511395": {"doc_hash": "9a0b64919558aa3cb1dd7c165a121b7da695cd051b648128e7f3a6b3f0fd57d9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "c03a63e1-25ee-4a68-9138-2502329a1305": {"doc_hash": "e7d3b63c1410822e667be580963aebc0e5d7bf71d1979b6d8c7943dfd478a1cb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "d41c218d-7d89-456e-b33c-a780722c64b2": {"doc_hash": "2a9cafea8f5cbe639690c1fa0336e39f5a61c90c92c6d0c8b71c8c4f505aefa1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "267ca3fe-b251-49b2-b6fb-b90c06fb40ca": {"doc_hash": "9a0b64919558aa3cb1dd7c165a121b7da695cd051b648128e7f3a6b3f0fd57d9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "4892b2a5-8ca4-4033-9314-0ef9038f7ef0": {"doc_hash": "e7d3b63c1410822e667be580963aebc0e5d7bf71d1979b6d8c7943dfd478a1cb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "cd6b9d04-df7f-423f-988a-3e8e33a760c0": {"doc_hash": "f92f375fb10a2f0cfb299124eed21ecca1a56f7870fc211f88d99014434d18bd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "adaec0f7-23ee-46ed-ab15-0d23cb8255f5": {"doc_hash": "3a64c01cc071e6fc7542e3724d6f3d2e434927a47a5204a3828580393d86e104", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "844e2005-5def-4c53-9586-cffa04a20dfd": {"doc_hash": "d3b3b333beed21476a63eb394effd8cd8a29a60ebafd081f511599bcd5b00779", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "890450b4-411e-4a3c-ae4a-3c6e9ce1fdbb": {"doc_hash": "e5f322c45b076adeba746a1c62cc970dac52ea09f52730c35a15ec34e699ba2e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "80cd8183-88a1-4478-be77-a6c1be7349f4": {"doc_hash": "4ef69e7530ca617432b125618ae4f2be05a7175a1c38028218134cf7574cf722", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "37c68533-1723-4951-8ade-fd6962c4e9be": {"doc_hash": "e8c68142788b84047dc31ac4cd7d83de19618aa7f5a0fdb1eb076036205d5cee", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "c17b12a3-9751-4a27-b1ec-35fb1ec0f714": {"doc_hash": "852ef726d27f23190f34e81f82c32d098b66ea311b655056bfc73d11c8b25224", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "8fa2f47f-fd5c-4362-b80b-afc4e654d3b3": {"doc_hash": "8fb290b2fe7c32a00542112b30dd7a7e5da91eb4eacf1b0ff8f93ceb1dc2b418", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "53bd0054-6806-4be8-a421-7b4beb57ed1a": {"doc_hash": "44b5376e3d45181a63d127448f2f299865fd9f78e558b12944c62267b7e2522f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "2effd1bf-74a9-4806-a3f6-b91a1df0c5fb": {"doc_hash": "43ff1fe61436de19a708f329dfcdc97bf7033f630dbdf87c474ccac3f6d3e2fe", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "ca9a43d5-c69b-4419-acf9-a743c4b5ae60": {"doc_hash": "e622869196ba3c37c755a0a607bca279f376339c2eacfef64b017ca13da34ee2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "dca40fa6-6512-4762-ad13-a124d1620665": {"doc_hash": "769be5e6dffa07005c0b2ae29c5e3a0784a58fd254b9acf4bb3bf50b0e7dd10e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "c84aa64e-14b3-49c6-b5cf-01054923401e": {"doc_hash": "fe9afb7ca69bdc9d802736be048e60794c97b90fdcb3db8eb7fca094a6e36094", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "c987d570-0b06-46f4-9d5b-9962c930be68": {"doc_hash": "c53beaafa7c42967b32e0854fc0484d099861e6a4e26d63b2951092c48d6bfe6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "dd5f8cd2-3162-4683-ad90-f573a3974d1f": {"doc_hash": "d1cbfe02030731d8ad697a4fa1e5a456464cfaccdba719a32f80cd3187d83507", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "50cd358d-c2be-4dcb-a444-a9c0e2bd10ec": {"doc_hash": "c8063bd6e40feb1cb59af75aae17e2f326e7ce50248d95a0003db1a50df72296", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "60b4552f-9495-4a77-be55-4c9610b2048c": {"doc_hash": "4e1e066399a30e9cc7b92b8575412f155408e5fc6ad4092ad4e0271c37d05411", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "83edf9cc-2c56-4e3e-a783-8a4cc9602629": {"doc_hash": "85076dfa2c7b6f3a0da57cba412a8a3f0504dedc0e25cf7e694e2dba1b757619", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "a3d1e384-1766-40ca-a943-7e3fafb8ee76": {"doc_hash": "5cbbd3708262555c402455e8f0ddd678d30cf8aba9ca8e07eb013c760df1e80b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "ebd14e5a-6aec-4cdc-a6d5-f59340b15933": {"doc_hash": "1252e865aae33375ff3113001633d894aee81ec5bf894a6948c530d365403f43", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "4237b3b6-8ee3-4caf-acf1-a5fb12003dc2": {"doc_hash": "fbae1bdd905fe0b2567c547e9ae00ffe3f57b7c7cd1e819dcdcee18c6ed9f4b5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "f8d51b4c-d422-48ec-9835-5c92c0db20fd": {"doc_hash": "038024685bf95ae90cc6cab9595097d1569cee548bdb25bc5bd6f8ac5137d1f3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "ad9507f7-2aca-40ca-ba72-114064ecbd2a": {"doc_hash": "1147dc6eefc41a08778046aa348821e99f2fcf47b4c94dab5ef48603e313233a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "c2f4e56a-c637-4a1c-8f86-dde4cad30142": {"doc_hash": "75601bf47ace535d189664b1fa92741c1c28f2fdcf0f8ee8ba9a17303d499938", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "ed8f540f-4642-434c-b8e9-4dac06d8df1e": {"doc_hash": "ce49dd6fe3648328530768a2ebf9749d1406d4cdd9d72dc18555548d829fa9e3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "26815d5a-2cdd-4aba-9fce-c1382d153986": {"doc_hash": "e0b8a4d978ce253ff4208b795d3df4aec16b17d7abc1459665281d3f0eb776b4", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "e5c95d48-ac60-47f5-a723-76937988cbaa": {"doc_hash": "8d768ab1451ce880f47c3ef1a75ecc811b91cedec199cce6a006ae43f472ae47", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "a5651519-d1d2-4289-aa68-a907b739f8b1": {"doc_hash": "49fe8610f19d426a28ba9ff12cc0b424c435999104e0e893933fbb79aeffdb70", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "8ed8b7a2-5dab-4cbc-bb07-6c279e5b7229": {"doc_hash": "efc7c2748e1f916c8db2d1b3814420e3f7f12200c20cb208c0d10c86d85d614c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "38b243ee-b354-41ee-a49b-72f4869e1cf5": {"doc_hash": "b034f0d2a57b3486d9a60be32235fc95bbb31ce1c5ee0b78f121cdb29f268edd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "9bb8374c-399e-4665-84cd-c6f38e477818": {"doc_hash": "b4d974cdb89313c6e5435729b7d2ba0e4310bf439d3cdd0ca05a62afa5f54c57", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "e9117ab4-073c-4c3f-b876-d8d2bf49bfc4": {"doc_hash": "1480e9249cd5a35efb32b8e30e7b356854f45e77c431be59f022562288e5af8a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "27ffa399-15fc-453b-8b47-1cfc7d90d073": {"doc_hash": "9754ad20e8a25bb4e3f163c7b6c1f5965a519dac4d798f2d7c28e149d7fedc38", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "8a7a68e6-722c-43a3-b2a6-f3486f5bb9c1": {"doc_hash": "86f121a23a3bd7db890ca360f66b1a68fffa3923ff8f44100ec73242b74a59c6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "f7c207e8-5bab-403c-ab59-bd4f9581a268": {"doc_hash": "88849a895d534d88f6fbdd6c579c03d4de0ffcb83c4e7575a1fbf8507e4e5af6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "3a0bdd21-90b0-4e60-926a-a08a9836e870": {"doc_hash": "cb4d9e8b5fef285b6f32ad4e8f3c715028a5e209c3bdef17d6b4ce890615fb2b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "6dcb5e8e-536e-4500-8dfc-32a0304e4445": {"doc_hash": "c4f287bbcd33f55c012519a859b178321902ed558d7192c669abd8030ba689ec", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "ec6dfe0e-ca07-4b2f-ad12-06ff8686adeb": {"doc_hash": "d482aab9514aad761eca779d0f5edca44b5b51acfd7b30520da2dc460fa75fdc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "b8da8d61-95ab-44e7-9ece-1096e1c2176a": {"doc_hash": "1e30c768f9ce13ef97f916b90ddd2e6ac3c7d2b37a6d2b97f59f6724f8f20947", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "44eae16a-3d54-4de7-9b43-b5e341e9797a": {"doc_hash": "82fbcef9a3084fbfaccfed4a94f34b5f040e7b8d02b87cf61ae557734b20aa16", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "4fd9e821-624f-4ddc-9838-b6379884f26d": {"doc_hash": "23a398190e92c71d2adfe0e963467e9bba75167ce59934f6234d9095dc76904e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "3264cdaf-7145-44f1-b2d6-db6d76037556": {"doc_hash": "d443ac773bd7d8817b700c0b8e64e5560ca9508f1fed2049209e55f5297ccae4", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "5a247cb7-fe2e-4f23-ac34-ad7fe134f255": {"doc_hash": "bac5939ea4017a6df4fb87db843c8590cd3de3f14aba427e192201e107d2a141", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "fb1ad426-cc9c-42f7-a25d-356300a24c76": {"doc_hash": "f0d517b53722b72cd34e8e4d8dc0d291754d87129904dab38904f217d3223c1e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "7595f677-646e-498f-b541-1330db2293e2": {"doc_hash": "83ed43dab59fdcbb33eddbb3b20030475340b607d521141e607fc42374e97ad7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "15b9f45f-fef0-41c7-a1f0-c3e87fde8cf8": {"doc_hash": "e1bef6a639d62edffe96ef80c5fb4ce70281119e9ea7065b71610d17421d2df8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "37d6843f-52a3-4903-a090-347780b80ca1": {"doc_hash": "b79e00578020a45afcb5bae7f12449558ab131be221d332c9497ef53ebe4b4f3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "70259326-07a8-4f9d-b3b5-6b8a038b4d94": {"doc_hash": "67c57e13395b1291dcfe9d3d646bc5c6b691077f221d8de64d570b61e3a5b32e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "eeab61f0-8040-4c5d-9e00-63191c35abbc": {"doc_hash": "df0f5b1f2ad285f2abedacb63f8d55110e92ff3dacfc4aca4c61082ecc2b0188", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "11e11d42-a0a5-4559-bc69-40a12e88d381": {"doc_hash": "06752071812abd23284ff0252f4e0987a949edc2dda9fd9dff8d8b25b89dfd52", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "26b8f05a-8164-4d6f-8157-1b5ad68dcf2e": {"doc_hash": "8536edc515e4cc43200716ff6269a29e1034adcf15fcf67f7d3d5aa19a91655e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "9066fafd-7d42-4ca2-84e0-e812f25906cc": {"doc_hash": "9caf0e04d1c96c781f5ba0d0bdd8f5f725441dcd2e378db67af9e1d4c0860ebf", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "b74c8e48-aee3-4149-8714-ba22a73f79bc": {"doc_hash": "3f08b54db6d9c67fe6349896cfbd9c90cd792f4848e3b0490f5ffa50b5bce30f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "5ff4f179-74dd-4833-a931-8a6894dfa8bf": {"doc_hash": "9c3ca2ff5e5781f98df635dbb2f104bc46e247c0abf78fe57ae5a844478e2c2f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "d2cf9ce0-82d6-4603-ad98-90e8ec39bfc5": {"doc_hash": "240f02e2528a5ad532298f050bac206b9db9af4269b717f58edad1c93fe11e93", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "a30f9db2-6708-4fcb-a526-55f612fda45e": {"doc_hash": "5532486810d7a26b250576e56e6b0f21800ddc9884e9a3ee5f50bf2f7c74167d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "d1b26f3c-da54-4f18-8805-6e51a829b7dd": {"doc_hash": "86a3e4e31dcfb5aa6c65c507f1f7c094887ccacaaf364a99f1b69687008bf548", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "627a737d-8c86-4f12-a92e-c8350b4bdc8a": {"doc_hash": "9a0c9dd8d474dacd40009256bf4a50c140a5376a2a3451577bedd9bc12aed30e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "d8c3c6bc-c8c5-4664-8ac3-f0fa2ddb285f": {"doc_hash": "c3561c21896cca389edd96f84649c6d74b41ea43bf23f95782838f6cf43728a4", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "7515ce6f-3c1c-4493-8410-2e21e71ac14a": {"doc_hash": "600b525ff4257508f2170e13fb324a3f2920d6619a392d10b04be7cf674f9bbf", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "d5c31810-4758-4042-b44e-3d21c6c3336e": {"doc_hash": "3506d6ce1a94a75c18de6038151c5fe7062986a4391ea9cfa05ff1f81d0fcf4a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "5f3cabe7-7a02-459e-adc1-fb69edb96957": {"doc_hash": "f59a73370bf74486b3c0eaa4280d1620d86ae3a52f62c00f41bdfeb59954a752", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "51b5d905-ece4-4529-92cd-1094dbc2f55a": {"doc_hash": "1e0e5893c45a5a22f7b1bbcddc5c6ba38acf812ca2d6f68ebfb944cf1d06bba5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "3d9fec61-97c6-4ac5-90a1-49185a038e62": {"doc_hash": "66dfa2a08434ff37aeef27f05ec15d70f7c3851be5d963769fdebe81a00bc0ab", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "cc7b3abf-c180-4a8d-ba3c-81d30bb8a299": {"doc_hash": "dd942f372ede4e0145d3a7fdafaf257fc506b2cb01925b8ad1f792c73d06d2aa", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "b02f5eee-462b-412a-aea0-d3b4d7ab5d07": {"doc_hash": "1de896bb3066590f3db052debe656eab00a94ffae0f7430148fafc4e5591f606", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "b4c3a51d-8bf1-4de0-ad46-83c549db4ba9": {"doc_hash": "db5b53cff1110f94412e9573f05d09378b07bf5898f6f781c30c5ddb32c2e9be", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "3c1b0741-1cac-4b04-89ea-a0ca8640b1d0": {"doc_hash": "c11167fff58edb2ad3475176817ea157ee31bccc5a6edbded9f40354b99c39cc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "2617f1c9-9be0-4bb8-abb9-111ee1b6d31d": {"doc_hash": "bd5d37d96dcd784e4a78928299ea6c2a91c76bd0087a8071ce44e60ba8484a1d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "f45badec-6d97-433d-9f1b-de4bf4dc146f": {"doc_hash": "fc5e496099f5bd8b2ac7da9afb590109d6d71abd812d3ec8c34a9d35960fe9fd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "5a51bb02-7264-4564-9e6a-a0ddac7f5fbc": {"doc_hash": "1ed8f83d85139c924999829d66285539c47b55ad112e4cdb343a9bd692a39374", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "5eae40bc-aea7-454f-9be5-ff9065e10b2a": {"doc_hash": "aa9352a9decd57b71e120869b5047cb7e5b5ad14bf0c6bd27050e28f9c269fae", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "7b18e431-1ae6-4bd9-b530-6f3149c864b3": {"doc_hash": "be551533bb0ed8fc92ebe227211fb2b29d373667ce174815d71e4811ec8bf4ae", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "f5104dde-612b-4da9-a3e2-5ae3eab0b0a0": {"doc_hash": "3de52d81fb5efcce369daa4f1c950b6fa298b590b8d56273c0e0f0417dfa90b5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "de1bba2e-8038-4056-82af-ab21c2bb3533": {"doc_hash": "d5875d7761cb8edf6998a6d96e7d4f18550f5d4aa7ebcb926cb0d245038793fa", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "c4a5a5c0-d280-44c5-8c28-017aac86330d": {"doc_hash": "7e1e61a2eaf49554412a9c3b6713f3be964999c34e7843febcf248af546dc1fb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "3d0711fd-4ece-475d-8540-0452799f2e9f": {"doc_hash": "9df04092b9322ea87b4c02c3866bc3df75ad534c8aad990319f8da19ec154b67", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "8526dd5b-3d8e-442e-b2c2-ce6e2f50482a": {"doc_hash": "4578a5e9abc3577434d5a8120ac746d02136c14c873cd6539e38797830fe10eb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "b2694119-f476-4d3f-8a21-89fff41abcb3": {"doc_hash": "b4bf3921a8d027a6b14027084d630b0add4e99ea4ff006d24039b20edb8f8763", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "f5254579-2d07-4115-ae95-88338a412a36": {"doc_hash": "c47a0b607714d67d6634da106e28d53001078f698b4f632d25b4c03b03315b76", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "e1c5d6c1-5384-4f7f-a260-9a0749afe0bc": {"doc_hash": "cc7779c0066f1be9d104f4bd5a316a39c8b9ca9da912827d5d246bd1ff313998", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "1194fa25-d58a-43c0-8848-9a8ceec7d985": {"doc_hash": "9e1dd22f94f1ce05c14932c529b03ed145f62ceb02505c6cdf33a3332de1ca34", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "19097887-91eb-4dbe-ab29-1d0fe3e46dae": {"doc_hash": "775797642e4dff508729528a64d99a7fab6bd8d4c632e243f105855f2b466b0d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "f7e05c07-469e-40b9-a258-1ca79e16000e": {"doc_hash": "b6c589e6ebcc58e28f16af66fc413bcd5861b94ed21fdf454f552287781b40a3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "499ac069-0437-4725-af5d-f45920be32df": {"doc_hash": "ec6a809d2d498981ccc509adb1bd894c96024129e6ebd834d9e8733a86c19533", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "e05b0377-7383-4dff-9094-2b7774acd686": {"doc_hash": "ce49dd6fe3648328530768a2ebf9749d1406d4cdd9d72dc18555548d829fa9e3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "2f3c33e7-0c47-429b-9568-262f51a01c4f": {"doc_hash": "bcc38d87260762fc51b1f445b62bb06b231617fcf75450d5737de82db8a14ace", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "31a66546-a080-4035-93f0-68f7fe5d625a": {"doc_hash": "40589bc407b2308a539af460024ee5fc3d6fd93e8028b31704dfde02d4319bdc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "d7fef8d0-5273-4882-a0e9-1ef09fcce340": {"doc_hash": "a27a1fbc200a542f600e8547aa9d61d3128cd1b9cdce399f077f0d128a0a83e9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "d62b6d8c-c24b-4fc0-8007-9bed085ff6c8": {"doc_hash": "673c6b45d3eda42842df385e9734ff51619b619efcdd959744a0c7b54e51bfcb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "e60ef1b1-dc9f-42ec-a0f2-4250c3800f17": {"doc_hash": "01b43cce15c01f76baaba2359ebb3cc72442cd715b47fd1c238ae3b35511011d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "4b5123d1-1b2a-4c30-aad7-d7903ad76254": {"doc_hash": "a35c15d4dfa83e88e944013e82bdec179cb0a6e9d8d6fd80ef24373bb53317b1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "16a852c8-932a-4fcf-9060-7b8178f45567": {"doc_hash": "017b0240e97ad90c10838e1f8abc7337120a16fe2761c3e7b0b9ef4ae2b71f67", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "6695cee5-9101-4c5d-9f44-600f88f1a617": {"doc_hash": "8d1b0fb36e2397d65f00bfba7657ba585437b93f5d50f6144e2e8c2a5be9c913", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "cd3d8818-768b-45c0-9802-ded4460d491a": {"doc_hash": "904c1847b285d3d7a0ade58f41c897cfaf6b54235e0312cb93219056c224d5d7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "5c7d91c3-d464-4a7e-a0ca-c16305069142": {"doc_hash": "e9b811e92425e9c6314cf81dcfa0d34f117ec277bfd048f9c67910a438f6a157", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "1c8d574b-2f2d-4fc8-b83f-1456a9739b5d": {"doc_hash": "42a1c24b1e33e39058c06579ca90da61ae8f5ec4abfcc81492434ae141648877", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "091dd910-213b-41e4-9c53-9dd6ff0bb16f": {"doc_hash": "120fa408d485f9d482d3050619f3c14ee8d5bfcbc6ca6df0bb7cde86ec2d1e33", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "08ee27fc-2100-4d2f-8070-e497f1a44c9c": {"doc_hash": "ce49dd6fe3648328530768a2ebf9749d1406d4cdd9d72dc18555548d829fa9e3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "ad0cfb61-29a4-4154-8e66-5a1b6feeb3e5": {"doc_hash": "5071b3302b46e217b301284180b4c4dc0821cd9bed0ea6a8e30495a78022f422", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "4e666599-af44-4415-8492-d39946c5b04b": {"doc_hash": "0802debfec640dd4dcfb0a8a61115bd6065dbf01e2fc9e722a89541da6884790", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "32a52e5c-1165-42fe-b110-41d1114fde62": {"doc_hash": "5ded812ab3dffe4e0603f83e42532e183335ee119e49e1f5b0ba6841c29fe367", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "2aa4a14d-3ee6-4e45-aa3f-f6adebc04a09": {"doc_hash": "bfc571c3192655e1704c13bb1f7fdc33746bc9fd250eb26b4497c43888cb9329", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "8bf2ba76-3186-4756-9394-bb8b65249acc": {"doc_hash": "4ab76d455174644ee0b4d9b4a8a3da296c151c84e7bf62c893397a3486fb08d2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "ffb775fe-762e-48c1-a3f6-b8f628bcfade": {"doc_hash": "bfd2c63074f0e28635cda095208fce9dfeb515be7488e49916eb0c44d2b016a5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "2a8ecd39-d4bc-4bd7-9b42-b78260d5efad": {"doc_hash": "267971175d858c889b945b37d7af651464b8ad4e6ecd148fda7ef3e8af9ee6fc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "623d66f6-5aec-438a-8453-1f23249323e9": {"doc_hash": "773c93f837bb67b4cf88b09d45360137418381ed06102d7a6569409770da8142", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "63cf1a5a-40f9-49a7-83ea-c5e34e109fbe": {"doc_hash": "57d136e8fb9939f696ac517c978771a60560bb549649d1b5c0866c15f8f5698b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "f0ea5197-4f30-4338-b42f-60e76036bcf4": {"doc_hash": "ee0992833120ea66dacd5545cd7535e6067a375f8abcfdf56b6fb1c7b065c2a5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "eefdc475-5bd6-478f-a30b-37e39b61f8f2": {"doc_hash": "653adbe39e18ea0c47451e3f480bd2b254efe7bacf9bff7391fd14be1e74bfe7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "c42c74a7-e693-4486-a231-1d1d2a35c1f1": {"doc_hash": "97ebe7b9628a184bf6e272f7ddf6999a48438c54ba4deab64d2c2160d8253dbd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "d440ec05-fa83-4ff2-b9a3-18b2df8b026d": {"doc_hash": "6f8c0dfacf47ed2cc302e3a5a5ddbb4d84225909a6cb813a343e7dfb69118607", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "8b5f4007-0919-4914-83ba-f9b4037275eb": {"doc_hash": "dd190799e8ea3a361beb5ebec78d36a6cc6dd0c6ed6c22dd4aa2641d2c5edfca", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "0f3910e6-60a5-4249-8af3-12fa10bb4fbd": {"doc_hash": "c0242518bb1ec943cb5f4d791f7213f8f26c323886192423f5ce9c9c31e96275", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "cd5c6df7-f609-4148-9f9e-990e52ae6611": {"doc_hash": "a2bc404fa81d9bd4e007ad8638c56f1abf72df7420b985bd972f61f9e6430999", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText"}, "10.36227/techrxiv.170629582.25144576/v1": {"doc_hash": "91466ff2cd55a5b348e43c06f91f6efcd60b6e608b92547f5ce61f9d62676daa"}, "ce6beca6-91b1-4a1e-bb9e-5e2aef65f7bf": {"doc_hash": "9cfb8ee4e42db5d961863f1923da7550420d6b2b600f811390027dc38c303f81", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "8a34a36d-0ff6-43b9-9b6f-8fdc4497a8dc": {"doc_hash": "c4fd1c61affc3a49da8b9fcd568f1fed9aa1397578f90697f7e6fb2cf3e6fccf", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "1e2ebfad-89b9-4d83-b74b-d5746082b6a2": {"doc_hash": "f5c44d5083537656ff20f9c9616844766ca8e19a1c1fc7b31d38e21a0fff522c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "b3e3932a-b0bb-4749-99ea-edf00ee93a5f": {"doc_hash": "5b5066ab9c824399ad3dfb368444da0328b7be607d0ee9cdff3a940c6d5e05aa", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "33178490-e400-4113-bbf7-7f978ef19fe2": {"doc_hash": "cc22f93536b9259f725386796ae2d65a7a083dbc6accf734743501ba14a06a56", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "29978223-f6ac-44b6-b30c-c94c15f525e0": {"doc_hash": "c9546906787c7bc0080356c389bdfe8b2e95af19ffb7f8e2ba345fdf32f8a7b7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "58c0c755-97ee-4e50-b41f-8395cf490ebd": {"doc_hash": "ef78120a9f7d8bfd0811f0359b789140a3fcea318d1a842fce2e7f69e803c321", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "c20a560a-e946-4202-bc5d-d7eafc0748e5": {"doc_hash": "10e4521d9910b3584d061e413a15a1ce5a0d9f51a4678e17994e0f549f7ee3bb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "4db71d11-1e03-4ced-9a5a-ecdbf4b8d893": {"doc_hash": "d6367e533604670d32eef6d0038070330125ff5237cb41e6a733aa62ae835ea5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "c256a51b-751d-4338-a3cf-f82f72e51766": {"doc_hash": "9b7beeeb1df49ebf9cbf44db70e963461e64200595131c3916cc3ffbda904067", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "3877773d-f394-4ada-bd5f-612912b7546e": {"doc_hash": "f5186d53a4a41351a084b2f4013a04f7e975d98ed4df3181da616cb013872258", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "dde9786d-e7bf-4fc1-baaa-639a4161b487": {"doc_hash": "7c666aac01623205c7ac2dd2480993fb918b37a6c0643aa4eaff6df1a5339a5b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "9d1f07fb-d417-425b-bacd-2e2091174c99": {"doc_hash": "be8d29636c87b0eedcf0fa9410a66f8281a6c5e49c322147445c4d3a35493af6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "a24779c4-6c4b-4e32-930d-c687149ca7f6": {"doc_hash": "5839e71cb6ad411f99a64b17676384e3f0d4a4cf914544c7500957d61b504efe", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "aa55ac90-ed41-41d5-aceb-1afdaa0a4b8a": {"doc_hash": "989541a6a59964a2bd4e5e0994848e2fe0af2a11cd50d1c66f2bd2018b7cbe26", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "02119eb9-8504-4083-ba41-22f37e599f29": {"doc_hash": "f2e5a17d9f041db2c725356bda0065cee077eed62934c73ed399b372d1be333a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "f7cfcfb5-ed7f-4c5f-9d78-ccfa5bed2ccc": {"doc_hash": "6f082c8c29b57646041f3886e1609667447e4ea55d615be08cdea11c1ac9264c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "9318ad85-4d09-4acc-8bb5-04e4182061b8": {"doc_hash": "414403053c98469a1e3496e911215cf975da05df9c868679fd050a7f3a369b31", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "78ea12e7-82c1-4cb3-8726-745b61e8741e": {"doc_hash": "78009af0922d05b37a8a69086a01d8d0ba7e8926faa2becbc30ea332a748adb5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "5859417b-2dd1-4b3c-8fa7-7cd5cd3ae3e6": {"doc_hash": "a935a745356acfc7fc87e51bb35eb1a50053066c43d4eed8e766c4514b997de2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "2996cee2-bdba-4ffc-b43f-b558d59696e4": {"doc_hash": "8ce69d37235f6833f54c00a7d96033878b57e43eb3c2fb2449dc0afb3649a266", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "e8714f23-6cd1-4440-b8cd-7fad4cd6d068": {"doc_hash": "6972f1caafa4629064423c1abb04a29ae674fdfd65f1e163a426fde28d2c8bde", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "baef1291-a654-4c00-8f4c-63c65a6d71ab": {"doc_hash": "4d66bc2bf57ebb45d28dc3401006dbb9ac3729c24493b96574f26306dbf5d2f7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "78a77e59-3b3c-4b1b-8ac3-6758df3750e1": {"doc_hash": "82501bec8b97e37558e8377436779400a0961efd55d9988e19388307a69b9726", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "7f2f15cc-42dc-4bea-9899-0221f9286888": {"doc_hash": "e7a7e68133cd6062743f24e3f8e093983c4b01ad4f7fd08d88e829017e58d717", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "a864e6a5-4b5a-46fa-98ee-4e060bead9a7": {"doc_hash": "25be9c6352f5f66709fdfecfb13f7cc5f1c6acafc1a3d80ab0b2a217ef487b80", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "1e5f7931-ee12-496a-a66c-02b52df3449c": {"doc_hash": "0ed293e9c677290b2129b00c27f59686fae7d4de78e577119dda286755623b1d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "216abd8a-aff8-4768-b247-11f3f384b8b7": {"doc_hash": "14b37b553adbe92c4cb77dde9bada4605a81743e71b022478912a46f7e0843ec", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "5d8da926-8381-4901-b1d2-9719080a6dc9": {"doc_hash": "566ecf2455b2a533461ffcfebe64bb766ccb2d18d0a565b4429d99f16b1fe318", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "1c54d84a-6815-4859-8152-c971e049b6e2": {"doc_hash": "09a89fedce9eb5b145058965c4e088dc29cff4b8418759f0743cb836c33d09a8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "7fbc1f57-49fe-4f17-90db-b249ba235f4a": {"doc_hash": "6a492ec0f9297a1441e85d2647ae137537b64c03c9feae7140e922faa476cf75", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "56631314-cff8-4964-9c57-d6ab1019aa40": {"doc_hash": "6c6f6331c4b3811d3d02d874f1eb39687cd84e089865a8761c16e9adb35a0eee", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "f4f5f59c-eab0-4eff-9506-716203d706d4": {"doc_hash": "9da8d8b3d0a67971b671d6c630d74433ffeb5958e5f39d41500232d831364b12", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "ded2ef5f-9f03-4ddf-adf9-3c0cdb3065d1": {"doc_hash": "c1d93d65de2e37cabd28c4ce29b22dd5f6a5fcd81855e1d80696372c9c535d92", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "09de1bb1-ddc6-4c63-8556-bc80420029d0": {"doc_hash": "5a66b13abc74b474b380c6875d8cb7718be6fe5d567c53cd7c52a9db59a3dc74", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "71da9476-65fd-44b0-bc1a-253d08c47b3a": {"doc_hash": "6e62fd3f5954ba2ffc96f7df65c69608f2f1f10ae02c461fb0553cc5f7b53156", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "854008a0-4e99-4b8e-b495-b88b9dc090a1": {"doc_hash": "79dc098c36faef36813c230735e384e3a5506edf7ed695bef4c88e07d9edec20", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "816881c4-a614-43c0-8d14-984014afd457": {"doc_hash": "c17b7baeac9273d14fba7cceb8e03a369dd3433aa0cf3ad5489929501586b5a3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "ce83d2a3-2280-4a71-93a1-e25407d1b548": {"doc_hash": "230f4a4c4da25caa0f32cd9f3059fed7414aaf30dd2ca291b9306fc7145cce4a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "dc9faad3-0c6a-48fb-be64-4e8b32de0891": {"doc_hash": "142545290cb40299ad0a4f41098946d4b0022d2037a6c1d561adbd8884204ddc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "54417010-3505-47e3-be62-a9740807383b": {"doc_hash": "2105a164f1bffae91bfd64179d332bc7cd2e24354ee2ba4fefc7996ae39f41bb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "670d4244-7cc5-4a33-95ab-5e03a994198c": {"doc_hash": "a1e74d7b08cd08e5a3e7f9e2e1b83e0ce720109826c27caecf099ddaabe580b7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "7409d33a-3652-4002-8c97-87fbdc4a7bfb": {"doc_hash": "36ddf2194b213cbd0e576ec1686883ecd04c47cce368e9685084d73bee3a9833", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "ce6c6eb7-e015-4572-94fe-81884a1c0a6b": {"doc_hash": "8a053472ce83a76e8469de36b14ed05aa2f9c2e7f8b6b9ab992c109ec64d1b8f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "471fb5ad-4434-49db-a405-c75619cbc511": {"doc_hash": "1628bd481328cb1cf2a8f6793cc133e268bfcf38628c39fba4ff15fd862aa66d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "537b0ab5-144d-4d5f-b014-b48212fc1c44": {"doc_hash": "f03dc5288e5d35ddd1a2d761c9ef289e349e45c6eeeb08f608299c618582bed0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "4b4bdbc7-f17b-4754-88a2-2704af18b3db": {"doc_hash": "2fb0da010a9a91ffb547b6bc1240514ed30318af5564dd105dac29434de159d0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "d7ab454b-d73e-4558-a4b3-3ab76b473001": {"doc_hash": "034a576eaf4340841e80c87d72d10adfecf8c84c971d7c16eaae886806c4b4b1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "557e8d1e-f6bf-4c69-8f84-8cf0e0762a34": {"doc_hash": "961746d6e951d93cd2dd872884cf0c89719e4c034998736889412046ce73b531", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "f934dfee-396b-4f19-a869-1f5c3ed52fad": {"doc_hash": "8e39460b0b41249f7134ebbdcf3f841bef92760642d9c10a7d011f835112d751", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "01253bd9-b089-49f8-a571-9b1a9b8ee380": {"doc_hash": "9318e5386f38f9e087999f34a0e2c32ad519fa7a49f25609fa7f96bfff90ef7f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "4c58947c-bfd2-4a53-9c22-0df6255f21e5": {"doc_hash": "e1fbcdf6a2b71d8d983e54e2b012453271bffd4bd2d6ccaea72773217c9de179", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "ec38f990-1634-4209-a465-676674df7c22": {"doc_hash": "29dd484dfdab2b101368a022dcea0ef74860b59bd00286d342c73da70df93853", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "d8c18495-ad01-4e85-81e4-d93ef23795b2": {"doc_hash": "1b2f6c3da80f32df4726eb6c5c73eb5bf1950c1bd25291bfad74636c9c74e321", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "17a8dc1d-2514-4bd0-bb6e-6ea070567a47": {"doc_hash": "e03cd79a214f59e7fd80f56410f924210a0e54993d40f3ce5f872b07f87364ea", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "5c796ebd-8863-4ae2-a86d-409a8ec7f21f": {"doc_hash": "4d1db2b8dfea536679675a3113d6f45f2b50ff34a3a4a10c00dcd0aae5a3e88e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "5eb85767-dbf4-48fd-a118-9b08e1bc849a": {"doc_hash": "dd00fe4958c3331f413eb440f64ccaa2d94a786fa483ee32f53a0942042ed37b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "3fb7aa01-72d7-4552-abb3-a30e88ba1c8a": {"doc_hash": "7986cf66c8888530f24a9e5516f2048cc884d28d27c9fe4f7f81103df68a832d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "00794754-fc63-45d5-9d6d-73a1d041162c": {"doc_hash": "ffd46ed9a8918d555f9d339d05248833994354e59ac96e80bd70ba8aef52e031", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "413c7c40-c43d-44ea-828d-1348a3b97ea4": {"doc_hash": "73b14af14cf0d2b891f086a414b832f6004516e8f20cda0e6036d3e35afc6cd1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "2c184e13-a2e1-4b6d-80a0-f96d5e819a63": {"doc_hash": "e69a71fc84540f253031c8cb74b97b11b4bcd895200683e1647961577eed8783", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "49e9d009-c89d-496a-beb5-dc968978421f": {"doc_hash": "78f61b86b6c192ed84c911a6b703295baef327adb62935548a27b6425569fd2b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "500f8d9b-1a0e-4bea-8443-34eb023678bb": {"doc_hash": "ce1db577fd923eced5101d20244fd300eb3fc9d03e162f2e95f6e79f76d7315e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "761432ee-0a5c-4331-96e3-98fccc17f028": {"doc_hash": "494164c48b1e8435c5b1b1c22673b5746fe853658b1d8387d242557f85057daa", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "3efb12e8-48aa-42b1-b8a0-7c17b031051b": {"doc_hash": "a14ee01a06346e9e160b131debb3719126cdf2912124148a62d03a487dbab871", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "cce391b7-00ab-4456-9032-fc4bd361911c": {"doc_hash": "96679602154faebfc21bfe884705a44cc7044ce16873851ca7e7b061b91cf76c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "12e75cd0-66b2-4c7e-b5fa-90ed1949b5da": {"doc_hash": "66f84738bade6178af173918a93cab80206cf08531f096c68edce4a300cbe4ee", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "18cd6ee6-e7c7-4314-8c31-f39a431205db": {"doc_hash": "b411d5f1cd993e1afe2f0068f027fd1985a7cdf0d46fa0e7c9cc5c089dfdbcef", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "dabff7c6-6f21-43ca-b1a1-dcc8b8c4e843": {"doc_hash": "bfac2b41af22aa9aa378c6a890318993ba93cd0c7bf5d147ff3bd2aaa52f92e4", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "7dcfcdbe-c07c-4039-acb1-d192252a1b50": {"doc_hash": "dad08b565f637d8a0f2facf868a5cb54f3763433ebcbe983049e349215727282", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "fb7f4a98-93be-46a4-b4ee-fe82bb1f2349": {"doc_hash": "f64d10afa3b84fda35772320b6c9d07003b982e58c9cbf915863964a18873882", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "a49871ef-1677-4bde-97d9-abc11fa8d738": {"doc_hash": "48093acb44135d55b879099dc59cca33627dc8b1f078fab00f69e6b1f9e112de", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "971a6b47-4916-45c6-9d97-2f2a1eb16643": {"doc_hash": "66d88559f752002447bfe7d3f6720489611cf40570dc80dea7848ba8b46970d7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "72ce1635-7d22-40d0-aee9-7f73daecc27f": {"doc_hash": "ea3f7b90bfebc41f8bbda2d4b4c6b263e06a7b681cfca10c5696706249d42320", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "001eb75c-bc35-4e70-9854-219ce8474aae": {"doc_hash": "11342dd690fbc16bfbb905db4457b0b157655d163b4dfde099a743b9a79fece5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "245911d7-bde2-4736-954a-5e097feae088": {"doc_hash": "7427699af82e940913fb7604dadc2cad7a23555d58ade3fcdf1cee58cd25d0cf", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "df27660a-ffba-4c72-adad-509aa46c096a": {"doc_hash": "432009e2d83926f569094b68fc988fc9582faf2c5cb6350c6de822044c89e3c2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "ec8798bc-3283-40e5-b2d1-9ddee2b41f2a": {"doc_hash": "840e1e95ac8d488f566599f5fdd8f58c1180b85d0b8ad8cd9994fb22241c5743", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "85c869f0-a5d1-4785-a5b9-0cec9c18b045": {"doc_hash": "e8a6926d68ca0fc108236ece54490e55926dc1d24e6f82a9b30c51e330b3282f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "03b403a4-e9bf-4301-8ef6-ef0ab9e5b204": {"doc_hash": "66e4d9f5ee2af667b5e33e43a5f0ddb1cacc7618c8ea0426cb3de176b3a23153", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "6a4f4cdb-c95c-4498-ab84-aacb1c5c9b4d": {"doc_hash": "258309c8c5c4eacc85847b9cc77d32dbcb91169711ba54187f97596b5a262607", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "778be731-0acb-41b8-a061-bb1e22ddca5e": {"doc_hash": "cd0233aef1790261090058fb55c9c77cfafe3a97c1dca27cddb1b394daf4506d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "18ac7c6d-a6af-4c41-bfc0-bf41eeed3cd8": {"doc_hash": "f4c9cb975988ce5f5f127e1e806e8e749430e10b2ba0265c666b3ef313dc97ce", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "12d254d7-14aa-4063-8d3a-a6b4155d3750": {"doc_hash": "3e48e8823babb64493b94f26890f5074a7c3ceb55990ea7c3c8d157e4f93bb6c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "4a172d1b-4b1b-492d-b96d-57e7810f3a6e": {"doc_hash": "0ef52e788430c1dff7ef7041c8d1545d47d00337882d4db71a6b42ccf49a6634", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "85b7c7f5-dff1-4eea-a6b4-7b5ad7493abc": {"doc_hash": "abb5f6e093177d99360a5d85b4713d5c9122a9d719864335140452f8f1db92dd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "c9929cc7-ed3d-421d-bf6b-0cfcfaa3041a": {"doc_hash": "93c8a87230b3d3c7058418a8a41ee0ee1bcb11a119755072e7ea8fb3c744135c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "37427d17-0b90-47e1-96db-dac275939524": {"doc_hash": "04837c0f687de1dd26b7b63977f332a8f0d7763b0f6fa0d0282c05d131ab9e74", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "1379cd16-9541-4653-93bb-b076d7d15e11": {"doc_hash": "f22e8e89026246128e1199216b2faab2316438f2920682fda2e3013da1c62f82", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "9c935663-660d-4652-b0d5-49ad7c2ab4ac": {"doc_hash": "6cd62c089b9b00e6a339ac7e332c39fd8a6c71d7334b3bd84154822df67f02d7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "9db3837b-faf5-46fc-b985-cdb443f76f25": {"doc_hash": "d65146e642aa82e8e59e69c5ec7f22bb8e1a69bff0acab896f48cd5f84a9dfbd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "0f4aa01c-f415-4fda-a8c9-bb332a4b6c1d": {"doc_hash": "b970a9c9565d2d42fee04a9d073255656b3a07415cff7fc18a0ceb106d32aad5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "5f154dde-95ef-4afe-b861-10abfd744e38": {"doc_hash": "5dc40e4ee87196d0eb0ac01df4eb0b259e524cb62db64f978ec868d4e64d209d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "327e205d-63cc-452f-9378-810c133eeabd": {"doc_hash": "15454de1c3e6841435ee43d5462445730743a604a625feff94b460711e824dde", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "1701c0e8-06f6-4410-9cb4-ec66139cfacf": {"doc_hash": "1b5ca345be3cef2fd88dec721a6d5d4e4d88db9d382a79938e879efe8c2c59b1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "8e333101-0b0a-418d-a9f2-ee9546ef71af": {"doc_hash": "b907c00bffe25fec82deb1daec636cd0acff66789685caaa1c1bf961302c9a08", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "95f9778e-5e99-404d-be88-5fc8e59e695d": {"doc_hash": "923c3f4f956b4ab0a1fbdd657567767f312687fa70c083f8e163becb2dc432a0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "277d09d8-91ee-47d8-af25-8d2f3c892b86": {"doc_hash": "b31484dd83aceef51d30a4973c3517c7c3700321db7ad808dddb799845478213", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "77ddb39f-f747-4ca6-98c2-7d1e8c2f4853": {"doc_hash": "5985bfee816ab954ef1c7ad93958f25e9a2f93b04987599e02aebf04d9258969", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "fa1f3b8b-9337-4e1b-99e2-de02e4d2e126": {"doc_hash": "c1b9d065ba5253b13884275becae0c6d4db793632d976358c9e79c5c495e0ec5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "4e2b87b3-85c1-40e8-a2d8-e69968573b61": {"doc_hash": "d402d4d61cb57bf823a033554e424722b879388b97113bcca63a815563b46eb0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "5728b055-b0f6-4f84-a469-d959e5aca37e": {"doc_hash": "b245d12e012f06cdaa6ec8a2481fd88ea887a078271d2452e36e9a3b9226052c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "6abbc3fa-b2f1-4845-a8e6-f04b90d662d8": {"doc_hash": "89a038b7302017804888c0bb7c79a53fec4c8dff9660f85a2e5e42c4813b5b04", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "ba128f38-243f-4a38-9819-b78ffffb84a6": {"doc_hash": "49c6e4e3faeabe5ffb2d8977653ab50326193e37a2bc1a9a6901c3fcd47cf2f4", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "991221b8-a930-4f83-9aa1-9fa1ed038555": {"doc_hash": "d4659be2f3d2362d71d136674205d741bc5fbc66faa29f11f0ea15c7033ed5ef", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "30030db8-9b2a-4ec7-9083-c3c773c89e2a": {"doc_hash": "1b9ccf38abd0e2fe89385a69898cd2b7eff381d120916d0d7b767e0042919f67", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "d9ce3809-fe35-47af-b1f9-6e6b1e45b34c": {"doc_hash": "714a4fb4744554ff84cbbbc4216e774c37a980ef2a9c39e25d883cc4a5864f7e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "7903e73b-3135-4461-8842-c7bcab8d5fd0": {"doc_hash": "59c3cd9daa2e0702204fbefd14a6bfbad53f64337deacd6bde1eaeab7dec29f2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "58c10eff-f1ca-4aae-a78d-2b3492de8699": {"doc_hash": "c74a4903a0aae4221314114f7521d28492e0081bafc4ae172d8fbc848481dfb9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "4bee446f-5cc1-4f67-95f7-d866d7cbd711": {"doc_hash": "d8fa837efc68683d3f354528fa93009caf584cc5fba4ad0910d337d5e22b471b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "f56c20b0-5491-4d65-9d23-3fc8eec4afd6": {"doc_hash": "514a1f6d7c5110a53ef74ba8b27586b8051036bab140e217aa0616652bfda054", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "e046c9ff-e1a8-4e85-a50f-d0607036e8ca": {"doc_hash": "587f92a13c1534dfd565ccc0af853194ff6afc3216eac7d56991a3f423b032e8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "8f29e913-0b92-4c8a-83dc-2f98d4f314bc": {"doc_hash": "e30ccac250dc7709b3654d9f89cee320453db15559a53cc3ff168ab9c64f619a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "282f896e-0556-4794-9cfb-6d2e0e8a601e": {"doc_hash": "27e823032d0f4f30c67d51a17569b5b42b169d3c47d64f09b645f5b854f7979d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "91e0412f-bfd0-44f4-887d-bdd9907d1ca4": {"doc_hash": "9dfcaa33bea30c0ec051c68c0e576d821e30b4d0ef1d3a2937bf015dc5a94e6d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "1d506d3b-eec9-4854-b9df-c579b2713bea": {"doc_hash": "2cd21a38d71f0b881e81569d3e884bf22a2bcedbec6a060495a9abd96fe93b47", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "b6ad5f65-0ecf-431a-b370-654f9037d95f": {"doc_hash": "f031e11a9ef091059eacaffb916ecf9e6566bc0144b8bfa32ef2a4e05614f85e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "35c6cbef-2e0e-47d8-b81e-a4dcea1502be": {"doc_hash": "8e520a0277a656666fe39979918439a815f3d563ac9a3a25e5562415f6292d9a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "bd6c2fe7-94f1-4704-8d0e-0bef046b3a5b": {"doc_hash": "1fe91d4693e83eb589ce6eeb28e2351df697826ca39da17aaff595fd2e0fddfc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "496676d1-ff3c-4d88-b62e-01a8896b1869": {"doc_hash": "1fe7a1b9e8aaf840f89780dcd59902161f570b103faf2e398854bd8e228149c5", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "05aa52b2-6751-4540-84e0-4a91056ececc": {"doc_hash": "64255e60fdcc81f7d17e7516c6ef856734bd6e06751032719851f256ed82a3aa", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "56fed4cc-c350-42c6-a812-e759579f7677": {"doc_hash": "0ee5c6e3424277d628b3768f4f0d3aa0335a1a24c5134fc6a3ebcb5a57bc4d77", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "ae04eaaf-33bd-419f-8f58-66fce7329ae7": {"doc_hash": "1f57c471355876372b9aa0d67e263b585cda78e111f9deea1885d6d6faf7cf78", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "2afb8b8f-d547-4cb8-b04b-7857ff1a6606": {"doc_hash": "dd578205846411e0ab46f18768b4990bf866f7594e6521fe2976f90d3a9e6d74", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "b4b359ad-eaca-4ba2-8a31-d315725895cc": {"doc_hash": "eeae3a529626030eb8ff9ffe8b3d921d9ea05bd57c29fde31449b307e23ce180", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "0b1bb255-2fbf-4bfc-b2af-c1b271e160e1": {"doc_hash": "9fbb288b5401ca96163265f3e81fa322b3aef9fd7a2eb9c845702335448045f0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "97c1cd5d-a391-443b-bbdf-c53a45dd536f": {"doc_hash": "3ae3f1f7895be89e637f01f7f345064c461420ea61c6cb3f6326efd39c177650", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "7388e734-dd8d-4459-b754-c773ec082e05": {"doc_hash": "41257749e4a5f0a43718e6530eca8975c9775ee3ea01a254323a365bbb47cda2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "da6be919-c62f-4dc3-ba9c-8e893701e92e": {"doc_hash": "5c1259b6226854fe06dc967fc846e78421cffbd9cf3bb7be7af20358dcaab070", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "f81466c6-0f61-488d-aba6-cd11ab2305a4": {"doc_hash": "39319925cb868ea261fc436b3c94425d3f3a884ccc5671306d0faeb9ede2a298", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "ff68e3a0-82be-4b72-b400-a02777a30c35": {"doc_hash": "0e74b817e68ed037203475a17287439ae3374f273d0e6dfd490255e8a8fb9b45", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "5e8073cb-01ee-459f-a27d-9ee328d6a9ef": {"doc_hash": "b4c6938fc3768b1ab0f4b1a67ce3728544b4726ac8118643638bac8c3cc9904a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "401d21a1-7f10-44bc-84a5-f5712336967d": {"doc_hash": "356de0bc689462dc24948765ef32848836198f0134042e2c310100fe7f81dcf1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "09afc8f6-2bd6-4caf-b6e3-5a01aa090e68": {"doc_hash": "b024adec424a186edbfe82d24d2d751bde5adbfed7c2557dd55c8759bd716865", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "2ae5b474-1b0c-41cf-b508-60338d005044": {"doc_hash": "445f607786cf73b9a691aa93b2a89a2f7fe9d538bc907ac6d3795b6d0463e22c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "e729d654-a5df-4c8a-a1d6-9e8e9623e471": {"doc_hash": "d701cad7ad5fb69e2a73739b99d57a24d671146c24d7792c733155fb9d3417d0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "64d5ecf7-8e50-42a0-b969-a8181296a2e7": {"doc_hash": "5cab10cccf883e66f2fc85889894b591512ca5aae24f17dbff2882b154db4fc3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "fc16d4f4-8226-4975-bc35-765dfc85b28a": {"doc_hash": "0f68d2a1d6bfb4715e13fc5c9ad8679d0f9891c5aee621bf0b4e1072fc315e54", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "c7a4c3e0-3d48-439e-bf42-be805a6538dd": {"doc_hash": "540c3bdfb2e847936e3ec554febd1e44e4c84bc0bcb4958e70b339bb3929b92c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "fa10dd7d-ce7a-4186-b844-8085b240b5a1": {"doc_hash": "2e31355950d062d30bc3ca39adced582cb123c06f7e1317fdcd0af15715f8c4c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "631a7a0d-247b-44f5-a0b7-1454d7f709b4": {"doc_hash": "7470b601ebe05b1c6eb0f5adfefab6cb46b9908cc895ac619983a7bb19ed7f7b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "e09d2c22-beaa-4ebd-919a-9b25c00c1bdc": {"doc_hash": "31f4bd0129ba515fa47c0eed15d8a219c22c0c876a464d5b66abb7f17cb43142", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "f604e1da-31a2-45d9-86fc-da0c6fe0b5b0": {"doc_hash": "b1202bf476bcc69f1f195e323f3cf168a935d200c745c1d08f4341d3960cbd48", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "1e3932f7-cc32-4a84-8a68-86445cd9f294": {"doc_hash": "855e93de3e9dddb5d998d45fb82018be2a5bd47607888aef8057ea18da28b451", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "6dcc80f4-ead8-4f1a-8f76-c4dfb16146f4": {"doc_hash": "83ac9a56eaff177da2a78c7ef902e8943d23061af019657a18e7b68acbc946a1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "fd55f36d-9909-46da-8ff2-820cbfdb84c2": {"doc_hash": "3d6df4d340d67f05b3c90640cd3a88ad0ed6266200d22df5947bdc7ad59e4fd0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "64efd21b-333a-4b1c-9899-9cec164d31ea": {"doc_hash": "e08f4e40505604cb1b07420a2febd8eb2487b597293a77584fd08fc0eb7a7861", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "d55ca5ed-7929-4410-a9e4-853529e82ef3": {"doc_hash": "0aeb069eb0e5ab658995dad4c121abf97d3b75c37cd0fb8d0c3c3e76ad99b511", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText"}, "10.1145/3620666.3651386": {"doc_hash": "d840471b6ba6c597356121a28ccc5484dc8c2d1346774f1402b7031d29d3d3d1"}, "d3ae19d3-aa0b-417d-bfb3-f31df984df31": {"doc_hash": "cd4a0915d61b416e8cc851a23bd0152f8a3f539719990adda01c93518ad46a16", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "5a7e07c2-6dd4-43e2-a6c0-22becd234523": {"doc_hash": "efbfa36634b4b0546bcef562b1b171c1cdd441a7e9351d27d4f0ee883d164101", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "f5dc9c18-9720-42d8-b9f6-38d547f7254a": {"doc_hash": "298555abc930fca533defeaa1b751768fd4e52ae22d09238fb6f2fc24033f5ee", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "b6e87dcb-e811-4c4e-98f2-2a384c8b31b7": {"doc_hash": "b4a3d6089a17acdd9ffa4706b82ee876a44048b766d68f839f89e37cf9b4e3c0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "728fd86c-0df5-482c-b8d4-730334ea7f93": {"doc_hash": "1c2d7b9391ce33663488830d58bff3a64dc922d786139cfde12f8bc47c72ac01", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "1bdfae30-83fc-4d76-9bfa-2adc5edb04d4": {"doc_hash": "dbe64e03f80abd6ef29330689842e59e750913bbe3d56973e6a8ecdc175360fb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "8c6d7c3a-bf16-4804-9ea9-3822049f0373": {"doc_hash": "6662b7d79067bd22cd5d97e318ff33d48079d26d86bc719db28776535cd73333", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "a22a2ca3-77b8-4a21-8f6e-a53fec7a0ad3": {"doc_hash": "d414cb0c552a98677de97b8bee688c88ea72d9679fa8fdb53a7b4f7e2e322a94", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "74b1c6bb-3764-4abf-b9cb-1a2a9f88ff1f": {"doc_hash": "0c7d55067f9eef06a48a045d4ac1ddee26c84c34fd7d31aff9e9df291cde8565", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "8f4d0025-6967-4729-aa31-5b5d317c521f": {"doc_hash": "686b2411b83ddfaebea5934ba1b6d5bbfeb1517bf2f4bb073e031a77a652775e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "cc9cb447-6d78-4d59-9daa-795b6236467c": {"doc_hash": "c50ce209f95537455eda1c11623dc851f1a6b42b0985edc1cde71cee2665af5a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "f01f355a-9948-4a16-9511-043226e6f4c7": {"doc_hash": "54e3c73159a0a33cf8b4eaf88a978f1c57edcf60235a365d9b59fca687d2f0eb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "12e51f59-5338-4bde-b6d9-ac58b4a8efe2": {"doc_hash": "2588e4ea8cadebf47d61f3f15b6188800660de0926ee8e2ae7ddc98d22a20d5d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "2487ff97-7439-45ec-8ea0-b72b4aa37da6": {"doc_hash": "dc0cdf8f86a1563ec0e954d8ce8a79e6c55c0580f8eed22d668475decc7c7c56", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "814efabd-7847-40c0-a39c-d8af3757b357": {"doc_hash": "3303bdf43fcde8bc9359c0efbd3e20aabdad118b8f2f2e6a5290fc68af5dc8a1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "cda1734a-9df0-43a1-ac37-aead62549627": {"doc_hash": "be7526a242d7e47fcaef0e311328be9a8f516d56c65b149a75424ff1bed0a55a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "fa3e398f-2316-4e36-a9e4-706aa288c7d3": {"doc_hash": "c199bd7f4b6555e8bc6f0e4e62cadcaf6286b5cd96e12c65a34d5e6e58ae62a4", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "ed7bbe97-f402-48e4-9746-a18a91c096a0": {"doc_hash": "74e7fe9fd749ac2bd0928d78f7360c6b0a194fe3ade02a5577f4d340dea54ba1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "f088e130-bfda-4ed1-baec-9f4d208b8a56": {"doc_hash": "9fb58a10f70fb8b8684b680469945e9cd1d5b3e7197a48f28237b33cb36a6a01", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "730c6909-f24b-45c4-80aa-f8eaac484169": {"doc_hash": "947b4f062a1268f97e2b7f53aa82de5edaf2accc433932a193663a41347912bf", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "baac4dc3-e5ae-4510-a94d-2c8e42948aca": {"doc_hash": "c6a40f816fb92e052868c213f503773da671719a8f153a69542f48934f9cf629", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "33c17ef5-a17b-44fe-8ac8-19391e0cde9f": {"doc_hash": "386e8c7931e4de4b4e17fb9aa87de23a1971f57c8951c9edcf4b2b215ac67db6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "3816d36f-9bf2-4ec3-85ae-e8a886a8e490": {"doc_hash": "be05a02b19f5391b29e746c0a967d85be8b252ee4eff370b2abb8fa5f22e8910", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "901c803f-e256-42e5-9aa7-c8304deb0cf9": {"doc_hash": "d3f1a11443af5f4835f7243cd9f6059b0346345370e5cf4f2a08e35b36f939a8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "d26c473d-a378-4bc7-a8de-914beb018f85": {"doc_hash": "32f2beb41315cd81d08ab2818857fd1a1f53a50a217e64f36d61c4e6df3639be", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "0ee57639-b5e4-4b9c-b67b-b8e0a2c8ba81": {"doc_hash": "163bfce6ba57a7840af9d26fa8634d93045e0513e6332e8f2088e554e6dea7df", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "2151de96-b998-4774-b23a-fae618ed43ed": {"doc_hash": "a067e9b6c230fcf6f77731a8af0b6f15ea558600433582a311b73264f591a0b4", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "edd47315-333b-46bb-953c-211b0ec105c8": {"doc_hash": "dbd24fa79641c42f03a57419af9b68e77ebf6426d915940dffa9d7a93d3e0468", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "29f86f38-2e31-45f2-822c-4ee5ebb15574": {"doc_hash": "06b26f9706e7268f14af27b0c3f4a3137b5c08bc7a6045eb4ca83576d52ef7c9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "48e6855e-0ba8-453e-89f3-ece7ae5a3d53": {"doc_hash": "eb8d76aabf6e7ac4b58748a0f1d9d8dae29d078daf5055e2aea81972b79579dc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "3bb6ebde-86a8-4f61-a8e8-f8a08d861e66": {"doc_hash": "82f17d06b45d54adf2b29760e12f0febd1395adf13d9901ea5f1826665775ecb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "786a24a6-813a-4c69-befa-69d4b5e89a35": {"doc_hash": "231a287c575ad87db87351a0417315ec22b54588a6d0b1fd7b16921b8480466d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "c7bb60d6-70df-4d54-acad-faeebb7d76d0": {"doc_hash": "9e3ce8fd91d0a1000000586dc23670ddddffa2eed60a8429c1ba70efa5904557", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "e839550d-9e47-4b97-94c1-2e73bebe411d": {"doc_hash": "8c390ca61a274f4c943eaf983ea64826c7a032a6c5c11565af44198b705fb28d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "5816647a-73e2-4ac7-a11a-057ddc2eb376": {"doc_hash": "e02eb7871690d0ea9617b56d3ba541e139e1c45810945264103378d27bee6a73", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "84f716f5-d65f-41cd-9889-b277271a8a23": {"doc_hash": "b460142b491da448c1d7919dddc6cdf7a8e9a8a9195f94b60cca9c3c9f672fd8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "9c25a12c-5d26-411f-a335-cde275a1cdb3": {"doc_hash": "43899adddb187b6b5a7e163418f9a3156874c79c37b559bdf5b94af2e42c853a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "c4078c9a-7d16-4a64-b8d7-b39677f41d80": {"doc_hash": "5712966de711cb2f843786e9ce6c5b6b6eda1179e921f538c967b34e5add9d22", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "9db0b3f3-6a36-4d90-8b1a-fdb0dcf551af": {"doc_hash": "a25b79cfff04dce8d041e7213039106a9887cb819f2c04cc07103c1138c46f4f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "2391d070-3b54-4f15-b97b-3dfa4adacfbe": {"doc_hash": "2f5dcf015b6d32bc5dd89cc281d980a8fd96d0db2446793dc206dc2685d82f06", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "1e49f2d8-c264-4fe5-8041-b96847310cd0": {"doc_hash": "221a159df4c4688b7204237b3c00afbff622816787d597e5f28ebfd2ddce17c2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "1bcd02b2-99a1-4a13-9043-20a1c8f432c8": {"doc_hash": "664fac6c0797af42494b489b4f2ae46b8d8be571b13a7c49d2d49c66abe3d5d8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "18be9a7b-ae16-412f-917d-8a7127b47d69": {"doc_hash": "f44dc2e9314a42992356b0e5b1f297bbaa63155bf51e0c6b3116a7898270eb43", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "0e920e28-34e8-4f9e-acef-2998fea02905": {"doc_hash": "41269a520432132a5f508aa033da61c131c6d33f89600b85ad24e2073e5652cd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "ff328eb8-1f09-4dcd-8706-dd160b6f7cdd": {"doc_hash": "05a905597ab89e864a9448b7885aa42efaedd26b610c3066e52d354c4bb1a56f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "166b8eab-c60e-4a52-83fa-948ba2ec2109": {"doc_hash": "5d637fc2359d0066dcd1574dc54b156d7cfb98f3960d64689c3645b3ebf0b819", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "f486df71-0c06-45b1-a45a-5c6f662ba243": {"doc_hash": "9af3883723723166ec7b417bb2f54931e9d77244c7f11fa277a3c5dd9df32e42", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "7e0694ca-48b9-4de4-9c75-9cc96c87f3dd": {"doc_hash": "769e22614347e4d3e2fbdb1ebc47e6392be056c0f4cff26ef507d60c126a1e65", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "f01407aa-78c3-4d06-beb4-ace5d05edda1": {"doc_hash": "23a214e9413b5d7e4f05ad110dcf4fd3ca50df62a27056fa6e9d6118dd448ce7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "ec137a4a-1c66-483b-85ab-cb3e5f97c354": {"doc_hash": "099b110e794f8fd67acfc7a9b0542ceabf74e0d4c594a8b6fa1064c7da7a2cad", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "7488f89b-8a3f-40cc-94cb-73d30ca8d5ee": {"doc_hash": "c8c9f25067ad4c4dd04962ece8784e3de60ce1745eddfabdd12fc16621533fc6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "30b68c42-066e-4641-affa-25da36e97709": {"doc_hash": "00d13484bce732a4882c4e038067cdc35cf12251016eea44f1a3a0df49f599f9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "bf635e73-8cfa-4246-8ea3-d1eefa59cae4": {"doc_hash": "377a9f5599ccb8b4ae4a60e2140f6020afbdce0c73e912ba5fb66c5aac60879b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "e6032776-bd28-48b7-a52d-1337ae85edb1": {"doc_hash": "6f5ed03f9caffe49f093350e9ea224f6e6337e1e3d15e054449df6a41cb3be0d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "1a95430b-f941-4dc7-b97c-db99e5a02163": {"doc_hash": "0e6d3361f1887f6fc5cc52b782bc95a8d3e1dfc473578617be0aae2a4bb265ca", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "a50743c2-1383-4511-9cbb-c7b9bde4eba9": {"doc_hash": "2daefc7e1ec7ae503256f6eb236adc798a606a90cb37caa2b07e42ef96d6593c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "5916c988-4dd2-41c8-85ea-dbcf1bdaf775": {"doc_hash": "3e724deab1a02b536db26e693e54185b172c7290a8e9719c82975b311ceb3ca2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "131b9936-99f7-459d-91b0-73a619f8492c": {"doc_hash": "ccc2c2e3c07a4c6e3ed4a61c892790e0c2526882c57b77c702674bfb636ebcc9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "ce2ab9a2-0f94-401b-9ed1-968d0a7e4b8a": {"doc_hash": "2e52794ac5da5cfe7300d7b01d2c3600747ffb9036c877679416ca6c99eb6433", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "65454d62-cf1a-44e6-b301-041249136f55": {"doc_hash": "e8068dc32b44799108c8ebc2e5dee5c41f84b0697e82f863991f9cec90903a57", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "1011f77c-a35f-4d6b-a620-da78e8ac9cb3": {"doc_hash": "871ff7a11ac4b4f26e7b5ad2e9a838e860df65d24672ba2c5bf57c0df95d572f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "13d90e69-51ed-4b76-aaa9-8ee59fa4dae8": {"doc_hash": "0e0d5df9a734af4255077b03470cd92c3d2d769c66ba2d134753096569976a33", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "6a60a4f7-d928-4020-99a1-13e5edfe04b7": {"doc_hash": "4470bdb457db811119c5db86ef1bf111bd2bdca0c9114ac0d218fc1bcd493e18", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "2c75efa3-3354-4488-9dff-16bfe0a7c630": {"doc_hash": "f2747620d6de61ce9f72827fd02c099acc482a44951b152a594fbb8a5bccbf7e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "946eb577-b93f-4c08-901a-cca01cc10d09": {"doc_hash": "8584497522ebc29ac4af130790cd7a07b7eacc229bcd53664e9ef249d161f603", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "e7887d5d-fe1c-403c-9583-a3453f71fc9c": {"doc_hash": "e6d04337f56e10bb82587c02e1dde111f6a5b4120d8e5a4d9f2cc5e11ced9fdf", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "96477ab0-f88c-4522-bb00-354110c85c8a": {"doc_hash": "fc2c18a059c5155027a22ac6d3acab70c90c974ff8371d6b0a10b9a47ef75824", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "d11f2831-dab5-46b8-b0e2-adae4a9b6f9a": {"doc_hash": "d7c1d93124a0e7a17ab7f23c334fc9244a0f953059c0961736394f0270065f53", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "5930a112-951d-4417-a7e0-1c0ee380c9e4": {"doc_hash": "a8a1be0e703d3dc8b8a6604748f7b9a783c810d32d9b0a96231bcb0c92f5148e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "6e9303bd-a766-4ffb-a89a-401ac929af1e": {"doc_hash": "1fbabef4d4c998c142485955030661bd6c61fd1b68f34552083baef021d85fbd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "64b61fdd-91d1-4083-9bb1-b2f0fa6b1346": {"doc_hash": "21ebb7e8bdd8e6599a6b6acf4561849bb046a762a2b4b582042ed594d98e5d2b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "3cb8820f-9f7c-46a0-b721-fa481020d96f": {"doc_hash": "d5459323dc05c749758f9cdbb427f57d6b299255122286a422f1c3eb20fa2553", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "db44e558-f3e8-4971-ac33-f8be2f7b1ec8": {"doc_hash": "2dac2d787806d5051b6a9e240a606604b965e805c8b296e953da54726da49267", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "c4140fe1-7410-4703-a922-a2ace8c833d9": {"doc_hash": "b1648653cba0842b925c7ee44d3bfac4405c366c3fc09ecbfdcee965176ecf17", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "ccb1104d-2553-408f-accd-febb66469018": {"doc_hash": "9f1c90dd12745f4358f2c77fc701179640821268853a602135709a50197f3edb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "21e1af3a-fd6b-4838-8daf-a1787afc18c6": {"doc_hash": "6751e7c3a3471eef82cf7b9b94539601352ddd16bfcd0269d578399564f939ef", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "0d368213-7f2a-4476-a7b0-ff42a57aabb0": {"doc_hash": "a99098b47dae530856b51b44851c953a891bdc475acf2420130f519cae756418", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "8f3f6f5d-e3eb-4bee-95d0-595c9e3ac97a": {"doc_hash": "95825486aa865af3e38b02099da7721e4e4b97b790544fdafd3307a7118ec2e7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "811f2858-7385-46d7-b22f-3a3f8919f484": {"doc_hash": "0c7e996c559613deb0178f2f33c09bb4b810562237c89476313a5b91d7d3cbc6", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "a02a361b-2fc6-46d1-b6ee-ac7da3f0d1ac": {"doc_hash": "1f78f50cefb06fdde8a0f2ce4f1b1e6d7f4063744f773d40c5b162902ef9a9cd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "fb6a1f4e-e3fd-45b2-8239-ae4853a8ed78": {"doc_hash": "0ddbfaa2742ccaff9a0179b61e442decb8588ba7b8b4edd2c6fb104043e9af47", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "aa9049b3-ffe6-4916-b095-885e60aba117": {"doc_hash": "411619d500481b79c2bf4fb11d6c276ae0a4d89063decece1e7f4e3a7776f6fb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "add5e0c5-8277-453f-8e87-0b5a980a95d8": {"doc_hash": "654e8598492847472e2bb6c73c979d4f056daca9590238a3d10d7aa8255d8671", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "a9df6ece-fd89-40cb-8d2a-76578d4f0cd7": {"doc_hash": "7a693ba962179e896daf986107e57236e5aacfc51cfbd19486100e7f99571632", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "7303f6d5-950b-4ef1-b527-74a463def7c8": {"doc_hash": "7cefa91074e4d55a64adbbd32375f85c82786d7b9509ed1c84437bc69306d1c3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "0acb7d48-8b0d-4068-8b78-5b75098eab09": {"doc_hash": "c813967765b313e40ab1c7f96b297c58639d1302bf044a09c0ea6cea825e6022", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "533e8751-04d2-47d0-8e64-bcd7ee462dfd": {"doc_hash": "04727baceca9615033568a2c0cbfa0ce9411d9f14b47aea0451181fc2496c833", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "20a292c9-a5be-4a54-80e1-6491d1ad655f": {"doc_hash": "71bd0bd1ce66b03713129ea14566956f5418c98db29176c29c7834a2ec3321c1", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "0e7a12a1-62ec-4f28-99e8-93fd581cc0d8": {"doc_hash": "eb687c85988be301686f2572034c7377a04832b0a60141ad2e4928d38ed6ccd0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "3dd7843e-e16f-4965-9b4f-c1d0f4146621": {"doc_hash": "007295623ffb746e663c1f601089bd3cd56a88ebd668353f9c7b6f812a4ebb35", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "9385e64e-d5a4-4d4c-84cb-f859d745f8f5": {"doc_hash": "cc51560774f10f8d4fd2fe11d904c020747a419b01c528d84446c4b847367903", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "7ee10036-70f0-4e5c-a310-80400ee46def": {"doc_hash": "f62f51f01a986168dc81042c2e872aacf68027c9dae268bd767962b361b2f414", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "a182dc1a-2aea-41ef-9cd2-8cea23807662": {"doc_hash": "a05b038e80ed0a22053e0c1d330ef806914a41c2c21c5c1113ecdc1761345acb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "6f8dcee6-529f-44e0-8f4b-1f2db164e133": {"doc_hash": "02dbbac43977d5e10bc6d965386f3ea7cb7d0d5fea146d2196cae11d61d5adcc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "b241cc96-de18-4a7b-be9a-77271c5bc125": {"doc_hash": "f50a3fd80e794655233acbf60e8f173331025e85dd25214e516bc8046eb45ca9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "25469213-9191-4a78-9f31-bfd562d3ce71": {"doc_hash": "2f2d77487c8f716c8df9b58c72b5076ecbef70a872ef4e375dcfb5a2d140f8ef", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "4b959207-cbb3-48e9-a73a-82ba017c6673": {"doc_hash": "22a94d59dc1159dbafcb8eadc4f616f1ecc6604ce5feac7450a6be2790b9befa", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "9084e53d-a3b5-4132-b865-cd4d6101302b": {"doc_hash": "bfef7dbaf08a8bd83a6ee635862ca2a4b6ca52afaf1b6454bf76aa61a8747c48", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "0d64c0a3-2bc2-4c5b-a284-577cc72df072": {"doc_hash": "5c11d91c3a888f4c83a3c64f4c2a157ece1810320f28aea1beabdd9ddac45d05", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "98473a04-c2e4-4376-b563-fb7e9d23871c": {"doc_hash": "cbae8afbf08a0dcb2b530ba9d3593ad33286138b3096b05439d4a4bf57e2efbd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "df2c8ad6-a2ad-453a-87fe-017e60bf787a": {"doc_hash": "70a924e66d5dd4324608ffea659be3d25e0fd23922a8bba828757e800a504165", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "65d6f179-674a-4762-893a-af6316bf0c23": {"doc_hash": "e4cba4eb530aa2796a463d0542f3de172b5471e4ee36636ac5c5ab8fe69c38ff", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "89c51624-4ace-45b5-92bd-beb86a011448": {"doc_hash": "850cd02b45b59ab52f15bd90ba678732fece0bb260a3e8d29e54af4271da23df", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "7048bba5-b259-44fa-a3ec-dbec58a52420": {"doc_hash": "44a6b0bd297403b28669464229ddbc330e0929e630fa71eff64f14fd2df9bfd0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "b8eecfff-ae1e-49cc-af79-6fee76ebe9ea": {"doc_hash": "78939eb1efa0de0a526b83d30bd0fc19d79e8da22577e245c44659c9a9283fab", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "4e4204ea-1459-4626-ae77-0af9bd17be77": {"doc_hash": "d081640ff3d803c5a8bf58a2bc76997276d57c1ce92af95ce282b96759cc58b2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "b4d57ef5-fd97-4269-a7c6-87c36f9c28c8": {"doc_hash": "980fcd8246843cf92946f01170eb80917ffd0da4446a8b8bbdeb5be009732663", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "72afdfb0-c290-4834-98e3-e0d68ba6b7fd": {"doc_hash": "23fe066329043262101e839c2a5f7ba6a85c98362bcd99e786250e4933638ccc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "5d253f0c-27fb-4cfb-a8ea-082717b4bdcd": {"doc_hash": "27af2aee7dad2b179479c094c8ffbf18283edd16d2ae7822e19a32ac29764da3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "66f77ccf-46c7-45af-a56d-c1696c546f05": {"doc_hash": "c11fe2abd1ac719ae08e16834dfae561393d74fca1304afc33735ffe53564f73", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "a67944fe-9abe-4082-aa63-bb0562efda06": {"doc_hash": "2cebeadc9cdc103d38c1f93d428f87c22315edc96129e79feb60c2675fd03933", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "1e4f061b-042f-4d35-9c93-3040c3a89140": {"doc_hash": "2e091a94a63b1dc60a762c1dd5228d7aa1742dc15d7f33979dc12acd8a7d43bb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "c6acd20d-06e0-4bc2-9e1d-7813e50ce73d": {"doc_hash": "649441050a1fd15817afd6bebf014b58548a4fe33692e1ff81d695f4e018b530", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "753408dd-545b-4377-ad0a-546e13225bea": {"doc_hash": "6f076a8de0995bd24dcc7632804e77e682a1d09f20695c9e5eca2e4dacd31275", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "6bcf6805-1d3d-45df-bd75-4157dfee23bd": {"doc_hash": "e4c3c1e58273e07a3b407be5ea0296cd5a4dd24c11a3dd34829fd6a7a7b49f1d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "3ff93fa7-fad9-4216-9035-10c64b29c928": {"doc_hash": "984c1ea9fd7781e0ef966d966a5221f9cfdb72c91de128986b244720dca78463", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "f7eb9ef0-7eb7-4e76-a581-32918df9288a": {"doc_hash": "75e0fc44baced29d5851d57dab82778b19bbc99e4d8a3952c44b12a0eb2a242c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "7f8683e7-f2e4-4e10-bcaa-306a580e7a98": {"doc_hash": "8c2478f1781e54ff3fba8606ac332a856a67a0f8b1689ec56ac58299f0f4215d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "97e67368-c103-433a-b808-5172fbeaf390": {"doc_hash": "6e9dcc035e4c7ce64e9107b30c01bf44bf7942a11d821d9ea59230c3f99ffdd2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "cfb8514d-9493-4708-a9da-2a997ee71fdd": {"doc_hash": "362e8a129b6d1de9d9926d1b42064b5fdde2390ca8b3de6fe5910e529576306e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "d156f5bb-f4b4-47f6-8d4d-b33a95a27b3e": {"doc_hash": "0b17b8914f71e6649d397472ff81852479bb029884403e69d5ddb0a9a3f749a3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "3b228fd9-bdfa-4266-8559-88dec33f6a0e": {"doc_hash": "ea6321464d42775894d85bab1a975b734b4231d442550c5f2984693081fefa3b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "0b23a1a4-fb99-4691-befd-6f5d513aa4bd": {"doc_hash": "d8dcedc7f50fd814e63f20961b2bb3cc1fd820ae27bf317618ddfacd52cb7ba8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "44186064-86d9-44bb-bdb7-c27a23d7c284": {"doc_hash": "9da061841dd3d3156b93598ae7a28d10e8772ed350d1b184f3cfe8f6da149db2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "7b84fb40-428c-4962-84fb-f265473d9bd5": {"doc_hash": "14746b26729f9055abeee1fc3632c0b8211663092eb775d9b1aeacd8f081df93", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "b4805184-cc85-4df9-990b-2a6ad5a21c43": {"doc_hash": "0c153f1340d43cad2a7f6b13d87b2076ce26b0f2ffaab1c3f62afecf2b792079", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "55422b37-2367-4f08-b2ba-352448c96424": {"doc_hash": "e83442bc5d406b317aa8629fa6219363382e0ad1478e23c01d6f5c83db2bd7cd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "71902272-2de6-4cd9-b6bb-c504b6c3e657": {"doc_hash": "5589bb5f9e316c394fd81d1756fa3f4c545dac066fe74d9463fa4d29f6fc837d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "d3628b81-dad8-4954-9cf4-d9170594e7b4": {"doc_hash": "480e3d01c753e792a16482f547d0f5c40637a8539e302da997fa7323a528585f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "73d0e642-7981-45ef-9ca6-287ea4299d46": {"doc_hash": "bf3c24a90828784536f5f8c394c39c66c20349698286853cd018761bc3266a02", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "7c3ca567-6e2f-433a-85cc-48656256688f": {"doc_hash": "ed6a5480e8e6b6ca23052128acc8c214dafad8d3ef604665dc206412ec487e0a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "afffc6de-4b84-4e6e-9605-177137f149c8": {"doc_hash": "d8280844df8af58cbfe0472e4e957cde37baff9232b236bd064e920e8b03cce3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "c94405ec-8091-475f-82fb-af342fd478f1": {"doc_hash": "725748e1bb8ec522f25eeaa1e2df69a91053b45b3076ca4c98b21884029b3cec", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "cc23c79d-5e73-457b-b022-67edd5a95d25": {"doc_hash": "f3d629d7e7cdd172787828eb73c43d15479015d9252694f230ad6e5b5e042325", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "00abe0e0-c902-4f44-8116-236ac44733a2": {"doc_hash": "f51af9c3303b8fc3f3cfa239c6db2211ab3d022455d4c62b4d0115a9e6e02990", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "c1a79e65-190b-4abb-8f3f-328332de4838": {"doc_hash": "d43a2b5b8b8b85d5891768ab78f633c7b9685ef93a4c34d6fdc4f38c915d5dfd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "04013ec3-e99c-4941-a3ce-639cacea06f7": {"doc_hash": "799f9cdc8244ea988d35428b9238a719e6d9226e1915e8243f9ef26dbc28164f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "8af6cba4-72c5-4c55-9671-84d3924034b3": {"doc_hash": "7caa281d6ce02c2bca2c799667a6dbd94c1c3a3f7e14031780d1ece40ab7d21f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "b9cee74f-4ae8-439c-887c-b4120b0f1a8c": {"doc_hash": "04606dd59fb07bc6841c3a98d939c2b19a7876d9557869e9516fe2607653f8b7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "4bbaca6d-21de-40aa-b72a-5131eee75c61": {"doc_hash": "2a6e82c8c91d23b6c09dd578c5c92b00ac4133e92a7718d97fe7b69352a7dbca", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "2c5cbc7f-e6ba-4b31-a886-8b211cbc52ed": {"doc_hash": "5d0746335841832cbc33059c51e4ab00bcefd719e4ae4fda2c058699577f5ca7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "546c4fc7-cfdf-4307-90ce-9add0588b0ca": {"doc_hash": "4aee9d8b3f0d60f60708cfa4ac694082d051ecbd6a143d1e9cf6153218b1a77d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "4cb70317-e840-44c4-8f61-b0ef5a49fab5": {"doc_hash": "1e3f936e3761ed9a71f61778be5d2b0462da4fb52d6332b4862c3f09a391411f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "d195d361-fc63-4315-8fc7-3f140bc13744": {"doc_hash": "caecfa24bbb432d5218e1f96606e8cd83e4a8561f46596fbe538513b3dd59448", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "35b0a8f7-0aa9-4837-8c58-3c180fff32a5": {"doc_hash": "7e0198ba864a2f20e48729c1c41e03c7bedc689d1f2e459819fc3b99c09a2903", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "1ac0724a-5cba-4014-bbda-3626cf04f5ef": {"doc_hash": "5fe71e3e9f8bc4ddec5fa8f0cf85b3634d33f647f83fedf68fcb6b46e97051aa", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "d97b1890-ebbd-495a-856f-a17415b6a208": {"doc_hash": "8b510266d146ff1078d44f6331183d8133be7c06d28ff5680bd7c52eee4fe7ae", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "5959cee1-6887-4d70-81ab-349054816bc5": {"doc_hash": "f02a146f155216035cf437ee1eb5f1b23dc306625e9ddcaf5d5cfc6dbd116c2e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "bdbdd574-6557-47f7-947b-3b1b5a07d17a": {"doc_hash": "45182dd7c932550570aabd16d476ce2a3d2c8fb578283e6ca0c9b8bc9c0d7e25", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "cdb97048-86ad-4596-a180-c5ee577bfad3": {"doc_hash": "abfb9c8e654c6dfe2233e79e5644afb20d2c1f9385fbd0da5608bc1e8eec322f", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "a184e5b0-3a97-470b-98cd-1f8b9e1acb09": {"doc_hash": "92ba8d006dc02c0d01b1b65fba1969b79421dcd890d9c1b0b2f550111c45d590", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "c069c6d9-2b0c-45a4-81c5-10c438813aa2": {"doc_hash": "61c28a4cde864610d99f249e9e8e3f5dd8ec92f680ccb6ea1532f5c6d148f606", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "6263bf63-7175-42c0-93df-82ddcfdfc4c9": {"doc_hash": "ac0e5920a7211dbe754fe62606229cce4f669e0e04ae017f5bdc991bd0bd682e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "9d58f374-c620-4eeb-abd1-5341196fabf5": {"doc_hash": "2650f62d67284278208cecbc45d44c0741231a415b711cd892b9801814072273", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "0a39b38e-6419-4ce9-b4ae-6db06d20cdac": {"doc_hash": "818ca215466d414474cec7d98b9266fc9bc19ab336d9a91c2fd6a3c734ebe9ea", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "ca649d76-1076-40b0-917c-fc515ee25dd5": {"doc_hash": "fd8947428292c89eece12a9bb94d94dadca7afc973d2e58bbe82980c66fe8ae7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "0713f40c-5bba-4f62-863f-f5df74e60599": {"doc_hash": "b532513e5ba1b1166834517fdc1651cd7708a72f0374989ad98c70c2221a07f4", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "d1a344f5-db37-4369-a773-e33274346cb1": {"doc_hash": "2a94f77eba46cb91c2d456040417944f406c157a06bb21ce6a5aa43fb92f00e2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "f55ae1bc-2d41-4058-9444-a1131cebd5ef": {"doc_hash": "7a035c3ab6e5cba12e9584c21fdf562601b6ac771dc1bfd940b5030626abd8cd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "679d9b84-6142-429c-9ef1-4b5ef77d27fe": {"doc_hash": "95f40fb8de96141c0bf3aa67b8daaa2d94dc4087bf843f7b20378953841eaafb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "98f0f3c1-d81b-4603-b0ce-c529d844818d": {"doc_hash": "6f44ad22f32b0c9a9a2776e267f8b8938c5562ba0f7588f5e5f217a648875a9d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "eddfd3bb-1f51-4635-87be-37b010a11127": {"doc_hash": "a6aee60478de07c4da7efaeec6cf33cda500cca9a6b630122096e310965667b3", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "9058e019-cc2e-45fd-bf9a-958b67c89a1f": {"doc_hash": "5072b4fae854000c46e0a50b296810643c844fc89279bb5c35320ae08493aba8", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "b5dc4aef-af8e-430e-bab8-bf0647274cf0": {"doc_hash": "acbce54ee2f4ed819a33e43a03df3337fc5e1101e2d6961100209cd189d98368", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "2ebd33d3-3c2b-4d46-87e1-8500b2da901c": {"doc_hash": "f838d1e4066e0050f674e10045b984a283d8b624c98e454383ea7cc13cccffa0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "032e29c9-31bf-4d4c-a141-75b44d8eb772": {"doc_hash": "92103a75150d1ced41d5012b8de92f6e3ee72ca8a8e1908ec2d38edb13b7862a", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "b61ca803-8a0c-426d-a376-31f0b920ab4e": {"doc_hash": "46c9a4c796ad32a7ea5055ca8b0b5c78493b1d18cb5bb9eb2f34d6c85cd9b1f0", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "f05985af-ddd1-4781-a431-89662ab67aba": {"doc_hash": "7076eb79c4b0d82431bf0a62727e03cf391490ef5ae3c493745057f5a2309465", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "4acb8290-def3-4c17-8463-491aa486ddbd": {"doc_hash": "5eb98f971a48bf3a26689c6a12fa3ad7e1661cc82e005180a7088e7633eb2479", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "076007d3-dbac-40e8-9072-f2e34c5d2d56": {"doc_hash": "d639086027053c661d737050bc644ffbf7efd2fccfe07ffee52b18ac75379736", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "9405157d-0ef7-43f3-bd50-fdcd2ee50d08": {"doc_hash": "d0d625cc671ad09f25592fab9335f1c36da5a588f588d189c01a8e1184ecb7aa", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "7da2ddf7-be0f-4a43-873c-d8bc0d9a81bd": {"doc_hash": "19f20ffa90cdf72d53a4bdb02df8c38996476e670fd9f8c8fe3c3d8108ec382b", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "b6905bfa-7ee1-4f4f-b610-d408f7084f6e": {"doc_hash": "af222a6e4b469361547ef179d6582d5e9cfba8520fa94ccb5aafe240711d950c", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "bfeeb803-6b00-476b-b25b-2d8381b45b7b": {"doc_hash": "cfa9a6971ba4791e332c81b732fc121dc107c8e1a63170657de3437e819269cb", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "8b4af47d-27ef-4ff6-a006-f9cb815c4598": {"doc_hash": "e5433883dfe0b9b375c6b994ea70fa9ea9534a0e8b64b959da4684e66485c226", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "36352219-00af-495f-9f79-b242ca2de9dc": {"doc_hash": "ec4456d70dc1a6011ecf0bf2ba08aaf4508324c09b5e67a2dbaba437b3b59650", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "02255e17-8105-4869-a336-2c8bc106cc36": {"doc_hash": "d1fe0e024709fff144774d22dd981a06ad41bf4b4354a50e31ca87d6880a8cdc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "07bf271f-a60f-46a8-a0f3-3c72c4542c0a": {"doc_hash": "a15b6a981db75ed52ebcc63000e19b7a6b94d7a560ce7d006f4131496cd3f18e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "fb077ce0-4c1f-4c65-94de-e35b4ab1c17b": {"doc_hash": "1936b8c3b64dfced2484df4fcd07f28645100f7c302a6b6721b8cb9c149f8d14", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "f1d435a9-ca53-459f-9bc7-c799f5d37e4f": {"doc_hash": "d39b60498a04af59c055c48f0495ea4f043da5b57a2896b0984471b4d0dbe9fc", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "a8e476a7-03fe-4c9f-9294-188e9f9d0dc0": {"doc_hash": "3895fd386ba8709df89d3104a41e2034fcb2c962d37babe603f80dd425bb8803", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText"}, "10.1145/3620665.3640359": {"doc_hash": "178e4390764202748e40c295553c29a626f30a40dd0a3cdeef5acf2b288d9c9e"}, "17f93544-8b86-4758-b5a0-db9d8d93c5e1": {"doc_hash": "b9382f7e639b67ce44e0485af3cce7fbc4e7584b660e37aaf652c1d83d1f5a9d", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "4dd1b630-56ee-419b-95aa-e119bd43cac4": {"doc_hash": "a08ac7406c1f69d81add6117bca4dc15a4a2a2ea1065b7f9cf0455cbee3ad637", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "a61f347a-3ad6-478c-bd3a-34aa9be50061": {"doc_hash": "ae47606036726d2fb26a76677806f74f3e2be4e50b3d28491ed41f7494355f04", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "cedebc15-8eff-4a16-b4d3-688515a66738": {"doc_hash": "58b073fb6710be480e16c54118ea2c55ef2fb5d4903dc260a04c2163beed85fe", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "98abcde2-874a-45bd-bd59-57c5974b5d64": {"doc_hash": "fa46c6b3a224db140a681b7da4c9c8a82d8614237d0f80c6423985a13b006326", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "750946c2-149f-4ead-b810-1c4132a618d1": {"doc_hash": "e35efb8510c2fd245d192c0e2e3b25bc2acd1102fcd2a2a0c0998b0cbdcebcbd", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "11fd5a8f-0034-41e4-b839-7256e6a5ed99": {"doc_hash": "6b6cf13943671118aee4e150a50bb034215239d9cf831a58debdb793cf964ba2", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "ce73f64b-0034-4847-a7fa-297f1f64516d": {"doc_hash": "ae6fbf61f85264a1275ab0898d6c13175add62b4c8dce8bcac36778282032af9", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "3a180214-0c81-41f3-b217-865bdc373458": {"doc_hash": "915901dc83ef5f8eaa2bc06e98f4de3310f39b50104c9701b0c00a60da0e16ea", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "b6f9b395-fe73-4520-b734-8deeeb67c05d": {"doc_hash": "c7fb7598c497f38cd0220660d538b7af6f829808cd555b25cad9bceab2841e2e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "0edc4b3c-fe16-415b-8b02-a903de626e43": {"doc_hash": "d0cdfb65fecb3eb057ebecfbfd847b9405ec122880cb6396d41c9fad93d7b4c7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "660493fc-edda-4c92-bbcf-dfa162356527": {"doc_hash": "3751eba9fc9154bc04db9113f6c390a16cf577ae2896d0d3db863bfb72a96aa7", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "ca6c40b0-025d-4e8e-a3c7-ae13e2e120c6": {"doc_hash": "589cda6ca8e2559eab325d31e3fa63e6d954997bb194ca15b585b7982cf0a98e", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "f37b1acf-e65f-4940-ab36-c89fe045d463": {"doc_hash": "1459aaad39593f6843f725a58da3f82e4910d808929eda2d342980e653e24493", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "8c999e23-13bf-4234-89cb-57d294f23ee9": {"doc_hash": "f7a9d90fd0976dc79e286c97ea8ba1629a6e3c20148f5e5b3fd7f36ee81b1056", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "ce9cb202-e79f-4a6d-abf9-4409ad296965": {"doc_hash": "54b3da8bd841b639a79b6d76369d38dffcefc5e497f735a1eebfea765b5ef1ed", "ref_doc_id": "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText"}, "10.1109/wccct56755.2023.10052488": {"doc_hash": "72d7eedaf014129d6f956823c852d0518dbce50df74ff9294790e719063b5594"}, "69f46ecb-64f0-4828-be69-6fdea351df8f": {"doc_hash": "31d06a2fd87e7f330b62dedbd0c1769b86898d5390ffa16bbbd97c0bc968f58c", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "0eb49248-6451-4dd4-af95-84debb4e62f2": {"doc_hash": "1927a8b52eba262e5ffb638ce227ca828dca0fe2d389e68d874723c0db21e31e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "3ef651ff-766a-4a1b-acc7-1b0fe4358d2a": {"doc_hash": "e8b01c9a0d1a55e9bfedb809c531488ebf9aae826bc42114ec7c542595e24660", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "a7e1eb8d-cc5e-4529-80cd-86880ab1c579": {"doc_hash": "d6b7cb81ae548d72731f70dbdeb96decac3131b47bb03a6b66d3e5dea4fa03d1", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "61b10b43-1127-46eb-9dac-29bf234c551d": {"doc_hash": "4ac414d4f738dcf2e67ab2879e65c92d32dc9e80067bafdf13a29029cb39a1c6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "6554170d-6516-4dad-9c9e-c0b3671ec8ad": {"doc_hash": "46d13f3424dda9a7d0cc22f5e654750522aa99ff371a0008e5d0ae8020e96787", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "887da0a6-a918-45f8-97f3-6eb363aec90b": {"doc_hash": "a45484ee488a97a292295a57a2efef1e4dc14edfaa180490f334ac32847d7c4f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "4b9365ff-ecad-478d-9922-4dccf7c469f4": {"doc_hash": "f7bde94144acf7d21da0a49b2e71752db776aaea00f4fa3cbae39aa4e576cb64", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "4e9dedfd-7b47-4cfd-9ba3-f37e71163773": {"doc_hash": "3fea2616c147b8ea3acd3b22a34c707e97e826c4cd66ea7b333afce2b54f07f5", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "9d112cca-1060-43ff-a555-823b666028ce": {"doc_hash": "3de282566a7637db2fe328478b82c2b929047925ff2bd771fd616318c3fda9ce", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "932ded43-76d8-4843-b571-05e6af61a054": {"doc_hash": "f08a8f4907c4b847822ef59451e0ba0e0bd0697e877fd94548d3e2f6c7fb851e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "49c1a25c-8aa6-4512-82e5-94fe17760319": {"doc_hash": "c46337ff6b896af5f347e9aa66fd5b23332796802db4d08975b544b204b7e842", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "74adacaa-4ffd-4d51-8bb1-af16038aa398": {"doc_hash": "2b63727ef0cee8d5effc31e396aacf585b780125eca007ff87ce91b5c0a9569b", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "fffd3454-7a13-4317-9cf4-93e6e1aaf4c9": {"doc_hash": "fb50a749bda939d3c41e8c89de01e3af48919149611d42087c2a62d038753bed", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "deddc4ff-b450-4c19-a581-605ba421c0bf": {"doc_hash": "0406abf90f3c4693e92ed51281674eb9d6eecbf77c0eea713f6848a012d9b288", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "8264a7a0-17c0-4a36-b3ba-5bdb87ca5fa3": {"doc_hash": "e964a16f34b03799a0ba92e4326e1bd1e9a796dd3b26c2a11a886b8abb9d6515", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "02ffb4cf-a7cf-4921-9b13-0f04fbd1c7e9": {"doc_hash": "909ceeff5100512930efa7c60820ce9c2c2fd07de4facd09334f73b97461a2cf", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "8f757292-10ed-4d25-8c32-eb8eaa7f60e1": {"doc_hash": "5a578085249e546642dd81c34fcb3a3a2d1f16b67c32adf9b6bade4dee7d08ae", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "11d58726-a1f4-4efd-be95-4060eb6e8311": {"doc_hash": "eec7d69a2d70940cb26e80ee28bd428bc8dfd6223f319c32b53ed7af420ade82", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "93432d76-aa5d-44c5-9966-232c066a1e11": {"doc_hash": "129492716b48f575c576941a92f825c34bbc94689513e3a475e4d2e4b6ab9da2", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "64fb4016-0551-46ef-9dd6-b376c71311bd": {"doc_hash": "089dbd818f0684ce1a34914053b1c4017a4f2f4d112aef35f70ca5f9cf3ce174", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "ddb2c5a2-1269-4c29-805e-009d49124ed5": {"doc_hash": "ddc5c1be39c2a5d31af45d43cdbfb55b236d558cb390ed994d554c27f2a8e31e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "f20b7187-4e07-4ee0-9f99-fa7eb00302a4": {"doc_hash": "9c69841a44d656a160f6b6f1bc12a92de0cd05b3d725c2b5d9152df24349be1d", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "da295ceb-8a43-4690-9fbe-b678aedf5db4": {"doc_hash": "8a9b5bea24beb9cef394776f52e25dfe6e0169050117fe7b178752aec5b337d7", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "eebd11f7-da9d-48df-8e8e-35717b84d6d6": {"doc_hash": "d39cbd6bd631c17b88b7f1b7f9097ae00e8517ca4a5ef084b2616bd1957237c9", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "05b7bc04-844d-401c-aed2-1ebf29dcfe55": {"doc_hash": "fd33d7f9240ebb23a396b45d0b27c5977596785762dc0050898849592fe6145b", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "99c461c9-39dc-441d-9e20-5658778103a8": {"doc_hash": "95f5dd5866086dc8f2abc003831e1d0dc9c575d5f5c76118adb7907a031a5101", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "ae4d523e-e7d3-441f-a3f9-eb7c5ce6ced2": {"doc_hash": "a77129191309986290f8dd4be47eb8029f2664a27aa90b180ce714a48a98ab50", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "1d58ac39-68bc-45a9-9c8b-2a79aa7e726f": {"doc_hash": "02c4172d1f28c1c7e508e1587cd285865b60624baed6c17aa3660d55b68637e6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "44c32e15-c836-42c4-86f6-7389163cbe01": {"doc_hash": "25f26fb81af43dc96f6084c7814eeea99fdbe7bf4dad7a75fee19579c0895ef4", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "56c304f4-a53e-4ce6-a7cc-eb1ec11e699c": {"doc_hash": "8d07dfb2bb7138cebf2987db48c2bdcbfe415d88a4a73cfc041c09a1160ce11a", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "53159cb3-d55d-4004-88e8-f2cffba985ed": {"doc_hash": "3da64b89f050582a9e4b5b74a80e78da1c0f2753c1883bdd7e4bfedbb5f815c8", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "0d832b5f-a58c-4190-850c-1393f8c82908": {"doc_hash": "73158d5cd8227b1e98453122e75cf9e884921b1fd3dc34be3a37d14ec18f3ca7", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "c6ce9951-3366-4b23-80c1-a54994cf97be": {"doc_hash": "fcc95eb82e0650aa4b6e64b3c0755ef474861e276609d584cdbfdb122549e7c2", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "bb99e91d-09d3-4f2c-956e-86b1fc15ea01": {"doc_hash": "77606b2a6cad4abdf110136cf8998c300bf3c63e8e1dc32726825324705cab11", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "cffb2891-1970-4436-ac95-293ace706841": {"doc_hash": "1f3b20468b46ef343c1329fd34d7daf1d1ef4ae6bcb5c2bca3b57353ac6dc2b2", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "916a3217-0ee1-4469-a9d1-d5470d1aae9c": {"doc_hash": "dc7259dbc338037caf08a344796566f96a6f6f0555044f3ada90367127d789a4", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "22f43c92-187b-41e0-aaf5-9fc7f4e3462b": {"doc_hash": "3a94b31996a602acea4791e92db1de63058e228b018f7759773fdcc403b9afce", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "0b24b883-1e13-4ae6-afe8-e17247b7242c": {"doc_hash": "a89f5c2db7a0a61c7d9cd41665c424586e48ce11e19baf342a2eda14df49221d", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "dab7f045-53f9-4fc6-aeb0-57e68ab6b8bc": {"doc_hash": "b59d7a1fac5ba7d530d184783db7c8826e68b7782e26101dfa81523e95bd6730", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "4dc6ef75-3d29-4e86-b3fc-a3a3da0bd193": {"doc_hash": "4e013c8047664c8abea7451c2cd8dfb7fd05e797e543a19591978a45fb6ee75f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "6e1ba783-6d7a-4167-bc12-3bf9d0e7c5a6": {"doc_hash": "efb9c6c31f56e1b9b130d4cfc7170f8c9c2c7052018c24d912200827c0cf8d4a", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "4b597f4b-6bc5-49d9-b319-9e51efece81a": {"doc_hash": "332cb42c6f7ded531e905542dc3e1cf1571c18aa7b3e43d7b997d17eb121192f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "1c915ba3-d40b-452e-8c5c-cca6b4e42c66": {"doc_hash": "01aca20568e703d91c10eea393fc187f4b15f5da7bfde50807cc3bf5ec2067a8", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "44658f6c-6fae-4c9c-a56f-102ff8d88f8a": {"doc_hash": "a4efed89b3f6f33d7095bb02ec1a0f50dc099f871ca7d1fc14f3392259910db7", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "0e9c3967-ccc0-4554-9aac-c88f870e3147": {"doc_hash": "ba7e6e68a290aba82754c47f50d6379e6da6bf1d79d2eced4d9eb7fddf27ddd1", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "846f23c6-7d45-4eec-bdc0-7d18e1d86468": {"doc_hash": "e9883cf3e33c2aff4b43c1d51713f83e494a658a1d4ee5828bedea45f4896b8d", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "83f1b13d-ca80-4ecb-88f4-feabe9c81073": {"doc_hash": "c3ae119ca7300cd2650357425d2a2fa6d329fc05704747e448910a2d0cc09938", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "568a6c63-8443-4a31-8170-3b13bee1aab7": {"doc_hash": "c56dbc36696936128dcd45b7e8f11645d4351114e4f26808240713ea79512278", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "2a967a35-c929-4297-934d-81c7294b6836": {"doc_hash": "f19be6eda07aa8c84628423e691acfe1eef9232e3c8a17e69b83d0eb7472d319", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "31586887-2b48-43d1-a7f9-fe6b0c57d3c3": {"doc_hash": "1d98cd38cc42df7b8bd1f3cee06e32b8506080953d317fe2c55a9b401f69ad55", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "19054bc3-9198-4776-b980-2bc762d0c322": {"doc_hash": "c30a11706e2d7171cfa22f2b167beb561dd60c726ab0c5fe4be2dd12d6eea6a6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "356f2f45-993a-4742-b1e2-2fbebeadc6fa": {"doc_hash": "4c46b26feaef680799b6a1be2babe72389cbd9b2936e2eb7d4afbb76cdbceca4", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "75244e7d-095e-4d01-8953-306760bca971": {"doc_hash": "8b669e81dd40d4738bc3c4c67eae89b5affc848eda1a7790dfa428325b7d639e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "bf094086-9c4a-48d8-bd6b-11d862fd889c": {"doc_hash": "4babc72861c7ab8e7ec80f75d6755c1222150bd48dcae0a7a8756a0106bba4b3", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "4e11fc2c-92b6-436f-ae81-ba6c4d60627d": {"doc_hash": "24f77d15e4ea3ce420da88e9a93d22e96387c23141b587e129ce172ff3b706b3", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "02587a1c-08eb-41f2-92d3-e5e732fccaef": {"doc_hash": "7095e42cb46f2d6bd053831e45559b331fdd330a96ee9a966e5110441ed0aa9b", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "9ce1be7a-a33b-41db-a132-2648c1b53f38": {"doc_hash": "1779f30d37ccf19683a93997ccda655fc3fcb205f90e668f4f8ca7129015806e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "9737dc88-cfee-4ce7-b6cb-534b1d0ae78f": {"doc_hash": "3159c4c0b080a0e4ec8170fe4460fb325b06ad2c388e096a2efa8e7857949117", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText"}, "a8b160eb-2ecf-433e-a204-98abb6371899": {"doc_hash": "afb014a03af0b217702fae8b37082e0a553e419acf85ff3dbf1afa578443088e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "04bff131-cee6-4985-9ec6-33c2658f405a": {"doc_hash": "443997784663acbaa674c2939cc5a8ccb83c5804937d4c9e4a7a0dd0104d2c6f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "b7c3291a-1802-4c9d-9c14-9cc5455b9191": {"doc_hash": "63fc4bf8d2e6b843ea6a85bb3f92ea17fa3d646d793b89eea8978a203b97aaa8", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "935fddc8-4640-486f-bd2c-e9ddc05cd514": {"doc_hash": "e801dffa81822688e975ca120d9dbc2adbca28281d80c393fe413f6e2d75e3f9", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "f67f804e-a91a-43b1-864b-0573cfbc4566": {"doc_hash": "9df4b10067977739136925e2a29130638354edb82bdf52d8682e58c039cc7d37", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "9f08e17b-1f33-4ae0-823c-b9455395cba7": {"doc_hash": "941828eb7fd07e4fac5198071ab79723edb6e1d5c39961d889cbce752b8cea94", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "6ca4ceb0-c5a7-45fe-b6d9-83e215954b9b": {"doc_hash": "147ec102dc08eb2accfd2233d69a04479e8476c8c203690fb570fabab69492de", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "3f32a1a3-cb24-4ce8-b5b8-0971a7b91306": {"doc_hash": "b81b58014484e67888ffefbe500d4892faa7259301d21b759013c28e7b4d124f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "c2dbc002-4aa9-4a30-9364-b71cee9431bf": {"doc_hash": "fae09f1ad847a7931cfaecaebd6044110a1fd6477a2111b3ae5f1287fb501de6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "fcba84b0-3be7-4f64-af6f-c137ccfc06d5": {"doc_hash": "89015b6a47bf78ca4076823931013fd5b8e01899046539023c16e7d3ef28fae1", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "91fac5af-354c-4a14-b045-b6c98a556d31": {"doc_hash": "6141d87ec7876ff32f0003cf5741f0b47647ea17a3361e881b265e6e6836159f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "d666af53-ea3e-4e9e-bac4-852c89e1ca31": {"doc_hash": "b43c2df615471c837ad7c0497ec096b96b6c7d947c2631ae6aeed810b2eab1b6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "52a16434-4e96-4cf7-b124-d6119d5b727a": {"doc_hash": "9a760f1173895539a43ea32ab56208a2997c45c7fcd705fdb4f68f01d1672b83", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "cea6327b-8740-46af-8a77-385dfda82840": {"doc_hash": "00da26e60e9ca350738ddf10890858b827f8260b6d1deee7206dc94656db481c", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "d8027022-cba1-4178-b36e-abe1f6bbb03c": {"doc_hash": "0f9f2e07ba682ce0a67368d1e86c2e7f06449e8b4a48f00446b897f397b53dca", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "84293e49-2a25-46bf-bd91-10048448e84d": {"doc_hash": "2b20a5d3341323a5895a059fd3fb2c5daefa340af52230667b0bb342de48229f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "a54162ee-38a0-4ce1-a5d0-4beafaa4b1bf": {"doc_hash": "3ba598a1ae3cbcce13017b6b8d03f45b2fc74ddd1f63c252e24d85f2718a08c0", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "ce4cb413-aa31-4b76-adf3-108fb53d2098": {"doc_hash": "77b0f3b49651b505653931bb98d5f6521474ddc80363f90dc3a7e00ea2002b87", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "5d1df985-c156-40cc-bdf5-7fcf777ae171": {"doc_hash": "e4ae9b25df08cbca563531af15a01a0652312b7e4bba79e325ddf4d162cb7eb3", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "ba48f775-1d63-4def-8b60-fc544042eaa7": {"doc_hash": "285a0ed7b225b038724dad00c1aa55f1ecf30117bf27ac67b9dbe7a6e24bf4a9", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "2d407c38-2a83-4e3a-8a82-c9424dbf9a5a": {"doc_hash": "42e2fbbbfbf8bea4ade6a66c0b312080a346a4707bbdbf3fb743aff0292c169b", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "a27a4f6f-6d75-40e7-b12e-6f88b75acead": {"doc_hash": "a8758061f205eec4943392919db9860c04ec67eaa671f7f7de98702d51beb6cc", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "df5526b1-3875-4a25-8eb8-10ae7adfc9f5": {"doc_hash": "d76f622e64c9aa946088c08cd4406c32765157746ff8fa95579ef2c38f79c2ae", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "fde06822-4570-445a-b422-4d9e65cb81fc": {"doc_hash": "19505efe601fb874661c49e8bd13def107fd4090c1dad45662cbf534f97b4b9a", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "4fb023f4-473e-4a4d-a360-94103ee37a57": {"doc_hash": "9c4e232cb1227a94b52aced7dd0c53dacd1aa35f049dc200e4d09f9876caf436", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "c040cd72-3d48-4732-a5db-4bc57ad52201": {"doc_hash": "38cc2f89f0460674fa8198febb1f0d3d09ed6a15803c2ef12b6bb59f2d3776a0", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "2eba354e-cfac-4c3d-bb1a-86abc71da489": {"doc_hash": "73bb58f2baf22dd1e3f8ae79b7edac413900478b4f0204edbd67e5eeb08b01b1", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "04040bb4-5ce0-40c0-9964-92d424691a82": {"doc_hash": "ab06920408cedb0dc745b7be12af25878d83134810cd40b9c8a9ad3685c13598", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "18c36cc6-f085-4274-8802-77b42302ae4d": {"doc_hash": "3972967887bba3c756f0088903c90654fe25232bdeb39f192296dc5a50faf415", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods"}, "10.1038/s41467-018-04484-2": {"doc_hash": "6636a3330cd5ee0d0d927d9152c1f053c011049579b0fb2298bba7a4b3e0c400"}, "536ef344-8e87-42a9-8b3f-464e6060d2fb": {"doc_hash": "fd2e7da3aad6df0646cef3073365ba16f32486356ae37ecde557f5bf570deb5f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "1185fde7-b191-4e05-95e3-574de68e57a5": {"doc_hash": "975190ee431b87aff0f22cd89249e2cf14111b13c741e44893afc2e2c454735e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "66e0628e-0300-434a-a4a1-1c934d2d3e75": {"doc_hash": "84d4c8fe122a76c59a70fabd0770a64a7d54cd6be7f096539e440500fa8e7fa7", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "c98cffa1-6f5f-46c6-a338-04a6ce47bcae": {"doc_hash": "d0bcfe621c6c8cf8dc1aa7f88899e7aeb0bce587a7fc195527c6a1881334930b", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "165cb3fc-631f-4a76-b259-23c8e922c438": {"doc_hash": "66fba88644b1b106bb5c23df0f3c112cc47f92c8c22517b58b3b0d5bf7d36fd0", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "4df2cc98-fc6b-478e-ad27-c14cbc113043": {"doc_hash": "ca794598b8ca32fc6038f6606b6cd3d6fcda857a46c4d105d3af554961987f6f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "92dc3828-21d4-4749-b6f0-f58d4d16ebf6": {"doc_hash": "b8cd3709fe1777beba2dd03b9f8e0eeb7a3248a9fea4ccb462c0f4b422223406", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "8f607fd6-7d9b-4ec8-9958-9eb7e53ac6c4": {"doc_hash": "5bbe693d0da84999180c135d3e7a863babc4751c450e19849b000a11a68af380", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "cf706117-2f30-4d1b-8e70-4c70d699cb36": {"doc_hash": "9ed7c4f0a1433d068679ce74eaf860a7fa42ed76e481638adcdd65bf3687c1e8", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "60295af4-15ab-4289-8fcd-15c3df4438c5": {"doc_hash": "76cfc3bf686e7a5375f7d4ae0d0cb26d5794338d7bf903b58cf2143fb0903a2c", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "cde84fb6-6a66-474d-b152-d7492a2b559b": {"doc_hash": "7ff27da982291bf455af1a8550a066d897d0abf521549ad5474a7daa4f7ca091", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "e0c9e927-0412-4f5c-88d7-602dd668ac86": {"doc_hash": "3d612d574a74f4683c4f45ef13706929fa3f88323a78c6b9aa8698310467dbbd", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "c920e1de-67e5-4586-92f4-303473cc251b": {"doc_hash": "2bb699a069d3a314ac769bf704bfbc4cb8d64e8fd2bb2b52d2590fd73855e52a", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "7b64ffab-b826-4bd6-b7eb-c7d703fb0b54": {"doc_hash": "ca3d7e186c4a69731ae500b09a8b91dfc806a9e08179ff32132ae4eda40e798e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "d2dd7cdf-1e30-4fb6-aa1c-37031f2c133b": {"doc_hash": "7af31361aaa1337abccf79c4e24c513f590d2f94b293188c757f6b9d6ea027ed", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "bc73a7d3-1d11-473a-a9c6-ce0c57836a45": {"doc_hash": "c49dd622c1edf59f5cec60f56acb83d23875e3f95a87aaded1934f6d8c556da0", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "a08b3032-0ea0-4c33-9e14-cf48b649224d": {"doc_hash": "2708428c112509a15d723491c47c4ae24c0d841df8640de36fcceae5d514d6c0", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "76540f75-6473-490d-9e0a-24bb98bd3c9f": {"doc_hash": "1001af32111d27ce30b6b5f03fb4a10c142411610b14cdbc743ea55cb0e62d84", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "3286bb37-a2dd-405b-a19b-eb4aa9fe19d2": {"doc_hash": "fd728780198abfcbe957ddb07bfa032dbe772067274797116fd31d2e9c374d6e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "504b8543-f5e3-4956-b5b4-054aea818f56": {"doc_hash": "e366b059bb3d27319573746bf191245cc22788c13f5c8039b94677ffa7dfe799", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "fc5cfc83-cc62-464d-96a9-8c0a0cd297b7": {"doc_hash": "2453865af664c0b8cfd51b31ab92f309183f966ff771dcf4150dade80326501c", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "69ec8af6-47b1-4544-893c-f5b3366afb8e": {"doc_hash": "6593528238bc3100e9c112d09b3b31e98d83e190efd768f31f9ff6a39b5f65f0", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "9131e166-368c-4b73-b994-da90529f11b9": {"doc_hash": "ffca9e92941ff9c1e2a9fe2a1ff5b8cad99e99fd5216d0b6da0c0b5c5ce15b07", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "47d3df59-f137-42b0-af4c-cb0db935322e": {"doc_hash": "d2a2d8949b89464d55abb60bce772d36152031597535af477d27ce802043c6ed", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "f1e90641-491e-4fb6-b147-00fe6a52a306": {"doc_hash": "77ec4e85fcbca8a74ece9d4052ba2dfd0bb965d420df67a035401b3d56e4a58c", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "c7db0e85-3af6-4128-9788-488b573a682c": {"doc_hash": "c99cf5d8829947f8319f5be1ea6576087141a245d2abc32acd960d181105da3f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "54da2ae5-79b9-4447-ab32-d643dfe0f5ae": {"doc_hash": "86426ade700cad37d69fb4b87274ff7f291b3b090ff82af729a2063316a5b54d", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "8c7f6748-59ee-43c6-9929-40cdfe9fb37f": {"doc_hash": "eb57c5b9b3f00124300a59eb422debc1b80e6be6ef1745682492574f7e32256f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "e3310836-8b30-40e7-8195-2e8c47ec8037": {"doc_hash": "e721e599df6004cef7660082df6e1c979a568c2cb9c36b09b57d51e1fe742db4", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "ddff0ee4-5311-4775-9ea0-6138d6a11005": {"doc_hash": "b625ed7f35d094c4646c65624bc42be5373c84e3128bafa2f0814938487ca179", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "c4f96757-a6b6-410a-80c8-35247ddf2999": {"doc_hash": "3566f5a3e2d6483aa6c8673ca2062b60619bb33e98573823d4039311c2e9f473", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "259bf45e-5516-4e01-b7dc-551b72742212": {"doc_hash": "0d9c9883fcf460c49c9d3055b24bfbba11f6784024a4f604b9f9ec8481c848ec", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "e856bd5e-e4b6-4762-b2f8-ec13b0c91a12": {"doc_hash": "969116fe666e0508c71b1ab8789617fb15337e022637ff6f2531b1f199f014a2", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "f18737d2-9be6-430c-9e16-d76a6eb0c46d": {"doc_hash": "ddcb85f76dfd5a80175cda515418867ed58251bd5412103738f44044256bba32", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "c71e47e0-f3fd-4832-81ad-006424afb3cf": {"doc_hash": "9eeb36d191f68e197447165e3f43ab042be9a12dfe0e001b15c63c631c4ea0a8", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "be01b8ee-0b58-43f9-a7c5-12f10dbb9280": {"doc_hash": "d6a404d12a4f7c44cb053bb30ee279a5a3ae6a33df984d50b420b6811b6c4312", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "3be32cca-1f81-4ead-be6a-81e55c832377": {"doc_hash": "deac2e95d9d8455cc73d6c873903fcf48e43261c7199f0fa6b39a2cff6e4675d", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "c0b76408-e9a5-4c25-83d9-4e0b7ebf70e9": {"doc_hash": "ea97652f4c03711f9e8abfe0f4277f60f6a854d053e958673eab7161d0784592", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "6411a181-99b2-41fc-9b30-c3ab03677adb": {"doc_hash": "fb35fb64166bb38ca59f8e6d92f6aa230136b582b34b23ee0cf077714f7f2014", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "21b6acde-09d6-4740-82b9-80a89aa41d61": {"doc_hash": "a0510857dca52472810df89352ea0286faf296d0d9c2bcef30d32b0800deda33", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "bd9a2a7d-8b7f-402a-bd96-c8bea4a3ed32": {"doc_hash": "a13783fa6037d0c0264d85719b6cdb0a35f0830b0c8b06127fdc2652fbc6aff6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "c19dc867-d31f-474a-8a57-e3d1a340f2a6": {"doc_hash": "db3914208e69262eb35bb95adc48cafa7351a89579a124ea9a88b99d6d1871cf", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "0b7ba1a6-ae1d-497e-b9e6-0a3d2caf8358": {"doc_hash": "68d6cea9cd62421edd8d4a90daa25865f8b96617121343fe70100b99edd71085", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "182a9b13-3a7b-4083-8c86-a509291959a9": {"doc_hash": "c81d9ec54e33332566f5b78220d99bf04cc03808c10694cb8107d9bb03a06f4f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "2c29c70a-7ce6-472c-9d07-dff88547737b": {"doc_hash": "88ee045172c9297193a20ad379232f5feef0bef7f7a0ebc59318b51a5ad66e53", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "323f77ef-7bc6-4196-9426-d0183681bd5e": {"doc_hash": "f4848e0098c8170ae0d50183769849e3e34896c71f8c512f7dbf400b1866b8f4", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "fadecbb8-3c29-4374-98f7-bc2a855fe909": {"doc_hash": "23bed5d6ce7c6e475501baa6dd1c8aed4d3c380f2e8f252335ea4c54210639fd", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "92aaea5d-b8e4-4510-b616-babdc0d59c77": {"doc_hash": "2826aff917a53464662cbd13d917b327a6790410ba43b8bc82417d20c7455834", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "71f930f0-880b-4c52-ae4a-9e8fd15f5dae": {"doc_hash": "e42f3a995668076e4158ebcc8f62fe6bd7d5469f36654d185319ba301c9e1972", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "0e2322e9-a46d-4f13-b539-91639ee755ba": {"doc_hash": "3aaf8bd6abd89f2d5fc81c06bf8d90cfa9f8e89edbffa2f92d8a35031838ad24", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "acea037c-6e2d-4b7b-a44b-1d1756ff2902": {"doc_hash": "9a636128f610ce8c3aaaf5f27e76504c36bf99d7c1d3c02a4cb0d87d41597826", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "79dfc1a5-b3f8-4ac4-b467-e023bd6f2abb": {"doc_hash": "efb625e65d4f43d1092d55ccdc9211d25011f4ccf7d55800687a579b11fa4a27", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "36ec4795-2693-4d92-93fe-b884bd1d14d7": {"doc_hash": "9b535c7d54a6ccef52057ae227e791666e387137515bfc9a95a73c713f51c016", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "ff108c4f-bcef-4bf5-b316-47c86e4507d6": {"doc_hash": "26a136873fee001108c9e444d5d10ccc7a8c4c3fc0b0bab099a18cae915ef64f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "da9c94a9-8bd5-49e3-bd4e-2abd6582c42c": {"doc_hash": "8a6b3afd5656c9a27ce72953b234d63502297111df7da91f355d13ad56d529f3", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "288fcfa9-ad7b-4df6-842c-87bba2c7fe9f": {"doc_hash": "0eb7b25b075f9f8094ee2f97e532173abc308582d5a9b9160670565e3119c4d0", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "4e37d045-1339-4f70-893e-74a7b3eb9f18": {"doc_hash": "ffce0be5d2c34a93923ef8811ec551e8b847e98039f9a741f2bcbcf5d2bfc3ce", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "ca62d585-1b19-400d-86b4-61a818687be5": {"doc_hash": "06bdbfee25daeb84e42e66bb39892805d5dfee41742d0b3c46459dfef2619132", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "87c50d04-f39e-4ca9-8228-a8bea1179753": {"doc_hash": "ac89d3ce4f9e7143294ed394e719d264a229d2eea7a05795845ccf8cd0ab1491", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "a81a973d-e24f-40a7-8bc0-1dc20fade299": {"doc_hash": "b39ba84e81d8f96d07c7cdf9add44192900d981214aa3d92238008af934bceea", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "cbc44d52-c54b-4635-a402-78c57db51d24": {"doc_hash": "c198a3f53fedbaafbce0c3bb97a90233d624c30bacff37df44adfa8bfb133d65", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "b7f02b11-382b-4227-99a7-b687d909489d": {"doc_hash": "4a04575a306640a64ed0fc5568f5612f78ee8a2ce563e8bb84be75a9d0dd5ef0", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "ffd3229e-9f7d-4d64-8474-80f5f682a9ee": {"doc_hash": "9a72366404180a86212790407b422736a42e3b10682e2699a36b4f47aba1808f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "bdda81c6-7282-4b9d-9cf8-f0f7438de133": {"doc_hash": "1c2279612f75d613f067dc98cac8369a06ecb5e571e6f2f854313ca34f626d43", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "cf9d4265-4d94-4829-91a9-78d555b55418": {"doc_hash": "f5835ecdd6a838dd0c37a4b6a46adf6fb7f2536bab3c9b51ffd46211200e138a", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "3c9885da-d240-4dc5-8dab-71a7fe893431": {"doc_hash": "376ff99dccb14c355c4dc0f722af3e7fc35f56bb0c3302a5c51b225760a88419", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "812d1682-2766-4ee2-ae8d-de8f8c573c84": {"doc_hash": "8c4d30fb0c3fcd032ddb78ce28f78670568bf73eb9275694b2cdb2e15305d318", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "52c140fb-0bb9-4b80-8cbf-21f9900257b6": {"doc_hash": "6655d176205ed8d18e73526de416b7f5a1807772c5b85fa5ff15758f13235307", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "922eca02-2805-4715-8c99-033021baf0b9": {"doc_hash": "d537931c9dda9aaa7ab611fa8dce30c7cc102734889c317d2f320b678579e517", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "5b87c2e4-e1ba-4c3c-b6e8-5423cfeb7ab7": {"doc_hash": "85ecd20cd57d14323160c36205e5a4c98a5817336f9de3eed089032d4789ef94", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "9274bfc9-e30e-498a-bb77-2d987c9fab45": {"doc_hash": "dc5cc1ab51af300cf8675ad4f59884bf9b57ebb9781896a28c597db476046d46", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "ff5c10cc-032d-427b-92ed-c18e28956e1b": {"doc_hash": "73e68de1dfec26abf47f715e4e7e4068e87d8a345ed9c03b20a8b9ec4870ed73", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "d2d97ba5-52ed-4a25-9c95-0509f6204908": {"doc_hash": "e430a1b15bb644de7a5d063436af6f0af69dcfe96841c9ec25a901975f49ff67", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "cd13a164-d1ed-40b7-8e48-7fce828b8be6": {"doc_hash": "afdda9a3c8fecc5a74bb49a93c52c5de593d7476a15872e8b46961a36f909869", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "c7375df0-64e7-4bb3-8f68-49d58f32ecbb": {"doc_hash": "f7cb79eba7242f5802c49e6c1064454adb97be6cc3021045ecfd319600e75458", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "358a3ab9-d84d-4f31-8068-0e2ae55137b5": {"doc_hash": "c0f740cb725bac1bf6e7168b9a92a60982bf18521f7cf89a21fd7c5549866bc0", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "03281470-88d8-4425-ae40-9c2d14940f75": {"doc_hash": "2a32adc59459017d4a7fe1403988bdf663f096736488a4b9dc3ceb33ba83b75e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "8ad2433a-caf2-4cd7-bd7d-b6613918c6de": {"doc_hash": "328addc4ae6f9eb9f5ff89e239adec202e03d29439486fbf9c1505b3eb622e79", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "967f9469-77fd-4076-bc9d-a8858767f772": {"doc_hash": "8b62a97a0fbc2daf9ec70050ff879dd949a44eb40efffebbf3260640f97b409d", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "d5bd77f8-aa6b-471d-9ff0-314f33009f44": {"doc_hash": "8df03da19d8271d095d3648888a568e5023a070aeb7339b73a8fe3ad8b75cd1e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "95b963d8-770e-404f-8ff9-8adc0248cac1": {"doc_hash": "e83f2533aecec783de24635b773b02f93e60a3f3c4fa8dde820da5dfc5fd6686", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "23b05784-67a8-41cb-a160-5ff6aebf8f83": {"doc_hash": "919d205ad1139fb70c474e3a249cba195ef8151ff7f7b6e5dbaed7f186024898", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText"}, "9315cfbb-c202-43c8-9ffe-649de42faeac": {"doc_hash": "8d2730820a08e36e55d1424cef6dc58cdc116862e2a0d1ba9d133224bf68ee7f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "27d30242-4c4d-4aa8-b422-37bcb0a3265b": {"doc_hash": "0755bcad7ae8d1f843cd975054b34eadfa234b6fded3eb2abd3e1a36a578f3c6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "2c56bc08-cdc6-4738-8d7a-ef17a91a3747": {"doc_hash": "8e86967dc6936bc1d55eac2c0cee36e14daf529b7043b1c3121bc5a9626eeaab", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "2ae09193-1dc8-49e9-abb4-0b2a232d034f": {"doc_hash": "30b0152d7fc4c8bb5b5533c57acffb3334859f4b07c30edffa4102edbdf5a2cf", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "10d9dc49-bb2d-4eef-973c-f863b9f8728f": {"doc_hash": "c09775941722ada1f58a468c132c07875dc49a5e0ce08678f396138b448b061f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "a3d022ba-01bb-462d-a949-c537174c1bba": {"doc_hash": "950c9a36f42f65466d712cac4303a75c1710e38c7d03948190c7fbd91f225813", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "ecf94287-c9ad-47f3-9563-9570446d9dce": {"doc_hash": "63c25df5aa685f4b1e44603e37c3cf0414a91a1ec9aeb70c027c3d3ead5264b4", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "dd8f4cce-e318-4425-8b99-74a073449928": {"doc_hash": "46f8559332e70918e6cf603061737aaa0996391daf1a31a283ec37e225cadce8", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "f1004139-76ac-4192-8ab2-db2e5335eadc": {"doc_hash": "60f12d91c77e989dfb6399a17df0f2a93f818b0a9ed80bed5047cc14587d9f93", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "9e1eae16-775e-408e-95b8-11187a140a24": {"doc_hash": "5664d1109439dbd2f076a32459c8f3d717f7dca54cc8083246f6f862595dda82", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "c0959c3a-c953-4561-8247-19e33ccf2761": {"doc_hash": "d313d42730a0f9f3853d95ebd89e9cb3a44c118f675cef5803d6630e4b18bcab", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "9bbaa51b-e138-417b-9958-9c9a69e0da00": {"doc_hash": "930b0fa761e2514097944fe44111aa3a8f194ef9e36107216d35e15ecab80446", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "258a74f5-76f2-47b7-ac21-46fc4e1309bd": {"doc_hash": "cdfad4dea055372cd329ca584d032ce7e550ae9b56ea314869c9ab21d3d529b9", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "5f64707f-be8e-4102-aaf5-2d5cb0eda120": {"doc_hash": "ba134e939e31a414ca07460f5ea388880d15f945be527446db04f74fe65facc7", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "7655e661-672a-421d-9b4e-b152784cd4c9": {"doc_hash": "3233b511ca2e7ca0e0c2543200986b07c9d9f257e3bfdbca8d0739d365aaee12", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "2c7a3f89-0ade-4ade-8648-c33d8b8c0dfb": {"doc_hash": "4312edd800264fdf5b53ab4e1d292c6fb2a712c2738fda5246fb1286a58697e4", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "be30074a-f899-4826-b5cc-484370ce5dd5": {"doc_hash": "bc7e2a3a42001d03a8a892bad639b824d56062b696e3ea1683196433581b5d15", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "7d112972-09cb-4a3d-8e7e-8ab58bc4ab90": {"doc_hash": "eb8b5699f5fdcfac549f9a973b26adb88117fcf292074dd164f2c0ef85f7ce16", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "4f208bd4-4dbb-4411-bce7-827a842a29e1": {"doc_hash": "8dbe47ab4c966c9ed26924057012fc92161f4fd6bf98243d1b45b97c81c8e4aa", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "13663ab3-c81f-4cb9-9074-7784689f73c3": {"doc_hash": "5e5896846d65d14f3c85a20ac931553908f5e967a7cdf043aa5c99998bc364a0", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "6b297d62-4349-4174-921b-e1631ae968e7": {"doc_hash": "439b924d67ab9237267a7a69f3462605382f773859d5f705f2d836117d15eadc", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods"}, "10.1038/s42256-019-0089-1": {"doc_hash": "e0529fccf1f26a56b7b5e6ea29e1b6197d0d3c56bef0195607a136928fef494e"}, "653d640e-3b26-4d0a-a02c-540557ceb834": {"doc_hash": "209086a3acb68c7285fb01e16cbb4c2ce196ffa92fc718b69a5fe26c0939f5d1", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "bd2537c7-0e85-4c8b-983f-e8ad539e4230": {"doc_hash": "cd603324315ac6e6fe371cf200491d0d724b04414d08ce6f163b5942136ca1da", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "f917288b-5cfc-4872-b640-40623ce72f6c": {"doc_hash": "bfc2a20c2109d618422eefb28bb5db00f47c2eb9fa50d64d67dffeaef6effd6b", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "8f6bbcc6-e4c7-422e-acfb-62438c84b885": {"doc_hash": "5013dacb8db718a2e762980216f7a304e4da23f0a4d57102c55316288d8e789c", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "ee6ca9d3-bce1-4235-8990-2726bd2659b8": {"doc_hash": "ceb11f35e220a9f68059f3844ecbddb680a0362932d57275e25ea3180cf2adaf", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "2b43bc75-11ea-4abe-b7dd-ddb3074a2255": {"doc_hash": "e49785539e1061a422b39557ee65bc2af7c1d6a282a4a4d7928e2a46052a3399", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "5fbb6874-d5b4-4ab0-90cc-0ac8f1558ca2": {"doc_hash": "2142bac2257eb2e60a74b296a5fa60b9951281230678888fb71c4c7ed5315679", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "65c79469-0da8-4033-a9f8-47ed14779a78": {"doc_hash": "1ec7ee820a29523c0d38b0c41d90d2c6f87a56f3c85da953b866576480fe6349", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "37013a73-33c7-4234-ad58-5384ea2073b4": {"doc_hash": "aa3a905a7c076c944a11aa3de8acd0618074b14d81bb62cff37f3da85fa538d6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "66a6267c-c7d1-40c5-b758-a556fbd81f81": {"doc_hash": "3e74e10a992f5456a83cd5f582a990dc27380cdc6911259db2099d539820dbf8", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "c6aefbcb-2b38-465d-a642-bc3904f2a37e": {"doc_hash": "75554dd107beee1a4d2b36b61ec2d2f5707eb69674ea5d75069a6c1f79a32fca", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "20e24256-4f4b-495d-834f-827477ab4680": {"doc_hash": "25562373cf50408b5022c959a296db7c1dd51d45fae7df0afdff9b7c946f7290", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "6f034d87-f9b3-4f17-a85f-62484b63ca3a": {"doc_hash": "7c7e467608367089f09a6387ca6499082a9158b647900febbefde0695c3ed597", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "bbf1f363-f629-4d0a-9f84-5ea2b7e46d95": {"doc_hash": "2e803b73f4117e21fd3ea19b0ae35c59495e33d2d45176d9060acb756291ea1f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "c59ba73a-a77e-49a2-a465-40aaf8c49915": {"doc_hash": "fc9342ad3ace48e83ce311c4bcc5314adb4d0e23093e1c01f6d5721092f24cf9", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "e7143612-55a0-481c-a46d-6878df23305d": {"doc_hash": "fc9d70216324d42e4dae97b94597f3826c4f8b16b82dda65d31d49db8651ffb6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "711754cc-08f9-4303-bea0-da807b32be97": {"doc_hash": "bb88532016a2e3becb72c54a2aadbb21fb49973435509249fa18e48d8faf6d7e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "8cad45bb-ee52-442e-ace9-99b8a2fddd24": {"doc_hash": "26de356861bf07a81d30c7738abe63b821c7b66ae41c933273a72b16256ba89f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "8e6df688-51de-4237-8883-c5fbceb88279": {"doc_hash": "3d8c2b0ae0928e63578fe86d78daed9485efa244976ed47edab70dfeac8cb1c3", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "667a099a-d403-4699-b667-eed09ba6cd24": {"doc_hash": "347a1dfa168b403bf216aa9bd3d74546f82d3745d7a3f5ac8f86806c45bd30a2", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "c4dccc2d-1b59-4438-b656-96c99e766059": {"doc_hash": "041fe7c85d61929befb45b323f8ceb1fe8a71007c7355018db8e323e38a5027e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "9d3e14b0-0f20-4535-9380-b09c4592ff35": {"doc_hash": "deef509fed52d9e09a13f1d5363940a35f0ac436cf0fa82857059f15126bd754", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText"}, "10.1109/cvpr.2018.00286": {"doc_hash": "a3ff32a182d7e0e79983392c13615f87637026a50853846fb660865c73ffa34b"}, "8b38de36-948e-4614-99da-9f15b9629e60": {"doc_hash": "8fd72bf03909293bdee723e3c679778e3f45e020f3b066d37ce5588ef318f1dd", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "45414ee3-3cc9-40e9-b846-d8c2a93d0d31": {"doc_hash": "c179c091830db3bb8d5b63de8113fd3fb28fabbe7c45db3f24bacd8c15f30bec", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "7ab3370c-75b2-4479-8a10-d9d83081db08": {"doc_hash": "b4ef1c5791007110918a85b7d864ccec442677f885c1aeee8eec6c2587100c26", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "b37585ad-1e7c-46de-916f-52c239038c42": {"doc_hash": "4316ce461073a969a89001ee024c20544c59e27adaf2db1b44f333c578aa0182", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "8d1a2099-19be-4747-b251-a655a6b55164": {"doc_hash": "eeb80144711c93ea6ebc96217383c4b7fc6eda7c734d1264efa1f82bbccbfa93", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "c2f4d688-296e-4551-aac3-d8afab3b182a": {"doc_hash": "d43d90d607f392c77cfe486ab5912bdd4e1baec8a1a6a0b065fce8bf0bfd273d", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "fac5e889-761e-49d4-bbb1-31693cf7d5e3": {"doc_hash": "96e5343cd8fb1e5247dacd420179bb5e1574b1a0a64f49be09d5c0f9a929d891", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "79b56430-ee52-43f4-84f5-fa57cb3dee54": {"doc_hash": "f8ba780dbd2245141644be122a73451bd365bf40966b463bafac200f5433015e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "4565c392-9058-45dc-89b9-d22d3b261983": {"doc_hash": "99fc73f3f6bf1061f91c395b054bc4c2b43a39782e462a0d66dec61a68ba47e2", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "3769e20d-58e0-4c7c-a3b0-ff460a883fa2": {"doc_hash": "0ba6873ed01a7ac0620642dbf0520f02d137263a74f8097fc7a9b3caf5540563", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "c87102f3-c101-43c6-8bf4-3c4a782d59fb": {"doc_hash": "45b1f34b27027406f87f7d8b2425ce89e3b8e36274b43bd241a5640aeee6eef0", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "3f6585b8-6463-4e59-9e00-bbadd8d8dff3": {"doc_hash": "f8fbf9e2cd97bbef9d705bb044e03e1e3843b2e3a30caf965d571cb4071bbfb5", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "ace8c8e8-c109-432b-87c6-c96dc89c67f0": {"doc_hash": "15bddfc7f52936c146d2fdb7272039656fdae23e604ae7065e5da1e2f2224c0c", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "0d13c6c7-3d6f-4d9e-8313-a6924a50c1df": {"doc_hash": "b8891e198e004d3fa936eac082e6e5b628447ad3825218199b23542fc5b916e5", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "d1cedde8-107a-4825-92c1-4ffce42e8681": {"doc_hash": "cc91437715e1cc80340b4be7fe336457619f2310cb153dd976af154271fc7082", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "cb26e56a-9512-4569-a49d-9e4b5d688040": {"doc_hash": "522697a10ff6661a501d4b6a2c028754867ab068a8a2393ca390037c9e9722b5", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "efc2fa00-4b18-474a-9839-6adaaaf173f3": {"doc_hash": "2d27154e426821841a4695d6f761eba37426a1c7c34426358922e7ae48b3f7ff", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "62bff64a-b30b-4d7c-b871-a7555a617676": {"doc_hash": "4bebd8f8198a17bd6177f0cdbcdcc02e26d90f9905433f8abd49ce41b81ccb32", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "ad6fbd61-69c5-4df4-97f1-d5e8142c7c8d": {"doc_hash": "ca15e5e38b8ac19c10897139f9ad94b57e02cc6bb45521db1fbfbabaf437424f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "a0747f83-f727-4f75-9b9a-2fbd94c61dcb": {"doc_hash": "448dd477644233fb9c3ea59a9937d3b303e4a87e2fd08154d43f75fa3eb29bbe", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "057a69ee-cc37-4a44-8ab5-17759486636d": {"doc_hash": "0707a83c69b7c899c1902f89da0c5d981d073ccaac8937979e22bca7b4e90278", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "a0be1da4-24c3-459a-a68b-ab0afc354bfa": {"doc_hash": "57b8d73336f7c3b7a4001be2e399f8724fb5ea8ca295e1c42bda7a78ce63b374", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "8b6f05d2-d2ac-42d3-833c-4406fd359c30": {"doc_hash": "f8f45bbd366ffcc9ae65d4bd12077148bf393c7cbabfbdeff79d82d3cde62568", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "8a2fe741-2ac4-444c-9594-ff5556bc5d49": {"doc_hash": "af2fca7ce23c58c17d6297f1ffa2c97d6bba2438616316d31ceef6b6a5eb2891", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "f5855de4-e6ee-4b3a-be83-26e3edd84711": {"doc_hash": "306af3c8766672df382841f17ccb7a2c38d97b396d790acb24791213e139fc8f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "3b883d14-bc1e-440e-8ccb-deea7b93d18e": {"doc_hash": "fe15cd59ca14e3677e01e599cd0385d717c56448ed0697262f814f1900c39d9e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "27d9d5aa-7553-41ac-81c8-9945ed56cefc": {"doc_hash": "c1bfada8b31e2750daf5d0c1331e3fae1cd46d635ef950f78ee3762cbf879bdf", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "a20e9cb2-ae72-48cb-89c1-ff4a78e825ba": {"doc_hash": "4b2c29dad94e45a893cf0b5bacba865d89b8a383a7c70a4402c29024524b92a3", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText"}, "10.1109/iccv.2019.00495": {"doc_hash": "42fc109db8e461b097a946c2a6d0cff793cf08b38c1a1756835ccfde2e42d574"}, "faf6e3e1-b949-4e5b-8f66-1a4e4cf4665c": {"doc_hash": "c59a62ce2dd3b5b44446940a6f7dc3619657b62473cc3df17e017667e05ed367", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "9fdd2b41-08ae-42c1-a646-a78abd8d4423": {"doc_hash": "4648450a1f519a5585c52fb7a697caa52d3dfdd59917a016b627effea8860cea", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "4af9d98f-6d4a-413b-a65d-ef8634b0435e": {"doc_hash": "36783edcc7452b611c2c7d318a08a84fb60ad5eb9b248c3959d82d226cff9e6b", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "ff3cc678-711e-459e-8766-64add4a5b79b": {"doc_hash": "b858bcc7af9ecd5b629d8d29e3a519f558f5f1d995d550839c1c2940a86b6c82", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "b546bc00-f93e-48a7-93ac-786f239a61c7": {"doc_hash": "4d8f3beef71601a1868963d77cb56b0620f0ac6737a1bfed7f53bac8021558b4", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "aa28498f-7e78-4bd0-a63b-67347822d054": {"doc_hash": "60718fc41ea2d64db53235b1870873a6eaf7027bebf384018265afa6a841f9cf", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "f5f0c874-9fea-4ed4-8ad3-353dc53704d4": {"doc_hash": "ca0e36145ae26f60ea0daa0b1d7e1a9815c460eb9b7783baaee29445f8781e36", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "856900f2-2a59-4eb8-b43a-b3655765c66e": {"doc_hash": "a5cf94b2310d8a87265ba7db5175838536598945bf8f609de3fa738ef3268225", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "442a996d-489a-4fc9-8512-8aab841141a0": {"doc_hash": "c7a02b638eb6f36285337b4e57438833aeda8e5d986a3a213711c583a743d8d6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "b58461df-014e-400f-8a3b-b7fcc899b4b8": {"doc_hash": "ce0920ef2b150e938e62e2470bda5eeeab0690ccf1280935becfce7b93fb155f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "e065c41a-5c38-4da7-8316-3a0eb1f37f4a": {"doc_hash": "8d668e9813c9d89936c12886f2f2372dcfebfb890a098df322d16346220e31fa", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "8ae8975d-4301-43bb-a39d-f9e02fc18f1b": {"doc_hash": "070b8db8efc8641e0a6e001ab80eed6837399e6e387403b3fc046fe0c53593d6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "91c75266-d293-4741-8856-db890e62e5db": {"doc_hash": "fc7bf9c45b8608cf12831dfbfed8e2eaf0d7438a756bf3ce984b71648a3c6460", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "0a586f64-69e3-4ed5-b217-246748cade34": {"doc_hash": "982ca1e6b6fc73217b7f86a11fd0804b531698ac7a2f5982f0ba782dbe0a2489", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "5556efd5-34f7-47f7-9c0d-cd10088a77da": {"doc_hash": "bbdb5e7c69aed3c6b25212e2b6df14d4c59230646707fcee414f6a7693b07dff", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "db041908-6a1b-49de-9e04-7158a955bbf6": {"doc_hash": "d35f37e052673540e52fd3b992e453b751ca2a317a5b303c234da42db8bb9178", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "fa56bd0b-aeee-4add-85fd-f1fddafeb944": {"doc_hash": "d2c182d687b3f6fc10103d1dd63f465264651f6354ddd7509a3440d5a3635d84", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "994c769d-f908-4f83-bdc5-6e732cfa53ad": {"doc_hash": "bde7551c12427310d7ac70c0802a9ba5a1c9dd50e2044cc09cb2ed1b29f4613d", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "97b6008a-66c1-4b50-bef0-2e8e54861b75": {"doc_hash": "cd27fd63cb6bd93f7b8feb4069e2e58adc25a6bbcfc520821cca0b7fe7a27bf3", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "f0954f18-142f-4c95-85c5-1e4596235197": {"doc_hash": "04f4982c6703e1be8ad9e321ec3bc1e770caed13467456804cf6b8eef0599989", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "1ed6baf4-44a7-4373-a63f-5f128f098efc": {"doc_hash": "be50021457500e058b1e6e107d952b0ac0b4d9b37f4e4154c47fa7851bdd38cb", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "df88479b-51f3-4218-8af7-478303a1dacb": {"doc_hash": "104795f33a88868739f3c05e967645222a529025aa8dc680d3172eafff3c4634", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "256d878d-b29d-4ce1-97ae-f144999fe5e3": {"doc_hash": "8f348ceed183f8a42d00b9dcf838f8d48578fc315afd5a35ce86404f91be2d5f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "1f5148c2-980e-4cce-9dc2-f06f3990764d": {"doc_hash": "fe4cf47c2ec51a862cf91599c089a5b19c240ffaa0c37f72d4e60acca1cb57ad", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "a4347006-e63b-4773-a42a-25cdbda90770": {"doc_hash": "ef1f94d48fa0c097a1c45ffa747c2ddeaebf70008d221d9037377f2f7ade2c47", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "d8650cb1-e3fc-461a-805c-329c9560f394": {"doc_hash": "4ef554d509e01e9c6c13d87ea5ffc2f7e8bb74cff4b7e8017a10386059989fa4", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "c7cbdd57-6a2b-43c3-bb41-9b55d76c0f0b": {"doc_hash": "94d1cf5b19d661d165a8e175c1c5b17a671e91e42c61ee8a2637592846ae146c", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "fd9bc8e1-56e3-4a3e-9e0e-0ba5a42765e4": {"doc_hash": "b4062f8043bdaaab0c7009029d3862920f129c8d2745c4756c1c808b13751833", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "36fe0d16-5f90-415d-97bb-8207ca42a491": {"doc_hash": "c1bfdd62b2e10c73b9021b7068b7c5db171116864d1313a9f04de537b6ab5c9e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "a1ddb1d8-d66a-443d-bb7e-f27adb0569d3": {"doc_hash": "3160750a4a344fb3d230cbe360323b40ee37a391380ead66e1c0a608967d3fba", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "9e1c208e-d993-46e0-a0e5-848afa168ce9": {"doc_hash": "6af4262ec102dee488d9a2bda5b88457a2404dfc2f7c8e6ac26f52cb486eddc1", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "3a65598f-d600-409b-b08c-4f3879639fd1": {"doc_hash": "6ed48b7e1f7c26867bf46b1808999438117e48c7a075793c6e18f20d549237c4", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "ee9dc51b-cb69-4e2f-88b3-03eb228aea79": {"doc_hash": "f6a455d68c7ea12d67e365262f601f0145dba572e76dadf7126c8b13cf7a537b", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "ed16d59b-d284-411b-af81-dac6b38d8325": {"doc_hash": "3741451cb668ef6d441cb64dc17befb6ede04e863ed3e8a84292dbab72c95629", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "b54be825-f95e-4cf8-b261-51dd9f7cea01": {"doc_hash": "73a7f8736a4c965c1e949d19cdca07e11a11e62182ac20b0a06ae3a9bba43beb", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "51f9ac20-bfcd-4b4e-8a64-e687f205f918": {"doc_hash": "bd0e4332afdab5891e6a8cd67967df351367efe95996b0308ea7f7336bcfd3c2", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "bd3c46c6-0611-4a40-b81e-7ca3240ad115": {"doc_hash": "002d44663de421b45d4ac5b3643f2fe834101597b5f8af0aea28b2357768ecb1", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "d90622b2-db1e-4595-b45a-88021548e613": {"doc_hash": "9ba8646a745debf1d4e12681802d1e3a68c3f6d9b3253f6a93cd211d0aa65685", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "3aa7adb4-b40f-4a03-9b10-5410ae180273": {"doc_hash": "2b5da070a46afcf8f8145fbd8b5125ce573326fa294f2427ba30803fbd3a68c6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "2926afe7-549a-42c9-b99f-7a235508a9af": {"doc_hash": "b72f01cce660a1ab003578ef15137b677f37d6864e8c06492ca93e34234f9917", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "f7cdaf08-647a-4d36-b6b2-35b4bd5ab93b": {"doc_hash": "1e46a58e6c99d15bd83e8ebe4b4feeae63682e7785702155e49b42cf4a794c7c", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "6c41a97a-bce7-43b4-afb6-8b20b267aee1": {"doc_hash": "89835e6841be6e448634497bdb2cc5a67a115b05085b9e8a5d58a10228177ef4", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "79c5db2a-a4ec-4b81-aed2-97905f7968bc": {"doc_hash": "5705316f52e543804fd2eeed2656c5055df8e4015d8f86473e2ccd4b0e7e8397", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "849f070a-a09f-4577-9e94-b57a991af1c7": {"doc_hash": "a30641307bb90aecaa92a3e651884ea09d2eabe3fd92a689edc519cd4027152a", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "2a50d1ab-c431-4d4b-90a6-2b1107ee143e": {"doc_hash": "23e748ec393edc01b07965c32503646e4ed3a849bff4a05347f55ac4b4c83cc7", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "7cf8d3cf-e074-4c78-8769-f7254411e3c0": {"doc_hash": "43221de08a45fa634765c04c14d6733811b051b205a841f0d2b41c23fae0aa87", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "5d9bd29d-615c-4477-aa4b-4832f9fa14a4": {"doc_hash": "6025e8aabd481ad9160720aad7fe2f10a7c63fe5fbbe143ccda648a4d966ec87", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "987df21d-13c3-4b91-970c-94a91a2f82ae": {"doc_hash": "5e7a353a5f375b3a5d6396ca770af2f3a77c2c00bd665f063c93e7de2f1f6474", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "d6642526-3af2-433a-82ab-f978c2caa3f6": {"doc_hash": "0af3b54a43ce809ca2b37cbcf3a4441b675391c875717fc9809378ce640607b5", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "e6d4c5f4-3cdc-403a-9c3a-66ed10ce59a2": {"doc_hash": "e0bc4ff71e40a0294eccce832b1d828daf3dbc01bea0e7f0209cc23bbeb74465", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "c5cc0672-8f70-412d-af6e-7ca91249b848": {"doc_hash": "3b95b2e11ce5002fc5644973d7615b315341d82489f56f509dc32943d046b79f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "c9867c00-0d52-406c-b6aa-4c4487e64bff": {"doc_hash": "37d08066eb909bac8f9db4ad0381bfa137752e1f6f41e55073cd80bf09b8e0c4", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "d3ff5b87-96c1-45ce-b887-03ca029cbe9c": {"doc_hash": "5c55a820d58c4ffdec85330a8720a548d617b1426b8b66ef007d5e7cd81576dc", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "d8bc747e-7478-409e-8b53-b16648b4e708": {"doc_hash": "0a19019d3518480290e13085851deb4f819bdc39ecbe3fe7df417f84182b032e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "7f3e12e4-a8da-464a-8f39-3607ca063b9f": {"doc_hash": "0826edb8d768403f7de06e336b9a4cf38ac2c5034a9b4f199833f18baa720a76", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "b94611cd-0027-4160-a6c8-46b243d25ab8": {"doc_hash": "36c8c8c8ba76ddcdc474e8048c163bafd10c21fb32b1ce00b18dafa485c51dd9", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "b5879588-e82d-4d60-8598-36520dc89449": {"doc_hash": "00908c88458d2091cc194a3104897999da1faa4735803df1c415875d2c9596b7", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "3dc2e943-b385-43cb-a390-4db54b5eb8fc": {"doc_hash": "4195cfc3fd86b41fd20d55ca8397c3f293de73b5ddf904baf76055006e4c27ba", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "2c528384-6c68-4c6a-9a11-6de467c656fc": {"doc_hash": "ac2e39dd7322f634d8365f4b9a8adc27ac6d55cea3dd327b13dd821032106e4b", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "81a437ce-4410-4a79-bdbf-d39059d0eead": {"doc_hash": "a2ef45fab82c26b57bcde69d02d6217b9403532131343975a982d9a750853f6c", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "4feffabc-f8b8-4783-b457-15644ca8de40": {"doc_hash": "f2ca050db5207dad7e9052bb3598d6a75dd2748ccb243ab3e3883468e837cfe1", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "b12b2e54-0c86-4c24-8791-746706842a2e": {"doc_hash": "0645e3ee5f5777c6527ea09976a86a3b96f1aac3b2c8d2834a5799a630c2d4fb", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "12b7697b-71a7-4392-951d-24cd554528b8": {"doc_hash": "5c5131c3c905c7b38f4f4069157cc075a9e852d05311488ab5c1477eb04da798", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "a1a84cae-a62d-45e0-acad-09c984d7a470": {"doc_hash": "1a41bbc5eff2bbce73d4045a6d798f0aaedaf4ef77d655d3aa50065678779a81", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "5a52ec5d-18c3-4107-94c7-660e459a6692": {"doc_hash": "15b4fb1558edf776fa6fdf7e525e40af6dd2195242afa632810bb21c62929abf", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "dc021ad9-7922-4548-8e04-f17492f46b7a": {"doc_hash": "3d35e05db9c77ebadd5987a017269f44b869120dd6dbac14d7dc4ba7e098beb2", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "e99832ef-4ee2-4207-80ac-3ed6607927ce": {"doc_hash": "6e7c66ae3838816024cfc2240b81ea7839a92bcd1463392d40471b2c096ca4a3", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "6cc45d1c-43df-4b9c-9462-f4dfd3e37555": {"doc_hash": "5a434a6a65a359d88d4f4637e0a22979dc8d96b736690cbbb80c845c9a810cd7", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "908a880e-de47-4c12-ab9c-e57b74a70e64": {"doc_hash": "27c5646de50c7479aa8d54479429adcc070823df846d5c3bf40919fc71ad8ba6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "a8d55467-1365-4c2c-a007-ac7303027784": {"doc_hash": "8affaeb68fa90297b14743a5183c6eae36cf1e39938ae42aa881d324a9e97836", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "80591771-6e15-4c26-8c9f-200b0b14708e": {"doc_hash": "9add70a22eae2c1dc3225f81e2e031e3d27a0e6c26771448fcfd62c1a0db8388", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "78bfa7e3-a4f9-4760-a73c-714c181f6716": {"doc_hash": "8b3daf8d53b5d98099059bd96248c8a7ab9e2378b26dc07f5354406dbb69fc9d", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "4d94de39-fa51-4cfe-bbb0-d287f46359a4": {"doc_hash": "c2815c33929525933de7db6c2c26fc953871f6363b829c74860389fdf69e59dd", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "45dc4261-9b7f-479b-8128-002f34667159": {"doc_hash": "07572ac7c956a6b646e47d9f63847cdcf5bb291e7fb370a8efc1b1f627e4e993", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "7fdb303f-f149-4dba-a3af-cb1e43bedeab": {"doc_hash": "ca40db0883bb34043934690a1ad5c0b932d27b56691f6cfb62f62607376fb5fb", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "6294cc3d-8b02-42e2-b564-accd8aea215b": {"doc_hash": "3d48c4da24789fe908a37188239cf4620c44bd4c0d7680e73d57da747a6a67b7", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "b5024c39-42e3-4b44-9a73-0d64a77e8bb8": {"doc_hash": "cffd7c9a90021e529db8d3386fb08a2f3db7d2935602929c4b263f1ae479af0e", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "69fd70be-78c0-49cf-baf1-2f0dade331d3": {"doc_hash": "df62907c4e373da252213f129c0090a3d54345ce3d08a63d23342d719fe27316", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "d76b9d13-2d70-4d35-80bc-64031c2e783b": {"doc_hash": "5948b81602d48f2a95fe3f0f5c271a307c9040679fba0d964f349c227bbf04f1", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "6ec54057-8962-4387-90e3-dc4fbad587fd": {"doc_hash": "5de92172c727e839bc79de1011021459b269295a939704396301cdb44e7145a7", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "45e7927a-ab5e-4f00-b4d4-7dfc96d8a5aa": {"doc_hash": "36db5aa7856038889d61001109e471312336dffad0999bbe7d984800d81da5cf", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "d5b334f2-f58f-49f8-9a54-e2582d5890d1": {"doc_hash": "8d9f55263a8e104aba5b73678e00b8cb37730041c3878230a988dd9c479878b3", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "fd03f1d7-9675-4e52-b49c-561cafc0d49d": {"doc_hash": "d5a662cc64cf4549d2fbdcac8611c529bea763c73003794013aa2c5c1ce8fcb6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "51cd05e9-61cb-47c8-91a2-5a20f9d639a9": {"doc_hash": "efa5e1d150c1e710658cf48afc295a4fd6d283a3cacb3016e727be22925d527b", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "6f766b16-d587-4d22-8e6d-57a8b6fc7d1f": {"doc_hash": "69ada83ffe30516d990b7320ee31877a82f9a55d6dbaebad6d25be07d4156f7d", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "bca7f5d9-6240-460b-8830-4ed286911cb3": {"doc_hash": "a176e0a134a93ae8b5705a1afb7ea0877840875a5382e37a7fe26abe9b4c6bbc", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "5f8cbf6c-781b-4a56-b30b-fe46b799bf79": {"doc_hash": "ee10b819840f46df0ea93f234f9abce88d72f5dfe741c55057e442aa04b58ec6", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "63a49a88-263d-44ac-ba3d-248447fb392f": {"doc_hash": "a4bbaa222cf50d7b659a59ee8afd5b7bbc9ee987722ba5e3d9b5de87dcfca90a", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "7e73f9de-3297-4b2b-87e9-1851a348105d": {"doc_hash": "e238825bcdd1fc88e7d2555ecd224b04ea8d37990e8e3ec3cc075884618c9295", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "4ac298ca-3662-40af-9534-e324e48d3a11": {"doc_hash": "91fc9d3a490b237fc0bc4f4cbaed0bae3847fb960631fcfd3be38bd175ae19f7", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "92d31c41-a3f7-4951-9db8-5dcec36a74cc": {"doc_hash": "847ecf212ad6767f96433525eff8f41ae8768d7998bfcc2e140b45c49badc41f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "d890d1e5-b8fd-426d-9597-93e0af0b44b7": {"doc_hash": "a7a68bf4f561c6a165ee34a4bddf1884f6c71e8c2b6f359e56179a6e96dfd834", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "735065a3-b136-48fa-93ad-ab0ed1a35b90": {"doc_hash": "e125f853204466a2c62a6de992db47004c77a21a4ba8c89bd8bea07c910aee6a", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "6bae563e-1d68-4634-b4fa-48292d321a02": {"doc_hash": "99fe961fe0d2f04ac03ad83497c4c6b3269148942d3f913f2c37b3a084d1ec3f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "67fe8c25-e9aa-4693-b753-b9d402bc7bbb": {"doc_hash": "142b4dbe34153ccc16e36a7665523fb3eff3b1c74872cde84127a346ecb2e87c", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "8e1cb92f-1d7d-4352-a448-7a32f8d86b6d": {"doc_hash": "500e7a80a132f1d77d2202b56d9e66df196b105e1db996ae3fafddd79f669b1f", "ref_doc_id": "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText"}, "10.1007/978-3-319-46493-0_32": {"doc_hash": "7138be6d31e6ee3ae05d01b5c9d6920244790a2be5564c4b3a4623fbe63303d9"}}, "docstore/ref_doc_info": {"documents\\shared_papers\\\u8d75\u61ff\u6668\\\u5f3a\u5316\u5b66\u4e60\\PPO.pdf_MainText": {"node_ids": ["a0caafa2-03a1-459a-89e1-708df0be7966", "2929254c-844e-4c67-b6d5-6e8bd409bee6", "8e0761f7-06e8-4475-acd5-025aeb072fb7", "6bfc782a-4235-49bc-966f-0b279ec4e6d1", "04666a9b-f40b-4843-804c-80d445aedceb", "c051e5fe-53c2-49b2-93be-866ba68773ac", "62315ef2-ad40-479b-9888-6c077637caec", "bb403b59-c170-4877-8a45-6fcdb3416f3d", "4aa3c1bb-2890-427e-9444-efbf4752b9fc", "9a7a7070-481f-44e4-b23e-931b8fb814ac", "3bb61a03-40f8-4bf3-ac28-9e1b5f349e87", "084f6aa2-2a95-4e36-ae95-7b8bdb900fca", "89fa1b5b-ec50-470c-882f-f81ffeacaaa6", "5c0bba9e-7133-4636-ba81-f84c210bc32c", "eac61337-ab4b-43c4-b972-ce0b9ed21ca8", "a922a896-544c-41b9-aae5-36ed21bd1c2e", "eb0b5c96-3abd-4026-8d8a-bc0fd325aca8", "7f715b14-44dc-46d3-b48c-439335722505", "6ea6e0be-82dd-4b21-8f07-8679f9c3cd63", "f7c3a139-ad6b-47e7-8515-6595d9d30330", "7a5aae2a-3bea-4c7a-b7d1-fe0148de4301", "16598399-70e9-4bec-bf7e-c884323b1790", "ec79c6a9-2dd3-4c95-aec9-3949c24e085f", "12501e2c-d18e-45d0-b4df-b04b171ac8df", "a666b670-4745-4078-a0c7-27a7e13a0565", "11f6fd5b-1ffd-4c0f-a81d-385b5f9a29ea", "80a0e15b-3324-483b-83cf-05ba7c5a709c", "25a0d61f-165b-4bef-be9c-2ae95bf4b427", "a2b8cfaa-16dd-4daa-a610-4fe00cdbc1f6", "80f926a7-357e-4b05-8df5-0811620a625e", "7d957296-e747-4823-8909-6284d238ef93", "27a0b647-3732-4092-8b98-1658cf568749", "9d4af09d-ff11-4898-904d-d6ec9fb04372", "c62002c7-d883-438c-aa54-5c80f7c1515f", "a044f4c4-f33d-49f8-8ae9-00c8dc0366ab", "56a890c1-62f2-4950-96ec-ef865a7028da", "d47d970d-a799-488f-8c02-ef44d3547b43", "c286534f-0ba3-42de-9e00-271c4aae63ff", "a5645c50-741d-46fa-8745-425d711dd8b6", "8b29399e-6a43-447a-9b97-3ffc55928232", "3c096904-122c-4e70-8ea0-f9522b7f06a6", "b2628aef-f602-4142-ab19-51b9c291faf2", "a699796d-44eb-4ede-8c66-3b790c2117eb", "72e1ac06-19c5-48ae-bf7c-821a8a508093", "d4789cdc-b420-4538-9a12-7363e468e8ee", "0189774e-86b0-4277-aaf5-98cffdfda3e0", "383cdbf4-b658-4f92-94e4-c1bdf1d79a9b", "7ec73f66-494f-4a41-9ba6-0d7a9cde8a8c", "2952169b-ea4d-4e3a-b1f9-a8deb75d3136", "f3d71d6c-29bf-47fd-828b-99ac385c81c3", "29fc2a74-fbc7-40c2-a0fc-84d8c7d55e07", "d88c2feb-d935-41e5-b5a0-21e8444ed341"], "metadata": {"node_type": "chunk_node"}}, "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\An_In-depth_Comparison_of_Compilers_for_Deep_Neural_Networks_on_Hardware.pdf_MainText": {"node_ids": ["ffdeaf3b-c475-43e3-a2c0-0e0472671663", "3f463d6a-73b7-4257-a0c2-a03497302442", "4d9c9c93-9028-4a08-8da6-6d1ab15344cc", "25f5135a-4134-4374-9d01-24c9a8465078", "ea2469e4-fcb7-47c1-adf3-357653468b6b", "620ea7d9-283d-4ca6-b2e4-cca42871147c", "b67bea5a-52e5-462f-a205-bbbf673641ac", "d23e8c7d-60bf-4541-b639-33eec97ead8c", "ecb2a8c5-dc92-4170-bb37-e2a825ed9e31", "0b153da7-104b-4f01-833c-78e94149c4e0", "d08751ae-097a-4b6a-90e2-c855556061ce", "b2c584ff-b29b-4966-97bf-dd610fb95927", "ae6f136c-33d0-43c8-8155-0e69e26e20d2", "48ac672e-3193-42e8-968c-f0703a53f97f", "baca1f34-761d-476b-8849-eea3e8d49820", "a64459c0-9aa7-4d65-b338-01a91a5f07a1", "8853a2dc-0d1d-43f3-bcc6-ebbcc09d66a3", "c05cf077-62c9-4238-bf7c-4f75ed74523f", "547c03f7-4cc7-47a8-9830-22151ce042c6", "b032fa00-a193-41da-a194-7776be2d46fd", "6152888b-1e9f-4ec1-bf9a-39bd5ecf6a19", "68a75f01-dabb-4d9b-885d-143a437b0d1a", "e125a4bd-9498-41af-b170-87ac268881a5", "50601c31-e74d-4160-b2e6-214aed85138b", "e09bab72-b099-484b-8dae-b70e1b650332", "9ca02669-dabf-4246-9691-62ed76bd22d8", "3470e867-a066-418e-8705-fbd1a4b89481", "8505cb06-bca1-4b0b-9832-fdaf92dfb907", "711dacfd-483b-45af-8ade-1864aa723de5", "a3d40b0c-3805-4d3b-8d23-d4dd4f2deccd", "7796bbc4-a338-43bd-bd86-5369a20e7c54", "a05685f1-eb4c-4e53-a231-f63d381fa96f", "eb1b185a-6e04-4913-bbbb-318f66778f2e", "9e6e0720-d784-4adf-a0ec-4585d5a44b5d", "e18a3d2b-d41d-4df9-a484-058727ff9c07", "25cc674e-c95e-4e1a-973b-e04f9abfa371", "ab10ee88-e59f-4c36-a5d7-904ed0a9d327", "43914e07-f635-4db0-b54c-4a66dc2d1a6a", "d282d820-b8fc-4b98-b9ea-35ffcd3a322c", "0acfc550-8ace-443a-9130-de2731b81860", "ffbdc075-f38f-4c62-a564-8608463c752c", "2b38ed1d-3ff4-4abb-a672-c426bb82ada8", "d24b05d3-94b4-4b7a-b890-8ffba811e88f", "9458e849-1aec-43ec-b3a4-7464f384192e", "bb16379e-43d3-421a-ba36-953e693b4d50", "1669afa0-c886-43d1-8cfb-16eca718dbaf", "f757e0d3-1100-40f0-9301-b235b47072d9", "8c75c395-bb32-4e4d-b4fa-defe48874fdf", "9011f739-c3ca-4321-b88d-00e36e4548ad", "fd2db383-b0c5-41f9-8aa8-2f303690613d", "a0f91275-eefe-48be-95ac-2f2cc648f1e1", "0f07ca8e-d4a5-4db8-a318-40c946baac6c", "9279c0f9-a48e-4994-aaed-f2fd7ebb2952", "44452aae-48f9-4dbe-9f02-a0c337763b66", "28ded4bf-d587-48c8-90fc-c32fc5afda76", "7e208ca0-6c5b-4703-84af-8c4caf5cc87a", "85cdb7f4-646a-4e90-bd6b-c32044d60bbd", "908332bf-44e8-4dec-b0f9-36b2e1add3dc", "abbd5bc3-434c-4649-b569-73a6f38ffd3b", "cd0a8396-0811-4e4c-b5c8-4d758f48924b", "b318501a-bd7c-4e9d-9772-98754952167c", "34297c71-1f08-4ea1-9408-279b2a29b126", "704aa060-2129-4118-80c7-6465ef9ea9a1", "5fb83834-55d4-4f6b-9327-73830378823b", "65744212-880c-4a95-b85d-b31f439da4ba", "7e451cf6-7fde-48d0-849a-bf867df138a1", "18e70dd0-d18b-406f-b432-7b645699e39a", "0d331d13-7298-4e53-b43b-8e246e25c6c8", "0dc8c9af-7ef0-4190-a630-e23b6a30b4d6", "33d33faf-7308-4dfb-b22d-f2009ca99fcf", "12749957-743b-40a6-83be-6e31eac48e2c", "60f10906-a74a-4861-b1fe-54a075b91b58", "6a2d98f3-08ca-4c89-a5ca-c21641b8da20", "18b9c7c3-c535-423b-a086-2de36ae1cc7f", "b566e6e7-c750-4be2-809a-debc38438160", "b0e9f081-2ead-40c5-9c38-974442a13794", "49cab8c8-6676-4019-bf25-f352c2ac6415", "f5a34c06-45e5-4e83-ba35-eb06b7a29214", "ad760e92-7e57-427b-9caf-23d430443131", "80c9f6b5-456f-43d2-9cde-399ea2abd084", "be070e71-e7e9-41dd-ab09-e70c95c061f3", "aed3a0b0-a415-4baa-a2d0-caa4f979f836", "b7a827f5-de3e-49bd-8df0-0d3ea48e7ec2", "1fb5cd21-691f-4418-bc6f-9bc31ccd8f36", "ab5145ef-8416-4b97-ab7a-9ceebd3ceeb9", "a107c5f5-57c0-4377-8fcf-9a7d171222fa", "1a4d7fe6-6afd-4b2a-bdfa-30b2cb79dd58", "46ff6c09-cfa3-45f7-9472-cf71c94da6bd", "90817541-872b-40c9-a477-10c2d47d5844", "e87ce1b6-9745-4143-bd3e-8ecc6a9fee72", "20bd3397-fcac-4188-86d8-8ff1e8881b72", "cf2f0faa-19f8-4803-9053-60d8768afbd4", "62e536d3-ca89-41ae-8450-a5ebec2a54d0", "d8503468-4dbf-4347-af65-57090e565847", "2e6de388-701b-47ed-b2db-98d488f6c476", "60ffd116-52a2-4465-bcab-6410b5c213a0", "96e74dd4-4384-4020-ad96-0c16b28bbce0", "6b9e58dd-8372-4df9-ab45-d9bfe068896b", "a988e371-88e7-4e72-9501-fa11de5c7221", "1cbf4c53-ddf2-4426-bc9a-9f2a4299c63f", "c8adb01c-fc25-4324-a599-5d5e419f3a17", "fcfa2fc7-6fa2-47ca-8483-dbee871c3c9b", "a7243164-eb89-4d64-9bce-e55a6e4b99d9", "9b68904b-d736-4f97-a746-8d7162ccb147", "922d55dd-3249-47b3-b670-22a4e15af335", "19eb1d3d-2306-48a4-baa9-ee9d888b7345", "423abd74-22b7-458c-8f92-d5d3bf8d9580", "ceefa223-c93b-4b61-b17c-c54857a4ce1a", "e14016dc-448b-46e4-8e69-5255990910a4", "00638092-5fc8-4fe5-8b09-d53b7eae4283", "0953a4c1-2cdb-4f6f-b52e-18ac5e0ec5ff", "9b479b7d-27d8-4e26-8939-6fc026af37c9", "600b05d3-80a2-4779-9a67-7324ba7a5f43", "463f2f3f-a4c9-42a7-aafb-79cb9b243eaa", "698fff3f-eff1-4dc6-b379-316790cbe937"], "metadata": {"node_type": "chunk_node"}}, "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures.pdf_MainText": {"node_ids": ["a00ab085-4161-40c6-8c30-e3ad607b89fd", "a3096672-db53-482c-a009-95e6b036cef1", "b6e222c2-08e1-434d-884b-ac775dc6bd98", "e3a6e9ad-0024-4a81-ad9c-73838f7602ac", "6cb7b791-e7c9-4c19-91b8-544737658d2f", "a9525ac8-4de6-4526-a69a-ceaa039ec724", "b9c24d88-1cc9-4fe4-939c-050cedd3510b", "2c98efe5-1ded-4666-a266-a0634c77a03e", "284a3228-e38d-4e17-80db-d78802d8f847", "e2f67fa7-b73b-4c7b-b178-d04c434b2f1a", "f4517e9c-4f7f-47a6-817a-4197c697ca82", "f9e854b5-4acf-403a-b8ce-86ab39c90784", "d73297d6-32ce-4901-824c-8fce79c77571", "c89d7d84-6dda-4677-9adf-0cc318efdff9", "9c0552e2-bfc9-4a44-9212-048333f21e01", "90643c3e-7f32-4875-8584-2071d08b76cc", "b5f4fba5-3d2f-4fcc-8de2-a1b3369330dc", "2106144d-642e-4960-b425-558691a5c921", "c5b89651-6775-409f-a4cd-6b7e9b7335c3", "e3cbb9b3-fb5c-43fa-85b6-ad6758f7041b", "59ce017a-b229-4751-ae99-2b35be85a5dd", "60db44b8-0543-4e04-a8e5-983e194d5060", "b91e7ef7-2c54-44fa-888d-bc700ea556d0", "21b8ae8b-bb04-436a-add1-104cb8dd909d", "777dcbac-c3e2-443f-91fd-d02d920f17e5", "b86ae0c3-0e53-44ec-a9e8-8e2ca25b0a30", "502ef66e-6dd2-4559-9c21-51caf7bca8c6", "a248eb7d-7045-48f9-b481-3521d40ad33f", "fe131852-5ce7-4d09-9faa-715544830cb0", "c8e6dd4c-902a-489f-911f-fc016ac3b818", "56afba75-0e7c-41f6-8310-43fb22925ca6", "1e6022f1-114a-44b0-b4f8-070f850231cb", "dc7673ad-6e8a-41ab-86ff-a347ac735b2b", "ce91620e-7eb7-4b78-9e15-53c97f6f9f5d", "54e970ed-7a06-43b4-bf52-ca552d9a7b8b", "2b6aaf40-171d-4c09-bd70-a207e25cb551", "2c8d0d18-2d69-4ab7-9e31-420aea81192f", "3c096f8d-52cd-48ab-8802-7364d73cafa2", "c2df2b22-d8be-40e2-8dd1-90773fc1c113", "0f50e789-1e08-4641-8cd4-b28d5f6b3460", "53e5906f-de74-465a-90c1-75e65dd77205", "f4629a79-99fd-4883-ae92-e054b85facc5", "6222df82-1d98-47ff-a0c9-db189f8c6937", "8cfadd58-f42a-4802-b49e-3d3a2ed8aaea", "04a5abff-9386-4cbd-b72d-85593cd205a3", "2e8528e1-bf1b-4bf5-81da-28e65910d1b6", "a76354e5-746f-4b01-becf-8353e6f0d923", "1b2e36ba-aa82-4011-8a43-ad89f8807cf4", "8f57d00d-c394-42c0-a3e8-7b29cf00fd12", "beddeded-41a9-41e2-b0d7-4d68fbef9c8a", "fe4ff507-003c-478d-8d23-7c649516db2b", "f2f79ecd-d00e-4320-bb3d-53171ac5ed78", "e5a01699-da84-4666-a2e6-f3696cc085bd", "8af28542-ba25-47f0-ba2f-6bf41ab2e18e", "7c04abd6-cef9-4c1c-8754-b992b75de0a9", "f33879f1-570c-4f85-9e46-5e17602302a4", "49b1d941-ef85-4e88-8b11-d1834a0ced47", "065d50ae-16a0-428e-95e2-756112bd9d65", "518c0cd7-d67a-4f81-827c-d5d132d6ad34", "9fef4540-a8f1-45bb-98fd-1c5cfabc8776", "0bc940a3-bd3f-49df-adfa-b3977e4a8a35", "0ad79ff6-aeef-4ff5-a3bd-312024872057", "0f83664e-f4e5-4c94-a082-2a48f14fb158", "c83150f1-456a-4ea4-bf4b-7ded0d77cefa", "28973d5e-e798-45b9-a244-8d8a2e33c7aa", "73de1ec2-b29c-4505-8822-68e4e0215bdf", "dcf1dd35-d48b-453a-a2a8-e0c57f97bffc", "c62a6ac3-6952-44eb-97c7-9f869b5fa962", "882bf157-6082-4418-a2d6-bdcd1b788533", "95fdd86a-5a07-446b-ae20-0fb1ab62c1b2", "78eeca5b-10d3-4bd2-beba-6c6331f17a3c", "254e8095-883b-4a57-8435-68a9c5ab72ce", "92dd26ee-30fe-4f33-92f6-5c8b12d4867a", "472c7f79-4cfc-43b2-9f2f-0827a8962d77", "7ba2b0c0-174f-4b19-baeb-84ad56a59329", "9437fd62-8259-41e4-8953-57e2c81f337e", "745a34b2-9238-4ca9-9859-a19cb95e6d63", "c2f806ed-9172-4a16-a400-d4b6c18ebabc", "51021f82-ef0a-4d15-8304-d82b3548a861", "3bb4d99d-65e9-4760-9542-7d74f8ee7545", "173b70b7-1aac-4dcd-b90f-ac71891ce5c7", "2764cd02-33d1-4750-9efd-8cdc99f927ad", "2bbf3931-5239-4fdf-814f-e1b7cc3b3e80", "023eaf60-3166-4103-a4f3-26084e42f378", "4348e3f2-7220-4f7a-aa3e-9ccf9abab9d9", "df172485-81b4-4ca1-aa82-75297446df18", "ea8821e7-a465-479c-9ede-fcda348b98a2", "63871b78-8b39-4786-a4b2-5743a0192158", "d4618e8f-9429-46ce-b8a2-aabb65355644", "3386c002-adce-43cb-9c7d-552cfa71796b", "afde5e1f-9ae6-40f7-b8df-b9f44ec544c9", "38bbec5d-8e77-4b46-8e10-2211408f8c2f", "0f9b1487-2428-4aa3-9f47-be1d6b3a0d8d", "312ecd15-f42a-48b9-8a82-6600e5fa0e28", "bd24967b-5200-4fcd-8140-16e238331f6d", "13d5c9a8-541d-4f57-98c4-1dfb90d00fb4", "af37f0aa-1182-4f6f-b2b3-2d18bb75f0f9", "fe7f1b9e-16c1-4b57-8361-5961c1311cd9", "45830ee6-afa4-4456-8ebe-1b7b71ebf662", "6edcc081-a266-4073-ac92-28cf9302839c", "cd50d53b-a36b-4f88-897b-1ecec7961fb4", "9afea05f-7150-406a-8608-a0b60157f561", "c15efbc1-a2fb-4250-8b40-62fd9eaa2825", "814aaeac-2eaf-4f8a-bd9d-03e74536fda0", "276029eb-491e-44af-b8fb-5090ddced02d", "7cfe1085-38de-41b5-99cd-dfbfdb1fa048", "c9d0931c-9237-4625-a349-323e48ff9912", "e358112d-4b6c-4589-ab6c-13424a213b33", "615beb3e-5614-4779-8bad-52087f6013ec", "99c84cf4-d116-4591-9175-4c43cf62995d", "7c718b15-d4f3-49ea-9a12-924fcdaf6ce8", "28ded052-665f-45cb-a075-5352cda84962", "e762895a-426d-4d44-9a17-154b0151c079", "5ba8ee52-d15a-4ce9-bec0-c8a43e165b4f", "cb1b4cf7-a033-4e60-aa94-ff4f9541db7b", "37917e4f-cf59-47fd-aa5f-4e4db1ba43a5", "ef343a51-c03b-45ac-a28b-d9a6c8a0458f", "6ce86b6c-571b-4270-815a-a28425a2eea7", "bc4a02e8-2566-4e40-abb9-0ba20265675a", "715d2610-c061-497e-bc9d-b973c002d5f7", "f51fcedb-a6e4-42b1-9a11-8b4f128b36f4", "627382b4-1d6a-4334-8028-bfe8ef264439", "4be65253-c5c1-49d7-a368-c4b338318f7b", "17ffb171-d7a1-4ff9-9478-ae0082cac502", "c68b5b8b-e902-47cd-a80a-1fbc0110520b", "cc0512d4-eb35-4c5f-80cc-caec67869c9a", "c731faed-4857-4b22-97c8-a79b7fc5542b", "afdfe7a3-0f5f-49e7-bf57-11645500f841", "13c63c38-4afd-4575-b896-d877cf9a9127", "bbc28024-fbaf-4bd5-b331-9ffa2063d3d9", "9d8995fe-d161-459b-8043-ef90a0bd76b2", "52a7d3ee-c29b-446c-9f34-d35a5008d881", "4dd37d7c-a5f5-4964-9439-703ead11d469", "7d9832d2-c9d8-4bf9-afa7-5b685f590008", "ffef8535-8176-45dd-a5b9-d413f0dda380", "cc1b8cf2-ebd7-40c3-9605-af705de9b9b0", "9893d134-8dbb-4656-9e92-45ac7b4fc86e", "6ee982fc-1e1e-4676-b822-26a01c688c4e", "517ef586-e04b-405f-a324-524868745a04", "f6b5e5df-534c-4e80-a530-35cc84be6677", "47cb6eea-6fdf-417d-9e32-62c7539645cb", "80d5b376-a62a-4664-be13-bc4b7fa94323", "58c87ddd-19e4-4729-8481-0bd4a49e897d", "1d2e6e11-b2bd-4d29-bed7-b13000ba1c97", "ab6d6655-7946-44f1-acab-626e95181823"], "metadata": {"node_type": "chunk_node"}}, "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\A_Compilation_Framework_for_SRAM_Computing-in-Memory_Systems_With_Optimized_Weight_Mapping_and_Error_Correction.pdf_MainText": {"node_ids": ["ac150157-944e-4376-ab03-a2adc0711b9e", "4036cd26-4a0c-47e6-a38c-5580571b84f2", "ddba2af4-55ac-4b35-bdd8-6b0cc7aa140c", "d69edfbc-1900-404e-85b4-19517f3146ec", "74605057-79af-413a-aef5-6bdb141f3201", "801ea761-ccc4-44d3-a49c-c99345bb67af", "7ee3e216-e1f3-450a-9106-c4afa90a077d", "a22e22a2-782d-48cf-9c42-1b77bc109742", "263be616-50ef-4b9c-a9fe-364ebc3c4be4", "5e4f548d-4156-4f8d-9ca3-7edc20df8b89", "654b133a-b06b-419f-b9db-64712eb02a5c", "9f7f2548-c016-49f9-bc8b-368200b3bfc4", "54372f18-4b51-4b04-b92c-018f74c8ddba", "c1e03c35-08e2-4e0f-8f85-0e108d31c967", "3737d0ab-d82a-4fa2-b916-7b6c2e2e57f0", "a097ab96-b7e9-43da-b1d6-2c581306db21", "a168720f-4e90-4475-9b55-2318ab043243", "a69f7089-ec4c-4cd8-ac38-91c8c79bea9d", "043ede7b-f2b6-48b7-b4c8-0d01b515cef3", "f2c53940-fad3-4ef6-b6fb-6621fe630357", "af55d924-20e3-4c1a-b472-e94d5620e5bf", "4d0e0363-8e64-47b2-876e-a4a2e7208c27", "b2a448bd-c0fe-48a2-bd2e-9948218f8176", "77d6e95b-5746-409a-889a-f2b033a180c0", "b54e23e9-b27c-4647-8365-5473311fa6c2", "125f6f54-7a86-4e57-b17f-c5d742dc864b", "3aa1a0b9-c4cb-43f4-b481-6949a4976ed7", "4261e0fa-e9a9-4175-b22d-931e66dc3b3e", "e6a4388e-24df-4d21-9f4f-f14035297a6e", "6303729d-8d4b-43f5-8b43-0a530b7e4445", "a293441b-a94c-445e-810e-1bbeea180301", "155cdde0-2686-4bf5-addb-697f5a7f296a", "256e98f6-ecbb-45fc-b674-6f7f2eba332e", "29084e2f-3623-4886-aa67-cb7f33768cae", "049171c9-1891-471d-ba6e-4e172cb9c0e9", "077a973e-2710-4c6a-9f9d-b31c3b511395", "c03a63e1-25ee-4a68-9138-2502329a1305", "d41c218d-7d89-456e-b33c-a780722c64b2", "267ca3fe-b251-49b2-b6fb-b90c06fb40ca", "4892b2a5-8ca4-4033-9314-0ef9038f7ef0", "cd6b9d04-df7f-423f-988a-3e8e33a760c0", "adaec0f7-23ee-46ed-ab15-0d23cb8255f5", "844e2005-5def-4c53-9586-cffa04a20dfd", "890450b4-411e-4a3c-ae4a-3c6e9ce1fdbb", "80cd8183-88a1-4478-be77-a6c1be7349f4", "37c68533-1723-4951-8ade-fd6962c4e9be", "c17b12a3-9751-4a27-b1ec-35fb1ec0f714", "8fa2f47f-fd5c-4362-b80b-afc4e654d3b3", "53bd0054-6806-4be8-a421-7b4beb57ed1a", "2effd1bf-74a9-4806-a3f6-b91a1df0c5fb", "ca9a43d5-c69b-4419-acf9-a743c4b5ae60", "dca40fa6-6512-4762-ad13-a124d1620665", "c84aa64e-14b3-49c6-b5cf-01054923401e", "c987d570-0b06-46f4-9d5b-9962c930be68", "dd5f8cd2-3162-4683-ad90-f573a3974d1f", "50cd358d-c2be-4dcb-a444-a9c0e2bd10ec", "60b4552f-9495-4a77-be55-4c9610b2048c", "83edf9cc-2c56-4e3e-a783-8a4cc9602629", "a3d1e384-1766-40ca-a943-7e3fafb8ee76", "ebd14e5a-6aec-4cdc-a6d5-f59340b15933", "4237b3b6-8ee3-4caf-acf1-a5fb12003dc2", "f8d51b4c-d422-48ec-9835-5c92c0db20fd", "ad9507f7-2aca-40ca-ba72-114064ecbd2a", "c2f4e56a-c637-4a1c-8f86-dde4cad30142", "ed8f540f-4642-434c-b8e9-4dac06d8df1e", "26815d5a-2cdd-4aba-9fce-c1382d153986", "e5c95d48-ac60-47f5-a723-76937988cbaa", "a5651519-d1d2-4289-aa68-a907b739f8b1", "8ed8b7a2-5dab-4cbc-bb07-6c279e5b7229", "38b243ee-b354-41ee-a49b-72f4869e1cf5", "9bb8374c-399e-4665-84cd-c6f38e477818", "e9117ab4-073c-4c3f-b876-d8d2bf49bfc4", "27ffa399-15fc-453b-8b47-1cfc7d90d073", "8a7a68e6-722c-43a3-b2a6-f3486f5bb9c1", "f7c207e8-5bab-403c-ab59-bd4f9581a268", "3a0bdd21-90b0-4e60-926a-a08a9836e870", "6dcb5e8e-536e-4500-8dfc-32a0304e4445", "ec6dfe0e-ca07-4b2f-ad12-06ff8686adeb", "b8da8d61-95ab-44e7-9ece-1096e1c2176a", "44eae16a-3d54-4de7-9b43-b5e341e9797a", "4fd9e821-624f-4ddc-9838-b6379884f26d", "3264cdaf-7145-44f1-b2d6-db6d76037556", "5a247cb7-fe2e-4f23-ac34-ad7fe134f255", "fb1ad426-cc9c-42f7-a25d-356300a24c76", "7595f677-646e-498f-b541-1330db2293e2", "15b9f45f-fef0-41c7-a1f0-c3e87fde8cf8", "37d6843f-52a3-4903-a090-347780b80ca1", "70259326-07a8-4f9d-b3b5-6b8a038b4d94", "eeab61f0-8040-4c5d-9e00-63191c35abbc", "11e11d42-a0a5-4559-bc69-40a12e88d381", "26b8f05a-8164-4d6f-8157-1b5ad68dcf2e", "9066fafd-7d42-4ca2-84e0-e812f25906cc", "b74c8e48-aee3-4149-8714-ba22a73f79bc", "5ff4f179-74dd-4833-a931-8a6894dfa8bf", "d2cf9ce0-82d6-4603-ad98-90e8ec39bfc5", "a30f9db2-6708-4fcb-a526-55f612fda45e", "d1b26f3c-da54-4f18-8805-6e51a829b7dd", "627a737d-8c86-4f12-a92e-c8350b4bdc8a", "d8c3c6bc-c8c5-4664-8ac3-f0fa2ddb285f", "7515ce6f-3c1c-4493-8410-2e21e71ac14a", "d5c31810-4758-4042-b44e-3d21c6c3336e", "5f3cabe7-7a02-459e-adc1-fb69edb96957", "51b5d905-ece4-4529-92cd-1094dbc2f55a", "3d9fec61-97c6-4ac5-90a1-49185a038e62", "cc7b3abf-c180-4a8d-ba3c-81d30bb8a299", "b02f5eee-462b-412a-aea0-d3b4d7ab5d07", "b4c3a51d-8bf1-4de0-ad46-83c549db4ba9", "3c1b0741-1cac-4b04-89ea-a0ca8640b1d0", "2617f1c9-9be0-4bb8-abb9-111ee1b6d31d", "f45badec-6d97-433d-9f1b-de4bf4dc146f", "5a51bb02-7264-4564-9e6a-a0ddac7f5fbc", "5eae40bc-aea7-454f-9be5-ff9065e10b2a", "7b18e431-1ae6-4bd9-b530-6f3149c864b3", "f5104dde-612b-4da9-a3e2-5ae3eab0b0a0", "de1bba2e-8038-4056-82af-ab21c2bb3533", "c4a5a5c0-d280-44c5-8c28-017aac86330d", "3d0711fd-4ece-475d-8540-0452799f2e9f", "8526dd5b-3d8e-442e-b2c2-ce6e2f50482a", "b2694119-f476-4d3f-8a21-89fff41abcb3", "f5254579-2d07-4115-ae95-88338a412a36", "e1c5d6c1-5384-4f7f-a260-9a0749afe0bc", "1194fa25-d58a-43c0-8848-9a8ceec7d985", "19097887-91eb-4dbe-ab29-1d0fe3e46dae", "f7e05c07-469e-40b9-a258-1ca79e16000e", "499ac069-0437-4725-af5d-f45920be32df", "e05b0377-7383-4dff-9094-2b7774acd686", "2f3c33e7-0c47-429b-9568-262f51a01c4f", "31a66546-a080-4035-93f0-68f7fe5d625a", "d7fef8d0-5273-4882-a0e9-1ef09fcce340", "d62b6d8c-c24b-4fc0-8007-9bed085ff6c8", "e60ef1b1-dc9f-42ec-a0f2-4250c3800f17", "4b5123d1-1b2a-4c30-aad7-d7903ad76254", "16a852c8-932a-4fcf-9060-7b8178f45567", "6695cee5-9101-4c5d-9f44-600f88f1a617", "cd3d8818-768b-45c0-9802-ded4460d491a", "5c7d91c3-d464-4a7e-a0ca-c16305069142", "1c8d574b-2f2d-4fc8-b83f-1456a9739b5d", "091dd910-213b-41e4-9c53-9dd6ff0bb16f", "08ee27fc-2100-4d2f-8070-e497f1a44c9c", "ad0cfb61-29a4-4154-8e66-5a1b6feeb3e5", "4e666599-af44-4415-8492-d39946c5b04b", "32a52e5c-1165-42fe-b110-41d1114fde62", "2aa4a14d-3ee6-4e45-aa3f-f6adebc04a09", "8bf2ba76-3186-4756-9394-bb8b65249acc", "ffb775fe-762e-48c1-a3f6-b8f628bcfade", "2a8ecd39-d4bc-4bd7-9b42-b78260d5efad", "623d66f6-5aec-438a-8453-1f23249323e9", "63cf1a5a-40f9-49a7-83ea-c5e34e109fbe", "f0ea5197-4f30-4338-b42f-60e76036bcf4", "eefdc475-5bd6-478f-a30b-37e39b61f8f2", "c42c74a7-e693-4486-a231-1d1d2a35c1f1", "d440ec05-fa83-4ff2-b9a3-18b2df8b026d", "8b5f4007-0919-4914-83ba-f9b4037275eb", "0f3910e6-60a5-4249-8af3-12fa10bb4fbd", "cd5c6df7-f609-4148-9f9e-990e52ae6611"], "metadata": {"node_type": "chunk_node"}}, "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\C4CAM_ACompiler for CAM-based In-memory Accelerators.pdf_MainText": {"node_ids": ["ce6beca6-91b1-4a1e-bb9e-5e2aef65f7bf", "8a34a36d-0ff6-43b9-9b6f-8fdc4497a8dc", "1e2ebfad-89b9-4d83-b74b-d5746082b6a2", "b3e3932a-b0bb-4749-99ea-edf00ee93a5f", "33178490-e400-4113-bbf7-7f978ef19fe2", "29978223-f6ac-44b6-b30c-c94c15f525e0", "58c0c755-97ee-4e50-b41f-8395cf490ebd", "c20a560a-e946-4202-bc5d-d7eafc0748e5", "4db71d11-1e03-4ced-9a5a-ecdbf4b8d893", "c256a51b-751d-4338-a3cf-f82f72e51766", "3877773d-f394-4ada-bd5f-612912b7546e", "dde9786d-e7bf-4fc1-baaa-639a4161b487", "9d1f07fb-d417-425b-bacd-2e2091174c99", "a24779c4-6c4b-4e32-930d-c687149ca7f6", "aa55ac90-ed41-41d5-aceb-1afdaa0a4b8a", "02119eb9-8504-4083-ba41-22f37e599f29", "f7cfcfb5-ed7f-4c5f-9d78-ccfa5bed2ccc", "9318ad85-4d09-4acc-8bb5-04e4182061b8", "78ea12e7-82c1-4cb3-8726-745b61e8741e", "5859417b-2dd1-4b3c-8fa7-7cd5cd3ae3e6", "2996cee2-bdba-4ffc-b43f-b558d59696e4", "e8714f23-6cd1-4440-b8cd-7fad4cd6d068", "baef1291-a654-4c00-8f4c-63c65a6d71ab", "78a77e59-3b3c-4b1b-8ac3-6758df3750e1", "7f2f15cc-42dc-4bea-9899-0221f9286888", "a864e6a5-4b5a-46fa-98ee-4e060bead9a7", "1e5f7931-ee12-496a-a66c-02b52df3449c", "216abd8a-aff8-4768-b247-11f3f384b8b7", "5d8da926-8381-4901-b1d2-9719080a6dc9", "1c54d84a-6815-4859-8152-c971e049b6e2", "7fbc1f57-49fe-4f17-90db-b249ba235f4a", "56631314-cff8-4964-9c57-d6ab1019aa40", "f4f5f59c-eab0-4eff-9506-716203d706d4", "ded2ef5f-9f03-4ddf-adf9-3c0cdb3065d1", "09de1bb1-ddc6-4c63-8556-bc80420029d0", "71da9476-65fd-44b0-bc1a-253d08c47b3a", "854008a0-4e99-4b8e-b495-b88b9dc090a1", "816881c4-a614-43c0-8d14-984014afd457", "ce83d2a3-2280-4a71-93a1-e25407d1b548", "dc9faad3-0c6a-48fb-be64-4e8b32de0891", "54417010-3505-47e3-be62-a9740807383b", "670d4244-7cc5-4a33-95ab-5e03a994198c", "7409d33a-3652-4002-8c97-87fbdc4a7bfb", "ce6c6eb7-e015-4572-94fe-81884a1c0a6b", "471fb5ad-4434-49db-a405-c75619cbc511", "537b0ab5-144d-4d5f-b014-b48212fc1c44", "4b4bdbc7-f17b-4754-88a2-2704af18b3db", "d7ab454b-d73e-4558-a4b3-3ab76b473001", "557e8d1e-f6bf-4c69-8f84-8cf0e0762a34", "f934dfee-396b-4f19-a869-1f5c3ed52fad", "01253bd9-b089-49f8-a571-9b1a9b8ee380", "4c58947c-bfd2-4a53-9c22-0df6255f21e5", "ec38f990-1634-4209-a465-676674df7c22", "d8c18495-ad01-4e85-81e4-d93ef23795b2", "17a8dc1d-2514-4bd0-bb6e-6ea070567a47", "5c796ebd-8863-4ae2-a86d-409a8ec7f21f", "5eb85767-dbf4-48fd-a118-9b08e1bc849a", "3fb7aa01-72d7-4552-abb3-a30e88ba1c8a", "00794754-fc63-45d5-9d6d-73a1d041162c", "413c7c40-c43d-44ea-828d-1348a3b97ea4", "2c184e13-a2e1-4b6d-80a0-f96d5e819a63", "49e9d009-c89d-496a-beb5-dc968978421f", "500f8d9b-1a0e-4bea-8443-34eb023678bb", "761432ee-0a5c-4331-96e3-98fccc17f028", "3efb12e8-48aa-42b1-b8a0-7c17b031051b", "cce391b7-00ab-4456-9032-fc4bd361911c", "12e75cd0-66b2-4c7e-b5fa-90ed1949b5da", "18cd6ee6-e7c7-4314-8c31-f39a431205db", "dabff7c6-6f21-43ca-b1a1-dcc8b8c4e843", "7dcfcdbe-c07c-4039-acb1-d192252a1b50", "fb7f4a98-93be-46a4-b4ee-fe82bb1f2349", "a49871ef-1677-4bde-97d9-abc11fa8d738", "971a6b47-4916-45c6-9d97-2f2a1eb16643", "72ce1635-7d22-40d0-aee9-7f73daecc27f", "001eb75c-bc35-4e70-9854-219ce8474aae", "245911d7-bde2-4736-954a-5e097feae088", "df27660a-ffba-4c72-adad-509aa46c096a", "ec8798bc-3283-40e5-b2d1-9ddee2b41f2a", "85c869f0-a5d1-4785-a5b9-0cec9c18b045", "03b403a4-e9bf-4301-8ef6-ef0ab9e5b204", "6a4f4cdb-c95c-4498-ab84-aacb1c5c9b4d", "778be731-0acb-41b8-a061-bb1e22ddca5e", "18ac7c6d-a6af-4c41-bfc0-bf41eeed3cd8", "12d254d7-14aa-4063-8d3a-a6b4155d3750", "4a172d1b-4b1b-492d-b96d-57e7810f3a6e", "85b7c7f5-dff1-4eea-a6b4-7b5ad7493abc", "c9929cc7-ed3d-421d-bf6b-0cfcfaa3041a", "37427d17-0b90-47e1-96db-dac275939524", "1379cd16-9541-4653-93bb-b076d7d15e11", "9c935663-660d-4652-b0d5-49ad7c2ab4ac", "9db3837b-faf5-46fc-b985-cdb443f76f25", "0f4aa01c-f415-4fda-a8c9-bb332a4b6c1d", "5f154dde-95ef-4afe-b861-10abfd744e38", "327e205d-63cc-452f-9378-810c133eeabd", "1701c0e8-06f6-4410-9cb4-ec66139cfacf", "8e333101-0b0a-418d-a9f2-ee9546ef71af", "95f9778e-5e99-404d-be88-5fc8e59e695d", "277d09d8-91ee-47d8-af25-8d2f3c892b86", "77ddb39f-f747-4ca6-98c2-7d1e8c2f4853", "fa1f3b8b-9337-4e1b-99e2-de02e4d2e126", "4e2b87b3-85c1-40e8-a2d8-e69968573b61", "5728b055-b0f6-4f84-a469-d959e5aca37e", "6abbc3fa-b2f1-4845-a8e6-f04b90d662d8", "ba128f38-243f-4a38-9819-b78ffffb84a6", "991221b8-a930-4f83-9aa1-9fa1ed038555", "30030db8-9b2a-4ec7-9083-c3c773c89e2a", "d9ce3809-fe35-47af-b1f9-6e6b1e45b34c", "7903e73b-3135-4461-8842-c7bcab8d5fd0", "58c10eff-f1ca-4aae-a78d-2b3492de8699", "4bee446f-5cc1-4f67-95f7-d866d7cbd711", "f56c20b0-5491-4d65-9d23-3fc8eec4afd6", "e046c9ff-e1a8-4e85-a50f-d0607036e8ca", "8f29e913-0b92-4c8a-83dc-2f98d4f314bc", "282f896e-0556-4794-9cfb-6d2e0e8a601e", "91e0412f-bfd0-44f4-887d-bdd9907d1ca4", "1d506d3b-eec9-4854-b9df-c579b2713bea", "b6ad5f65-0ecf-431a-b370-654f9037d95f", "35c6cbef-2e0e-47d8-b81e-a4dcea1502be", "bd6c2fe7-94f1-4704-8d0e-0bef046b3a5b", "496676d1-ff3c-4d88-b62e-01a8896b1869", "05aa52b2-6751-4540-84e0-4a91056ececc", "56fed4cc-c350-42c6-a812-e759579f7677", "ae04eaaf-33bd-419f-8f58-66fce7329ae7", "2afb8b8f-d547-4cb8-b04b-7857ff1a6606", "b4b359ad-eaca-4ba2-8a31-d315725895cc", "0b1bb255-2fbf-4bfc-b2af-c1b271e160e1", "97c1cd5d-a391-443b-bbdf-c53a45dd536f", "7388e734-dd8d-4459-b754-c773ec082e05", "da6be919-c62f-4dc3-ba9c-8e893701e92e", "f81466c6-0f61-488d-aba6-cd11ab2305a4", "ff68e3a0-82be-4b72-b400-a02777a30c35", "5e8073cb-01ee-459f-a27d-9ee328d6a9ef", "401d21a1-7f10-44bc-84a5-f5712336967d", "09afc8f6-2bd6-4caf-b6e3-5a01aa090e68", "2ae5b474-1b0c-41cf-b508-60338d005044", "e729d654-a5df-4c8a-a1d6-9e8e9623e471", "64d5ecf7-8e50-42a0-b969-a8181296a2e7", "fc16d4f4-8226-4975-bc35-765dfc85b28a", "c7a4c3e0-3d48-439e-bf42-be805a6538dd", "fa10dd7d-ce7a-4186-b844-8085b240b5a1", "631a7a0d-247b-44f5-a0b7-1454d7f709b4", "e09d2c22-beaa-4ebd-919a-9b25c00c1bdc", "f604e1da-31a2-45d9-86fc-da0c6fe0b5b0", "1e3932f7-cc32-4a84-8a68-86445cd9f294", "6dcc80f4-ead8-4f1a-8f76-c4dfb16146f4", "fd55f36d-9909-46da-8ff2-820cbfdb84c2", "64efd21b-333a-4b1c-9899-9cec164d31ea", "d55ca5ed-7929-4410-a9e4-853529e82ef3"], "metadata": {"node_type": "chunk_node"}}, "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIM-MLC_AMulti-level Compilation Stack for Computing-in-memory accelerators.pdf_MainText": {"node_ids": ["d3ae19d3-aa0b-417d-bfb3-f31df984df31", "5a7e07c2-6dd4-43e2-a6c0-22becd234523", "f5dc9c18-9720-42d8-b9f6-38d547f7254a", "b6e87dcb-e811-4c4e-98f2-2a384c8b31b7", "728fd86c-0df5-482c-b8d4-730334ea7f93", "1bdfae30-83fc-4d76-9bfa-2adc5edb04d4", "8c6d7c3a-bf16-4804-9ea9-3822049f0373", "a22a2ca3-77b8-4a21-8f6e-a53fec7a0ad3", "74b1c6bb-3764-4abf-b9cb-1a2a9f88ff1f", "8f4d0025-6967-4729-aa31-5b5d317c521f", "cc9cb447-6d78-4d59-9daa-795b6236467c", "f01f355a-9948-4a16-9511-043226e6f4c7", "12e51f59-5338-4bde-b6d9-ac58b4a8efe2", "2487ff97-7439-45ec-8ea0-b72b4aa37da6", "814efabd-7847-40c0-a39c-d8af3757b357", "cda1734a-9df0-43a1-ac37-aead62549627", "fa3e398f-2316-4e36-a9e4-706aa288c7d3", "ed7bbe97-f402-48e4-9746-a18a91c096a0", "f088e130-bfda-4ed1-baec-9f4d208b8a56", "730c6909-f24b-45c4-80aa-f8eaac484169", "baac4dc3-e5ae-4510-a94d-2c8e42948aca", "33c17ef5-a17b-44fe-8ac8-19391e0cde9f", "3816d36f-9bf2-4ec3-85ae-e8a886a8e490", "901c803f-e256-42e5-9aa7-c8304deb0cf9", "d26c473d-a378-4bc7-a8de-914beb018f85", "0ee57639-b5e4-4b9c-b67b-b8e0a2c8ba81", "2151de96-b998-4774-b23a-fae618ed43ed", "edd47315-333b-46bb-953c-211b0ec105c8", "29f86f38-2e31-45f2-822c-4ee5ebb15574", "48e6855e-0ba8-453e-89f3-ece7ae5a3d53", "3bb6ebde-86a8-4f61-a8e8-f8a08d861e66", "786a24a6-813a-4c69-befa-69d4b5e89a35", "c7bb60d6-70df-4d54-acad-faeebb7d76d0", "e839550d-9e47-4b97-94c1-2e73bebe411d", "5816647a-73e2-4ac7-a11a-057ddc2eb376", "84f716f5-d65f-41cd-9889-b277271a8a23", "9c25a12c-5d26-411f-a335-cde275a1cdb3", "c4078c9a-7d16-4a64-b8d7-b39677f41d80", "9db0b3f3-6a36-4d90-8b1a-fdb0dcf551af", "2391d070-3b54-4f15-b97b-3dfa4adacfbe", "1e49f2d8-c264-4fe5-8041-b96847310cd0", "1bcd02b2-99a1-4a13-9043-20a1c8f432c8", "18be9a7b-ae16-412f-917d-8a7127b47d69", "0e920e28-34e8-4f9e-acef-2998fea02905", "ff328eb8-1f09-4dcd-8706-dd160b6f7cdd", "166b8eab-c60e-4a52-83fa-948ba2ec2109", "f486df71-0c06-45b1-a45a-5c6f662ba243", "7e0694ca-48b9-4de4-9c75-9cc96c87f3dd", "f01407aa-78c3-4d06-beb4-ace5d05edda1", "ec137a4a-1c66-483b-85ab-cb3e5f97c354", "7488f89b-8a3f-40cc-94cb-73d30ca8d5ee", "30b68c42-066e-4641-affa-25da36e97709", "bf635e73-8cfa-4246-8ea3-d1eefa59cae4", "e6032776-bd28-48b7-a52d-1337ae85edb1", "1a95430b-f941-4dc7-b97c-db99e5a02163", "a50743c2-1383-4511-9cbb-c7b9bde4eba9", "5916c988-4dd2-41c8-85ea-dbcf1bdaf775", "131b9936-99f7-459d-91b0-73a619f8492c", "ce2ab9a2-0f94-401b-9ed1-968d0a7e4b8a", "65454d62-cf1a-44e6-b301-041249136f55", "1011f77c-a35f-4d6b-a620-da78e8ac9cb3", "13d90e69-51ed-4b76-aaa9-8ee59fa4dae8", "6a60a4f7-d928-4020-99a1-13e5edfe04b7", "2c75efa3-3354-4488-9dff-16bfe0a7c630", "946eb577-b93f-4c08-901a-cca01cc10d09", "e7887d5d-fe1c-403c-9583-a3453f71fc9c", "96477ab0-f88c-4522-bb00-354110c85c8a", "d11f2831-dab5-46b8-b0e2-adae4a9b6f9a", "5930a112-951d-4417-a7e0-1c0ee380c9e4", "6e9303bd-a766-4ffb-a89a-401ac929af1e", "64b61fdd-91d1-4083-9bb1-b2f0fa6b1346", "3cb8820f-9f7c-46a0-b721-fa481020d96f", "db44e558-f3e8-4971-ac33-f8be2f7b1ec8", "c4140fe1-7410-4703-a922-a2ace8c833d9", "ccb1104d-2553-408f-accd-febb66469018", "21e1af3a-fd6b-4838-8daf-a1787afc18c6", "0d368213-7f2a-4476-a7b0-ff42a57aabb0", "8f3f6f5d-e3eb-4bee-95d0-595c9e3ac97a", "811f2858-7385-46d7-b22f-3a3f8919f484", "a02a361b-2fc6-46d1-b6ee-ac7da3f0d1ac", "fb6a1f4e-e3fd-45b2-8239-ae4853a8ed78", "aa9049b3-ffe6-4916-b095-885e60aba117", "add5e0c5-8277-453f-8e87-0b5a980a95d8", "a9df6ece-fd89-40cb-8d2a-76578d4f0cd7", "7303f6d5-950b-4ef1-b527-74a463def7c8", "0acb7d48-8b0d-4068-8b78-5b75098eab09", "533e8751-04d2-47d0-8e64-bcd7ee462dfd", "20a292c9-a5be-4a54-80e1-6491d1ad655f", "0e7a12a1-62ec-4f28-99e8-93fd581cc0d8", "3dd7843e-e16f-4965-9b4f-c1d0f4146621", "9385e64e-d5a4-4d4c-84cb-f859d745f8f5", "7ee10036-70f0-4e5c-a310-80400ee46def", "a182dc1a-2aea-41ef-9cd2-8cea23807662", "6f8dcee6-529f-44e0-8f4b-1f2db164e133", "b241cc96-de18-4a7b-be9a-77271c5bc125", "25469213-9191-4a78-9f31-bfd562d3ce71", "4b959207-cbb3-48e9-a73a-82ba017c6673", "9084e53d-a3b5-4132-b865-cd4d6101302b", "0d64c0a3-2bc2-4c5b-a284-577cc72df072", "98473a04-c2e4-4376-b563-fb7e9d23871c", "df2c8ad6-a2ad-453a-87fe-017e60bf787a", "65d6f179-674a-4762-893a-af6316bf0c23", "89c51624-4ace-45b5-92bd-beb86a011448", "7048bba5-b259-44fa-a3ec-dbec58a52420", "b8eecfff-ae1e-49cc-af79-6fee76ebe9ea", "4e4204ea-1459-4626-ae77-0af9bd17be77", "b4d57ef5-fd97-4269-a7c6-87c36f9c28c8", "72afdfb0-c290-4834-98e3-e0d68ba6b7fd", "5d253f0c-27fb-4cfb-a8ea-082717b4bdcd", "66f77ccf-46c7-45af-a56d-c1696c546f05", "a67944fe-9abe-4082-aa63-bb0562efda06", "1e4f061b-042f-4d35-9c93-3040c3a89140", "c6acd20d-06e0-4bc2-9e1d-7813e50ce73d", "753408dd-545b-4377-ad0a-546e13225bea", "6bcf6805-1d3d-45df-bd75-4157dfee23bd", "3ff93fa7-fad9-4216-9035-10c64b29c928", "f7eb9ef0-7eb7-4e76-a581-32918df9288a", "7f8683e7-f2e4-4e10-bcaa-306a580e7a98", "97e67368-c103-433a-b808-5172fbeaf390", "cfb8514d-9493-4708-a9da-2a997ee71fdd", "d156f5bb-f4b4-47f6-8d4d-b33a95a27b3e", "3b228fd9-bdfa-4266-8559-88dec33f6a0e", "0b23a1a4-fb99-4691-befd-6f5d513aa4bd", "44186064-86d9-44bb-bdb7-c27a23d7c284", "7b84fb40-428c-4962-84fb-f265473d9bd5", "b4805184-cc85-4df9-990b-2a6ad5a21c43", "55422b37-2367-4f08-b2ba-352448c96424", "71902272-2de6-4cd9-b6bb-c504b6c3e657", "d3628b81-dad8-4954-9cf4-d9170594e7b4", "73d0e642-7981-45ef-9ca6-287ea4299d46", "7c3ca567-6e2f-433a-85cc-48656256688f", "afffc6de-4b84-4e6e-9605-177137f149c8", "c94405ec-8091-475f-82fb-af342fd478f1", "cc23c79d-5e73-457b-b022-67edd5a95d25", "00abe0e0-c902-4f44-8116-236ac44733a2", "c1a79e65-190b-4abb-8f3f-328332de4838", "04013ec3-e99c-4941-a3ce-639cacea06f7", "8af6cba4-72c5-4c55-9671-84d3924034b3", "b9cee74f-4ae8-439c-887c-b4120b0f1a8c", "4bbaca6d-21de-40aa-b72a-5131eee75c61", "2c5cbc7f-e6ba-4b31-a886-8b211cbc52ed", "546c4fc7-cfdf-4307-90ce-9add0588b0ca", "4cb70317-e840-44c4-8f61-b0ef5a49fab5", "d195d361-fc63-4315-8fc7-3f140bc13744", "35b0a8f7-0aa9-4837-8c58-3c180fff32a5", "1ac0724a-5cba-4014-bbda-3626cf04f5ef", "d97b1890-ebbd-495a-856f-a17415b6a208", "5959cee1-6887-4d70-81ab-349054816bc5", "bdbdd574-6557-47f7-947b-3b1b5a07d17a", "cdb97048-86ad-4596-a180-c5ee577bfad3", "a184e5b0-3a97-470b-98cd-1f8b9e1acb09", "c069c6d9-2b0c-45a4-81c5-10c438813aa2", "6263bf63-7175-42c0-93df-82ddcfdfc4c9", "9d58f374-c620-4eeb-abd1-5341196fabf5", "0a39b38e-6419-4ce9-b4ae-6db06d20cdac", "ca649d76-1076-40b0-917c-fc515ee25dd5", "0713f40c-5bba-4f62-863f-f5df74e60599", "d1a344f5-db37-4369-a773-e33274346cb1", "f55ae1bc-2d41-4058-9444-a1131cebd5ef", "679d9b84-6142-429c-9ef1-4b5ef77d27fe", "98f0f3c1-d81b-4603-b0ce-c529d844818d", "eddfd3bb-1f51-4635-87be-37b010a11127", "9058e019-cc2e-45fd-bf9a-958b67c89a1f", "b5dc4aef-af8e-430e-bab8-bf0647274cf0", "2ebd33d3-3c2b-4d46-87e1-8500b2da901c", "032e29c9-31bf-4d4c-a141-75b44d8eb772", "b61ca803-8a0c-426d-a376-31f0b920ab4e", "f05985af-ddd1-4781-a431-89662ab67aba", "4acb8290-def3-4c17-8463-491aa486ddbd", "076007d3-dbac-40e8-9072-f2e34c5d2d56", "9405157d-0ef7-43f3-bd50-fdcd2ee50d08", "7da2ddf7-be0f-4a43-873c-d8bc0d9a81bd", "b6905bfa-7ee1-4f4f-b610-d408f7084f6e", "bfeeb803-6b00-476b-b25b-2d8381b45b7b", "8b4af47d-27ef-4ff6-a006-f9cb815c4598", "36352219-00af-495f-9f79-b242ca2de9dc", "02255e17-8105-4869-a336-2c8bc106cc36", "07bf271f-a60f-46a8-a0f3-3c72c4542c0a", "fb077ce0-4c1f-4c65-94de-e35b4ab1c17b", "f1d435a9-ca53-459f-9bc7-c799f5d37e4f", "a8e476a7-03fe-4c9f-9294-188e9f9d0dc0"], "metadata": {"node_type": "chunk_node"}}, "documents\\shared_papers\\\u8d75\u61ff\u6668\\\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\\CIM\\CIMAX-Compiler_An_End-to-End_ANN_Compiler_for_Heterogeneous_Computing-in-Memory_Platform.pdf_MainText": {"node_ids": ["17f93544-8b86-4758-b5a0-db9d8d93c5e1", "4dd1b630-56ee-419b-95aa-e119bd43cac4", "a61f347a-3ad6-478c-bd3a-34aa9be50061", "cedebc15-8eff-4a16-b4d3-688515a66738", "98abcde2-874a-45bd-bd59-57c5974b5d64", "750946c2-149f-4ead-b810-1c4132a618d1", "11fd5a8f-0034-41e4-b839-7256e6a5ed99", "ce73f64b-0034-4847-a7fa-297f1f64516d", "3a180214-0c81-41f3-b217-865bdc373458", "b6f9b395-fe73-4520-b734-8deeeb67c05d", "0edc4b3c-fe16-415b-8b02-a903de626e43", "660493fc-edda-4c92-bbcf-dfa162356527", "ca6c40b0-025d-4e8e-a3c7-ae13e2e120c6", "f37b1acf-e65f-4940-ab36-c89fe045d463", "8c999e23-13bf-4234-89cb-57d294f23ee9", "ce9cb202-e79f-4a6d-abf9-4409ad296965"], "metadata": {"node_type": "chunk_node"}}, "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_MainText": {"node_ids": ["69f46ecb-64f0-4828-be69-6fdea351df8f", "0eb49248-6451-4dd4-af95-84debb4e62f2", "3ef651ff-766a-4a1b-acc7-1b0fe4358d2a", "a7e1eb8d-cc5e-4529-80cd-86880ab1c579", "61b10b43-1127-46eb-9dac-29bf234c551d", "6554170d-6516-4dad-9c9e-c0b3671ec8ad", "887da0a6-a918-45f8-97f3-6eb363aec90b", "4b9365ff-ecad-478d-9922-4dccf7c469f4", "4e9dedfd-7b47-4cfd-9ba3-f37e71163773", "9d112cca-1060-43ff-a555-823b666028ce", "932ded43-76d8-4843-b571-05e6af61a054", "49c1a25c-8aa6-4512-82e5-94fe17760319", "74adacaa-4ffd-4d51-8bb1-af16038aa398", "fffd3454-7a13-4317-9cf4-93e6e1aaf4c9", "deddc4ff-b450-4c19-a581-605ba421c0bf", "8264a7a0-17c0-4a36-b3ba-5bdb87ca5fa3", "02ffb4cf-a7cf-4921-9b13-0f04fbd1c7e9", "8f757292-10ed-4d25-8c32-eb8eaa7f60e1", "11d58726-a1f4-4efd-be95-4060eb6e8311", "93432d76-aa5d-44c5-9966-232c066a1e11", "64fb4016-0551-46ef-9dd6-b376c71311bd", "ddb2c5a2-1269-4c29-805e-009d49124ed5", "f20b7187-4e07-4ee0-9f99-fa7eb00302a4", "da295ceb-8a43-4690-9fbe-b678aedf5db4", "eebd11f7-da9d-48df-8e8e-35717b84d6d6", "05b7bc04-844d-401c-aed2-1ebf29dcfe55", "99c461c9-39dc-441d-9e20-5658778103a8", "ae4d523e-e7d3-441f-a3f9-eb7c5ce6ced2", "1d58ac39-68bc-45a9-9c8b-2a79aa7e726f", "44c32e15-c836-42c4-86f6-7389163cbe01", "56c304f4-a53e-4ce6-a7cc-eb1ec11e699c", "53159cb3-d55d-4004-88e8-f2cffba985ed", "0d832b5f-a58c-4190-850c-1393f8c82908", "c6ce9951-3366-4b23-80c1-a54994cf97be", "bb99e91d-09d3-4f2c-956e-86b1fc15ea01", "cffb2891-1970-4436-ac95-293ace706841", "916a3217-0ee1-4469-a9d1-d5470d1aae9c", "22f43c92-187b-41e0-aaf5-9fc7f4e3462b", "0b24b883-1e13-4ae6-afe8-e17247b7242c", "dab7f045-53f9-4fc6-aeb0-57e68ab6b8bc", "4dc6ef75-3d29-4e86-b3fc-a3a3da0bd193", "6e1ba783-6d7a-4167-bc12-3bf9d0e7c5a6", "4b597f4b-6bc5-49d9-b319-9e51efece81a", "1c915ba3-d40b-452e-8c5c-cca6b4e42c66", "44658f6c-6fae-4c9c-a56f-102ff8d88f8a", "0e9c3967-ccc0-4554-9aac-c88f870e3147", "846f23c6-7d45-4eec-bdc0-7d18e1d86468", "83f1b13d-ca80-4ecb-88f4-feabe9c81073", "568a6c63-8443-4a31-8170-3b13bee1aab7", "2a967a35-c929-4297-934d-81c7294b6836", "31586887-2b48-43d1-a7f9-fe6b0c57d3c3", "19054bc3-9198-4776-b980-2bc762d0c322", "356f2f45-993a-4742-b1e2-2fbebeadc6fa", "75244e7d-095e-4d01-8953-306760bca971", "bf094086-9c4a-48d8-bd6b-11d862fd889c", "4e11fc2c-92b6-436f-ae81-ba6c4d60627d", "02587a1c-08eb-41f2-92d3-e5e732fccaef", "9ce1be7a-a33b-41db-a132-2648c1b53f38", "9737dc88-cfee-4ce7-b6cb-534b1d0ae78f"], "metadata": {"node_type": "chunk_node"}}, "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2018 NC Efficient and self-adaptive in-situ learning in multilayer memristor neural networks.pdf_Methods": {"node_ids": ["a8b160eb-2ecf-433e-a204-98abb6371899", "04bff131-cee6-4985-9ec6-33c2658f405a", "b7c3291a-1802-4c9d-9c14-9cc5455b9191", "935fddc8-4640-486f-bd2c-e9ddc05cd514", "f67f804e-a91a-43b1-864b-0573cfbc4566", "9f08e17b-1f33-4ae0-823c-b9455395cba7", "6ca4ceb0-c5a7-45fe-b6d9-83e215954b9b", "3f32a1a3-cb24-4ce8-b5b8-0971a7b91306", "c2dbc002-4aa9-4a30-9364-b71cee9431bf", "fcba84b0-3be7-4f64-af6f-c137ccfc06d5", "91fac5af-354c-4a14-b045-b6c98a556d31", "d666af53-ea3e-4e9e-bac4-852c89e1ca31", "52a16434-4e96-4cf7-b124-d6119d5b727a", "cea6327b-8740-46af-8a77-385dfda82840", "d8027022-cba1-4178-b36e-abe1f6bbb03c", "84293e49-2a25-46bf-bd91-10048448e84d", "a54162ee-38a0-4ce1-a5d0-4beafaa4b1bf", "ce4cb413-aa31-4b76-adf3-108fb53d2098", "5d1df985-c156-40cc-bdf5-7fcf777ae171", "ba48f775-1d63-4def-8b60-fc544042eaa7", "2d407c38-2a83-4e3a-8a82-c9424dbf9a5a", "a27a4f6f-6d75-40e7-b12e-6f88b75acead", "df5526b1-3875-4a25-8eb8-10ae7adfc9f5", "fde06822-4570-445a-b422-4d9e65cb81fc", "4fb023f4-473e-4a4d-a360-94103ee37a57", "c040cd72-3d48-4732-a5db-4bc57ad52201", "2eba354e-cfac-4c3d-bb1a-86abc71da489", "04040bb4-5ce0-40c0-9964-92d424691a82", "18c36cc6-f085-4274-8802-77b42302ae4d"], "metadata": {"node_type": "chunk_node"}}, "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_MainText": {"node_ids": ["536ef344-8e87-42a9-8b3f-464e6060d2fb", "1185fde7-b191-4e05-95e3-574de68e57a5", "66e0628e-0300-434a-a4a1-1c934d2d3e75", "c98cffa1-6f5f-46c6-a338-04a6ce47bcae", "165cb3fc-631f-4a76-b259-23c8e922c438", "4df2cc98-fc6b-478e-ad27-c14cbc113043", "92dc3828-21d4-4749-b6f0-f58d4d16ebf6", "8f607fd6-7d9b-4ec8-9958-9eb7e53ac6c4", "cf706117-2f30-4d1b-8e70-4c70d699cb36", "60295af4-15ab-4289-8fcd-15c3df4438c5", "cde84fb6-6a66-474d-b152-d7492a2b559b", "e0c9e927-0412-4f5c-88d7-602dd668ac86", "c920e1de-67e5-4586-92f4-303473cc251b", "7b64ffab-b826-4bd6-b7eb-c7d703fb0b54", "d2dd7cdf-1e30-4fb6-aa1c-37031f2c133b", "bc73a7d3-1d11-473a-a9c6-ce0c57836a45", "a08b3032-0ea0-4c33-9e14-cf48b649224d", "76540f75-6473-490d-9e0a-24bb98bd3c9f", "3286bb37-a2dd-405b-a19b-eb4aa9fe19d2", "504b8543-f5e3-4956-b5b4-054aea818f56", "fc5cfc83-cc62-464d-96a9-8c0a0cd297b7", "69ec8af6-47b1-4544-893c-f5b3366afb8e", "9131e166-368c-4b73-b994-da90529f11b9", "47d3df59-f137-42b0-af4c-cb0db935322e", "f1e90641-491e-4fb6-b147-00fe6a52a306", "c7db0e85-3af6-4128-9788-488b573a682c", "54da2ae5-79b9-4447-ab32-d643dfe0f5ae", "8c7f6748-59ee-43c6-9929-40cdfe9fb37f", "e3310836-8b30-40e7-8195-2e8c47ec8037", "ddff0ee4-5311-4775-9ea0-6138d6a11005", "c4f96757-a6b6-410a-80c8-35247ddf2999", "259bf45e-5516-4e01-b7dc-551b72742212", "e856bd5e-e4b6-4762-b2f8-ec13b0c91a12", "f18737d2-9be6-430c-9e16-d76a6eb0c46d", "c71e47e0-f3fd-4832-81ad-006424afb3cf", "be01b8ee-0b58-43f9-a7c5-12f10dbb9280", "3be32cca-1f81-4ead-be6a-81e55c832377", "c0b76408-e9a5-4c25-83d9-4e0b7ebf70e9", "6411a181-99b2-41fc-9b30-c3ab03677adb", "21b6acde-09d6-4740-82b9-80a89aa41d61", "bd9a2a7d-8b7f-402a-bd96-c8bea4a3ed32", "c19dc867-d31f-474a-8a57-e3d1a340f2a6", "0b7ba1a6-ae1d-497e-b9e6-0a3d2caf8358", "182a9b13-3a7b-4083-8c86-a509291959a9", "2c29c70a-7ce6-472c-9d07-dff88547737b", "323f77ef-7bc6-4196-9426-d0183681bd5e", "fadecbb8-3c29-4374-98f7-bc2a855fe909", "92aaea5d-b8e4-4510-b616-babdc0d59c77", "71f930f0-880b-4c52-ae4a-9e8fd15f5dae", "0e2322e9-a46d-4f13-b539-91639ee755ba", "acea037c-6e2d-4b7b-a44b-1d1756ff2902", "79dfc1a5-b3f8-4ac4-b467-e023bd6f2abb", "36ec4795-2693-4d92-93fe-b884bd1d14d7", "ff108c4f-bcef-4bf5-b316-47c86e4507d6", "da9c94a9-8bd5-49e3-bd4e-2abd6582c42c", "288fcfa9-ad7b-4df6-842c-87bba2c7fe9f", "4e37d045-1339-4f70-893e-74a7b3eb9f18", "ca62d585-1b19-400d-86b4-61a818687be5", "87c50d04-f39e-4ca9-8228-a8bea1179753", "a81a973d-e24f-40a7-8bc0-1dc20fade299", "cbc44d52-c54b-4635-a402-78c57db51d24", "b7f02b11-382b-4227-99a7-b687d909489d", "ffd3229e-9f7d-4d64-8474-80f5f682a9ee", "bdda81c6-7282-4b9d-9cf8-f0f7438de133", "cf9d4265-4d94-4829-91a9-78d555b55418", "3c9885da-d240-4dc5-8dab-71a7fe893431", "812d1682-2766-4ee2-ae8d-de8f8c573c84", "52c140fb-0bb9-4b80-8cbf-21f9900257b6", "922eca02-2805-4715-8c99-033021baf0b9", "5b87c2e4-e1ba-4c3c-b6e8-5423cfeb7ab7", "9274bfc9-e30e-498a-bb77-2d987c9fab45", "ff5c10cc-032d-427b-92ed-c18e28956e1b", "d2d97ba5-52ed-4a25-9c95-0509f6204908", "cd13a164-d1ed-40b7-8e48-7fce828b8be6", "c7375df0-64e7-4bb3-8f68-49d58f32ecbb", "358a3ab9-d84d-4f31-8068-0e2ae55137b5", "03281470-88d8-4425-ae40-9c2d14940f75", "8ad2433a-caf2-4cd7-bd7d-b6613918c6de", "967f9469-77fd-4076-bc9d-a8858767f772", "d5bd77f8-aa6b-471d-9ff0-314f33009f44", "95b963d8-770e-404f-8ff9-8adc0248cac1", "23b05784-67a8-41cb-a160-5ff6aebf8f83"], "metadata": {"node_type": "chunk_node"}}, "documents\\shared_papers\\\u6768\u518d\u6b63\\\u5b58\u7b97\u4e00\u4f53\\2019 wangzhongrui In situ training of feed-forward and recurrent convolutional memristor network.pdf_Methods": {"node_ids": ["9315cfbb-c202-43c8-9ffe-649de42faeac", "27d30242-4c4d-4aa8-b422-37bcb0a3265b", "2c56bc08-cdc6-4738-8d7a-ef17a91a3747", "2ae09193-1dc8-49e9-abb4-0b2a232d034f", "10d9dc49-bb2d-4eef-973c-f863b9f8728f", "a3d022ba-01bb-462d-a949-c537174c1bba", "ecf94287-c9ad-47f3-9563-9570446d9dce", "dd8f4cce-e318-4425-8b99-74a073449928", "f1004139-76ac-4192-8ab2-db2e5335eadc", "9e1eae16-775e-408e-95b8-11187a140a24", "c0959c3a-c953-4561-8247-19e33ccf2761", "9bbaa51b-e138-417b-9958-9c9a69e0da00", "258a74f5-76f2-47b7-ac21-46fc4e1309bd", "5f64707f-be8e-4102-aaf5-2d5cb0eda120", "7655e661-672a-421d-9b4e-b152784cd4c9", "2c7a3f89-0ade-4ade-8648-c33d8b8c0dfb", "be30074a-f899-4826-b5cc-484370ce5dd5", "7d112972-09cb-4a3d-8e7e-8ab58bc4ab90", "4f208bd4-4dbb-4411-bce7-827a842a29e1", "13663ab3-c81f-4cb9-9074-7784689f73c3", "6b297d62-4349-4174-921b-e1631ae968e7"], "metadata": {"node_type": "chunk_node"}}, "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\post-training-quantization\\Quantization_quantizatoin and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference.pdf_MainText": {"node_ids": ["653d640e-3b26-4d0a-a02c-540557ceb834", "bd2537c7-0e85-4c8b-983f-e8ad539e4230", "f917288b-5cfc-4872-b640-40623ce72f6c", "8f6bbcc6-e4c7-422e-acfb-62438c84b885", "ee6ca9d3-bce1-4235-8990-2726bd2659b8", "2b43bc75-11ea-4abe-b7dd-ddb3074a2255", "5fbb6874-d5b4-4ab0-90cc-0ac8f1558ca2", "65c79469-0da8-4033-a9f8-47ed14779a78", "37013a73-33c7-4234-ad58-5384ea2073b4", "66a6267c-c7d1-40c5-b758-a556fbd81f81", "c6aefbcb-2b38-465d-a642-bc3904f2a37e", "20e24256-4f4b-495d-834f-827477ab4680", "6f034d87-f9b3-4f17-a85f-62484b63ca3a", "bbf1f363-f629-4d0a-9f84-5ea2b7e46d95", "c59ba73a-a77e-49a2-a465-40aaf8c49915", "e7143612-55a0-481c-a46d-6878df23305d", "711754cc-08f9-4303-bea0-da807b32be97", "8cad45bb-ee52-442e-ace9-99b8a2fddd24", "8e6df688-51de-4237-8883-c5fbceb88279", "667a099a-d403-4699-b667-eed09ba6cd24", "c4dccc2d-1b59-4438-b656-96c99e766059", "9d3e14b0-0f20-4535-9380-b09c4592ff35"], "metadata": {"node_type": "chunk_node"}}, "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf_MainText": {"node_ids": ["8b38de36-948e-4614-99da-9f15b9629e60", "45414ee3-3cc9-40e9-b846-d8c2a93d0d31", "7ab3370c-75b2-4479-8a10-d9d83081db08", "b37585ad-1e7c-46de-916f-52c239038c42", "8d1a2099-19be-4747-b251-a655a6b55164", "c2f4d688-296e-4551-aac3-d8afab3b182a", "fac5e889-761e-49d4-bbb1-31693cf7d5e3", "79b56430-ee52-43f4-84f5-fa57cb3dee54", "4565c392-9058-45dc-89b9-d22d3b261983", "3769e20d-58e0-4c7c-a3b0-ff460a883fa2", "c87102f3-c101-43c6-8bf4-3c4a782d59fb", "3f6585b8-6463-4e59-9e00-bbadd8d8dff3", "ace8c8e8-c109-432b-87c6-c96dc89c67f0", "0d13c6c7-3d6f-4d9e-8313-a6924a50c1df", "d1cedde8-107a-4825-92c1-4ffce42e8681", "cb26e56a-9512-4569-a49d-9e4b5d688040", "efc2fa00-4b18-474a-9839-6adaaaf173f3", "62bff64a-b30b-4d7c-b871-a7555a617676", "ad6fbd61-69c5-4df4-97f1-d5e8142c7c8d", "a0747f83-f727-4f75-9b9a-2fbd94c61dcb", "057a69ee-cc37-4a44-8ab5-17759486636d", "a0be1da4-24c3-459a-a68b-ab0afc354bfa", "8b6f05d2-d2ac-42d3-833c-4406fd359c30", "8a2fe741-2ac4-444c-9594-ff5556bc5d49", "f5855de4-e6ee-4b3a-be83-26e3edd84711", "3b883d14-bc1e-440e-8ccb-deea7b93d18e", "27d9d5aa-7553-41ac-81c8-9945ed56cefc", "a20e9cb2-ae72-48cb-89c1-ff4a78e825ba"], "metadata": {"node_type": "chunk_node"}}, "documents\\shared_papers\\\u6768\u518d\u6b63\\\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\\training-aware-quantization\\XNOR-Net.pdf_MainText": {"node_ids": ["faf6e3e1-b949-4e5b-8f66-1a4e4cf4665c", "9fdd2b41-08ae-42c1-a646-a78abd8d4423", "4af9d98f-6d4a-413b-a65d-ef8634b0435e", "ff3cc678-711e-459e-8766-64add4a5b79b", "b546bc00-f93e-48a7-93ac-786f239a61c7", "aa28498f-7e78-4bd0-a63b-67347822d054", "f5f0c874-9fea-4ed4-8ad3-353dc53704d4", "856900f2-2a59-4eb8-b43a-b3655765c66e", "442a996d-489a-4fc9-8512-8aab841141a0", "b58461df-014e-400f-8a3b-b7fcc899b4b8", "e065c41a-5c38-4da7-8316-3a0eb1f37f4a", "8ae8975d-4301-43bb-a39d-f9e02fc18f1b", "91c75266-d293-4741-8856-db890e62e5db", "0a586f64-69e3-4ed5-b217-246748cade34", "5556efd5-34f7-47f7-9c0d-cd10088a77da", "db041908-6a1b-49de-9e04-7158a955bbf6", "fa56bd0b-aeee-4add-85fd-f1fddafeb944", "994c769d-f908-4f83-bdc5-6e732cfa53ad", "97b6008a-66c1-4b50-bef0-2e8e54861b75", "f0954f18-142f-4c95-85c5-1e4596235197", "1ed6baf4-44a7-4373-a63f-5f128f098efc", "df88479b-51f3-4218-8af7-478303a1dacb", "256d878d-b29d-4ce1-97ae-f144999fe5e3", "1f5148c2-980e-4cce-9dc2-f06f3990764d", "a4347006-e63b-4773-a42a-25cdbda90770", "d8650cb1-e3fc-461a-805c-329c9560f394", "c7cbdd57-6a2b-43c3-bb41-9b55d76c0f0b", "fd9bc8e1-56e3-4a3e-9e0e-0ba5a42765e4", "36fe0d16-5f90-415d-97bb-8207ca42a491", "a1ddb1d8-d66a-443d-bb7e-f27adb0569d3", "9e1c208e-d993-46e0-a0e5-848afa168ce9", "3a65598f-d600-409b-b08c-4f3879639fd1", "ee9dc51b-cb69-4e2f-88b3-03eb228aea79", "ed16d59b-d284-411b-af81-dac6b38d8325", "b54be825-f95e-4cf8-b261-51dd9f7cea01", "51f9ac20-bfcd-4b4e-8a64-e687f205f918", "bd3c46c6-0611-4a40-b81e-7ca3240ad115", "d90622b2-db1e-4595-b45a-88021548e613", "3aa7adb4-b40f-4a03-9b10-5410ae180273", "2926afe7-549a-42c9-b99f-7a235508a9af", "f7cdaf08-647a-4d36-b6b2-35b4bd5ab93b", "6c41a97a-bce7-43b4-afb6-8b20b267aee1", "79c5db2a-a4ec-4b81-aed2-97905f7968bc", "849f070a-a09f-4577-9e94-b57a991af1c7", "2a50d1ab-c431-4d4b-90a6-2b1107ee143e", "7cf8d3cf-e074-4c78-8769-f7254411e3c0", "5d9bd29d-615c-4477-aa4b-4832f9fa14a4", "987df21d-13c3-4b91-970c-94a91a2f82ae", "d6642526-3af2-433a-82ab-f978c2caa3f6", "e6d4c5f4-3cdc-403a-9c3a-66ed10ce59a2", "c5cc0672-8f70-412d-af6e-7ca91249b848", "c9867c00-0d52-406c-b6aa-4c4487e64bff", "d3ff5b87-96c1-45ce-b887-03ca029cbe9c", "d8bc747e-7478-409e-8b53-b16648b4e708", "7f3e12e4-a8da-464a-8f39-3607ca063b9f", "b94611cd-0027-4160-a6c8-46b243d25ab8", "b5879588-e82d-4d60-8598-36520dc89449", "3dc2e943-b385-43cb-a390-4db54b5eb8fc", "2c528384-6c68-4c6a-9a11-6de467c656fc", "81a437ce-4410-4a79-bdbf-d39059d0eead", "4feffabc-f8b8-4783-b457-15644ca8de40", "b12b2e54-0c86-4c24-8791-746706842a2e", "12b7697b-71a7-4392-951d-24cd554528b8", "a1a84cae-a62d-45e0-acad-09c984d7a470", "5a52ec5d-18c3-4107-94c7-660e459a6692", "dc021ad9-7922-4548-8e04-f17492f46b7a", "e99832ef-4ee2-4207-80ac-3ed6607927ce", "6cc45d1c-43df-4b9c-9462-f4dfd3e37555", "908a880e-de47-4c12-ab9c-e57b74a70e64", "a8d55467-1365-4c2c-a007-ac7303027784", "80591771-6e15-4c26-8c9f-200b0b14708e", "78bfa7e3-a4f9-4760-a73c-714c181f6716", "4d94de39-fa51-4cfe-bbb0-d287f46359a4", "45dc4261-9b7f-479b-8128-002f34667159", "7fdb303f-f149-4dba-a3af-cb1e43bedeab", "6294cc3d-8b02-42e2-b564-accd8aea215b", "b5024c39-42e3-4b44-9a73-0d64a77e8bb8", "69fd70be-78c0-49cf-baf1-2f0dade331d3", "d76b9d13-2d70-4d35-80bc-64031c2e783b", "6ec54057-8962-4387-90e3-dc4fbad587fd", "45e7927a-ab5e-4f00-b4d4-7dfc96d8a5aa", "d5b334f2-f58f-49f8-9a54-e2582d5890d1", "fd03f1d7-9675-4e52-b49c-561cafc0d49d", "51cd05e9-61cb-47c8-91a2-5a20f9d639a9", "6f766b16-d587-4d22-8e6d-57a8b6fc7d1f", "bca7f5d9-6240-460b-8830-4ed286911cb3", "5f8cbf6c-781b-4a56-b30b-fe46b799bf79", "63a49a88-263d-44ac-ba3d-248447fb392f", "7e73f9de-3297-4b2b-87e9-1851a348105d", "4ac298ca-3662-40af-9534-e324e48d3a11", "92d31c41-a3f7-4951-9db8-5dcec36a74cc", "d890d1e5-b8fd-426d-9597-93e0af0b44b7", "735065a3-b136-48fa-93ad-ab0ed1a35b90", "6bae563e-1d68-4634-b4fa-48292d321a02", "67fe8c25-e9aa-4693-b753-b9d402bc7bbb", "8e1cb92f-1d7d-4352-a448-7a32f8d86b6d"], "metadata": {"node_type": "chunk_node"}}}}